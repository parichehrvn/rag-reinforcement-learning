{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-07T17:53:56.618305Z",
     "start_time": "2025-09-07T17:53:56.612010Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:07.468989Z",
     "start_time": "2025-09-07T17:54:07.465557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:2081\""
   ],
   "id": "6fedcccc88926c04",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:09.724175Z",
     "start_time": "2025-09-07T17:54:09.074945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider='google_genai')"
   ],
   "id": "3b2253d32bcd2b7f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:10.703405Z",
     "start_time": "2025-09-07T17:54:10.696543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # Specify the OpenAI model\n",
    "    api_key=API_KEY\n",
    ")"
   ],
   "id": "10387477855c27ee",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:14.546730Z",
     "start_time": "2025-09-07T17:54:13.032823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "llm.invoke([HumanMessage('hello there')])"
   ],
   "id": "6b097e4b7ba5bfbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--33e5f36a-92e7-472f-9094-8c54e88a2741-0', usage_metadata={'input_tokens': 3, 'output_tokens': 35, 'total_tokens': 38, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:25.634329Z",
     "start_time": "2025-09-07T17:54:16.150260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ],
   "id": "5d514f9636606e17",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parichehr/PycharmProjects/NLP/rag-reinforcement-learning/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/parichehr/PycharmProjects/NLP/rag-reinforcement-learning/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:28.820791Z",
     "start_time": "2025-09-07T17:54:27.427254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "vector_store = Chroma(\n",
    "    collection_name='rag_test',\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory='../storage'\n",
    ")"
   ],
   "id": "8032ecbe676e826a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:34.996284Z",
     "start_time": "2025-09-07T17:54:34.993529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, OnlinePDFLoader\n",
    "import bs4\n",
    "\n",
    "blog_urls = [\n",
    "    \"https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1\",\n",
    "    \"https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92\",\n",
    "    \"https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c\",\n",
    "    \"https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e\"\n",
    "]\n",
    "\n",
    "pdf_urls = [\n",
    "    \"https://arxiv.org/pdf/cs/9605103.pdf\",  # Reinforcement Learning: A Survey\n",
    "    \"https://arxiv.org/pdf/2312.14925.pdf\",   # A Survey of Reinforcement Learning from Human Feedback\n",
    "    \"https://arxiv.org/pdf/2308.14328.pdf\",   # Reinforcement Learning for Generative AI: A Survey\n",
    "    \"https://arxiv.org/pdf/2312.10256.pdf\"    # Multi-agent Reinforcement Learning: A Comprehensive Survey\n",
    "]"
   ],
   "id": "b63a2d54a238620e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T16:14:18.663390Z",
     "start_time": "2025-09-07T16:14:17.203758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(blog_urls[0],),\n",
    ")\n",
    "loader.load()[0].page_content"
   ],
   "id": "4f3c410ad3d4d129",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inReinforcement Learning: An introduction (Part 1/4)Cédric Vandelaer10 min read·Aug 20, 2022--2ListenShareHi and welcome to the first part of a series on Reinforcement Learning.Press enter or click to view image in full sizeIf you somehow ended up here without having heard of Reinforcement Learning (RL) before, then let me summarize it as follows: “RL is a general framework for training an artificial intelligence model to solve a certain task or goal” … or in layman’s terms, we make AI do cool things!The goal of this blog series is to learn about RL and simultaneously explore some of the more recent research later on. We will start from the very basics and work our way towards more advanced topics. Even if you have almost no prior programming and/or mathematics knowledge, you should be able to follow along pretty smoothly.The first mini-series will be split into four parts:Part 1: What is Reinforcement learning?Part 2: RL terminology and formal conceptsPart 3: The REINFORCE algorithmPart 4: Implementing the REINFORCE algorithmAt the same time, this mini-series will be the introduction to future posts with increasing complexity. Feel free to skip to the next part if you are already familiar with the content.Note: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.That sounds cool! … but what can I do with RL?Reinforcement learning is a framework to learn any task. In theory, RL can solve any problem that is phrased as a Markov Decision Process. We will explain what that means later on. For now, let’s have a look at some successful applications. If you enjoy these examples, please be sure to also check out the work from the original authors.GeneralOne of the videos I love showing is the hide and seek video from OpenAI. The video is a nice example of how RL can help us with finding novel solutions to problems, without explicitly programming tactics or solution methods.GamesOne of the early successes of Deep RL (which is combining RL with neural networks), was the ability to learn how to play Atari games, straight from pixels. Later on, researchers took it upon themselves to not only evaluate RL on the (relatively) simple Atari games, but rather evaluate it on the hardest competitive games out there. The assumption here is, that if RL can solve these complex games, it can also generalize to challenging real-world settings. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.RoboticsSolving tasks in simulations and video games is one thing, but what about real life? Another popular field where RL is often applied (or at least holds great promise), is robotics. Robotics are significantly harder than simulations for various reasons. Think for example about the time it takes to repeatedly make a robot try out a certain action. Or think about the safety requirements involved for robotics. In the example below, you can see how the ANYmal robot from the Robotics System Lab in Zürich learned to recover from a fall.Real world examplesRL can be applied to many other domains than the ones I just mentioned. Advertising, finance, healthcare, … just to name a few. As a final example, I present a goal-oriented chatbot, trained to negotiate about sales (source: https://siddharthverma314.github.io/research/chai-acl-2022/ ).Press enter or click to view image in full sizeRL: The basicsA description of the RL framework is as follows: We have an agent that tries to solve a task in a certain environment. The concept of agent should be taken very broadly here, an agent can be a robot, a chatbot, a virtual character, etc. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in.The RL problem is trying to maximize the cumulative reward the agent gets over time.Press enter or click to view image in full sizeImagine our agent is a monkey and the task we want the monkey to solve, is to pick up as many bananas as possible. At every timestep, the monkey needs to decide to take an action. The actions could be to step towards the tree, grab something, climb, … Perhaps the reward at every timestep can be defined as the number of bananas the monkey got at that timestep. After every action, the monkey will also be in a new state. Maybe we define the state of the monkey as its position in the world. So when the monkey takes a step, the state at the next timestep would be the coordinates of the monkey at the next timestep. We are now searching for the optimal behavior, the best sequence of actions the monkey can take, to maximize the cumulative number of bananas it will get.How does RL fit in the bigger picture?You might look at this framework and think: “Hey, isn’t that exactly what people are already studying in the field of …?”. And in fact, you might be right.In other domains like engineering or mathematics, people have often been studying the same problems with different names and methods. Or in fields like neuroscience and psychology, some similarities can be found in the way our brain “rewards” us by releasing dopamine.The likely reason for this intersection of domains, is that reinforcement learning is the study of a fundamental problem. It is essentially the science of decision taking. In these series, we will be looking at it from the umbrella of computer science and machine learning.Press enter or click to view image in full sizeThis general applicability is also what makes RL so interesting to me personally. RL is one of the potential technologies that could get us closer to general AI: an AI system that can solve any task, in contrast to a narrow set of tasks. There are also other technologies (e.g. big language models, graph neural networks, …) that are making some strides in this regard, but the problem statement of RL in particular seems to be the most ambitious.Another way of situating RL, is by looking at how it compares to other learning paradigms. Within the field of machine learning, people often distinguish between Supervised learning, Unsupervised learning and Reinforcement learning.Press enter or click to view image in full sizeWhen it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to. Our AI model is successful when it correctly predicts a label y given an (unseen) sample x.As an example, say we have a dataset consisting of cat and dog images. Each sample (image) has a label, stating whether the image contains a “dog” or a “cat”. The goal of our supervised model, is now to learn how to map a dog-image to the label “dog” and a cat-image to the label “cat”. After it has learned this mapping, the hope is that this AI model can repeat this process for new images that it has not seen before.In an Unsupervised learning context, we no longer provide any labels Y. So the task for the AI system now becomes learning some general statistics about the dataset. We could for example give an AI the task to generate a new sample, similar to the ones seen in the dataset. In this case the model would be considered successful if it manages to correctly learn the interesting characteristics of a dataset.Reinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in. The “dataset” we need to consider here, are the actions our agent took and the accumulated rewards it got by taking those actions. An added difficulty here is that our dataset is non-static. Say that our agent acts in a certain way, we can then collect some data of the actions our agent took and we can try to optimize these (e.g. do more of the actions that led to a successful result). But as a result of this optimization, we have now changed the behavior of this agent, and thus we will need to collect new data to see how well our agent fares now.If RL is so great, then why isn’t everyone using RL?After reading all this, you might be wondering why people aren’t using RL to solve all imaginable problems. The truth is that even though the field has made a lot of advancements in the last few years, there are still a few fundamental problems to be solved. Progress is being made all the time, but to give you an idea of what you might encounter, I’ll list a few common ones.Sample (in-)efficiencyIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task. In this sense, RL is very inefficient compared to humans, for example, it doesn’t take a human dozens of hours to learn how to play an Atari game.This sample-efficiency can in part be explained by the fact that humans can leverage a lot of their previous knowledge (priors) when they encounter a new task. A human can for example reuse some of the knowledge and skills of previous games and/or concepts they already acquired from other experiences throughout their life. An RL-agent in contrast, starts the learning process without any assumptions.Another thing to mention is that leveraging knowledge from previous tasks is also an active research topic. I’ll just put one example (out of many) here to give you an idea.Deepmind GatoGoogle Jump-Start RLThe exploration-exploitation trade-offWhile the previous problem sounds more like an engineering effort (it’s not), the exploration-exploitation trade-off seems more fundamental. Whenever we train an RL-agent, the agent will need some time to explore, it needs to take some actions that it hasn’t taken before, in order to discover how to solve the problem. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing. Sometimes we want the agent to leverage what it has already learned to try and optimize further. This is the exploration-exploitation trade-off, we want an automated way to strike a good balance between letting the agent explore and taking actions for which it already knows what they will lead to.For a lot problems, it is quite possible that the agent gets stuck in a local optimum.Press enter or click to view image in full sizeThe exploration-exploitation trade-off sounds very much tractable at first, but it turns out to be one of the hardest problems for RL to solve. To give you an idea about how hard this problem is: The problem was originally considered by the scientists of the allied forces, but was suggested to be dropped over to Germany, because it was deemed so intractable that they wanted the German scientists to also waste their time on it.No silver bullet for this problem has been found and there are many people working on various solutions. I will leave a link here to a previously proposed solution called “curiosity-driven exploration” which I found particularly interesting.Curiosity-driven exploration by Self-supervised PredictionThe sparse-reward problemAnother rather fundamental problem, is the so called Sparse-reward problem. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve.Imagine for example this mountain car. The agent needs to move the car left and right, such that it gets enough momentum to reach the top. Initially though, the agent doesn’t know that it needs to move the car back and forth to reach the top. If we only give our agent a reward (a positive feedback signal) when we have reached the flag, it might not ever get a positive feedback signal, simply because it might never reach the flag by taking random actions (exploration).Something commonly done to counteract this problem, is by “reward shaping”. We will modify the reward such that the agent gets more feedback signals to learn from. In case of the mountain car, we could for example also give the agent a positive reward based on the speed or altitude it achieves. However, reward shaping is not a scalable solution. Luckily other solutions are being sought after.I’m leaving another example here which I think is an interesting (but not generally applicable approach) for counteracting sparse rewards.Hindsight Experience ReplayThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made.ConclusionPhew, that was a lot of information in a very short period, but you made it through! This first introduction should give you a good idea of what RL is all about.In the next article, we will start formalizing some of the ideas and concepts which we have briefly touched upon in this post, as a preparation for getting our hands dirty and implementing these ideas ourselves.Artificial IntelligenceReinforcement LearningDeep LearningPolicy GradientData Science----2Written by Cédric Vandelaer192 followers·8 followingResponses (2)See all responsesHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T16:54:08.839153Z",
     "start_time": "2025-09-07T16:52:10.670423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = OnlinePDFLoader(pdf_urls[0])\n",
    "loader.load()[0].page_content"
   ],
   "id": "6e284810baea941f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Journal of Artificial Intelligence Research 4 (1996) 237-285 Submitted 9/95; published 5/96\\n\\nReinforcement Learning: A Survey\\n\\nLeslie Pack Kaelbling LPK@CS.BROWN.EDU Michael L. Littman MLITTMAN @Cs .BROWN.EDU Computer Science Department, Box 1910, Brown University\\n\\nProvidence, RI 02912-1910 USA\\n\\nAndrew W. Moore AWM@CS.CMU.EDU Smith Hall 221, Carnegie Mellon University, 5000 Forbes Avenue Pittsburgh, PA 15213 USA\\n\\nAbstract\\n\\nThis paper surveys the field of reinforcement learning from a computer-science per- spective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.\\n\\n1. Introduction\\n\\nReinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last five to ten years, it has attracted rapidly increasing interest in the machine learning and artificial intelligence communities. Its promise is beguiling—a way of programming agents by reward and punishment without needing to specify how the task is to be achieved. But there are formidable computational obstacles to fulfilling the promise. This paper surveys the historical basis of reinforcement learning and some of the current work from a computer science perspective. We give a high-level overview of the field and a taste of some specific approaches. It is, of course, impossible to mention all of the important work in the field; this should not be taken to be an exhaustive account.\\n\\nReinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment. The work described here has a strong family resemblance to eponymous work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” It is appropriately thought of as a class of problems, rather than as a set of techniques.\\n\\nThere are two main strategies for solving reinforcement-learning problems. The first is to search in the space of behaviors in order to find one that performs well in the environment. This approach has been taken by work in genetic algorithms and genetic programming,\\n\\n©1996 AT Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nFigure 1: The standard reinforcement-learning model.\\n\\nas well as some more novel search techniques (Schmidhuber, 1996). The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world. This paper is devoted almost entirely to the second set of techniques because they take advantage of the special structure of reinforcement-learning problems that is not available in optimization problems in general. It is not yet clear which set of approaches is best in which circumstances.\\n\\nThe rest of this section is devoted to establishing notation and describing the basic reinforcement-learning model. Section 2 explains the trade-off between exploration and exploitation and presents some solutions to the most basic case of reinforcement-learning problems, in which we want to maximize the immediate reward. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them. Section 4 considers some classic model-free algorithms for reinforcement learning from delayed reward: adaptive heuristic critic, TD(A) and Q-learning. Section 5 demonstrates a continuum of algorithms that are sensitive to the amount of computation an agent can perform between actual steps of action in the environment. Generalization—the cornerstone of mainstream machine learning research—has the potential of considerably aiding reinforcement learning, as described in Section 6. Section 7 considers the problems that arise when the agent does not have complete perceptual access to the state of the environment. Section 8 catalogs some of reinforcement learning’s successful applications. Finally, Section 9 concludes with some speculations about important open problems and the future of reinforcement learning.\\n\\n1.1 Reinforcement-Learning Model\\n\\nIn the standard reinforcement-learning model, an agent is connected to its environment via perception and action, as depicted in Figure 1. On each step of interaction the agent receives as input, 27, some indication of the current state, s, of the environment; the agent then chooses an action, a, to generate as output. The action changes the state of the environment, and the value of this state transition is communicated to the agent through a scalar reinforcement signal, r. The agent’s behavior, B, should choose actions that tend to increase the long-run sum of values of the reinforcement signal. It can learn to do this over time by systematic trial and error, guided by a wide variety of algorithms that are the subject of later sections of this paper.\\n\\n238\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nFormally, the model consists of\\n\\ne a discrete set of environment states, S;\\n\\ne adiscrete set of agent actions, A; and\\n\\ne aset of scalar reinforcement signals; typically {0,1}, or the real numbers.\\n\\nThe figure also includes an input function J, which determines how the agent views the environment state; we will assume that it is the identity function (that is, the agent perceives the exact state of the environment) until we consider partial observability in Section 7.\\n\\nAn intuitive way to understand the relation between the agent and its environment is with the following example dialogue.\\n\\nEnvironment: You are in state 65. You have 4 possible actions.\\n\\nAgent: T\\'ll take action 2.\\n\\nEnvironment: You received a reinforcement of 7 units. You are now in state 15. You have 2 possible actions.\\n\\nAgent: T\\'ll take action 1.\\n\\nEnvironment: You received a reinforcement of -4 units. You are now in state\\n\\n65. You have 4 possible actions.\\n\\nAgent: T\\'ll take action 2. Environment: You received a reinforcement of 5 units. You are now in state\\n\\n44. You have 5 possible actions.\\n\\nThe agent’s job is to find a policy 7, mapping states to actions, that maximizes some long-run measure of reinforcement. We expect, in general, that the environment will be non-deterministic; that is, that taking the same action in the same state on two different occasions may result in different next states and/or different reinforcement values. This happens in our example above: from state 65, applying action 2 produces differing rein- forcements and differing states on two occasions. However, we assume the environment is stationary; that is, that the probabilities of making state transitions or receiving specific reinforcement signals do not change over time.!\\n\\nReinforcement learning differs from the more widely studied problem of supervised learn- ing in several ways. The most important difference is that there is no presentation of in- put/output pairs. Instead, after choosing an action the agent is told the immediate reward and the subsequent state, but is not told which action would have been in its best long-term\\n\\ninterests. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally. Another difference from supervised learning is that on-line performance is important: the evaluation of the system is often concurrent with learning.\\n\\n1. This assumption may be disappointing; after all, operation in non-stationary environments is one of the motivations for building learning systems. In fact, many of the algorithms described in later sections are effective in slowly-varying non-stationary environments, but there is very little theoretical analysis in this area.\\n\\n239\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nSome aspects of reinforcement learning are closely related to search and planning issues in artificial intelligence. AI search algorithms generate a satisfactory trajectory through a graph of states. Planning operates in a similar manner, but typically within a construct with more complexity than a graph, in which states are represented by compositions of logical expressions instead of atomic symbols. These A] algorithms are less general than the reinforcement-learning methods, in that they require a predefined model of state transitions, and with a few exceptions assume determinism. On the other hand, reinforcement learning, at least in the kind of discrete cases for which theory has been developed, assumes that the entire state space can be enumerated and stored in memory—an assumption to which conventional search algorithms are not tied.\\n\\n1.2 Models of Optimal Behavior\\n\\nBefore we can start thinking about algorithms for learning to behave optimally, we have to decide what our model of optimality will be. In particular, we have to specify how the agent should take the future into account in the decisions it makes about how to behave now. There are three models that have been the subject of the majority of work in this area.\\n\\nThe finite-horizon model is the easiest to think about; at a given moment in time, the agent should optimize its expected reward for the next h steps:\\n\\nh EQ ri) :\\n\\nt=0 it need not worry about what will happen after that. In this and subsequent expressions, r, represents the scalar reward received t steps into the future. This model can be used in two ways. In the first, the agent will have a non-stationary policy; that is, one that changes over time. On its first step it will take what is termed a h-step optimal action. This is defined to be the best action available given that it has h steps remaining in which to act and gain reinforcement. On the next step it will take a (h — 1)-step optimal action, and so on, until it finally takes a 1-step optimal action and terminates. In the second, the agent does receding-horizon control, in which it always takes the h-step optimal action. The agent always acts according to the same policy, but the value of h limits how far ahead it looks in choosing its actions. The finite-horizon model is not always appropriate. In many cases we may not know the precise length of the agent’s life in advance.\\n\\nThe infinite-horizon discounted model takes the long-run reward of the agent into ac-\\n\\ncount, but rewards that are received in the future are geometrically discounted according to discount factor +, (where 0 < 7 < 1):\\n\\noo\\n\\nBD yr\\n\\nt=0\\n\\nWe can interpret y in several ways. It can be seen as an interest rate, a probability of living another step, or as a mathematical trick to bound the infinite sum. The model is conceptu-\\n\\nally similar to receding-horizon control, but the discounted model is more mathematically tractable than the finite-horizon model. This is a dominant reason for the wide attention this model has received.\\n\\n240\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nAnother optimality criterion is the average-reward model, in which the agent is supposed to take actions that optimize its long-run average reward:\\n\\nh\\n\\nlim BEY n) .\\n\\nhoo +=0\\n\\nSuch a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1 (Bertsekas, 1995). One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount of reward in the initial phases and the other of whic does not. Reward gained on any initial prefix of the agent’s life is overshadowed by the long-run average performance. It is possible to generalize this model so that it takes into account both the long run average and the amount of initial reward than can be gained. In the generalized, bias optimal model, a policy is preferred if it maximizes the long-run average and ties are broken by the initial extra reward.\\n\\nFigure 2 contrasts these models of optimality by providing an environment in whic changing the model of optimality changes the optimal policy. In this example, circles\\n\\nrepresent the states of the environment and arrows are state transitions. There is only a single action choice from every state except the start state, which is in the upper left and marked with an incoming arrow. All rewards are zero except where marked. Under a finite-horizon model with h = 5, the three actions yield rewards of +6.0, +0.0, and +0.0, so the first action should be chosen; under an infinite-horizon discounted model with 7 = 0.9, the three choices yield +16.2, +59.0, and +58.5 so the second action should be chosen; and under the average reward model, the third action should be chosen since it leads to an average reward of +11. If we change h to 1000 and ¥ to 0.2, then the second action is optimal for the finite-horizon model and the first for the infinite-horizon discounted model; however, the average reward model will always prefer the best long-term average. Since the choice of optimality model and parameters matters so much, it is important to choose it\\n\\ncarefully in any application.\\n\\nThe finite-horizon model is appropriate when the agent’s lifetime is known; one im- portant aspect of this model is that as the length of the remaining lifetime decreases, the agent’s policy may change. A system with a hard deadline would be appropriately modeled this way. The relative usefulness of infinite-horizon discounted and bias-optimal models is still under debate. Bias-optimality has the advantage of not requiring a discount parameter;\\n\\nhowever, algorithms for finding bias-optimal policies are not yet as well-understood as those for finding optimal infinite-horizon discounted policies.\\n\\n1.38 Measuring Learning Performance\\n\\nThe criteria given in the previous section can be used to assess the policies learned by a given algorithm. We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use.\\n\\ne Eventual convergence to optimal. Many algorithms come with a provable guar-\\n\\nantee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992). This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau\\n\\n241\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n+2\\n\\nFinite horizon, h=4 +10\\n\\nInfinite horizon, y=0.9\\n\\nOOO0-0-00\"\\n\\nAverage reward\\n\\nFigure 2: Comparing models of optimality. All unlabeled arrows produce a reward of zero.\\n\\nat 99% of optimality may, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate.\\n\\ne Speed of convergence to optimality. Optimality is usually an asymptotic result, and so convergence speed is an ill-defined measure. More practical is the speed of convergence to near-optimality. This measure begs the definition of how near to optimality is sufficient. A related measure is level of performance after a given time, which similarly requires that someone define the given time.\\n\\nIt should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accu- racy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework (Valiant, 1984), there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance. That is usually an inappropriate view for an agent with a long existence in a complex environment.\\n\\nIn spite of the mismatch between embedded reinforcement learning and the train/test perspective, Fiechter (1994) provides a PAC analysis for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views.\\n\\nMeasures related to speed of learning have an additional weakness. An algorithm that merely tries to achieve optimality as fast as possible may incur unnecessarily large penalties during the learning period. A less aggressive strategy taking longer to achieve optimality, but gaining greater total reinforcement during its learning might be preferable.\\n\\ne Regret. A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret (Berry & Fristedt, 1985). It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain.\\n\\n242\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n1.4 Reinforcement Learning and Adaptive Control\\n\\nAdaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algo- rithms for improving a sequence of decisions from experience. Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and ac- tions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory. A very common formulation of cost functions in adaptive control are quadratic penalties on deviation from desired state and action vectors. Most importantly, although the dynamic model of the system is not known in advance, and must be esti- mated from data, the structure of the dynamic model is fixed, leaving model estimation as a parameter estimation problem. These assumptions permit deep, elegant and powerful mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive control algorithms.\\n\\n2. Exploitation versus Exploration: The Single-State Case\\n\\nOne major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the roblems of exploration, we treat a very simple case in this section. The fundamental issues and approaches described here will, in many cases, transfer to the more complex instances of reinforcement learning discussed later in the paper.\\n\\nThe simplest possible reinforcement-learning problem is known as the k-armed bandit\\n\\nproblem, which has been the subject of a great deal of study in the statistics and applied mathematics literature (Berry & Fristedt, 1985). The agent is in a room with a collection of k gambling machines (each called a “one-armed bandit” in colloquial English). The agent is ermitted a fixed number of pulls, h. Any arm may be pulled on each turn. The machines do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal machine. When arm ? is pulled, machine 7 pays off 1 or 0, according to some underlying probability parameter p;, where payoffs are independent events and the p;s are unknown. What should the agent’s strategy be? This problem illustrates the fundamental tradeoff between exploitation and exploration. The agent might believe that a particular arm has a fairly high payoff probability; should it choose that arm all the time, or should it choose another one that it has less information about, but seems to be worse? Answers to these questions depend on how long the agent is expected to play the game; the longer the game lasts, the worse the consequences of prematurely converging on a sub-optimal arm, and the more the agent should explore.\\n\\na\\n\\nThere is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt (1985). We use the term “action” to indicate the agent’s choice of arm to pull. This eases the transition into delayed reinforcement models in Section 3. It is very important to note that bandit problems fit our definition of a reinforcement-learning environment with a single state with only self transitions.\\n\\nSection 2.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case.\\n\\n243\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nSection 2.2 presents three techniques that are not formally justified, but that have had wide use in practice, and can be applied (with similar lack of guarantee) to the general case.\\n\\n2.1 Formally Justified Techniques\\n\\nThere is a fairly well-developed formal theory of exploration for very simple problems. Although it is instructive, the methods it provides do not scale well to more complex problems.\\n\\n2.1.1 DYNAMIC-PROGRAMMING APPROACH\\n\\nIf the agent is going to be acting for a total of A steps, it can use basic Bayesian reasoning to solve for an optimal strategy (Berry & Fristedt, 1985). This requires an assumed prior joint distribution for the parameters {p;}, the most natural of which is that each p; is independently uniformly distributed between 0 and 1. We compute a mapping from belief states (summaries of the agent’s experiences during this run) to actions. Here, a belief state\\n\\ncan be represented as a tabulation of action choices and payoffs: {n1, wi, na, W2,..., Mk, We} denotes a state of play in which each arm 7 has been pulled n; times with w; payoffs. We write V*(n1, wi,-.-,2k, WE) as the expected payoff remaining, given that a total of h pulls\\n\\nare available, and we use the remaining pulls optimally.\\n\\nIf 0; nj; = h, then there are no remaining pulls, and V*(nj, wy,..., nz, we) = 0. This is the basis of a recursive definition. If we know the V* value for all belief states with ¢ pulls remaining, we can compute the V* value of any belief state with t+ 1 pulls remaining:\\n\\nVe (ny.wp..-.,npewp) = max; B Future payoff if agent takes action a, then acts optimally for remaining pulls\\n\\n= max; piV™ (ny, W;,---,2i +1, wie+1,---, MK. We)+ (1 = pi) V*(m1, Wi, ee ME 1, Wi, +. Me, WE)\\n\\nwhere p; is the posterior subjective probability of action 7 paying off given n;, w; and our prior probability. For the uniform priors, which result in a beta distribution, p; =\\n\\nThe expense of filling in the table of V* values in this way for all attainable belief states is linear in the number of belief states times actions, and thus exponential in the horizon.\\n\\n2.1.2 GITTINS ALLOCATION INDICES\\n\\nGittins gives an “allocation index” method for finding the optimal choice of action at each step in k-armed bandit problems (Gittins, 1989). The technique only applies under the discounted expected reward criterion. For each action, consider the number of times it has been chosen, n, versus the number of times it has paid off, w. For certain discount factors, there are published tables of “index values,” I(n,w) for each pair of n and w. Look up the index value for each action 7, I(nj,w;). It represents a comparative measure of the combined value of the expected payoff of action i (given its history of payoffs) and the value of the information that we would get by choosing it. Gittins has shown that choosing the action with the largest index value guarantees the optimal balance between exploration and exploitation.\\n\\n244\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\na=0 a=1 KL0+—O+—0 +++ O40 O90 + 0 0-00 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=1 a=0 a=1\\n\\n1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=0\\n\\nFigure 3: A Tsetlin automaton with 2N states. The top row shows the state transitions that are made when the previous action resulted in a reward of 1; the bottom row shows transitions after a reward of 0. In states in the left half of the figure, action 0 is taken; in those on the right, action 1 is taken.\\n\\nBecause of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward (Salganicoff & Ungar, 1995). Unfortunately, no one has yet been able to find an analog of index values for delayed reinforcement problems.\\n\\n2.1.3 LEARNING AUTOMATA\\n\\nA branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar (1989), which were originally described explicitly as finite state automata. The Tsetlin automaton shown in Figure 3 provides an example that solves a 2-armed bandit arbitrarily near optimally as N approaches infinity.\\n\\nIt is inconvenient to describe algorithms as finite-state automata, so a move was made to describe the internal state of the agent as a probability distribution according to which actions would be chosen. The probabilities of taking different actions would be adjusted according to their previous successes and failures.\\n\\nAn example, which stands among a set of algorithms independently developed in the mathematical psychology literature (Hilgard & Bower, 1975), is the linear reward-inaction algorithm. Let p; be the agent’s probability of taking action 7.\\n\\ne When action a; succeeds,\\n\\nDi t= pita(l—p) Pj (= py— ap; for 7 #2\\n\\ne When action a; fails, p; remains unchanged (for all j).\\n\\nThis algorithm converges with probability 1 to a vector containing a single 1 and the rest 0’s (choosing a particular action with probability 1). Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making a small (Narendra & Thathachar, 1974). There is no literature on the regret of this algorithm.\\n\\n245\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n2.2 Ad-Hoc Techniques\\n\\nIn reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun (1992) has surveyed a variety of these techniques.\\n\\n2.2.1 GREEDY STRATEGIES\\n\\nThe first strategy that comes to mind is to always choose the action with the highest esti- mated payoff. The flaw is that early unlucky sampling might indicate that the best action’s reward is less than the reward obtained from a suboptimal action. The suboptimal action will always be picked, leaving the true optimal action starved of data and its superiority never discovered. An agent must explore to ameliorate this outcome.\\n\\nA useful heuristic is optimism in the face of uncertainty in which actions are selected greedily, but strongly optimistic prior beliefs are put on their payoffs so that strong negative evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrar- ily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the ez- ploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993).\\n\\n2.2.2 RANDOMIZED STRATEGIES\\n\\nAnother simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random. Some versions of this strategy start with a large value of p to encourage initial exploration, which is slowly decreased.\\n\\nAn objection to the simple strategy is that when it experiments with a non-greedy action it is no more likely to try a promising alternative than a clearly hopeless alternative. A slightly more sophisticated strategy is Boltzmann exploration. In this case, the expected reward for taking action a, E.R(a) is used to choose an action probabilistically according to the distribution\\n\\n(ER(a)/T\\n\\nPO Sea PROT\\n\\nThe temperature parameter T can be decreased over time to decrease exploration. This method works well if the best action is well separated from the others, but suffers somewhat when the values of the actions are close. It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care.\\n\\n2.2.3 INTERVAL-BASED TECHNIQUES\\n\\nExploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling’s interval estimation algorithm (1993b) stores statistics for each action a;: w; is the number of successes and n; the number of trials. An action is chosen by computing the upper bound of a 100-(1—a)%\\n\\n246\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nconfidence interval on the success probability of each action and choosing the action with the highest upper bound. Smaller values of the a parameter encourage greater exploration. When payoffs are boolean, the normal approximation to the binomial distribution can be used to construct the confidence interval (though the binomial should be used for small n). Other payoff distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods (Box & Draper, 1987), which are used for comparing multiple treatments (for example, fertilizers\\n\\nor drugs) to determine which treatment (if any) is best in as small a set of experiments as possible.\\n\\n2.3 More General Problems\\n\\nWhen there are multiple states, but reinforcement is still immediate, then any of the above solutions can be replicated, once for each state. However, when generalization is required, these solutions must be integrated with generalization methods (see section 6); this is straightforward for the simple ad-hoc methods, but it is not understood how to maintain theoretical guarantees.\\n\\nMany of these techniques focus on converging to some regime in which exploratory actions are taken rarely or never; this is appropriate when the environment is stationary. However, when the environment is non-stationary, exploration must continue to take place, in order to notice changes in the world. Again, the more ad-hoc techniques can be modified to deal with this in a plausible manner (keep temperature parameters from going to 0; decay the statistics in interval estimation), but none of the theoretically guaranteed methods can be applied.\\n\\n3. Delayed Reward\\n\\nIn the general case of the reinforcement learning problem, the agent’s actions determine not only its immediate reward, but also (at least probabilistically) the next state of the environment. Such environments can be thought of as networks of bandit problems, but the agent must take into account the next state as well as the immediate reward when it decides which action to take. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account. The agent will have to be able to learn from delayed reinforcement: it may take a long sequence of actions, receiving insignificant reinforcement, then finally arrive at a state with high reinforcement. The agent must be able to learn which of its actions are desirable based on reward that can take place arbitrarily far in the future.\\n\\n3.1 Markov Decision Processes\\n\\nProblems with delayed reinforcement are well modeled as Markov decision processes (MDPs). An MDP consists of\\n\\ne@ aset of states S,\\n\\ne a set of actions A,\\n\\n247\\n\\nKAELBLING, LITTMAN, & Moore\\n\\ne a reward function R:S x A> ®R, and\\n\\ne astate transition function T : S x A — TI(S), where a member of II(S) is a probability istribution over the set S (i.e. it maps states to probabilities). We write T(s, a, s’) for the probability of making a transition from state s to state s’ using action a.\\n\\nThe state transition function probabilistically specifies the next state of the environment as a function of its current state and the agent’s action. The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994).\\n\\nAlthough general MDPs may have infinite (even uncountable) state and action spaces, we will only discuss methods for solving finite-state and finite-action problems. In section 6, we discuss methods for solving problems with continuous input and output spaces.\\n\\n3.2 Finding a Policy Given a Model\\n\\nBefore we consider algorithms for learning to behave in MDP environments, we will ex- plore techniques for determining the optimal policy given a correct model. These dynamic programming techniques will serve as the foundation and inspiration for the learning al- gorithms to follow. We restrict our attention mainly to finding optimal policies for the infinite-horizon discounted model, but most of these algorithms have analogs for the finite- horizon and average-case models as well. We rely on the result that, for the infinite-horizon discounted model, there exists an optimal deterministic stationary policy (Bellman, 1957).\\n\\nWe will speak of the optimal value of a state—it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy. Using 7 as a complete decision policy, it is written\\n\\nV*(s)= max E (>: on)\\n\\nt=0\\n\\nThis optimal value function is unique and can be defined as the solution to the simultaneous equations\\n\\nsles\\n\\nV*(s) = max (n a+7>> renner) WseS, (1)\\n\\nwhich assert that the value of a state s is the expected instantaneous reward plus the expected discounted value of the next state, using the best available action. Given the optimal value function, we can specify the optimal policy as\\n\\nx*(s) = argmax | R(s,a) +7 Ss T(s,a,8\\')V*(s\") “ ES 3.2.1 VALUE ITERATION\\n\\nOne way, then, to find an optimal policy is to find the optimal value function. It can be determined by a simple iterative algorithm called value iteration that can be shown to converge to the correct V* values (Bellman, 1957; Bertsekas, 1987).\\n\\n248\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\ninitialize V(s) arbitrarily loop until policy good enough loop for s€S loop for aE A Q(s,4) = R(s,a) +7 Dyes Ts, 4, 8)V (6% V(s) := max, Q(s, a) end loop end loop\\n\\nIt is not obvious when to stop the value iteration algorithm. One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function (Williams & Baird, 1993b). It says that if the maximum difference between two successive value functions is less than ¢, then the value of the greedy policy, he policy obtained by choosing, in every state, the action that maximizes the estimated discounted reward, using the current estimate of the value function) differs from the value function of the optimal policy by no more than 2ey/(1— 7) at any state. This provides an effective stopping criterion for the algorithm. Puterman (1994) discusses another stopping criterion, based on the span semi-norm, which may result in earlier termination. Another mportant result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged (Bertsekas, 1987). And in practice, the greedy policy is often optimal long before the value function has converged.\\n\\n=\\n\\nValue iteration is very flexible. The assignments to V need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value of every state gets updated infinitely often on an infinite run. These issues are treated extensively by Bertsekas (1989), who also proves convergence results.\\n\\nUpdates based on Equation 1 are known as full backups since they make use of infor- mation from all possible successor states. It can be shown that updates of the form\\n\\nQs, a) = Qls,a) +a(r +7 maxQ(s!,a) — Q(s,a))\\n\\ncan also be used as long as each pairing of a and s is updated infinitely often, s’ is sampled from the distribution T(s, a, s’), r is sampled with mean R(s,a) and bounded variance, and the learning rate a is decreased slowly. This type of sample backup (Singh, 1993) is critical to the operation of the model-free methods discussed in the next section.\\n\\nThe computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions. Com- monly, the transition probabilities T(s, a, s’) are sparse. If there are on average a constant number of next states with non-zero probability then the cost per iteration is linear in the number of states and linear in the number of actions. The number of iterations required to reach the optimal value function is polynomial in the number of states and the magnitude of the largest reward if the discount factor is held constant. However, in the worst case the number of iterations grows polynomially in 1/(1— y), so the convergence rate slows considerably as the discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b).\\n\\n249\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n3.2.2 Poticy ITERATION\\n\\nThe policy iteration algorithm manipulates the policy directly, rather than finding it indi- rectly via the optimal value function. It operates as follows:\\n\\nchoose an arbitrary policy 7’ loop wisn compute the value function of policy 7: solve the linear equations V,(s) = R(s,7(s)) + ¥ Nores T(s, 7(8), 8)Vi(s’) improve the policy at each state: n\\'(s) := argmax, (R(s, a) + 7 Dees T(s, a, 8’) Vz(s\\'))\\n\\nuntil t= 7’\\n\\nThe value function of a policy is just the expected infinite discounted reward that will be gained, at each state, by executing that policy. It can be determined by solving a set of linear equations. Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. If it can, we change the policy to take the new action whenever it is in that situation. This step is guaranteed to strictly improve the performance of the policy. When no improvements are possible, then the policy is guaranteed to be optimal.\\n\\nSince there are at most |A||s| distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations (Puter- man, 1994). However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP (Littman et al., 1995b).\\n\\n3.2.3 ENHANCEMENT TO VALUE ITERATION AND POLicy ITERATION\\n\\nIn practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems. Puterman’s modified policy iteration algorithm (Puterman & Shin, 1978) provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of V,. Instead of finding an exact value for V,, we can perform a few steps of a modified value-iteration step where the policy is held fixed over successive iterations. This can be shown to produce an approximation to V, that converges linearly in 7. In practice, this can result in substantial speedups.\\n\\nSeveral standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution (Riide, 1993). State aggre- gation works by collapsing groups of states to a single meta-state solving the abstracted problem (Bertsekas & Castafion, 1989).\\n\\n250\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n3.2.4 COMPUTATIONAL COMPLEXITY\\n\\nValue iteration works by producing successive approximations of the optimal value function. Each iteration can be performed in O(|A||S|?) steps, or faster if there is sparsity in the ransition function. However, the number of iterations required can grow exponentially in he discount factor (Condon, 1992); as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of O(|A]|.$|?+|S|?) can be prohibitive. There is no known tight worst-case bound available or policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some ractictioners (Rust, 1996).\\n\\nLinear programming (Schrijver, 1986) is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D’Epenoux, 1963; Hoffman & Karp, 1966). An advantage of this approach is that commercial-quality inear-programming packages are available, although the time and space requirements can still be quite high. From a theoretic perspective, linear programming is the only known algorithm that can solve MDPs in polynomial time, although the theoretically efficient algorithms have not been shown to be efficient in practice.\\n\\n4. Learning an Optimal Policy: Model-free Methods\\n\\nIn the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model. The model consists of knowledge of the state tran- sition probability function T(s, a, s’) and the reinforcement function R(s,a). Reinforcement learning is primarily concerned with how to obtain the optimal policy when such a model is not known in advance. The agent must interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy. At this point, there are two ways to proceed.\\n\\ne Model-free: Learn a controller without learning a model. e Model-based: Learn a model, and use it to derive a controller.\\n\\nWhich approach is better? This is a matter of some debate in the reinforcement-learning community. A number of algorithms have been proposed on both sides. This question also appears in other fields, such as adaptive control, where the dichotomy is between direct and indirect adaptive control.\\n\\nThis section examines model-free learning, and Section 5 examines model-based meth- ods.\\n\\nThe biggest problem facing a reinforcement-learning agent is temporal credit assignment. How do we know whether the action just taken is a good one, when it might have far- reaching effects? One strategy is to wait until the “end” and reward the actions taken if the result was good and punish them if the result was bad. In ongoing tasks, it is difficult to know what the “end” is, and this might require a great deal of memory. Instead, we will use insights from value iteration to adjust the estimated value of a state based on\\n\\n251\\n\\nKAELBLING, LITTM.\\n\\n— 4\\n\\nAN, & Moore\\n\\nT’4\\n\\nFigure 4: Architecture for\\n\\nthe immediate reward and the estimated value is known as temporal difference methods (Sutt: temporal-difference learning strategies for the d\\n\\n4.1 Adaptive Heuristic Critic and TD(\\\\)\\n\\nThe adaptive heuristic critic algorithm is an ada\\n\\nSutton, & Anderson, 1983) in which the value mented by solving a set of linear equations, but TD(0). A block\\n\\nnents: a critic (la reinforcement-lear rithms, modified acting to maximiz v, that is computed by the critic. The critic us learn to map states to their expected discounted is the one currently instantia We can see the analogy wi\\n\\no deal with multiple states a:\\n\\nvalue function V, i new policy 7’ tha however, both components op: can be guaranteed to converge\\n\\nand Baird explored the convergence properties\\n\\nfor that po\\n\\nmaximizes the new value fun\\n\\no the optimal pol\\n\\ncall “incremental variants of\\n\\niagram for this approach is gi beled AHC), and a reinforcemen ning component can be an instance of any of the k-armed bandit algo-\\n\\ne instantaneous reward, it will ed in the RL comp h modified policy i\\n\\nworking in alternation. The policy 7 implemented by RL is fixed and t cy. Now we fix the critic and let the RL com\\n\\nolicy iteration” (Williams & Baird, 1993a).\\n\\nhe adaptive heuristic critic.\\n\\nof the next state. This class of algorithms ‘on, 1988). We will consider two different iscounted infinite-horizon model.\\n\\ntive version of policy iteration (Barto, -function computation is no longer imple- is instead computed by an algorithm called iven in Figure 4. It consists of two compo- -learning component (labeled RL). The nd non-stationary rewards. But instead of he heuristic value, he real external reinforcement signal to ues given that the policy onent.\\n\\nbe acting to maximize es\\n\\nval being executed\\n\\neration if we imagine these components e critic learns the onent learn a plementations,\\n\\nction, and so on. In most im\\n\\nerate simultaneously. Only the alternating implementation\\n\\nicy, under appropriate conditions. Williams of a class of AHC-related algorithms they\\n\\nIt remains to explain how the critic can learn the value of a policy. We define (s, a, r,s’)\\n\\nto be an experience tuple summarizing a single t\\n\\nransition in the environment. Here s is the\\n\\nagent’s state before the transition, a is its choice of action, r the instantaneous reward it\\n\\nreceives, and s’ its resulting state. The value o algorithm (Sutton, 1988) which uses the update\\n\\nV(s):\\n\\na policy is learned using Sutton’s TD(0) rule\\n\\nV(s)\\n\\nWhenever a state s is visited, its estimated val since r is the instantaneous reward received and occurring next state. This is analogous to the sa:\\n\\nVV (s\\')—V(s))\\n\\nue is updated to be closer to r+ yV(s‘), V(s’) is the estimated value of the actually mple-backup rule from value iteration—the\\n\\nonly difference is that the sample is drawn from the real world rather than by simulating\\n\\na known model. The key idea is that r + yV(s’\\n\\n252\\n\\nis a sample of the value of V(s), and it is\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nmore likely to be correct because it incorporates the real r. If the learning rate a is adjusted properly (it must be slowly decreased) and the policy is held fixed, TD(0) is guaranteed to converge to the optimal value function.\\n\\nThe TD(0) rule as presented above is really an instance of a more general class of algorithms called TD(X), with 4 = 0. T.D(0) looks only one step ahead when adjusting value estimates; although it will eventually arrive at the correct answer, it can take quite a while to do so. The general TD(A) rule is similar to the TD(0) rule given above,\\n\\nV(u) = V(u) ta(rt+yV(s\\') —V(s)je(u) ,\\n\\nbut it is applied to every state according to its eligibility e(u), rather than just to the immediately previous state, s. One version of the eligibility trace is defined to be\\n\\nt : _ lifs=s e(s) = Say! *S sq , where 35,5, = { 0 otherwise k=1\\n\\nThe eligibility of a state s is the degree to which it has been visited in the recent past; when a reinforcement is received, it is used to update all the states that have been recently visited, according to their eligibility. When \\\\ = 0 this is equivalent to TD(0). When \\\\ = 1, it is roughly equivalent to updating all the states according to the number of times they were visited by the end of a run. Note that we can update the eligibility online as follows:\\n\\ne(s) i= yAe(s)+1 if s= current state yAe(s) otherwise\\n\\nIt is computationally more expensive to execute the general TD(X), though it often converges considerably faster for large \\\\ (Dayan, 1992; Dayan & Sejnowski, 1994). There has been some recent work on making the updates more efficient (Cichosz & Mulawka, 1995) and on changing the definition to make T D(A) more consistent with the certainty-equivalent method (Singh & Sutton, 1996), which is discussed in Section 5.1.\\n\\n4.2 Q-learning\\n\\nThe work of the two components of AHC can be accomplished in a unified manner by Watkins’ Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation. Let Q*(s,a) be the expected discounted reinforcement of taking action ain state s, then continuing by choosing actions optimally. Note that V*(s) is the value of s assuming the best action is taken initially, and so V*(s) = max, Q*(s, a). Q*(s, a) can hence be written recursively as\\n\\nQ*(s,a) = R(s,a) +7 Ss T(s,a, 8’) max Q*(s’, a’). ES e\\n\\nNote also that, since V*(s) = max, Q*(s,a), we have x*(s) = argmax, Q*(s,a) as an optimal policy.\\n\\nBecause the Q function makes the action explicit, we can estimate the Q values on- line using a method essentially the same as TD(0), but also use them to define the policy,\\n\\n253\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nbecause an action can be chosen just by taking the one wit\\n\\ncurrent state. The Q-learning rule is\\n\\nQs.) = Qls,a) + alr +7 maxQ(s\\'a!\\n\\nwhere (s,a,r,s’) is an experie\\n\\neach state an infinite number of Q values will converge with probability 1 to Q* (Watkins,\\n\\nJordan, & Singh, 1994). Q-le more than one step previously,\\n\\nWhen the Q values are ne the agent to act greedily,\\n\\nthe maximum Q value for the\\n\\n— Q(s.4))\\n\\nnce tuple as described earlier. If each action is executed in a is decayed appropriately, the 989; Tsitsiklis, 1994; Jaakkola, arning can also be extended to update states that occurred . as in TD(A) (Peng & Williams, 1994).\\n\\ny converged to their optimal values, it is appropriate for\\n\\nimes on an infinite run and\\n\\nar.\\n\\naking, in each situation, the action with the highest @ value.\\n\\nDuring learning, however, there is a difficult exploitation versus exploration trade-off to be\\n\\nmade. There are no good, forma pt one of the\\n\\nstandard practice is to ado\\n\\nAHC architectures seem to It can be hard to get components converge togethe is, that the Q values will con\\n\\nlevel.\\n\\nly justified approaches to this problem in the general case; ad hoc methods discussed in section 2.2.\\n\\nifficult to work with than Q-learning on a practical e relative learning rates right in AHC so that the two In addition, Q-learning is exploration insensitive: that verge to the optimal values, independent of how the agent\\n\\nbe more t Tr.\\n\\nbehaves while the data is being collected (as long as all state-action pairs are tried often\\n\\nenough). This means that, in Q-learning, the details learning algorithm. For t most effective model-free however, address any of\\n\\noO\\n\\nese\\n\\ngo\\n\\na.\\n\\nalthough the exploration-exploitation issue must be addressed the explora\\n\\nion strategy will not affect the convergence of the -learning is the most popular and seems to be the earning from delayed reinforcement. It does not,\\n\\nreasons, Q rithm for\\n\\nhe issues involved in generalizing over large state and/or action spaces. In addition, it may converge qui\\n\\ne slowly to a good policy.\\n\\n4.3 Model-free Learning With Average Reward\\n\\nAs described, Q-learning can\\n\\nbe applied to discounted infinite-horizon MDPs. It can also\\n\\nbe applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset.\\n\\nSchwartz (1993) examine framework. Although his R-le some MDPs, severa problem they wish to solve th Q-learning (Mahade\\n\\nreward policies. Mahadevan ( a reinforcement-learning persp In particu (and some dynamic cies. Jaakkola, Jor\\n\\nresearchers have found the average-reward\\n\\nvan, 1994). With that in mind, researchers have studied the problem o\\n\\nar, he showed that existing reinforcement-learning alg programming algorithms) do not always an and Singh (1995) described an average-reward learning algorithm\\n\\nthe problem of adapting Q-learning to an average-reward arning algorithm seems to exhibit convergence problems for criterion closer to the true an a discounted criterion and therefore prefer R-learning to\\n\\nlearning optimal average- 996) surveyed model-based average-reward algorithms from ective and found several difficulties with existing algorithms. orithms for average reward roduce bias-optimal poli-\\n\\nwith guaranteed convergence properties. It uses a Monte-Carlo component to estimate the\\n\\nexpected\\n\\nuture reward for each state as the agent moves through the environment. In\\n\\n254\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\naddition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new textbook (1995). Although this recent work provides a much needed theoretical foundation to this area of reinforcement learning, many important problems remain unsolved.\\n\\n5. Computing Optimal Policies by Learning Models\\n\\nThe previous section showed how it is possible to learn an optimal policy without knowing the models T(s, a, s’) or R(s,a) and without even learning those models en route. Although many of these methods are guaranteed to find optimal policies eventually and use very little computation time per experience, they make extremely inefficient use of the data they gather and therefore often require a great deal of experience to achieve good performance. In this section we still begin by assuming that we don’t know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are\\n\\nespecially important in applications in which computation is considered to be cheap and real-world experience costly.\\n\\n5.1 Certainty Equivalent Methods\\n\\nWe begin with the most conceptually straightforward method: first, learn the T and R functions by exploring the environment and keeping statistics about the results of each action; next, compute an optimal policy using one of the methods of Section 3. This method is known as certainty equivlance (Kumar & Varaiya, 1986).\\n\\nThere are some serious objections to this method:\\n\\ne It makes an arbitrary division between the learning phase and the acting phase.\\n\\ne How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data (Whitehead, 1991) than a system that interleaves experience gathering with policy-building more tightly (Koenig & Simmons, 1993). See Figure 5 for an example.\\n\\ne The possibility of changes in the environment is also problematic. Breaking up an agent’s life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment changes.\\n\\nA variation on this idea is certainty equivalence, in which the model is learned continually through the agent’s lifetime and, at each step, the current model is used to compute an optimal policy and value function. This method makes very effective use of available data, but still ignores the question of exploration and is extremely computationally demanding, even for fairly small state spaces. Fortunately, there are a number of other model-based algorithms that are more practical.\\n\\n5.2 Dyna\\n\\nSutton’s Dyna architecture (1990, 1991) exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than\\n\\n255\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nFigure 5: In this environment, due to Whitehead (1991), random exploration would take take O(2\") steps to reach the goal even once, whereas a more intelligent explo- ration strategy (e.g. “assume any untried action leads directly to goal”) would require only O(n”) steps.\\n\\nthe certainty-equivalence approach. It simultaneously uses experience to build a model (T and R), uses experience to adjust the policy, and uses the model to adjust the policy.\\n\\nDyna operates in a loop of interaction with the environment. Given an experience tuple (s,a,s\\',r), it behaves as follows:\\n\\ne Update the model, incrementing statistics for the transition from s to s’ on action a and for receiving reward r for taking action a in state s. The updated models are T and R.\\n\\ne Update the policy at state s based on the newly updated model using the rule\\n\\nQls.a) = Risa) +7 Do F(s,0,8!) maxQls\\'.a’) .\\n\\nwhich is a version of the value-iteration update for Q values.\\n\\ne Perform k additional updates: choose k state-action pairs at random and update them according to the same rule as before:\\n\\nQ(sp, ag) :=R(sp, ag) + + oT (sn, ap, 8’) max Q(s\\', a’).\\n\\ne Choose an action a’ to perform in state s’, based on the Q values but perhaps modified by an exploration strategy.\\n\\nThe Dyna algorithm requires about f times the computation of Q-learning per instance, but this is typically vastly less than for the naive model-based method. A reasonable value of & can be determined based on the relative speeds of computation and of taking action.\\n\\nFigure 6 shows a grid world in which in each cell the agent has four actions (N, S, E, W) and transitions are made deterministically to an adjacent cell, unless there is a block, in which case no movement occurs. As we will see in Table 1, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational effort, however.\\n\\n256\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nFigure 6: A 3277-state grid world. This was formulated as a shortest-path reinforcement- learning problem, which yields the same result as if a reward of 1 is given at the goal, a reward of zero elsewhere and a discount factor is used.\\n\\nSteps before Backups before\\n\\nconvergence convergence Q-learning 531,000 531,000 Dyna 62,000 3,055,000 prioritized sweeping 28,000 1,010,000\\n\\nTable 1: The performance of three algorithms described in the text. All methods used the exploration heuristic of “optimism in the face of uncertainty”: any state not previously visited was assumed by default to be a goal state. Q-learning used its optimal learning rate parameter for a deterministic maze: a@ = 1. Dyna and prioritized sweeping were permitted to take k = 200 backups per transition. For prioritized sweeping, the priority queue often emptied before all backups were used.\\n\\n257\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n5.3 Prioritized Sweeping / Queue-Dyna\\n\\nAlthough Dyna is a great improvement on previous methods, it suffers from being relatively undirected. It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the “interesting” parts of the state space. These problems are addressed by prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams, 1993), which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail.\\n\\nThe algorithm is similar to Dyna, except that updates are no longer chosen at random and values are now associated with states (as in value iteration) instead of state-action pairs (as in Q-learning). To make appropriate choices, we must store additional information in the model. Each state remembers its predecessors: the states that have a non-zero transition probability to it under some action. In addition, each state has a priority, initially set to zero.\\n\\nInstead of updating & random state-action pairs, prioritized sweeping updates k states with the highest priority. For each high-priority state s, it works as follows:\\n\\ne Remember the current value of the state: Vou = V(s).\\n\\ne Update the state’s value\\n\\nV(s) = max (i. al+y>oT(s, 4, vie)\\n\\n3!\\n\\ne Set the state’s priority back to 0.\\n\\ne Compute the value change A = |V,7aq — V(s)|.\\n\\nUse A to modify the priorities of the predecessors of s.\\n\\nIf we have updated the V value for state s’ and it has changed by amount A, then the immediate predecessors of s’ are informed of this event. Any state s for which there exists an action a such that T(s,a,s’) # 0 has its priority promoted to A - T(s,a,s’), unless its priority already exceeded that value.\\n\\nThe global behavior of this algorithm is that when a real-world transition is “surprising” (the agent happens upon a goal state, for instance), then lots of computation is directed to propagate this new information back to relevant predecessor states. When the real- world transition is “boring” (the actual result is very similar to the predicted result), then computation continues in the most deserving part of the space.\\n\\nRunning prioritized sweeping on the problem in Figure 6, we see a large improvement over Dyna. The optimal policy is reached in about half the number of steps of experience and one-third the computation as Dyna required (and therefore about 20 times fewer steps and twice the computational effort of Q-learning).\\n\\n258\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n5.4 Other Model-Based Methods\\n\\nMethods proposed for based methods as well. RTDP (real-time\\n\\nmodel-based method t of the state-space that he agent is trying to By taking into accoun without necessarily vis’\\n\\nsolving MDPs given a model can be used in the context of model-\\n\\nynamic programming) (Barto, Bradtke, & Singh, 1995) is another at uses Q-learning to concentrate computational effort on the areas the agent is most likely to occupy. It is specific to problems in which\\n\\nachieve a particular goal state and the reward everywhere else is 0.\\n\\nthe start state, it can find a short path from the start to the goal, iting the rest of the state space.\\n\\nThe Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman, 994) exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent’s current state and the goal state, if there is one. States that are not in the envelope are summarized by a single “out” state. The planning process is an alternation between finding an optimal policy on the approximate MDP and to the envelope. Action may take place in parallel with planning, in states are also pruned out of the envelope.\\n\\nadding useful states which case irrelevan\\n\\n6. Generalization\\n\\nAll of the previous discussion has tacitly assumed that it is possible to enumerate the state and action spaces and store tables of values over them. Except in very small environments, his means impractical memory requirements. It also makes inefficient use of experience. In a large, smooth state space we generally expect similar states to have similar values and sim- actions. Surely, therefore, there should be some more compact representation han a table. Most problems will have continuous or large discrete state spaces; some wil have large or continuous action spaces. The problem of learning in large spaces is addresse: hrough generalization techniques, which allow compact storage of learned information an er of knowledge between “s:\\n\\nilar optima.\\n\\nrans imilar” states and actions. The large literature of genera. ues from inductive concept learning can be applied to reinforcement learning niques often need to be tailored to specific details of the problem. In the following sections, we explore the application of standar unction-approximation techniques, adaptive resolution models, and hierarchical methods o the problem of reinforcement learning.\\n\\nT\\n\\nhe s\\n\\nization techni\\n\\n. However, tec.\\n\\np D.\\n\\nhe reinforcement-learning architectures and algorithms discussed above have include orage of a variety of mappings, including S — A (policies), S > R (value functions), Sx A— * (Q functions and rewards), S x A > S (deterministic transitions), and S x Ax § => [0,1] (transition probabilities). Some of these mappings, such as transitions an immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervise learning that support noisy training examples. Popular techniques include various neural- network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods. Other mappings, especially the policy\\n\\n259\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nmapping, typically need specialized algorithms because training sets of input-output pairs are not available.\\n\\n6.1 Generalization over Input\\n\\nA reinforcement-learning agent’s current state plays a central role in its selection of reward- maximizing actions. Viewing the agent as a state-free black box, a description of the current state is its input. Depending on the agent architecture, its output is either an action selection, or an evaluation of the current state that can be used to select an action. The problem of deciding how the different aspects of an input affect the value of the output is sometimes called the “structural credit-assignment” problem. This section examines approaches to generating actions or evaluations as a function of a description of the agent’s current state. The first group of techniques covered here is specialized to the case when reward is not delayed; the second group is more generally applicable.\\n\\n6.1.1 IMMEDIATE REWARD\\n\\nWhen the agent’s actions do not influence state transitions, the resulting problem becomes one of choosing actions to maximize immediate reward as a function of the agent’s current state. These problems bear a resemblance to the bandit problems discussed in Section 2 except that the agent should condition its action selection on the current state. For this reason, this class of problems has been described as associative reinforcement learning. The algorithms in this section address the problem of learning from immediate boolean reinforcement where the state is vector valued and the action is a boolean vector. Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4.1. They can also be generalized to real-valued reward through reward comparison methods (Sutton, 1984).\\n\\nCRBP_ Thecomplementary reinforcement backpropagation algorithm (Ackley & Littman, 1990) (CRBP) consists of a feed-forward network mapping an encoding of the state to an encoding of the action. The action is determined probabilistically from the activation of the output units: if output unit ¢ has activation y;, then bit ¢ of the action vector has value 1 with probability y;, and 0 otherwise. Any neural-network supervised training procedure can be used to adapt the network as follows. If the result of generating action a is r = 1, then the network is trained with input-output pair (s,a). If the result is r = 0, then the network is trained with input-output pair (s,@), where @ = (1— a,...,1— ay).\\n\\nThe idea behind this training rule is that whenever an action fails to generate reward, CRBP will try to generate an action that is different from the current choice. Although it seems like the algorithm might oscillate between an action and its complement, that does not happen. One step of training a network will only change the action slightly and since the output probabilities will tend to move toward 0.5, this makes action selection more random and increases search. The hope is that the random distribution will generate an action that works better, and then that action will be reinforced.\\n\\nARC The associative reinforcement comparison (ARC) algorithm (Sutton, 1984) is an instance of the AHc architecture for the case of boolean actions, consisting of two feed-\\n\\n260\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nforward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units.\\n\\nIn the simplest case, the entire system learns only to optimize immediate reward. First, let us consider the behavior of the network that learns the policy, a mapping from a vector describing s to a Q or 1. If the output unit has activation y;, then a, the action generated, will be 1 if y+ v > 0, where v is normal noise, and 0 otherwise.\\n\\nThe adjustment for the output unit is, in the simplest case,\\n\\ne=r(a—1/2) ,\\n\\nwhere the first factor is the reward received for taking the most recent action and the second encodes which action was taken. The actions are encoded as 0 and 1, so a—1/2 always has the same magnitude; if the reward and the action have the same sign, then action 1 will be made more likely, otherwise action 0 will be.\\n\\nAs described, the network will tend to seek actions that given positive reward. To extend this approach to maximize reward, we can compare the reward to some baseline, b. This changes the adjustment to\\n\\n© =(r—d)a~1/2) |\\n\\nwhere 6 is the output of the second network. The second network is trained in a standard supervised mode to estimate r as a function of the input state s.\\n\\nVariations of this approach have been used in a variety of applications (Anderson, 1986; Barto et al., 1983; Lin, 1993b; Sutton, 1984).\\n\\nREINFORCE Algorithms Williams (1987, 1992) studied the problem of choosing ac- tions to maximize immedate reward. He identified a broad class of update rules that per- form gradient descent on the expected reward and showed how to integrate these rules with backpropagation. This class, called REINFORCE algorithms, includes linear reward-inaction (Section 2.1.3) as a special case.\\n\\nThe generic REINFORCE update for a parameter w;; can be written\\n\\nAw = a4j(r - badger Ina)\\n\\nwhere a;; is a non-negative factor, r the current reinforcement, 6;; a reinforcement baseline, and g; is the probability density function used to randomly generate actions based on unit activations. Both a;; and b;; can take on different values for each w;;, however, when a;; is constant throughout the system, the expected update is exactly in the direction of the expected reward gradient. Otherwise, the update is in the same half space as the gradient but not necessarily in the direction of steepest increase.\\n\\nWilliams points out that the choice of baseline, 6 convergence speed of the algorithm.\\n\\nij, can have a profound effect on the\\n\\nLogic-Based Methods Another strategy for generalization in reinforcement learning is to reduce the learning problem to an associative problem of learning boolean functions. A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive\\n\\n261\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nthe generalization process (Kaelbling, 1994b); the other searches the space of syntactic descriptions of functions using a simple generate-and-test method (Kaelbling, 1994a).\\n\\nThe restriction to a single boolean output makes these techniques difficult to apply. In very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output. In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The CASCADE method (Kaelbling, 1993b) allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort.\\n\\n6.1.2 DELAYED REWARD\\n\\nAnother method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning. Here, a function approximator is used o represent the value function by mapping a state description to a value. Many reseachers have experimented with this approach: Boyan and Moore (1995) used local memory-based methods in conjunction with value iteration; Lin (1991) used backprop- agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992, 995) used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich (1995) used backpropagation and T D(A) to learn good strategies for job-shop scheduling.\\n\\nAlthough there have been some positive examples, in general there are unfortunate in- eractions between function approximation and the learning rules. In discrete environments\\n\\nhere is a guarantee that any operation that updates the value function (according to the\\n\\nBellman equations) can only reduce the error between the current value function and the optimal value function. This guarantee no longer holds when generalization is used. These issues are discussed by Boyan and Moore (1995), who give some simple examples of value unction errors growing arbitrarily large when generalization is used with value iteration. Their solution to this, applicable only to certain classes of problems, discourages such diver-\\n\\ngence by only permitting updates whose estimated values can be shown to be near-optimal via a battery of Monte-Carlo experiments.\\n\\nThrun and Schwartz (1993) theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the “max” operator in the definition of the value function.\\n\\nSeveral recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show how the appro- priate choice of function approximator can guarantee convergence, though not necessarily to the optimal values. Baird’s residual gradient technique (Baird, 1995) provides guaranteed convergence to locally optimal solutions.\\n\\nPerhaps the gloominess of these counter-examples is misplaced. Boyan and Moore (1995) report that their counter-examples can be made to work with problem-specific hand-tuning despite the unreliability of untuned algorithms that provably converge in discrete domains. Sutton (1996) shows how modified versions of Boyan and Moore’s examples can converge successfully. An open question is whether general principles, ideally supported by theory, can help us understand when value function approximation will succeed. In Sutton’s com-\\n\\n262\\n\\nREINFORCEMENT LEARNING\\n\\n: A SURVEY\\n\\nparative experiments with Boyan and Moore’s counter-examples, he changes four aspects\\n\\nof the experiments:\\n\\n1. Small changes to\\n\\nthe task specifications.\\n\\n2. A very different kind of function approximator (CMAC (Albus, 1975)) that has weak\\n\\ngeneralization.\\n\\n3. A different learning algorithm: SARSA (Rummery & Niranjan, 1994) instead of value\\n\\niteration.\\n\\n4. A different training regime. Boyan and Moore sampled sta whereas Sutton’s method sampled along empirical traject\\n\\nThere are intuitive reasons to believe that the fourth factor is\\n\\nmore careful research is needed.\\n\\nAdaptive Resolution Models the environment into regions of states that can be considered t: learning and generating actions. Without detailed is very difficult to know what granularity or placement of par use adaptive resolution; during the course of learning, artition is constructed that is appropriate to the\\n\\nproblem is overcome in a\\n\\nDecision Trees va.\\n\\nIn environments ued variables, it is possible to learn compact decision trees for representing Q values. The\\n\\nmethods tha\\n\\nhat are charac\\n\\nes uniformly in state space, ories.\\n\\narticularly important, but\\n\\nIn many cases, what we would like to do is partition\\n\\ne same for the purposes of rior knowledge of the environment, it itions is appropriate. This\\n\\nenvironment.\\n\\nerized by a set of boolean or discrete-\\n\\nworks as fol\\n\\nG-learning algorithm (Chapman & Kk hat no partitioning is necessary an if it were one state. In parallel with input bits: it asks the question whe\\n\\nstates in which 6 = 0. If such a bit he process is repeated in each of t\\n\\naelbling, 1991), tries to learn this process, i her there is some bit 6 in\\n\\nhat the Q values for states in which 6 = 1 are significantly\\n\\nis found, it is used to spl\\n\\nQ values for gathers statistics based on individua\\n\\ne leaves. This method was able to learn very smal\\n\\nows. It starts by assuming the entire environment as\\n\\nthe state description such ifferent from Q values for it the decision tree. Then\\n\\n;\\n\\nre\\n\\ngame environment and or dealing with partial cannot, however, acqui (such as those needed\\n\\nVariable Resolution enables conventional\\n\\nresentations of the Q function in noisy state attributes. It outperformed Q-learning with backpro\\n\\nwas used by McCallum (1995 observability re partitions in which attribu o solve parity problems).\\n\\nDynamic Programming ynamic programming to be\\n\\nhe presence of an overwhel\\n\\nto learn behaviors in a complex driving-simulator. I\\n\\nming number of irrelevant, agation in a simple video- (in conjunction with other techniques\\n\\nes are only significant in combination\\n\\nThe VRDP algorithm (Moore, 1991 performed in real-valued multivariate\\n\\nstate-spaces where straightforward discretization would fall prey to the curse of dimension-\\n\\nality. A kd-tree (simi\\n\\nregions. The coarse regions are refined into detailed\\n\\nspace which are predic\\n\\nning “mental trajectories” through state space. This algorithm\\n\\ned to be important. This no\\n\\nof problems for which disadvantage of requiri\\n\\null high-resolution arrays wo ng a guess at an initially vali\\n\\n263\\n\\nar to a decision tree) is used to parti\\n\\ntion state space into coarse regions, but only in parts of the state ion of importance is obtained by run- proved effective on a number uld have been impractical. It has the trajectory through state-space.\\n\\n(a)\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n(b)\\n\\n(c)\\n\\nStart\\n\\nFigure 7: (a) A two-dimensional maze pro start to goal without crossing an PartiGame during the entire first\\n\\nGoal\\n\\nNF\\n\\ni i\\n\\ninal fo EE\\n\\nBs\\n\\nThe point robot must find a path from y of the barrier lines. (b) The path taken by rial. It begins with intense exploration to find a\\n\\nblem.\\n\\nroute out of the almost entirely enclosed start region. Having eventually reached\\n\\na sufficiently high resolution, it discovers the gap and proceeds gree\\n\\nily towards\\n\\nthe goal, only to be temporarily blocked by the goal’s barrier region. (c) The\\n\\nsecond\\n\\ntrial.\\n\\nPartiGame Algorithm Moore’s PartiGame algorithm (Moore, 1994) is another solution to the problem of learning to achieve goal configurations in deterministic high-dimensional\\n\\ncontinuous s\\n\\naces by learning an adaptive-resolution model. It also divides the environment\\n\\ninto cells; but in each cell, the actions available consist of aiming at the neighboring cells\\n\\n(this aiming problem sta incremental\\n\\nis accomplished by a local controller, which must be provided as ement). The graph of cell transitions is solved for shortest paths in an online manner, but a minimax criterion is used to detect when a group of cells is\\n\\nart of the\\n\\ntoo coarse to prevent movement between obstacles or to avoid limit cycles. The offending\\n\\ncells are spli choose appropria An important fea it also struc the agent wi small local c\\n\\nures 1 ini ang\\n\\nto higher resolution. Eventually, the environment is divided up jus\\n\\ne actions for ach\\n\\nexploration of s ially try someth: es when all the\\n\\nFigure 7a shows a two-dimens\\'\\n\\nof a robot using second trial, star\\n\\nThis is a very than a minute. T limits its applica methods.\\n\\ned from a slight\\n\\nfast algorithm, | e restriction of bility, however.\\n\\nture is that, as well as reducing memory and computational re\\n\\nenough to ieving the goal, but no unnecessary distinctions are made. uirements, ate space in a multi-resolution manner. Given a failure, ing very different to rectify the failure, and only resort to ualitatively different strategies have been exhausted.\\n\\nional continuous maze. Figure 7b shows the performance\\n\\nhe PartiGame algorithm during the very first trial. Figure 7c shows the\\n\\ny different position.\\n\\nearning policies in spaces of up to nine dimensions in less he current implementation to deterministic environments McCallum (1995) suggests some related tree-structured\\n\\n264\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n6.2 Generalization over Actions\\n\\nThe networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well.\\n\\nIn cases such as this when actions are described combinatorially, it is important to generalize over actions to avoid keeping separate statistics for the huge number of actions that can be chosen. In continuous action spaces, the need for generalization is even more pronounced.\\n\\nWhen estimating Q values using a neural network, it is possible to use either a distinct network for each action, or a network with a distinct output for each action. When the action space is continuous, neither approach is possible. An alternative strategy is to use a single network with both the state and action as input and Q value as the output. Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value (Baird & Klopf, 1993).\\n\\nGullapalli (1990, 1992) has developed a “neural” reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts\\n\\nthe mean and variance based on previous experience. When the chosen actions are not performing well, the variance is high, resulting in exploration of the range of choices. When an action performs well, the mean is moved in that direction and the variance decreased, resulting in a tendency to generate more action values near the successful one. This method was successfully employed to learn to control a robot arm with many continuous degrees of\\n\\nfreedom.\\n\\n6.3 Hierarchical Methods\\n\\nAnother strategy for dealing with large state spaces is to treat them as a hierarchy of learning problems. In many cases, hierarchical solutions introduce slight sub-optimality in performance, but potentially gain a good deal of efficiency in execution time, learning time, and space. Hierarchical learners are commonly structured as gated behaviors, as shown in Figure 8. There is a collection of behaviors that map environment states into low-level actions and a gating function that decides, based on the state of the environment, which behavior’s actions should be switched through and actually executed. Maes and Brooks (1990) used a version of this architecture in which the individual behaviors were fixed a priori and the gating function was learned from reinforcement. Mahadevan and Connell (1991b) used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned. Lin (1993a) and Dorigo and Colombetti (1995, 1994) both used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework.\\n\\n6.3.1 FEUDAL Q-LEARNING\\n\\nFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external environment. Its actions consist of commands that\\n\\n265\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nFigure 8: A structure of gated behaviors.\\n\\nit can give to the low-level learner. When the master generates a particular command to the slave, it must reward the slave for taking actions that satisfy the command, even if they do not result in external reinforcement. The master, then, learns a mapping from states to commands. The slave learns a mapping from commands and states to external actions. The set of “commands” and their associated reinforcement functions are established in advance of the learning.\\n\\nThis is really an instance of the general “gated behaviors” approach, in which the slave can execute any of the behaviors depending on its command. The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels.\\n\\n6.3.2 COMPOSITIONAL Q-LEARNING\\n\\nSingh’s compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of condi- tions in sequential order. The achievement of the conditions provides reinforcement for the elemental tasks, which are trained first to achieve individual subgoals. Then, the gating function learns to switch the elemental tasks in order to achieve the appropriate high-level sequential goal. This method was used by Tham and Prager (1994) to learn to control a simulated multi-link robot arm.\\n\\n6.3.3 HIBRARCHICAL DISTANCE TO GOAL\\n\\nEspecially if we consider reinforcement learning modules to be part of larger agent archi- tectures, it is important to consider problems in which goals are dynamically input to the learner. Kaelbling’s HDG algorithm (1993a) uses a hierarchical approach to solving prob- lems when goals of achievement (the agent should get to a particular state as quickly as possible) are given to an agent dynamically.\\n\\nThe HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned (a priori, but more recent work (Ashar, 1994) addresses the case of learning the partition) into a set of regions whose centers are known as “landmarks.” If the agent is\\n\\n266\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\noffice\\n\\nFigure 9: An example of a partially observable environment.\\n\\ncurrently in the same region as the goal, then it uses low-level actions to move to the goal. If not, then high-level information is used to determine the next landmark on the shortest path from the agent’s closest landmark to the goal’s closest landmark. Then, the agent uses low-level information to aim toward that next landmark. If errors in action cause deviations in the path, there is no problem; the best aiming point is recomputed on every step.\\n\\n7. Partially Observable Environments\\n\\nIn many real-world environments, it will not be possible for the agent to have perfect and complete perception of the state of the environment. Unfortunately, complete observability is necessary for learning methods based on MDPs. In this section, we consider the case in which the agent makes observations of the state of the environment, but these observations may be noisy and provide incomplete information. In the case of a robot, for instance, it might observe whether it is in a corridor, an open room, a T-junction, etc., and those observations might be error-prone. This problem is also referred to as the problem of “incomplete perception,” “perceptual aliasing,” or “hidden state.”\\n\\nIn this section, we will consider extensions to the basic MDP framework for solving partially observable problems. The resulting formal model is called a partially observable Markov decision process or POMDP.\\n\\n7.1 State-Free Deterministic Policies\\n\\nThe most naive strategy for dealing with partial observability is to ignore it. That is, to reat the observations as if they were the states of the environment and try to learn to behave. Figure 9 shows a simple environment in which the agent is attempting to get to he printer from an office. If it moves from the office, there is a good chance that the agent will end up in one of two places that look like “hall”, but that require different actions for getting to the printer. If we consider these states to be the same, then the agent cannot ossibly behave optimally. But how well can it do?\\n\\nThe resulting problem is not Markovian, and Q-learning cannot be guaranteed to con- verge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate (Chrisman &\\n\\n267\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nLittman, 1993). It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations. Unfortunately, when the environment is not Markovian, the transition probabilities depend on the policy being executed, so this new policy will induce a new set of transition probabilities. This approach may yield plausible results in some cases, but again, there are no guarantees.\\n\\nIt is reasonable, though, to ask what the optimal policy (mapping from observations to actions, in this case) is. It is NP-hard (Littman, 1994b) to find this mapping, and even the best mapping can have very poor performance. In the case of our agent trying to get to the printer, for instance, any deterministic state-free policy takes an infinite number of steps to reach the goal on average.\\n\\n7.2 State-Free Stochastic Policies\\n\\nSome improvement can be gained by considering stochastic policies; these are mappings from observations to probability distributions over actions. If there is randomness in the agent’s actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan (1995) have developed an algorithm for finding locally-optimal stochastic policies, but finding a globally optimal policy is still NP hard.\\n\\nIn our example, it turns out that the optimal stochastic policy is for the agent, when in a state that looks like a hall, to go east with probability 2 — /2 % 0.6 and west with probability /2 —1 % 0.4. This policy can be found by solving a simple (in this case) quadratic program. The fact that such a simple example can produce irrational numbers gives some indication that it is a difficult problem to solve exactly.\\n\\n7.3 Policies with Internal State\\n\\nThe only way to behave truly effectively in a wide-range of environments is to use memory of previous actions and observations to disambiguate the current state. There are a variety of approaches to learning policies with internal state.\\n\\nRecurrent Q-learning One intuitively simple approach is to use a recurrent neural net- work to learn Q values. The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain “history features” to predict value. This approach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin & Mitchell, 1992; Schmidhuber, 1991b). It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems.\\n\\nClassifier Systems Classifier systems (Holland, 1975; Goldberg, 1989) were explicitly developed to solve problems with delayed reward, including those requiring short-term memory. The internal mechanism typically used to pass reward back through chains of decisions, called the bucket brigade algorithm, bears a close resemblance to Q-learning. In spite of some early successes, the original design does not appear to handle partially ob- served environments robustly.\\n\\nRecently, this approach has been reexamined using insights from the reinforcement- learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems (Dorigo & Bersini, 1994). Cliff and Ross (1994) start with Wilson’s zeroth-\\n\\n268\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nFigure 10: Structure of a POMDP agent.\\n\\nevel classifier system (Wilson, 1995) and add one and two-bit memory registers. They find hat, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments.\\n\\nDorigo and Colombetti applied classifier systems to a moderately complex problem of learning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti, 994).\\n\\nFinite-history-window Approach One way to restore the Markov property is to allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell (1992) used a fixed-width finite history window to learn a pole balancing task. McCallum (1995) describes the “utile suffix memory” which learns a variable-width window hat serves simultaneously as a model of the environment and a finite-memory policy. This system has had excellent results in a very complex driving-simulation domain (McCallum, 995). Ring (1994) has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations.\\n\\nPOMDP Approach Another strategy consists of using hidden Markov model (HMM) echniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Monahan, 1982). Chrisman (1992) showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum (1993), also gave heuristic state- splitting rules to attempt to learn the smallest possible model for a given environment. The resulting model can then be used to integrate information from the agent’s observations in order to make decisions.\\n\\nFigure 10 illustrates the basic structure for a perfect-memory controller. The component on the left is the state estimator, which computes the agent’s belief state, b as a function of the old belief state, the last action a, and the current observation 7. In this context, a belief state is a probability distribution over states of the environment, indicating the likelihood, given the agent’s past experience, that the environment is actually in each of those states. The state estimator can be constructed straightforwardly using the estimated world model and Bayes’ rule.\\n\\nNow we are left with the problem of finding a policy mapping belief states into action. This problem can be formulated as an MDP, but it is difficult to solve using the techniques described earlier, because the input space is continuous. Chrisman’s approach (1992) does not take into account future uncertainty, but yields a policy after a small amount of com- putation. A standard approach from the operations-research literature is to solve for the\\n\\n269\\n\\nKAELBLING, LITTMAN, & Moore\\n\\noptimal policy (or a close approximation thereof) based on its representation as a piecewise- linear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations (Cassandra et al., 1994; Littman, Cassandra, & Kaelbling, 1995a).\\n\\n8. Reinforcement Learning Applications\\n\\nOne reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act. But it is unsurprising that it has also been used by a number of researchers as a practical computational tool for constructing autonomous systems that improve themselves with experience. These applications have ranged from robotics, to industrial manufacturing, to combinatorial search problems such as computer game playing.\\n\\nPractical applications provide a test of the efficacy and usefulness of learning algorithms. They are also an inspiration for deciding which components of the reinforcement learning framework are of practical importance. For example, a researcher with a real robotic task can provide a data point to questions such as:\\n\\ne How important is optimal exploration? Can we break the learning period into explo- ration phases and exploitation phases?\\n\\ne What is the most useful model of long-term reward: Finite horizon? Discounted? Infinite horizon?\\n\\ne How much computation is available between agent decisions and how should it be used?\\n\\ne What prior knowledge can we build into the system, and which algorithms are capable of using that knowledge?\\n\\nLet us examine a set of practical applications of reinforcement learning, while bearing these questions in mind.\\n\\n8.1 Game Playing\\n\\nGame playing has dominated the Artificial Intelligence world as a problem domain ever since he field was born. Two-player games do not fit into the established reinforcement-learning ramework since the optimality criterion for games is not one of maximizing reward in the ace of a fixed environment, but one of maximizing reward against an optimal adversary (minimax). Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games (Littman, 1994a) and many researchers have used reinforcement earning in these environments. One application, spectacularly far ahead of its time, was Samuel’s checkers playing system (Samuel, 1959). This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning.\\n\\nMore recently, Tesauro (1992, 1994, 1995) applied the temporal difference algorithm o backgammon. Backgammon has approximately 107° states, making table-based rein- orcement learning impossible. Instead, Tesauro used a backpropagation-based three-layer\\n\\n270\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nTraining Hidden Results Games Units Basic Poor TD 1.0 300,000 80 Lost by 13 points in 51 games TD 2.0 800,000 40 Lost by 7 points in 38 games TD 2.1 1,500,000 80 Lost by 1 point in 40 games\\n\\nTable 2: TD-Gammon’s performance in\\n\\ngames against the top human professional players.\\n\\nA backgammon tournament involves playing a series of games for points until one player reaches a set target. TD-Gammon won none of these tournaments but came\\n\\nsufficiently close that it is now\\n\\nconsidered one of the best few players in the world.\\n\\nneural network as a function approximator for the value function\\n\\nBoard Position > Probability of victory for current player.\\n\\nTwo versions of the learning algorithm\\n\\nwere used. The first, which we will call Basic TD-\\n\\nGammon, used very little predefined knowledge of the game, and the representation of a board position was virtually a raw encoding, sufficiently powerful only to permit the neural network to distinguish between conceptually different positions. The second, TD-Gammon,\\n\\nwas provided with the same raw state crafted features of backgammon board\\n\\ninformation supplemented by a number of hand- positions. Providing hand-crafted features in this\\n\\nmanner is a good example of how inductive biases from human knowledge of the task can\\n\\nbe supplied to a learning algorithm. The training of both learning algorit\\n\\nwas achieved by constant self-play. No\\n\\ngreedily chose the move with the larges\\n\\nms required several months of computer time, and exploration strategy was used—the system always expected probability of victory. This naive explo-\\n\\nration strategy proved entirely adequate for this environment, which is perhaps surprising given the considerable work in the reinforcement-learning literature which has produced numerous counter-examples to show that greedy exploration can lead to poor learning per-\\n\\nformance. Backgammon, however, has is followed, every game is guaranteed information is obtained fairly frequent’\\n\\nwo important properties. Firstly, whatever policy o end in finite time, meaning that useful reward\\n\\ny. Secondly, the state transitions are sufficiently\\n\\nstochastic that independent of the policy, all states will occasionally be visited—a wrong initial value function has little danger of starving us from visiting a critical part of state space from which important information could be obtained.\\n\\nThe results (Table 2) of TD-Gammon are impressive. It has competed at the very top level of international human play. Basic TD-Gammon played respectably, but not at a\\n\\nprofessional standard.\\n\\n271\\n\\nFigure 11: Schaal and Atkeson’s devil-sticking robot. The tapered stick is hit alternately by each of the two hand sticks. The task is to keep the devil stick from falling for as many hits as possible. The robot has three motors indicated by torque vectors 71,72, 73.\\n\\nAlthough experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess (Thrun, 1995). It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains.\\n\\n8.2 Robotics and Control\\n\\nIn recent years there have been many robotics and control applications that have used reinforcement learning. Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway.\\n\\n1. Schaal and Atkeson (1994) constructed a two-armed robot, shown in Figure 11, tha earns to juggle a device known as a devil-stick. This is a complex non-linear contro ask involving a six-dimensional state space and less than 200 msecs per control deci- sion. After about 40 initial attempts the robot learns to keep juggling for hundreds o hits. A typical human learning the task requires an order of magnitude more practice o achieve proficiency at mere tens of hits.\\n\\nThe juggling robot learned a world model from experience, which was generalize o unvisited states by a function approximation scheme known as locally weighte regression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). Between each trial, a form of dynamic programming specific to linear control policies and locally linear ransitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design (Sage & White, 1977).\\n\\n272\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n2. Mahadevan and Connell (1991a) discuss a task in which a mobile robot pushes large\\n\\nearned.\\n\\n3. Mataric (1994) describes a robotics experiment with, from t\\n\\nical reinforcement learning, an unthinka\\n\\nnals called progress estimators were use This was achieved in a robust manner he estimators, but had the freedom to Secondly, control was decentralized. Eac.\\n\\nwithout explicit communication with the ot\\n\\nrofi\\n\\nquantized into a small number of discre\\n\\nof the Q-learned policies were almost as he job.\\n\\n4. Q-learning has been used in an elevator dispa’ problem, which has been implemented in simulation only at this stage, involved four elevators servicing ten floors. The objective was to minimize the average squared wait time for passengers, discounted into future time. The problem can be posed as a discrete Markov system, but there are 10?? states even in the most simplified version of he problem. Crites and Barto used neural ne provided an excellent comparison study of their Q-learning approach against the most popular and the most sophisticated elevator dispatching algorithms. The squared wait ime of their controller was approximately 7% (“Empty the System” heuristic with a receding horizon controller) and less than half he squared wait time of the controller most frequently used in real elevator systems.\\n\\n5. The final example concerns an application o authors of this survey to a packaging task from a food processing industry. The roblem involves filling containers with varia The product characteristics also vary with time, but can be sensed. Depending on he task, various constraints are placed on the container-filling procedure. Here are\\n\\nhree examples:\\n\\ngoo\\n\\nbly high dimensional\\n\\nin which the robots\\n\\nfrom the induc\\n\\nive bias\\n\\nh robot learned its own policy\\n\\ners. Thirdly, s\\n\\nas a simple hand-crafte\\n\\nching task (Crites & Bar\\n\\nboxes for extended periods of time. Box-pushing is a well-known difficult robotics roblem, characterized by immense uncertainty in the results of actions. Q-learning was used in conjunction with some novel clustering techniques designed to enable a higher-dimensional input than a tabular approach would have permitted. The robot earned to perform competitively with the performance of a human-programmed so- ution. Another aspect of this work, mentioned in Section 6.3, was a breakdown of the monolithic task description into a set of lower level tasks to be\\n\\npre-programmed\\n\\ne viewpoint of theoret- state space, containing many dozens of degrees of freedom. Four mobile robots traveled within an enclo- sure collecting small disks and transporting them to a destination region. There were hree enhancements to the basic Q-learning algorithm. Firstly, pre-programmed sig- to break the monolithic task into subtasks.\\n\\nwere not forced to use\\n\\nhey provided. independently\\n\\nate space was brutally e states according to values o ber of pre-programmed boolean features of the underlying sensors. The performance\\n\\na small num-\\n\\ncontroller for\\n\\n0, 1996). The\\n\\nworks for function approximation and\\n\\ness than the best alternative algorithm\\n\\nreinforcement learning by one of the\\n\\nble numbers of non-identical products.\\n\\ne The mean weight of all containers produced by a shift must not be below the\\n\\nmanufacturer’s declared weight W.\\n\\n273\\n\\nKAELBLING, LI\\n\\nTTMAN, & Moore\\n\\ne The number of containers below the declared weight must be less than P%.\\n\\ne No containers may be produced below weight W’.\\n\\nSuch tasks are controlled by machinery which operates according to various setpoints. Conventional practice is that setpoints are chosen by human operators, but this choice\\n\\nis not easy as it is task constraints. The task was posed\\n\\nependent on the\\n\\nThe dependency is\\n\\ncurrent product characteristics and the current often difficult to model and highly non-linear.\\n\\nas a finite-horizon Markov decision task in which the state of the\\n\\nsystem is a function of the product characteristics, the amount of time remaining in\\n\\nthe production\\n\\nso far. The system\\n\\nregression was\\n\\ning was used to ma information was ob typically with wast. deployed successful\\n\\nSome interesting aspects of practical rein\\n\\nexamples. The mos\\n\\nnecessary to supplement plying extra knowledge comes a\\n\\nSup the system is subse\\n\\nthese, a knowledge-fr the finite lifetime of What forms did\\n\\nlinearity for the jugg\\n\\nshif was discretized i use intain an optimal\\n\\nage reduced by a y in several factor:\\n\\nstriking is that in al\\n\\nhe fundamental al\\n\\nuently less aw ee approach woul. he robots. his pre-programmed ing robot’s policy,\\n\\nno\\n\\nthe two mobile-robo\\n\\nthe @ values which assumed loca ionally used a manual dimensions and so required correspon\\n\\naddi\\n\\nsumption of local pie in the amoun\\n\\nT\\n\\nysis\\n\\nof lear e exploration s o judge were able to plora T strategies mir yet all prove Finally, it They\\n\\nwhere\\n\\nion.\\n\\nwere al\\n\\nThe\\n\\nearn well wit e packaging task use rors theoretically op adequate.\\n\\nis also worth considering the compu very different, which indicates that t various reinforcement learning algorithms do indeed hav juggler needed to make very fast decisions with had long periods (30 seconds and more) between each\\n\\nhe\\n\\nconsis\\n\\nexamples, while\\n\\nly €\\n\\ny discr ing\\n\\nand the mean was\\n\\nained. In simulate\\n\\na price: onomous.\\n\\nized state space. T\\n\\nage and percent below declared in the shift nto 200,000 discrete states and local weighted\\n\\nto learn and generalize a transition model. Prioritized sweep-\\n\\nvalue function as each new piece of transition experiments the savings were considerable, actor of ten. Since then the system has been\\n\\nies within the United States.\\n\\norcement learning come to light from these cases, to make a real system work it proved gorithm with extra pre-programmed knowledge. more human effort and insight is required and But it is also clear that for tasks such as have achieved worthwhile performance within\\n\\nknowledge take? It included an assumption of\\n\\na manual breaking up of the task into subtasks for\\n\\nbox-pusher also used a clustering technique for tent @ values. The four disk-collecting robots e packaging example had far fewer y weaker assumptions, but there, too, the as-\\n\\ncewise continui\\n\\nning\\n\\nyin t\\n\\nata required.\\n\\ntrategies are inter\\n\\nto profitably gree\\n\\nex te) imal (bu\\n\\ny\\n\\nesting too. T experiment. loration—alway: ptimism in t\\n\\ne transition model enabled massive reductions\\n\\ne juggler used careful statistical anal- However, both mobile robot applications 8 exploiting without deliberate ex- e face of uncertainty. None of these t computationally intractable) exploration, and of these experiments. utational demands of\\n\\national regimes iffering com ean array of differing applications. ow latency between each hit, but rial to consolidate the experiences\\n\\ne\\n\\ncollected on the previous trial and to perform the more aggressive computation necessary\\n\\nto produce a new reactive controller on the next trial. T\\n\\ne box-pushing robot was meant to\\n\\n274\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\noperate autonomously for hours and so had to make decisions with a uniform length control cycle. The cycle was sufficiently long for quite substantial computations beyond simple Q- learning backups. The four disk-collecting robots were particularly interesting. Each robot had a short life of less than 20 minutes (due to battery constraints) meaning that substantial number crunching was impractical, and any significant combinatorial search would have used a significant fraction of the robot’s learning lifetime. The packaging task had easy constraints. One decision was needed every few minutes. This provided opportunities for fully computing the optimal value function for the 200,000-state system between every control cycle, in addition to performing massive cross-validation-based optimization of the transition model being learned.\\n\\nA great deal of further work is currently in progress on practical implementations of reinforcement learning. The insights and task constraints that they produce will have an important effect on shaping the kind of algorithms that are developed in future.\\n\\n9. Conclusions\\n\\nThere are a variety of reinforcement-learning techniques that work effectively on a variety of small problems. But very few of these techniques scale well to larger problems. This is not because researchers have done a bad job of inventing learning techniques, but because it is very difficult to solve arbitrary problems in the general case. In order to solve highly complex problems, we must give up tabula rasa learning techniques and begin to incorporate bias that will give leverage to the learning process.\\n\\nThe necessary bias can come in a variety of forms, including the following:\\n\\nshaping: The technique of shaping is used in training animals (Hilgard & Bower, 1975); a teacher presents very simple problems to solve first, then gradually exposes the learner to more complex problems. Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).\\n\\nlocal reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local. In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly (Mataric, 1994).\\n\\nimitation: An agent can learn by “watching” another agent perform the task (Lin, 1991). For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel (Pomerleau, 1993).\\n\\nproblem decomposition: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very power- ful technique for biasing learning. Most interesting examples of robotic reinforcement learning employ this technique to some extent (Connell & Mahadevan, 1993).\\n\\nreflexes: One thing that keeps agents that know nothing from learning anything is that they have a hard time even finding the interesting parts of the space; they wander\\n\\n275\\n\\nlea:\\n\\nKAELBLING, LITTMAN, & Moore\\n\\naround at random never getting near the goal, or they are always “killed” immediately. These problems can be ameliorated by programming a set of “reflexes” that cause the agent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto, Grupen, & Connolly, 1994). These reflexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn. Recent work by Millan (1996) explores the use of reflexes to make robot learning safer and more efficient.\\n\\nWith appropriate biases, supplied by human programmers or teachers, complex reinforcement- rning problems will eventually be solvable. There is still much work to be done and many\\n\\ninteresting questions remaining for learning techniques and especially regarding methods for approximating, decomposing, and incorporating bias into problems.\\n\\nAcknowledgements\\n\\nT to\\n\\nanks to Marco Dorigo and three anonymous reviewers for comments that have helped improve this paper. Also thanks to our many colleagues in the reinforcement-learning\\n\\ncommunity who have done this work and explained it to us.\\n\\n93\\n\\nin\\n\\nLeslie Pack Kaelbling was supported in part by NSF grants IRI-9453383 and IRI 2395. Michael Littman was supported in part by Bellcore. Andrew Moore was supported art by an NSF Research Initiation Award and by 3M Corporation.\\n\\nReferences\\n\\nAc\\n\\nkley, D. H., & Littman, M. L. (1990). Generalization and scaling in reinforcement learn- ing. In Touretzky, D. S. (Ed.), Advances in Neural Information Processing Systems 2, pp. 550-557 San Mateo, CA. Morgan Kaufmann.\\n\\nAlbus, J. S. (1975). A new approach to manipulator control: Cerebellar model articulation\\n\\ncontroller (emac). Journal of Dynamic Systems, Measurement and Control, 97, 220- 227.\\n\\nAlbus, J. S. (1981). Brains, Behavior, and Robotics. BYTE Books, Subsidiary of McGraw-\\n\\nHill, Peterborough, New Hampshire.\\n\\nAnderson, C. W. (1986). Learning and Problem Solving with Multilayer Connectionist\\n\\nSystems. Ph.D. thesis, University of Massachusetts, Amherst, MA.\\n\\nAshar, R. R. (1994). Hierarchical learning in stochastic domains. Master’s thesis, Brown\\n\\nUniversity, Providence, Rhode Island.\\n\\nBaird, L. (1995). Residual algorithms: Reinforcement learning with function approxima-\\n\\ntion. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 30-37 San Francisco, CA. Morgan Kaufmann.\\n\\nBaird, L. C., & Klopf, A. H. (1993). Reinforcement learning with high-dimensional, con-\\n\\ntinuous actions. Tech. rep. WL-TR-93-1147, Wright-Patterson Air Force Base Ohio: Wright Laboratory.\\n\\n276\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nBarto, A. G., Bradtke, 8. J., & Singh, 8. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1), 81-138.\\n\\nBarto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5), 834-846.\\n\\nBellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.\\n\\nBerenji, H. R. (1991). Artificial neural networks and approximate reasoning for intelligent control in space. In American Control Conference, pp. 1075-1080.\\n\\nBerry, D. A., & Fristedt, B. (1985). Bandit Problems: Sequential Allocation of Experiments. Chapman and Hall, London, UK.\\n\\nBertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic Models. Prentice-Hall, Englewood Cliffs, NJ.\\n\\nBertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena Scientific, Belmont, Massachusetts. Volumes 1 and 2.\\n\\nBertsekas, D. P., & Castafion, D. A. (1989). Adaptive aggregation for infinite horizon dynamic programming. IEEE Transactions on Automatic Control, 34 (6), 589-598.\\n\\nBertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel and Distributed Computation: Numer- ical Methods. Prentice-Hall, Englewood Cliffs, NJ.\\n\\nBox, G. E. P., & Draper, N. R. (1987). Empirical Model-Building and Response Surfaces. Wiley.\\n\\nBoyan, J. A., & Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximating the value function. In Tesauro, G., Touretzky, D. S., & Leen, T. K. (Eds.), Advances in Neural Information Processing Systems 7 Cambridge, MA. The MIT Press.\\n\\nBurghes, D., & Graham, A. (1980). Introduction to Control Theory including Optimal Control. Ellis Horwood.\\n\\nCassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially observable stochastic domains. In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, WA.\\n\\nChapman, D., & Kaelbling, L. P. (1991). Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. In Proceedings of the Interna- tional Joint Conference on Artificial Intelligence Sydney, Australia.\\n\\nChrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. In Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 183-188 San Jose, CA. AAAT Press.\\n\\n277\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nChrisman, L., & Littman, M. (1993). Hidden state and short-term memory.. Presentation at Reinforcement Learning Workshop, Machine Learning Conference.\\n\\nCichosz, P., & Mulawka, J. J. (1995). Fast and efficient reinforcement learning with trun- cated temporal differences. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 99-107 San Francisco, CA. Morgan Kaufmann.\\n\\nCleveland, W. S., & Delvin, S. J. (1988). Locally weighted regression: An approach to regression analysis by local fitting. Journal of the American Statistical Association, 83(403), 596-610.\\n\\nCliff, D., & Ross, S. (1994). Adding temporary memory to ZCS. Adaptive Behavior, 3(2), 101-150.\\n\\nCondon, A. (1992). The complexity of stochastic games. Information and Computation, 96 (2), 203-224.\\n\\nConnell, J., & Mahadevan, S. (1993). Rapid task learning for real robots. In Robot Learning. Kluwer Academic Publishers.\\n\\nCrites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcement learning. In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural Information Processing Systems 8.\\n\\nDayan, P. (1992). The convergence of TD(A) for general \\\\. Machine Learning, 8(3), 341- 362.\\n\\nDayan, P., & Hinton, G. E. (1993). Feudal reinforcement learning. In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), Advances in Neural Information Processing Systems 5 San Mateo, CA. Morgan Kaufmann.\\n\\nDayan, P., & Sejnowski, T. J. (1994). TD(A) converges with probability 1. Machine Learn- ing, 14(3).\\n\\nDean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning with deadlines in stochastic domains. In Proceedings of the Eleventh National Conference on Artificial Intelligence Washington, DC.\\n\\nD’Epenoux, F. (1963). A probabilistic production and inventory problem. Management Science, 10, 98-108.\\n\\nDerman, C. (1970). Finite State Markovian Decision Processes. Academic Press, New York.\\n\\nDorigo, M., & Bersini, H. (1994). A comparison of q-learning and classifier systems. In From Animals to Animats: Proceedings of the Third International Conference on the Simulation of Adaptive Behamor Brighton, UK.\\n\\nDorigo, M., & Colombetti, M. (1994). Robot shaping: Developing autonomous agents through learning. Arteficial Intelligence, 71(2), 321-370.\\n\\n278\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nDorigo, M. (1995). Alecsys and the AutonoMouse: Learning to control a real robot by distributed classifier systems. Machine Learning, 19.\\n\\nFiechter, C.-N. (1994). Efficient reinforcement learning. In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, pp. 88-97. Association of Computing Machinery.\\n\\nGittins, J. C. (1989). Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. Wiley, Chichester, NY.\\n\\nGoldberg, D. (1989). Genetic algorithms in search, optimization, and machine learning. Addison-Wesley, MA.\\n\\nGordon, G. J. (1995). Stable function approximation in dynamic programming. In Priedi- tis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 261-268 San Francisco, CA. Morgan Kaufmann.\\n\\nGullapalli, V. (1990). A stochastic reinforcement learning algorithm for learning real-valued functions. Neural Networks, 3, 671-692.\\n\\nGullapalli, V. (1992). Reinforcement learning and its application to control. Ph.D. thesis, University of Massachusetts, Amherst, MA.\\n\\nHilgard, E. R., & Bower, G. H. (1975). Theories of Learning (fourth edition). Prentice-Hall, Englewood Cliffs, NJ.\\n\\nHoffman, A. J., & Karp, R. M. (1966). On nonterminating stochastic games. Management Science, 12, 359-370.\\n\\nHolland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor, MI.\\n\\nHoward, R. A. (1960). Dynamic Programming and Markov Processes. The MIT Press, Cambridge, MA.\\n\\nJaakkola, T., Jordan, M.1., & Singh, S. P. (1994). On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6(6).\\n\\nJaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Monte-carlo reinforcement learning in non-Markovian decision problems. In Tesauro, G., Touretzky, D. S., & Leen, T. kK. (Eds.), Advances in Neural Information Processing Systems 7 Cambridge, MA. The MIT Press.\\n\\nKaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Preliminary results. In Proceedings of the Tenth International Conference on Machine Learning Amherst, MA. Morgan Kaufmann.\\n\\nKaelbling, L. P. (1993b). Learning in Embedded Systems. The MIT Press, Cambridge, MA.\\n\\nKaelbling, L. P. (1994a). Associative reinforcement learning: A generate and test algorithm. Machine Learning, 15 (3).\\n\\n279\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nKaelbling, L. P. (1994b). Associative reinforcement learning: Functions in k-DNF. Machine Learning, 15(3).\\n\\nKirman, J. (1994). Predicting Real-Time Planner Performance by Domain Characterization. Ph.D. thesis, Department of Computer Science, Brown University.\\n\\nKoenig, S., & Simmons, R. G. (1993). Complexity analysis of real-time reinforcement learning. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pp. 99-105 Menlo Park, California. AAAI Press/MIT Press.\\n\\nKumar, P. R., & Varaiya, P. P. (1986). Stochastic Systems: Estimation, Identification, and Adaptive Control. Prentice Hall, Englewood Cliffs, New Jersey.\\n\\nLee, C. C. (1991). A self learning rule-based controller employing approximate reasoning and neural net concepts. International Journal of Intelligent Systems, 6(1), 71-93.\\n\\nLin, L.-J. (1991). Programming robots using reinforcement learning and teaching. In Proceedings of the Ninth National Conference on Artificial Intelligence.\\n\\nLin, L.-J. (1993a). Hierachical learning of robot skills by reinforcement. In Proceedings of the International Conference on Neural Networks.\\n\\nLin, L.-J. (1993b). Reinforcement Learning for Robots Using Neural Networks. Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\\n\\nLin, L.-J., & Mitchell, T. M. (1992). Memory approaches to reinforcement learning in non- Markovian domains. Tech. rep. CMU-CS-92-138, Carnegie Mellon University, School of Computer Science.\\n\\nLittman, M. L. (1994a). Markov games as a framework for multi-agent reinforcement learn- ing. In Proceedings of the Eleventh International Conference on Machine Learning, pp. 157-163 San Francisco, CA. Morgan Kaufmann.\\n\\nLittman, M. L. (1994b). Memoryless policies: Theoretical limitations and practical results. In Cliff, D., Husbands, P., Meyer, J-A., & Wilson, S. W. (Eds.), From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior Cambridge, MA. The MIT Press.\\n\\nLittman, M. L., Cassandra, A., & Kaelbling, L. P. (1995a). Learning policies for partially observable environments: Scaling up. In Prieditis, A., & Russell, S. (Eds.), Proceed- ings of the Twelfth International Conference on Machine Learning, pp. 362-370 San Francisco, CA. Morgan Kaufmann.\\n\\nLittman, M. L., Dean, T. L., & Kaelbling, L. P. (1995b). On the complexity of solving Markov decision problems. In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI-95) Montreal, Québec, Canada.\\n\\nLovejoy, W. S. (1991). A survey of algorithmic methods for partially observable Markov decision processes. Annals of Operations Research, 28, 47-66.\\n\\n280\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nMaes, P., & Brooks, R. A. (1990). Learning to coordinate behaviors. In Proceedings Eighth National Conference on Artificial Intelligence, pp. 796-802. Morgan Kaufmann.\\n\\nMahadevan, S. (1994). To discount or not to discount in reinforcement learning: A case study comparing R learning and Q learning. In Proceedings of the Eleventh Inter- national Conference on Machine Learning, pp. 164-172 San Francisco, CA. Morgan Kaufmann.\\n\\nMahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empirical results. Machine Learning, 22(1).\\n\\nMahadevan, S., & Connell, J. (1991a). Automatic programming of behavior-based robots using reinforcement learning. In Proceedings of the Ninth National Conference on Artificial Intelligence Anaheim, CA.\\n\\nMahadevan, S., & Connell, J. (1991b). Scaling reinforcement learning to robotics by ex- ploiting the subsumption architecture. In Proceedings of the Eighth International Workshop on Machine Learning, pp. 328-332.\\n\\nMataric, M. J. (1994). Reward functions for accelerated learning. In Cohen, W. W., & Hirsh, H. (Eds.), Proceedings of the Eleventh International Conference on Machine Learning. Morgan Kaufmann.\\n\\nMcCallum, A. K. (1995). Reinforcement Learning with Selective Perception and Hidden State. Ph.D. thesis, Department of Computer Science, University of Rochester.\\n\\nMcCallum, R. A. (1993). Overcoming incomplete perception with utile distinction memory. In Proceedings of the Tenth International Conference on Machine Learning, pp. 190- 196 Amherst, Massachusetts. Morgan Kaufmann.\\n\\nMcCallum, R. A. (1995). Instance-based utile distinctions for reinforcement learning with hidden state. In Proceedings of the Twelfth International Conference Machine Learn- ing, pp. 387-395 San Francisco, CA. Morgan Kaufmann.\\n\\nMeeden, L., McGraw, G., & Blank, D. (1993). Emergent control and planning in an au- tonomous vehicle. In Touretsky, D. (Ed.), Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, pp. 735-740. Lawerence Erlbaum Associates, Hills- dale, NJ.\\n\\nMillan, J. d. R. (1996). Rapid, safe, and incremental learning of navigation strategies. [EEE Transactions on Systems, Man, and Cybernetics, 26 (3).\\n\\nMonahan, G. E. (1982). A survey of partially observable Markov decision processes: Theory, models, and algorithms. Management Science, 28, 1-16.\\n\\nMoore, A. W. (1991). Variable resolution dynamic programming: Efficiently learning ac- tion maps in multivariate real-valued spaces. In Proc. Eighth International Machine Learning Workshop.\\n\\n281\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nMoore, A. W. (1994). The parti-game algorithm for variable resolution reinforcement learn- ing in multidimensional state-spaces. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural Information Processing Systems 6, pp. 711-718 San Mateo, CA. Morgan Kaufmann.\\n\\nMoore, A. W., & Atkeson, C. G. (1992). An investigation of memory-based function ap- proximators for learning control. Tech. rep., MIT Artifical Intelligence Laboratory, Cambridge, MA.\\n\\nMoore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less data and less real time. Machine Learning, 13.\\n\\nMoore, A. W., Atkeson, C. G., & Schaal, S. (1995). Memory-based learning for control. Tech. rep. CMU-RI-TR-95-18, CMU Robotics Institute.\\n\\nNarendra, K., & Thathachar, M. A. L. (1989). Learning Automata: An Introduction. Prentice-Hall, Englewood Cliffs, NJ.\\n\\nNarendra, K. §., & Thathachar, M. A. L. (1974). Learning automata—a survey. IEEE Transactions on Systems, Man, and Cybernetics, 4 (4), 323-334.\\n\\nPeng, J., & Williams, R. J. (1993). Efficient learning and planning within the Dyna frame- work. Adaptive Behavior, 1(4), 437-454.\\n\\nPeng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. In Proceedings of the Eleventh International Conference on Machine Learning, pp. 226-232 San Francisco, CA. Morgan Kaufmann.\\n\\nPomerleau, D. A. (1993). Neural network perception for mobile robot guidance. Kluwer Academic Publishing.\\n\\nPuterman, M. L. (1994). Markov Decision Processes—Discrete Stochastic Dynamic Pro- gramming. John Wiley & Sons, Inc., New York, NY.\\n\\nPuterman, M. L., & Shin, M. C. (1978). Modified policy iteration algorithms for discounted Markov decision processes. Management Science, 24, 1127-1137.\\n\\nRing, M. B. (1994). Continual Learning in Reinforcement Environments. Ph.D. thesis, University of Texas at Austin, Austin, Texas.\\n\\nRiide, U. (1993). Mathematical and computational techniques for multilevel adaptive meth- ods. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania.\\n\\nRumelhart, D. E., & McClelland, J. L. (Eds.). (1986). Parallel Distributed Processing: Explorations in the microstructures of cognition. Volume 1: Foundations. The MIT Press, Cambridge, MA.\\n\\nRummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems. Tech. rep. CUED/F-INFENG/TR166, Cambridge University.\\n\\n282\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nRust, J. (1996). Numerical dynamic programming in economics. In Handbook of Computa- tional Economics. Elsevier, North Holland.\\n\\nSage, A. P., & White, C. C. (1977). Optimum Systems Control. Prentice Hall.\\n\\nSalganicoff, M., & Ungar, L. H. (1995). Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 480-487 San Francisco, CA. Morgan Kaufmann.\\n\\nSamuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal of Research and Development, 8, 211-229. Reprinted in E. A. Feigenbaum and J. Feldman, editors, Computers and Thought, McGraw-Hill, New York 1963.\\n\\nSchaal, S., & Atkeson, C. (1994). Robot juggling: An implementation of memory-based learning. Control Systems Magazine, 14.\\n\\nSchmidhuber, J. (1996). A general method for multi-agent learning and incremental self- improvement in unrestricted environments. In Yao, X. (Ed.), Evolutionary Computa- tion: Theory and Applications. Scientific Publ. Co., Singapore.\\n\\nSchmidhuber, J. H. (1991a). Curious model-building control systems. In Proc. International Joint Conference on Neural Networks, Singapore, Vol. 2, pp. 1458-1463. IEEE.\\n\\nSchmidhuber, J. H. (1991b). Reinforcement learning in Markovian and non-Markovian environments. In Lippman, D. S., Moody, J. E., & Touretzky, D. S. (Eds.), Advances in Neural Information Processing Systems 3, pp. 500-506 San Mateo, CA. Morgan Kaufmann.\\n\\nSchraudolph, N. N., Dayan, P., & Sejnowski, T. J. (1994). Temporal difference learning of position evaluation in the game of Go. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural Information Processing Systems 6, pp. 817-824 San Mateo, CA. Morgan Kaufmann.\\n\\nSchrijver, A. (1986). Theory of Linear and Integer Programming. Wiley-Interscience, New York, NY.\\n\\nSchwartz, A. (1993). A reinforcement learning method for maximizing undiscounted re- wards. In Proceedings of the Tenth International Conference on Machine Learning, pp. 298-305 Amherst, Massachusetts. Morgan Kaufmann.\\n\\nSingh, S. P., Barto, A. G., Grupen, R., & Connolly, C. (1994). Robust reinforcement learning in motion planning. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural Information Processing Systems 6, pp. 655-662 San Mateo, CA. Morgan Kaufmann.\\n\\nSingh, 5. P., & Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces. Machine Learning, 22(1).\\n\\n283\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nSingh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract models. In Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 202-207 San Jose, CA. AAAT Press.\\n\\nSingh, S. P. (1992b). Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3), 323-340.\\n\\nSingh, S. P. (1993). Learning to Solve Markovian Decision Processes. Ph.D. thesis, Depart- ment of Computer Science, University of Massachusetts. Also, CMPSCI Technical Report 93-77.\\n\\nStengel, R. F. (1986). Stochastic Optimal Control. John Wiley and Sons.\\n\\nSutton, R. S. (1996). Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. In Touretzky, D., Mozer, M., Hasselmo, M. (Eds.), Neural Information Processing Systems 8. &\\n\\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis, University of Massachusetts, Amherst, MA.\\n\\nSutton, R. S. (1988). Learning to predict by the method of temporal differences. Machine Learning, 3(1), 9-44.\\n\\nSutton, R. 8. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the Seventh International Conference on Machine Learning Austin, TX. Morgan Kaufmann.\\n\\nSutton, R. S$. (1991). Planning by incremental dynamic programming. In Proceedings of the Eighth International Workshop on Machine Learning, pp. 353-357. Morgan Kaufmann.\\n\\nTesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8, 257-277.\\n\\nTesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master- level play. Neural Computation, 6(2), 215-219.\\n\\nTesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of the ACM, 38(3), 58-67.\\n\\nTham, C.-K., Prager, R. W. (1994). A modular q-learning architecture for manipula- tor task decomposition. In Proceedings of the Eleventh International Conference on Machine Learning San Francisco, CA. Morgan Kaufmann. &\\n\\nThrun, S. (1995). Learning to play the game of chess. In Tesauro, G., Touretzky, D. S., Leen, T. K. (Eds.), Advances in Neural Information Processing Systems 7 Cambridge, MA. The MIT Press.\\n\\n284\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nThrun, S., & Schwartz, A. (1993). Issues in using function approximation for reinforcement learning. In Mozer, M., Smolensky, P., Touretzky, D., Elman, J., & Weigend, A. (Eds.), Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum.\\n\\nThrun, S. B. (1992). The role of exploration in learning control. In White, D. A., & Sofge, D. A. (Eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. Van Nostrand Reinhold, New York, NY.\\n\\nTsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine Learning, 16(3).\\n\\nTsitsiklis, J. N., & Van Roy, B. (1996). Feature-based methods for large scale dynamic programming. Machine Learning, 22(1).\\n\\nValiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11), 1134-1142.\\n\\nWatkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, King’s College, Cambridge, UK.\\n\\nWatkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8(3), 279-292.\\n\\nWhitehead, S. D. (1991). Complexity and cooperation in Q-learning. In Proceedings of the Eighth International Workshop on Machine Learning Evanston, IL. Morgan Kauf- mann.\\n\\nWilliams, R. J. (1987). A class of gradient-estimating algorithms for reinforcement learning in neural networks. In Proceedings of the IEEE First International Conference on Neural Networks San Diego, CA.\\n\\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3), 229-256.\\n\\nWilliams, R. J., & Baird, III, L. C. (1993a). Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems. Tech. rep. NU-CCS-93-11, Northeastern University, College of Computer Science, Boston, MA.\\n\\nWilliams, R. J., & Baird, III, L. C. (1993b). Tight performance bounds on greedy policies based on imperfect value functions. Tech. rep. NU-CCS-93-14, Northeastern Univer- sity, College of Computer Science, Boston, MA.\\n\\nWilson, 8. (1995). Classifier fitness based on accuracy. Evolutionary Computation, 3(2), 147-173.\\n\\nZhang, W., & Dietterich, T. G. (1995). A reinforcement learning approach to job-shop scheduling. In Proceedings of the International Joint Conference on Artificial Intel- lience.\\n\\n285'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-07T17:54:39.807036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "blog_docs = []\n",
    "for url in blog_urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    blog_docs.append(loader.load()[0].page_content)\n",
    "\n",
    "pdf_docs = []\n",
    "for url in pdf_urls:\n",
    "    loader = OnlinePDFLoader(url)\n",
    "    pdf_docs.append(loader.load())"
   ],
   "id": "f174dd71f2c137ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:34:24.484110Z",
     "start_time": "2025-09-07T17:34:24.360946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess medium blogs\n",
    "def clean_medium_content(content: str) -> str:\n",
    "    # Remove common Medium UI/boilerplate\n",
    "    lines = content.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    skip_patterns = [\n",
    "        r'Sitemap|Open in app|Sign up|Sign in|Medium Logo|Write|Listen|Share',\n",
    "        r'Press enter or click to view image in full size',\n",
    "        r'followers|following|Responses \\(\\d+\\)|See all responses|Help|Status|About|Careers|Press|Blog|Privacy|Rules|Terms|Text to speech',\n",
    "        r'Written by .*?Medium',  # Author footer\n",
    "        r'^\\s*$'  # Empty lines\n",
    "    ]\n",
    "    for line in lines:\n",
    "        for pattern in skip_patterns:\n",
    "            line = re.sub(pattern, '', line, flags=re.IGNORECASE)\n",
    "        # Then apply the length check and append if it passes\n",
    "        if len(line.strip()) > 20 and not re.match(r'^--?\\d+$', line.strip()):\n",
    "            cleaned_lines.append(line.strip())\n",
    "\n",
    "    cleaned = ' '.join(cleaned_lines)\n",
    "    # Remove extra spaces/multiple newlines\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "cleaned_blog_docs = []\n",
    "for doc in blog_docs:\n",
    "    cleaned_blog_docs.append(clean_medium_content(doc))"
   ],
   "id": "15029b0c7affb9f3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blog_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     23\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cleaned.strip()\n\u001B[32m     25\u001B[39m cleaned_blog_docs = []\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m \u001B[43mblog_docs\u001B[49m:\n\u001B[32m     27\u001B[39m     cleaned_blog_docs.append(clean_medium_content(doc))\n",
      "\u001B[31mNameError\u001B[39m: name 'blog_docs' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "import re\n",
    "\n",
    "# Initialize loader with language specification (if supported)\n",
    "loader = OnlinePDFLoader(file_path=\"/tmp/tmp56jk68sq/tmp.pdf\", language=\"en\")\n",
    "\n",
    "# Load document\n",
    "documents = loader.load()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(doc):\n",
    "    text = doc.page_content\n",
    "\n",
    "    # Remove metadata (journal, authors, copyright)\n",
    "    text = re.sub(r\"Journal of Artificial Intelligence Research.*?\\n\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"Leslie Pack Kaelbling.*?(?=\\nAbstract)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"©1996 AI Access Foundation.*?\\n\", \"\", text)\n",
    "\n",
    "    # Remove references section\n",
    "    text = re.sub(r\"References\\n.*\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove figure and table captions\n",
    "    text = re.sub(r\"Figure \\d+:.*?\\n\", \"\", text)\n",
    "    text = re.sub(r\"Table \\d+:.*?\\n\", \"\", text)\n",
    "\n",
    "    # Remove inline citations\n",
    "    text = re.sub(r\"\\(\\w+ et al., \\d{4}\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\(\\w+, \\d{4}\\)\", \"\", text)\n",
    "\n",
    "    # Remove footnotes\n",
    "    text = re.sub(r\"\\d+\\.\\s.*?\\n\", \"\", text)\n",
    "\n",
    "    # Normalize special characters and line breaks\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "cleaned_text = preprocess_text(documents[0])\n",
    "\n",
    "# Update document content\n",
    "documents[0].page_content = cleaned_text\n",
    "\n",
    "# Proceed with downstream tasks"
   ],
   "id": "4b0a64021d07fe61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step: Ingesting data from Medium blogs and PDF papers on Reinforcement Learning\n",
    "# Prerequisites (same as before):\n",
    "# - pip install langchain langchain-community beautifulsoup4 requests\n",
    "# - For PDFs: pip install unstructured  # Required for OnlinePDFLoader\n",
    "# - Note: OnlinePDFLoader fetches and parses online PDFs. If issues, download PDFs locally and use PyMuPDFLoader (pip install pymupdf).\n",
    "# Import bs4 for SoupStrainer\n",
    "import bs4\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader, OnlinePDFLoader\n",
    "\n",
    "# List of Medium blog URLs (same as before)\n",
    "blog_urls = [\n",
    "    \"https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1\",\n",
    "    \"https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92\",\n",
    "    \"https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c\",\n",
    "    \"https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e\"\n",
    "]\n",
    "\n",
    "# List of PDF paper URLs (same as before)\n",
    "pdf_urls = [\n",
    "    \"https://arxiv.org/pdf/cs/9605103.pdf\",  # Reinforcement Learning: A Survey\n",
    "    \"https://arxiv.org/pdf/2312.14925.pdf\",   # A Survey of Reinforcement Learning from Human Feedback\n",
    "    \"https://arxiv.org/pdf/2308.14328.pdf\",   # Reinforcement Learning for Generative AI: A Survey\n",
    "    \"https://arxiv.org/pdf/2312.10256.pdf\"    # Multi-agent Reinforcement Learning: A Comprehensive Survey\n",
    "]\n",
    "\n",
    "# Load blog documents with Medium-specific parsing\n",
    "blog_docs = []\n",
    "for url in blog_urls:\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"pw-post-title\", \"pw-subtitle\", \"pw-post-body-paragraph\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    blog_docs.extend(loader.load())\n",
    "\n",
    "# Load PDF documents (unchanged)\n",
    "pdf_docs = []\n",
    "for url in pdf_urls:\n",
    "    loader = OnlinePDFLoader(url)\n",
    "    pdf_docs.extend(loader.load())\n",
    "\n",
    "# Combine all documents\n",
    "all_documents = blog_docs + pdf_docs\n",
    "\n",
    "# Print summary to verify\n",
    "print(f\"Loaded {len(all_documents)} documents.\")\n",
    "for doc in all_documents:\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}, Length: {len(doc.page_content)} chars\")"
   ],
   "id": "5a9cb8bb1a0bc5a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
