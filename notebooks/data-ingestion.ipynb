{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-08T20:52:37.138196Z",
     "start_time": "2025-09-08T20:52:37.134334Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:52:37.735608Z",
     "start_time": "2025-09-08T20:52:37.732634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:2081\""
   ],
   "id": "6fedcccc88926c04",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:09.724175Z",
     "start_time": "2025-09-07T17:54:09.074945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "# llm = init_chat_model(\"gemini-2.5-flash\", model_provider='google_genai')"
   ],
   "id": "3b2253d32bcd2b7f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:52:39.954446Z",
     "start_time": "2025-09-08T20:52:39.484637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=API_KEY\n",
    ")"
   ],
   "id": "10387477855c27ee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T17:54:14.546730Z",
     "start_time": "2025-09-07T17:54:13.032823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "llm.invoke([HumanMessage('hello there')])"
   ],
   "id": "6b097e4b7ba5bfbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--33e5f36a-92e7-472f-9094-8c54e88a2741-0', usage_metadata={'input_tokens': 3, 'output_tokens': 35, 'total_tokens': 38, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:52:57.505793Z",
     "start_time": "2025-09-08T20:52:48.381813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ],
   "id": "5d514f9636606e17",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parichehr/PycharmProjects/NLP/rag-reinforcement-learning/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/parichehr/PycharmProjects/NLP/rag-reinforcement-learning/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:53:03.121915Z",
     "start_time": "2025-09-08T20:53:01.557408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "vector_store = Chroma(\n",
    "    collection_name='rag_rl',\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory='../storage'\n",
    ")"
   ],
   "id": "8032ecbe676e826a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:53:13.848528Z",
     "start_time": "2025-09-08T20:53:13.845467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, OnlinePDFLoader\n",
    "import bs4\n",
    "\n",
    "blog_urls = [\n",
    "    \"https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1\",\n",
    "    \"https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92\",\n",
    "    \"https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c\",\n",
    "    \"https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e\"\n",
    "]\n",
    "\n",
    "pdf_urls = [\n",
    "    \"https://arxiv.org/pdf/cs/9605103.pdf\",  # Reinforcement Learning: A Survey\n",
    "    \"https://arxiv.org/pdf/2312.14925.pdf\",   # A Survey of Reinforcement Learning from Human Feedback\n",
    "    \"https://arxiv.org/pdf/2308.14328.pdf\",   # Reinforcement Learning for Generative AI: A Survey\n",
    "    \"https://arxiv.org/pdf/2312.10256.pdf\"    # Multi-agent Reinforcement Learning: A Comprehensive Survey\n",
    "]"
   ],
   "id": "b63a2d54a238620e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:29:39.314310Z",
     "start_time": "2025-09-08T20:29:37.910857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(blog_urls[0],),\n",
    ")\n",
    "loader.load()[0].page_content"
   ],
   "id": "4f3c410ad3d4d129",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inReinforcement Learning: An introduction (Part 1/4)Cédric Vandelaer10 min read·Aug 20, 2022--2ListenShareHi and welcome to the first part of a series on Reinforcement Learning.Press enter or click to view image in full sizeIf you somehow ended up here without having heard of Reinforcement Learning (RL) before, then let me summarize it as follows: “RL is a general framework for training an artificial intelligence model to solve a certain task or goal” … or in layman’s terms, we make AI do cool things!The goal of this blog series is to learn about RL and simultaneously explore some of the more recent research later on. We will start from the very basics and work our way towards more advanced topics. Even if you have almost no prior programming and/or mathematics knowledge, you should be able to follow along pretty smoothly.The first mini-series will be split into four parts:Part 1: What is Reinforcement learning?Part 2: RL terminology and formal conceptsPart 3: The REINFORCE algorithmPart 4: Implementing the REINFORCE algorithmAt the same time, this mini-series will be the introduction to future posts with increasing complexity. Feel free to skip to the next part if you are already familiar with the content.Note: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.That sounds cool! … but what can I do with RL?Reinforcement learning is a framework to learn any task. In theory, RL can solve any problem that is phrased as a Markov Decision Process. We will explain what that means later on. For now, let’s have a look at some successful applications. If you enjoy these examples, please be sure to also check out the work from the original authors.GeneralOne of the videos I love showing is the hide and seek video from OpenAI. The video is a nice example of how RL can help us with finding novel solutions to problems, without explicitly programming tactics or solution methods.GamesOne of the early successes of Deep RL (which is combining RL with neural networks), was the ability to learn how to play Atari games, straight from pixels. Later on, researchers took it upon themselves to not only evaluate RL on the (relatively) simple Atari games, but rather evaluate it on the hardest competitive games out there. The assumption here is, that if RL can solve these complex games, it can also generalize to challenging real-world settings. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.RoboticsSolving tasks in simulations and video games is one thing, but what about real life? Another popular field where RL is often applied (or at least holds great promise), is robotics. Robotics are significantly harder than simulations for various reasons. Think for example about the time it takes to repeatedly make a robot try out a certain action. Or think about the safety requirements involved for robotics. In the example below, you can see how the ANYmal robot from the Robotics System Lab in Zürich learned to recover from a fall.Real world examplesRL can be applied to many other domains than the ones I just mentioned. Advertising, finance, healthcare, … just to name a few. As a final example, I present a goal-oriented chatbot, trained to negotiate about sales (source: https://siddharthverma314.github.io/research/chai-acl-2022/ ).Press enter or click to view image in full sizeRL: The basicsA description of the RL framework is as follows: We have an agent that tries to solve a task in a certain environment. The concept of agent should be taken very broadly here, an agent can be a robot, a chatbot, a virtual character, etc. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in.The RL problem is trying to maximize the cumulative reward the agent gets over time.Press enter or click to view image in full sizeImagine our agent is a monkey and the task we want the monkey to solve, is to pick up as many bananas as possible. At every timestep, the monkey needs to decide to take an action. The actions could be to step towards the tree, grab something, climb, … Perhaps the reward at every timestep can be defined as the number of bananas the monkey got at that timestep. After every action, the monkey will also be in a new state. Maybe we define the state of the monkey as its position in the world. So when the monkey takes a step, the state at the next timestep would be the coordinates of the monkey at the next timestep. We are now searching for the optimal behavior, the best sequence of actions the monkey can take, to maximize the cumulative number of bananas it will get.How does RL fit in the bigger picture?You might look at this framework and think: “Hey, isn’t that exactly what people are already studying in the field of …?”. And in fact, you might be right.In other domains like engineering or mathematics, people have often been studying the same problems with different names and methods. Or in fields like neuroscience and psychology, some similarities can be found in the way our brain “rewards” us by releasing dopamine.The likely reason for this intersection of domains, is that reinforcement learning is the study of a fundamental problem. It is essentially the science of decision taking. In these series, we will be looking at it from the umbrella of computer science and machine learning.Press enter or click to view image in full sizeThis general applicability is also what makes RL so interesting to me personally. RL is one of the potential technologies that could get us closer to general AI: an AI system that can solve any task, in contrast to a narrow set of tasks. There are also other technologies (e.g. big language models, graph neural networks, …) that are making some strides in this regard, but the problem statement of RL in particular seems to be the most ambitious.Another way of situating RL, is by looking at how it compares to other learning paradigms. Within the field of machine learning, people often distinguish between Supervised learning, Unsupervised learning and Reinforcement learning.Press enter or click to view image in full sizeWhen it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to. Our AI model is successful when it correctly predicts a label y given an (unseen) sample x.As an example, say we have a dataset consisting of cat and dog images. Each sample (image) has a label, stating whether the image contains a “dog” or a “cat”. The goal of our supervised model, is now to learn how to map a dog-image to the label “dog” and a cat-image to the label “cat”. After it has learned this mapping, the hope is that this AI model can repeat this process for new images that it has not seen before.In an Unsupervised learning context, we no longer provide any labels Y. So the task for the AI system now becomes learning some general statistics about the dataset. We could for example give an AI the task to generate a new sample, similar to the ones seen in the dataset. In this case the model would be considered successful if it manages to correctly learn the interesting characteristics of a dataset.Reinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in. The “dataset” we need to consider here, are the actions our agent took and the accumulated rewards it got by taking those actions. An added difficulty here is that our dataset is non-static. Say that our agent acts in a certain way, we can then collect some data of the actions our agent took and we can try to optimize these (e.g. do more of the actions that led to a successful result). But as a result of this optimization, we have now changed the behavior of this agent, and thus we will need to collect new data to see how well our agent fares now.If RL is so great, then why isn’t everyone using RL?After reading all this, you might be wondering why people aren’t using RL to solve all imaginable problems. The truth is that even though the field has made a lot of advancements in the last few years, there are still a few fundamental problems to be solved. Progress is being made all the time, but to give you an idea of what you might encounter, I’ll list a few common ones.Sample (in-)efficiencyIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task. In this sense, RL is very inefficient compared to humans, for example, it doesn’t take a human dozens of hours to learn how to play an Atari game.This sample-efficiency can in part be explained by the fact that humans can leverage a lot of their previous knowledge (priors) when they encounter a new task. A human can for example reuse some of the knowledge and skills of previous games and/or concepts they already acquired from other experiences throughout their life. An RL-agent in contrast, starts the learning process without any assumptions.Another thing to mention is that leveraging knowledge from previous tasks is also an active research topic. I’ll just put one example (out of many) here to give you an idea.Deepmind GatoGoogle Jump-Start RLThe exploration-exploitation trade-offWhile the previous problem sounds more like an engineering effort (it’s not), the exploration-exploitation trade-off seems more fundamental. Whenever we train an RL-agent, the agent will need some time to explore, it needs to take some actions that it hasn’t taken before, in order to discover how to solve the problem. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing. Sometimes we want the agent to leverage what it has already learned to try and optimize further. This is the exploration-exploitation trade-off, we want an automated way to strike a good balance between letting the agent explore and taking actions for which it already knows what they will lead to.For a lot problems, it is quite possible that the agent gets stuck in a local optimum.Press enter or click to view image in full sizeThe exploration-exploitation trade-off sounds very much tractable at first, but it turns out to be one of the hardest problems for RL to solve. To give you an idea about how hard this problem is: The problem was originally considered by the scientists of the allied forces, but was suggested to be dropped over to Germany, because it was deemed so intractable that they wanted the German scientists to also waste their time on it.No silver bullet for this problem has been found and there are many people working on various solutions. I will leave a link here to a previously proposed solution called “curiosity-driven exploration” which I found particularly interesting.Curiosity-driven exploration by Self-supervised PredictionThe sparse-reward problemAnother rather fundamental problem, is the so called Sparse-reward problem. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve.Imagine for example this mountain car. The agent needs to move the car left and right, such that it gets enough momentum to reach the top. Initially though, the agent doesn’t know that it needs to move the car back and forth to reach the top. If we only give our agent a reward (a positive feedback signal) when we have reached the flag, it might not ever get a positive feedback signal, simply because it might never reach the flag by taking random actions (exploration).Something commonly done to counteract this problem, is by “reward shaping”. We will modify the reward such that the agent gets more feedback signals to learn from. In case of the mountain car, we could for example also give the agent a positive reward based on the speed or altitude it achieves. However, reward shaping is not a scalable solution. Luckily other solutions are being sought after.I’m leaving another example here which I think is an interesting (but not generally applicable approach) for counteracting sparse rewards.Hindsight Experience ReplayThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made.ConclusionPhew, that was a lot of information in a very short period, but you made it through! This first introduction should give you a good idea of what RL is all about.In the next article, we will start formalizing some of the ideas and concepts which we have briefly touched upon in this post, as a preparation for getting our hands dirty and implementing these ideas ourselves.Artificial IntelligenceReinforcement LearningDeep LearningPolicy GradientData Science----2Written by Cédric Vandelaer192 followers·8 followingResponses (2)See all responsesHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T16:54:08.839153Z",
     "start_time": "2025-09-07T16:52:10.670423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = OnlinePDFLoader(pdf_urls[0])\n",
    "loader.load()[0].page_content"
   ],
   "id": "6e284810baea941f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Journal of Artificial Intelligence Research 4 (1996) 237-285 Submitted 9/95; published 5/96\\n\\nReinforcement Learning: A Survey\\n\\nLeslie Pack Kaelbling LPK@CS.BROWN.EDU Michael L. Littman MLITTMAN @Cs .BROWN.EDU Computer Science Department, Box 1910, Brown University\\n\\nProvidence, RI 02912-1910 USA\\n\\nAndrew W. Moore AWM@CS.CMU.EDU Smith Hall 221, Carnegie Mellon University, 5000 Forbes Avenue Pittsburgh, PA 15213 USA\\n\\nAbstract\\n\\nThis paper surveys the field of reinforcement learning from a computer-science per- spective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.\\n\\n1. Introduction\\n\\nReinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last five to ten years, it has attracted rapidly increasing interest in the machine learning and artificial intelligence communities. Its promise is beguiling—a way of programming agents by reward and punishment without needing to specify how the task is to be achieved. But there are formidable computational obstacles to fulfilling the promise. This paper surveys the historical basis of reinforcement learning and some of the current work from a computer science perspective. We give a high-level overview of the field and a taste of some specific approaches. It is, of course, impossible to mention all of the important work in the field; this should not be taken to be an exhaustive account.\\n\\nReinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment. The work described here has a strong family resemblance to eponymous work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” It is appropriately thought of as a class of problems, rather than as a set of techniques.\\n\\nThere are two main strategies for solving reinforcement-learning problems. The first is to search in the space of behaviors in order to find one that performs well in the environment. This approach has been taken by work in genetic algorithms and genetic programming,\\n\\n©1996 AT Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nFigure 1: The standard reinforcement-learning model.\\n\\nas well as some more novel search techniques (Schmidhuber, 1996). The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world. This paper is devoted almost entirely to the second set of techniques because they take advantage of the special structure of reinforcement-learning problems that is not available in optimization problems in general. It is not yet clear which set of approaches is best in which circumstances.\\n\\nThe rest of this section is devoted to establishing notation and describing the basic reinforcement-learning model. Section 2 explains the trade-off between exploration and exploitation and presents some solutions to the most basic case of reinforcement-learning problems, in which we want to maximize the immediate reward. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them. Section 4 considers some classic model-free algorithms for reinforcement learning from delayed reward: adaptive heuristic critic, TD(A) and Q-learning. Section 5 demonstrates a continuum of algorithms that are sensitive to the amount of computation an agent can perform between actual steps of action in the environment. Generalization—the cornerstone of mainstream machine learning research—has the potential of considerably aiding reinforcement learning, as described in Section 6. Section 7 considers the problems that arise when the agent does not have complete perceptual access to the state of the environment. Section 8 catalogs some of reinforcement learning’s successful applications. Finally, Section 9 concludes with some speculations about important open problems and the future of reinforcement learning.\\n\\n1.1 Reinforcement-Learning Model\\n\\nIn the standard reinforcement-learning model, an agent is connected to its environment via perception and action, as depicted in Figure 1. On each step of interaction the agent receives as input, 27, some indication of the current state, s, of the environment; the agent then chooses an action, a, to generate as output. The action changes the state of the environment, and the value of this state transition is communicated to the agent through a scalar reinforcement signal, r. The agent’s behavior, B, should choose actions that tend to increase the long-run sum of values of the reinforcement signal. It can learn to do this over time by systematic trial and error, guided by a wide variety of algorithms that are the subject of later sections of this paper.\\n\\n238\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nFormally, the model consists of\\n\\ne a discrete set of environment states, S;\\n\\ne adiscrete set of agent actions, A; and\\n\\ne aset of scalar reinforcement signals; typically {0,1}, or the real numbers.\\n\\nThe figure also includes an input function J, which determines how the agent views the environment state; we will assume that it is the identity function (that is, the agent perceives the exact state of the environment) until we consider partial observability in Section 7.\\n\\nAn intuitive way to understand the relation between the agent and its environment is with the following example dialogue.\\n\\nEnvironment: You are in state 65. You have 4 possible actions.\\n\\nAgent: T\\'ll take action 2.\\n\\nEnvironment: You received a reinforcement of 7 units. You are now in state 15. You have 2 possible actions.\\n\\nAgent: T\\'ll take action 1.\\n\\nEnvironment: You received a reinforcement of -4 units. You are now in state\\n\\n65. You have 4 possible actions.\\n\\nAgent: T\\'ll take action 2. Environment: You received a reinforcement of 5 units. You are now in state\\n\\n44. You have 5 possible actions.\\n\\nThe agent’s job is to find a policy 7, mapping states to actions, that maximizes some long-run measure of reinforcement. We expect, in general, that the environment will be non-deterministic; that is, that taking the same action in the same state on two different occasions may result in different next states and/or different reinforcement values. This happens in our example above: from state 65, applying action 2 produces differing rein- forcements and differing states on two occasions. However, we assume the environment is stationary; that is, that the probabilities of making state transitions or receiving specific reinforcement signals do not change over time.!\\n\\nReinforcement learning differs from the more widely studied problem of supervised learn- ing in several ways. The most important difference is that there is no presentation of in- put/output pairs. Instead, after choosing an action the agent is told the immediate reward and the subsequent state, but is not told which action would have been in its best long-term\\n\\ninterests. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally. Another difference from supervised learning is that on-line performance is important: the evaluation of the system is often concurrent with learning.\\n\\n1. This assumption may be disappointing; after all, operation in non-stationary environments is one of the motivations for building learning systems. In fact, many of the algorithms described in later sections are effective in slowly-varying non-stationary environments, but there is very little theoretical analysis in this area.\\n\\n239\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nSome aspects of reinforcement learning are closely related to search and planning issues in artificial intelligence. AI search algorithms generate a satisfactory trajectory through a graph of states. Planning operates in a similar manner, but typically within a construct with more complexity than a graph, in which states are represented by compositions of logical expressions instead of atomic symbols. These A] algorithms are less general than the reinforcement-learning methods, in that they require a predefined model of state transitions, and with a few exceptions assume determinism. On the other hand, reinforcement learning, at least in the kind of discrete cases for which theory has been developed, assumes that the entire state space can be enumerated and stored in memory—an assumption to which conventional search algorithms are not tied.\\n\\n1.2 Models of Optimal Behavior\\n\\nBefore we can start thinking about algorithms for learning to behave optimally, we have to decide what our model of optimality will be. In particular, we have to specify how the agent should take the future into account in the decisions it makes about how to behave now. There are three models that have been the subject of the majority of work in this area.\\n\\nThe finite-horizon model is the easiest to think about; at a given moment in time, the agent should optimize its expected reward for the next h steps:\\n\\nh EQ ri) :\\n\\nt=0 it need not worry about what will happen after that. In this and subsequent expressions, r, represents the scalar reward received t steps into the future. This model can be used in two ways. In the first, the agent will have a non-stationary policy; that is, one that changes over time. On its first step it will take what is termed a h-step optimal action. This is defined to be the best action available given that it has h steps remaining in which to act and gain reinforcement. On the next step it will take a (h — 1)-step optimal action, and so on, until it finally takes a 1-step optimal action and terminates. In the second, the agent does receding-horizon control, in which it always takes the h-step optimal action. The agent always acts according to the same policy, but the value of h limits how far ahead it looks in choosing its actions. The finite-horizon model is not always appropriate. In many cases we may not know the precise length of the agent’s life in advance.\\n\\nThe infinite-horizon discounted model takes the long-run reward of the agent into ac-\\n\\ncount, but rewards that are received in the future are geometrically discounted according to discount factor +, (where 0 < 7 < 1):\\n\\noo\\n\\nBD yr\\n\\nt=0\\n\\nWe can interpret y in several ways. It can be seen as an interest rate, a probability of living another step, or as a mathematical trick to bound the infinite sum. The model is conceptu-\\n\\nally similar to receding-horizon control, but the discounted model is more mathematically tractable than the finite-horizon model. This is a dominant reason for the wide attention this model has received.\\n\\n240\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nAnother optimality criterion is the average-reward model, in which the agent is supposed to take actions that optimize its long-run average reward:\\n\\nh\\n\\nlim BEY n) .\\n\\nhoo +=0\\n\\nSuch a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1 (Bertsekas, 1995). One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount of reward in the initial phases and the other of whic does not. Reward gained on any initial prefix of the agent’s life is overshadowed by the long-run average performance. It is possible to generalize this model so that it takes into account both the long run average and the amount of initial reward than can be gained. In the generalized, bias optimal model, a policy is preferred if it maximizes the long-run average and ties are broken by the initial extra reward.\\n\\nFigure 2 contrasts these models of optimality by providing an environment in whic changing the model of optimality changes the optimal policy. In this example, circles\\n\\nrepresent the states of the environment and arrows are state transitions. There is only a single action choice from every state except the start state, which is in the upper left and marked with an incoming arrow. All rewards are zero except where marked. Under a finite-horizon model with h = 5, the three actions yield rewards of +6.0, +0.0, and +0.0, so the first action should be chosen; under an infinite-horizon discounted model with 7 = 0.9, the three choices yield +16.2, +59.0, and +58.5 so the second action should be chosen; and under the average reward model, the third action should be chosen since it leads to an average reward of +11. If we change h to 1000 and ¥ to 0.2, then the second action is optimal for the finite-horizon model and the first for the infinite-horizon discounted model; however, the average reward model will always prefer the best long-term average. Since the choice of optimality model and parameters matters so much, it is important to choose it\\n\\ncarefully in any application.\\n\\nThe finite-horizon model is appropriate when the agent’s lifetime is known; one im- portant aspect of this model is that as the length of the remaining lifetime decreases, the agent’s policy may change. A system with a hard deadline would be appropriately modeled this way. The relative usefulness of infinite-horizon discounted and bias-optimal models is still under debate. Bias-optimality has the advantage of not requiring a discount parameter;\\n\\nhowever, algorithms for finding bias-optimal policies are not yet as well-understood as those for finding optimal infinite-horizon discounted policies.\\n\\n1.38 Measuring Learning Performance\\n\\nThe criteria given in the previous section can be used to assess the policies learned by a given algorithm. We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use.\\n\\ne Eventual convergence to optimal. Many algorithms come with a provable guar-\\n\\nantee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992). This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau\\n\\n241\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n+2\\n\\nFinite horizon, h=4 +10\\n\\nInfinite horizon, y=0.9\\n\\nOOO0-0-00\"\\n\\nAverage reward\\n\\nFigure 2: Comparing models of optimality. All unlabeled arrows produce a reward of zero.\\n\\nat 99% of optimality may, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate.\\n\\ne Speed of convergence to optimality. Optimality is usually an asymptotic result, and so convergence speed is an ill-defined measure. More practical is the speed of convergence to near-optimality. This measure begs the definition of how near to optimality is sufficient. A related measure is level of performance after a given time, which similarly requires that someone define the given time.\\n\\nIt should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accu- racy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework (Valiant, 1984), there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance. That is usually an inappropriate view for an agent with a long existence in a complex environment.\\n\\nIn spite of the mismatch between embedded reinforcement learning and the train/test perspective, Fiechter (1994) provides a PAC analysis for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views.\\n\\nMeasures related to speed of learning have an additional weakness. An algorithm that merely tries to achieve optimality as fast as possible may incur unnecessarily large penalties during the learning period. A less aggressive strategy taking longer to achieve optimality, but gaining greater total reinforcement during its learning might be preferable.\\n\\ne Regret. A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret (Berry & Fristedt, 1985). It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain.\\n\\n242\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n1.4 Reinforcement Learning and Adaptive Control\\n\\nAdaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algo- rithms for improving a sequence of decisions from experience. Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and ac- tions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory. A very common formulation of cost functions in adaptive control are quadratic penalties on deviation from desired state and action vectors. Most importantly, although the dynamic model of the system is not known in advance, and must be esti- mated from data, the structure of the dynamic model is fixed, leaving model estimation as a parameter estimation problem. These assumptions permit deep, elegant and powerful mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive control algorithms.\\n\\n2. Exploitation versus Exploration: The Single-State Case\\n\\nOne major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the roblems of exploration, we treat a very simple case in this section. The fundamental issues and approaches described here will, in many cases, transfer to the more complex instances of reinforcement learning discussed later in the paper.\\n\\nThe simplest possible reinforcement-learning problem is known as the k-armed bandit\\n\\nproblem, which has been the subject of a great deal of study in the statistics and applied mathematics literature (Berry & Fristedt, 1985). The agent is in a room with a collection of k gambling machines (each called a “one-armed bandit” in colloquial English). The agent is ermitted a fixed number of pulls, h. Any arm may be pulled on each turn. The machines do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal machine. When arm ? is pulled, machine 7 pays off 1 or 0, according to some underlying probability parameter p;, where payoffs are independent events and the p;s are unknown. What should the agent’s strategy be? This problem illustrates the fundamental tradeoff between exploitation and exploration. The agent might believe that a particular arm has a fairly high payoff probability; should it choose that arm all the time, or should it choose another one that it has less information about, but seems to be worse? Answers to these questions depend on how long the agent is expected to play the game; the longer the game lasts, the worse the consequences of prematurely converging on a sub-optimal arm, and the more the agent should explore.\\n\\na\\n\\nThere is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt (1985). We use the term “action” to indicate the agent’s choice of arm to pull. This eases the transition into delayed reinforcement models in Section 3. It is very important to note that bandit problems fit our definition of a reinforcement-learning environment with a single state with only self transitions.\\n\\nSection 2.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case.\\n\\n243\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nSection 2.2 presents three techniques that are not formally justified, but that have had wide use in practice, and can be applied (with similar lack of guarantee) to the general case.\\n\\n2.1 Formally Justified Techniques\\n\\nThere is a fairly well-developed formal theory of exploration for very simple problems. Although it is instructive, the methods it provides do not scale well to more complex problems.\\n\\n2.1.1 DYNAMIC-PROGRAMMING APPROACH\\n\\nIf the agent is going to be acting for a total of A steps, it can use basic Bayesian reasoning to solve for an optimal strategy (Berry & Fristedt, 1985). This requires an assumed prior joint distribution for the parameters {p;}, the most natural of which is that each p; is independently uniformly distributed between 0 and 1. We compute a mapping from belief states (summaries of the agent’s experiences during this run) to actions. Here, a belief state\\n\\ncan be represented as a tabulation of action choices and payoffs: {n1, wi, na, W2,..., Mk, We} denotes a state of play in which each arm 7 has been pulled n; times with w; payoffs. We write V*(n1, wi,-.-,2k, WE) as the expected payoff remaining, given that a total of h pulls\\n\\nare available, and we use the remaining pulls optimally.\\n\\nIf 0; nj; = h, then there are no remaining pulls, and V*(nj, wy,..., nz, we) = 0. This is the basis of a recursive definition. If we know the V* value for all belief states with ¢ pulls remaining, we can compute the V* value of any belief state with t+ 1 pulls remaining:\\n\\nVe (ny.wp..-.,npewp) = max; B Future payoff if agent takes action a, then acts optimally for remaining pulls\\n\\n= max; piV™ (ny, W;,---,2i +1, wie+1,---, MK. We)+ (1 = pi) V*(m1, Wi, ee ME 1, Wi, +. Me, WE)\\n\\nwhere p; is the posterior subjective probability of action 7 paying off given n;, w; and our prior probability. For the uniform priors, which result in a beta distribution, p; =\\n\\nThe expense of filling in the table of V* values in this way for all attainable belief states is linear in the number of belief states times actions, and thus exponential in the horizon.\\n\\n2.1.2 GITTINS ALLOCATION INDICES\\n\\nGittins gives an “allocation index” method for finding the optimal choice of action at each step in k-armed bandit problems (Gittins, 1989). The technique only applies under the discounted expected reward criterion. For each action, consider the number of times it has been chosen, n, versus the number of times it has paid off, w. For certain discount factors, there are published tables of “index values,” I(n,w) for each pair of n and w. Look up the index value for each action 7, I(nj,w;). It represents a comparative measure of the combined value of the expected payoff of action i (given its history of payoffs) and the value of the information that we would get by choosing it. Gittins has shown that choosing the action with the largest index value guarantees the optimal balance between exploration and exploitation.\\n\\n244\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\na=0 a=1 KL0+—O+—0 +++ O40 O90 + 0 0-00 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=1 a=0 a=1\\n\\n1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=0\\n\\nFigure 3: A Tsetlin automaton with 2N states. The top row shows the state transitions that are made when the previous action resulted in a reward of 1; the bottom row shows transitions after a reward of 0. In states in the left half of the figure, action 0 is taken; in those on the right, action 1 is taken.\\n\\nBecause of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward (Salganicoff & Ungar, 1995). Unfortunately, no one has yet been able to find an analog of index values for delayed reinforcement problems.\\n\\n2.1.3 LEARNING AUTOMATA\\n\\nA branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar (1989), which were originally described explicitly as finite state automata. The Tsetlin automaton shown in Figure 3 provides an example that solves a 2-armed bandit arbitrarily near optimally as N approaches infinity.\\n\\nIt is inconvenient to describe algorithms as finite-state automata, so a move was made to describe the internal state of the agent as a probability distribution according to which actions would be chosen. The probabilities of taking different actions would be adjusted according to their previous successes and failures.\\n\\nAn example, which stands among a set of algorithms independently developed in the mathematical psychology literature (Hilgard & Bower, 1975), is the linear reward-inaction algorithm. Let p; be the agent’s probability of taking action 7.\\n\\ne When action a; succeeds,\\n\\nDi t= pita(l—p) Pj (= py— ap; for 7 #2\\n\\ne When action a; fails, p; remains unchanged (for all j).\\n\\nThis algorithm converges with probability 1 to a vector containing a single 1 and the rest 0’s (choosing a particular action with probability 1). Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making a small (Narendra & Thathachar, 1974). There is no literature on the regret of this algorithm.\\n\\n245\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n2.2 Ad-Hoc Techniques\\n\\nIn reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun (1992) has surveyed a variety of these techniques.\\n\\n2.2.1 GREEDY STRATEGIES\\n\\nThe first strategy that comes to mind is to always choose the action with the highest esti- mated payoff. The flaw is that early unlucky sampling might indicate that the best action’s reward is less than the reward obtained from a suboptimal action. The suboptimal action will always be picked, leaving the true optimal action starved of data and its superiority never discovered. An agent must explore to ameliorate this outcome.\\n\\nA useful heuristic is optimism in the face of uncertainty in which actions are selected greedily, but strongly optimistic prior beliefs are put on their payoffs so that strong negative evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrar- ily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the ez- ploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993).\\n\\n2.2.2 RANDOMIZED STRATEGIES\\n\\nAnother simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random. Some versions of this strategy start with a large value of p to encourage initial exploration, which is slowly decreased.\\n\\nAn objection to the simple strategy is that when it experiments with a non-greedy action it is no more likely to try a promising alternative than a clearly hopeless alternative. A slightly more sophisticated strategy is Boltzmann exploration. In this case, the expected reward for taking action a, E.R(a) is used to choose an action probabilistically according to the distribution\\n\\n(ER(a)/T\\n\\nPO Sea PROT\\n\\nThe temperature parameter T can be decreased over time to decrease exploration. This method works well if the best action is well separated from the others, but suffers somewhat when the values of the actions are close. It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care.\\n\\n2.2.3 INTERVAL-BASED TECHNIQUES\\n\\nExploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling’s interval estimation algorithm (1993b) stores statistics for each action a;: w; is the number of successes and n; the number of trials. An action is chosen by computing the upper bound of a 100-(1—a)%\\n\\n246\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nconfidence interval on the success probability of each action and choosing the action with the highest upper bound. Smaller values of the a parameter encourage greater exploration. When payoffs are boolean, the normal approximation to the binomial distribution can be used to construct the confidence interval (though the binomial should be used for small n). Other payoff distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods (Box & Draper, 1987), which are used for comparing multiple treatments (for example, fertilizers\\n\\nor drugs) to determine which treatment (if any) is best in as small a set of experiments as possible.\\n\\n2.3 More General Problems\\n\\nWhen there are multiple states, but reinforcement is still immediate, then any of the above solutions can be replicated, once for each state. However, when generalization is required, these solutions must be integrated with generalization methods (see section 6); this is straightforward for the simple ad-hoc methods, but it is not understood how to maintain theoretical guarantees.\\n\\nMany of these techniques focus on converging to some regime in which exploratory actions are taken rarely or never; this is appropriate when the environment is stationary. However, when the environment is non-stationary, exploration must continue to take place, in order to notice changes in the world. Again, the more ad-hoc techniques can be modified to deal with this in a plausible manner (keep temperature parameters from going to 0; decay the statistics in interval estimation), but none of the theoretically guaranteed methods can be applied.\\n\\n3. Delayed Reward\\n\\nIn the general case of the reinforcement learning problem, the agent’s actions determine not only its immediate reward, but also (at least probabilistically) the next state of the environment. Such environments can be thought of as networks of bandit problems, but the agent must take into account the next state as well as the immediate reward when it decides which action to take. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account. The agent will have to be able to learn from delayed reinforcement: it may take a long sequence of actions, receiving insignificant reinforcement, then finally arrive at a state with high reinforcement. The agent must be able to learn which of its actions are desirable based on reward that can take place arbitrarily far in the future.\\n\\n3.1 Markov Decision Processes\\n\\nProblems with delayed reinforcement are well modeled as Markov decision processes (MDPs). An MDP consists of\\n\\ne@ aset of states S,\\n\\ne a set of actions A,\\n\\n247\\n\\nKAELBLING, LITTMAN, & Moore\\n\\ne a reward function R:S x A> ®R, and\\n\\ne astate transition function T : S x A — TI(S), where a member of II(S) is a probability istribution over the set S (i.e. it maps states to probabilities). We write T(s, a, s’) for the probability of making a transition from state s to state s’ using action a.\\n\\nThe state transition function probabilistically specifies the next state of the environment as a function of its current state and the agent’s action. The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994).\\n\\nAlthough general MDPs may have infinite (even uncountable) state and action spaces, we will only discuss methods for solving finite-state and finite-action problems. In section 6, we discuss methods for solving problems with continuous input and output spaces.\\n\\n3.2 Finding a Policy Given a Model\\n\\nBefore we consider algorithms for learning to behave in MDP environments, we will ex- plore techniques for determining the optimal policy given a correct model. These dynamic programming techniques will serve as the foundation and inspiration for the learning al- gorithms to follow. We restrict our attention mainly to finding optimal policies for the infinite-horizon discounted model, but most of these algorithms have analogs for the finite- horizon and average-case models as well. We rely on the result that, for the infinite-horizon discounted model, there exists an optimal deterministic stationary policy (Bellman, 1957).\\n\\nWe will speak of the optimal value of a state—it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy. Using 7 as a complete decision policy, it is written\\n\\nV*(s)= max E (>: on)\\n\\nt=0\\n\\nThis optimal value function is unique and can be defined as the solution to the simultaneous equations\\n\\nsles\\n\\nV*(s) = max (n a+7>> renner) WseS, (1)\\n\\nwhich assert that the value of a state s is the expected instantaneous reward plus the expected discounted value of the next state, using the best available action. Given the optimal value function, we can specify the optimal policy as\\n\\nx*(s) = argmax | R(s,a) +7 Ss T(s,a,8\\')V*(s\") “ ES 3.2.1 VALUE ITERATION\\n\\nOne way, then, to find an optimal policy is to find the optimal value function. It can be determined by a simple iterative algorithm called value iteration that can be shown to converge to the correct V* values (Bellman, 1957; Bertsekas, 1987).\\n\\n248\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\ninitialize V(s) arbitrarily loop until policy good enough loop for s€S loop for aE A Q(s,4) = R(s,a) +7 Dyes Ts, 4, 8)V (6% V(s) := max, Q(s, a) end loop end loop\\n\\nIt is not obvious when to stop the value iteration algorithm. One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function (Williams & Baird, 1993b). It says that if the maximum difference between two successive value functions is less than ¢, then the value of the greedy policy, he policy obtained by choosing, in every state, the action that maximizes the estimated discounted reward, using the current estimate of the value function) differs from the value function of the optimal policy by no more than 2ey/(1— 7) at any state. This provides an effective stopping criterion for the algorithm. Puterman (1994) discusses another stopping criterion, based on the span semi-norm, which may result in earlier termination. Another mportant result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged (Bertsekas, 1987). And in practice, the greedy policy is often optimal long before the value function has converged.\\n\\n=\\n\\nValue iteration is very flexible. The assignments to V need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value of every state gets updated infinitely often on an infinite run. These issues are treated extensively by Bertsekas (1989), who also proves convergence results.\\n\\nUpdates based on Equation 1 are known as full backups since they make use of infor- mation from all possible successor states. It can be shown that updates of the form\\n\\nQs, a) = Qls,a) +a(r +7 maxQ(s!,a) — Q(s,a))\\n\\ncan also be used as long as each pairing of a and s is updated infinitely often, s’ is sampled from the distribution T(s, a, s’), r is sampled with mean R(s,a) and bounded variance, and the learning rate a is decreased slowly. This type of sample backup (Singh, 1993) is critical to the operation of the model-free methods discussed in the next section.\\n\\nThe computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions. Com- monly, the transition probabilities T(s, a, s’) are sparse. If there are on average a constant number of next states with non-zero probability then the cost per iteration is linear in the number of states and linear in the number of actions. The number of iterations required to reach the optimal value function is polynomial in the number of states and the magnitude of the largest reward if the discount factor is held constant. However, in the worst case the number of iterations grows polynomially in 1/(1— y), so the convergence rate slows considerably as the discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b).\\n\\n249\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n3.2.2 Poticy ITERATION\\n\\nThe policy iteration algorithm manipulates the policy directly, rather than finding it indi- rectly via the optimal value function. It operates as follows:\\n\\nchoose an arbitrary policy 7’ loop wisn compute the value function of policy 7: solve the linear equations V,(s) = R(s,7(s)) + ¥ Nores T(s, 7(8), 8)Vi(s’) improve the policy at each state: n\\'(s) := argmax, (R(s, a) + 7 Dees T(s, a, 8’) Vz(s\\'))\\n\\nuntil t= 7’\\n\\nThe value function of a policy is just the expected infinite discounted reward that will be gained, at each state, by executing that policy. It can be determined by solving a set of linear equations. Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. If it can, we change the policy to take the new action whenever it is in that situation. This step is guaranteed to strictly improve the performance of the policy. When no improvements are possible, then the policy is guaranteed to be optimal.\\n\\nSince there are at most |A||s| distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations (Puter- man, 1994). However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP (Littman et al., 1995b).\\n\\n3.2.3 ENHANCEMENT TO VALUE ITERATION AND POLicy ITERATION\\n\\nIn practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems. Puterman’s modified policy iteration algorithm (Puterman & Shin, 1978) provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of V,. Instead of finding an exact value for V,, we can perform a few steps of a modified value-iteration step where the policy is held fixed over successive iterations. This can be shown to produce an approximation to V, that converges linearly in 7. In practice, this can result in substantial speedups.\\n\\nSeveral standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution (Riide, 1993). State aggre- gation works by collapsing groups of states to a single meta-state solving the abstracted problem (Bertsekas & Castafion, 1989).\\n\\n250\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n3.2.4 COMPUTATIONAL COMPLEXITY\\n\\nValue iteration works by producing successive approximations of the optimal value function. Each iteration can be performed in O(|A||S|?) steps, or faster if there is sparsity in the ransition function. However, the number of iterations required can grow exponentially in he discount factor (Condon, 1992); as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of O(|A]|.$|?+|S|?) can be prohibitive. There is no known tight worst-case bound available or policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some ractictioners (Rust, 1996).\\n\\nLinear programming (Schrijver, 1986) is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D’Epenoux, 1963; Hoffman & Karp, 1966). An advantage of this approach is that commercial-quality inear-programming packages are available, although the time and space requirements can still be quite high. From a theoretic perspective, linear programming is the only known algorithm that can solve MDPs in polynomial time, although the theoretically efficient algorithms have not been shown to be efficient in practice.\\n\\n4. Learning an Optimal Policy: Model-free Methods\\n\\nIn the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model. The model consists of knowledge of the state tran- sition probability function T(s, a, s’) and the reinforcement function R(s,a). Reinforcement learning is primarily concerned with how to obtain the optimal policy when such a model is not known in advance. The agent must interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy. At this point, there are two ways to proceed.\\n\\ne Model-free: Learn a controller without learning a model. e Model-based: Learn a model, and use it to derive a controller.\\n\\nWhich approach is better? This is a matter of some debate in the reinforcement-learning community. A number of algorithms have been proposed on both sides. This question also appears in other fields, such as adaptive control, where the dichotomy is between direct and indirect adaptive control.\\n\\nThis section examines model-free learning, and Section 5 examines model-based meth- ods.\\n\\nThe biggest problem facing a reinforcement-learning agent is temporal credit assignment. How do we know whether the action just taken is a good one, when it might have far- reaching effects? One strategy is to wait until the “end” and reward the actions taken if the result was good and punish them if the result was bad. In ongoing tasks, it is difficult to know what the “end” is, and this might require a great deal of memory. Instead, we will use insights from value iteration to adjust the estimated value of a state based on\\n\\n251\\n\\nKAELBLING, LITTM.\\n\\n— 4\\n\\nAN, & Moore\\n\\nT’4\\n\\nFigure 4: Architecture for\\n\\nthe immediate reward and the estimated value is known as temporal difference methods (Sutt: temporal-difference learning strategies for the d\\n\\n4.1 Adaptive Heuristic Critic and TD(\\\\)\\n\\nThe adaptive heuristic critic algorithm is an ada\\n\\nSutton, & Anderson, 1983) in which the value mented by solving a set of linear equations, but TD(0). A block\\n\\nnents: a critic (la reinforcement-lear rithms, modified acting to maximiz v, that is computed by the critic. The critic us learn to map states to their expected discounted is the one currently instantia We can see the analogy wi\\n\\no deal with multiple states a:\\n\\nvalue function V, i new policy 7’ tha however, both components op: can be guaranteed to converge\\n\\nand Baird explored the convergence properties\\n\\nfor that po\\n\\nmaximizes the new value fun\\n\\no the optimal pol\\n\\ncall “incremental variants of\\n\\niagram for this approach is gi beled AHC), and a reinforcemen ning component can be an instance of any of the k-armed bandit algo-\\n\\ne instantaneous reward, it will ed in the RL comp h modified policy i\\n\\nworking in alternation. The policy 7 implemented by RL is fixed and t cy. Now we fix the critic and let the RL com\\n\\nolicy iteration” (Williams & Baird, 1993a).\\n\\nhe adaptive heuristic critic.\\n\\nof the next state. This class of algorithms ‘on, 1988). We will consider two different iscounted infinite-horizon model.\\n\\ntive version of policy iteration (Barto, -function computation is no longer imple- is instead computed by an algorithm called iven in Figure 4. It consists of two compo- -learning component (labeled RL). The nd non-stationary rewards. But instead of he heuristic value, he real external reinforcement signal to ues given that the policy onent.\\n\\nbe acting to maximize es\\n\\nval being executed\\n\\neration if we imagine these components e critic learns the onent learn a plementations,\\n\\nction, and so on. In most im\\n\\nerate simultaneously. Only the alternating implementation\\n\\nicy, under appropriate conditions. Williams of a class of AHC-related algorithms they\\n\\nIt remains to explain how the critic can learn the value of a policy. We define (s, a, r,s’)\\n\\nto be an experience tuple summarizing a single t\\n\\nransition in the environment. Here s is the\\n\\nagent’s state before the transition, a is its choice of action, r the instantaneous reward it\\n\\nreceives, and s’ its resulting state. The value o algorithm (Sutton, 1988) which uses the update\\n\\nV(s):\\n\\na policy is learned using Sutton’s TD(0) rule\\n\\nV(s)\\n\\nWhenever a state s is visited, its estimated val since r is the instantaneous reward received and occurring next state. This is analogous to the sa:\\n\\nVV (s\\')—V(s))\\n\\nue is updated to be closer to r+ yV(s‘), V(s’) is the estimated value of the actually mple-backup rule from value iteration—the\\n\\nonly difference is that the sample is drawn from the real world rather than by simulating\\n\\na known model. The key idea is that r + yV(s’\\n\\n252\\n\\nis a sample of the value of V(s), and it is\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nmore likely to be correct because it incorporates the real r. If the learning rate a is adjusted properly (it must be slowly decreased) and the policy is held fixed, TD(0) is guaranteed to converge to the optimal value function.\\n\\nThe TD(0) rule as presented above is really an instance of a more general class of algorithms called TD(X), with 4 = 0. T.D(0) looks only one step ahead when adjusting value estimates; although it will eventually arrive at the correct answer, it can take quite a while to do so. The general TD(A) rule is similar to the TD(0) rule given above,\\n\\nV(u) = V(u) ta(rt+yV(s\\') —V(s)je(u) ,\\n\\nbut it is applied to every state according to its eligibility e(u), rather than just to the immediately previous state, s. One version of the eligibility trace is defined to be\\n\\nt : _ lifs=s e(s) = Say! *S sq , where 35,5, = { 0 otherwise k=1\\n\\nThe eligibility of a state s is the degree to which it has been visited in the recent past; when a reinforcement is received, it is used to update all the states that have been recently visited, according to their eligibility. When \\\\ = 0 this is equivalent to TD(0). When \\\\ = 1, it is roughly equivalent to updating all the states according to the number of times they were visited by the end of a run. Note that we can update the eligibility online as follows:\\n\\ne(s) i= yAe(s)+1 if s= current state yAe(s) otherwise\\n\\nIt is computationally more expensive to execute the general TD(X), though it often converges considerably faster for large \\\\ (Dayan, 1992; Dayan & Sejnowski, 1994). There has been some recent work on making the updates more efficient (Cichosz & Mulawka, 1995) and on changing the definition to make T D(A) more consistent with the certainty-equivalent method (Singh & Sutton, 1996), which is discussed in Section 5.1.\\n\\n4.2 Q-learning\\n\\nThe work of the two components of AHC can be accomplished in a unified manner by Watkins’ Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation. Let Q*(s,a) be the expected discounted reinforcement of taking action ain state s, then continuing by choosing actions optimally. Note that V*(s) is the value of s assuming the best action is taken initially, and so V*(s) = max, Q*(s, a). Q*(s, a) can hence be written recursively as\\n\\nQ*(s,a) = R(s,a) +7 Ss T(s,a, 8’) max Q*(s’, a’). ES e\\n\\nNote also that, since V*(s) = max, Q*(s,a), we have x*(s) = argmax, Q*(s,a) as an optimal policy.\\n\\nBecause the Q function makes the action explicit, we can estimate the Q values on- line using a method essentially the same as TD(0), but also use them to define the policy,\\n\\n253\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nbecause an action can be chosen just by taking the one wit\\n\\ncurrent state. The Q-learning rule is\\n\\nQs.) = Qls,a) + alr +7 maxQ(s\\'a!\\n\\nwhere (s,a,r,s’) is an experie\\n\\neach state an infinite number of Q values will converge with probability 1 to Q* (Watkins,\\n\\nJordan, & Singh, 1994). Q-le more than one step previously,\\n\\nWhen the Q values are ne the agent to act greedily,\\n\\nthe maximum Q value for the\\n\\n— Q(s.4))\\n\\nnce tuple as described earlier. If each action is executed in a is decayed appropriately, the 989; Tsitsiklis, 1994; Jaakkola, arning can also be extended to update states that occurred . as in TD(A) (Peng & Williams, 1994).\\n\\ny converged to their optimal values, it is appropriate for\\n\\nimes on an infinite run and\\n\\nar.\\n\\naking, in each situation, the action with the highest @ value.\\n\\nDuring learning, however, there is a difficult exploitation versus exploration trade-off to be\\n\\nmade. There are no good, forma pt one of the\\n\\nstandard practice is to ado\\n\\nAHC architectures seem to It can be hard to get components converge togethe is, that the Q values will con\\n\\nlevel.\\n\\nly justified approaches to this problem in the general case; ad hoc methods discussed in section 2.2.\\n\\nifficult to work with than Q-learning on a practical e relative learning rates right in AHC so that the two In addition, Q-learning is exploration insensitive: that verge to the optimal values, independent of how the agent\\n\\nbe more t Tr.\\n\\nbehaves while the data is being collected (as long as all state-action pairs are tried often\\n\\nenough). This means that, in Q-learning, the details learning algorithm. For t most effective model-free however, address any of\\n\\noO\\n\\nese\\n\\ngo\\n\\na.\\n\\nalthough the exploration-exploitation issue must be addressed the explora\\n\\nion strategy will not affect the convergence of the -learning is the most popular and seems to be the earning from delayed reinforcement. It does not,\\n\\nreasons, Q rithm for\\n\\nhe issues involved in generalizing over large state and/or action spaces. In addition, it may converge qui\\n\\ne slowly to a good policy.\\n\\n4.3 Model-free Learning With Average Reward\\n\\nAs described, Q-learning can\\n\\nbe applied to discounted infinite-horizon MDPs. It can also\\n\\nbe applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset.\\n\\nSchwartz (1993) examine framework. Although his R-le some MDPs, severa problem they wish to solve th Q-learning (Mahade\\n\\nreward policies. Mahadevan ( a reinforcement-learning persp In particu (and some dynamic cies. Jaakkola, Jor\\n\\nresearchers have found the average-reward\\n\\nvan, 1994). With that in mind, researchers have studied the problem o\\n\\nar, he showed that existing reinforcement-learning alg programming algorithms) do not always an and Singh (1995) described an average-reward learning algorithm\\n\\nthe problem of adapting Q-learning to an average-reward arning algorithm seems to exhibit convergence problems for criterion closer to the true an a discounted criterion and therefore prefer R-learning to\\n\\nlearning optimal average- 996) surveyed model-based average-reward algorithms from ective and found several difficulties with existing algorithms. orithms for average reward roduce bias-optimal poli-\\n\\nwith guaranteed convergence properties. It uses a Monte-Carlo component to estimate the\\n\\nexpected\\n\\nuture reward for each state as the agent moves through the environment. In\\n\\n254\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\naddition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new textbook (1995). Although this recent work provides a much needed theoretical foundation to this area of reinforcement learning, many important problems remain unsolved.\\n\\n5. Computing Optimal Policies by Learning Models\\n\\nThe previous section showed how it is possible to learn an optimal policy without knowing the models T(s, a, s’) or R(s,a) and without even learning those models en route. Although many of these methods are guaranteed to find optimal policies eventually and use very little computation time per experience, they make extremely inefficient use of the data they gather and therefore often require a great deal of experience to achieve good performance. In this section we still begin by assuming that we don’t know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are\\n\\nespecially important in applications in which computation is considered to be cheap and real-world experience costly.\\n\\n5.1 Certainty Equivalent Methods\\n\\nWe begin with the most conceptually straightforward method: first, learn the T and R functions by exploring the environment and keeping statistics about the results of each action; next, compute an optimal policy using one of the methods of Section 3. This method is known as certainty equivlance (Kumar & Varaiya, 1986).\\n\\nThere are some serious objections to this method:\\n\\ne It makes an arbitrary division between the learning phase and the acting phase.\\n\\ne How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data (Whitehead, 1991) than a system that interleaves experience gathering with policy-building more tightly (Koenig & Simmons, 1993). See Figure 5 for an example.\\n\\ne The possibility of changes in the environment is also problematic. Breaking up an agent’s life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment changes.\\n\\nA variation on this idea is certainty equivalence, in which the model is learned continually through the agent’s lifetime and, at each step, the current model is used to compute an optimal policy and value function. This method makes very effective use of available data, but still ignores the question of exploration and is extremely computationally demanding, even for fairly small state spaces. Fortunately, there are a number of other model-based algorithms that are more practical.\\n\\n5.2 Dyna\\n\\nSutton’s Dyna architecture (1990, 1991) exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than\\n\\n255\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nFigure 5: In this environment, due to Whitehead (1991), random exploration would take take O(2\") steps to reach the goal even once, whereas a more intelligent explo- ration strategy (e.g. “assume any untried action leads directly to goal”) would require only O(n”) steps.\\n\\nthe certainty-equivalence approach. It simultaneously uses experience to build a model (T and R), uses experience to adjust the policy, and uses the model to adjust the policy.\\n\\nDyna operates in a loop of interaction with the environment. Given an experience tuple (s,a,s\\',r), it behaves as follows:\\n\\ne Update the model, incrementing statistics for the transition from s to s’ on action a and for receiving reward r for taking action a in state s. The updated models are T and R.\\n\\ne Update the policy at state s based on the newly updated model using the rule\\n\\nQls.a) = Risa) +7 Do F(s,0,8!) maxQls\\'.a’) .\\n\\nwhich is a version of the value-iteration update for Q values.\\n\\ne Perform k additional updates: choose k state-action pairs at random and update them according to the same rule as before:\\n\\nQ(sp, ag) :=R(sp, ag) + + oT (sn, ap, 8’) max Q(s\\', a’).\\n\\ne Choose an action a’ to perform in state s’, based on the Q values but perhaps modified by an exploration strategy.\\n\\nThe Dyna algorithm requires about f times the computation of Q-learning per instance, but this is typically vastly less than for the naive model-based method. A reasonable value of & can be determined based on the relative speeds of computation and of taking action.\\n\\nFigure 6 shows a grid world in which in each cell the agent has four actions (N, S, E, W) and transitions are made deterministically to an adjacent cell, unless there is a block, in which case no movement occurs. As we will see in Table 1, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational effort, however.\\n\\n256\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nFigure 6: A 3277-state grid world. This was formulated as a shortest-path reinforcement- learning problem, which yields the same result as if a reward of 1 is given at the goal, a reward of zero elsewhere and a discount factor is used.\\n\\nSteps before Backups before\\n\\nconvergence convergence Q-learning 531,000 531,000 Dyna 62,000 3,055,000 prioritized sweeping 28,000 1,010,000\\n\\nTable 1: The performance of three algorithms described in the text. All methods used the exploration heuristic of “optimism in the face of uncertainty”: any state not previously visited was assumed by default to be a goal state. Q-learning used its optimal learning rate parameter for a deterministic maze: a@ = 1. Dyna and prioritized sweeping were permitted to take k = 200 backups per transition. For prioritized sweeping, the priority queue often emptied before all backups were used.\\n\\n257\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n5.3 Prioritized Sweeping / Queue-Dyna\\n\\nAlthough Dyna is a great improvement on previous methods, it suffers from being relatively undirected. It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the “interesting” parts of the state space. These problems are addressed by prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams, 1993), which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail.\\n\\nThe algorithm is similar to Dyna, except that updates are no longer chosen at random and values are now associated with states (as in value iteration) instead of state-action pairs (as in Q-learning). To make appropriate choices, we must store additional information in the model. Each state remembers its predecessors: the states that have a non-zero transition probability to it under some action. In addition, each state has a priority, initially set to zero.\\n\\nInstead of updating & random state-action pairs, prioritized sweeping updates k states with the highest priority. For each high-priority state s, it works as follows:\\n\\ne Remember the current value of the state: Vou = V(s).\\n\\ne Update the state’s value\\n\\nV(s) = max (i. al+y>oT(s, 4, vie)\\n\\n3!\\n\\ne Set the state’s priority back to 0.\\n\\ne Compute the value change A = |V,7aq — V(s)|.\\n\\nUse A to modify the priorities of the predecessors of s.\\n\\nIf we have updated the V value for state s’ and it has changed by amount A, then the immediate predecessors of s’ are informed of this event. Any state s for which there exists an action a such that T(s,a,s’) # 0 has its priority promoted to A - T(s,a,s’), unless its priority already exceeded that value.\\n\\nThe global behavior of this algorithm is that when a real-world transition is “surprising” (the agent happens upon a goal state, for instance), then lots of computation is directed to propagate this new information back to relevant predecessor states. When the real- world transition is “boring” (the actual result is very similar to the predicted result), then computation continues in the most deserving part of the space.\\n\\nRunning prioritized sweeping on the problem in Figure 6, we see a large improvement over Dyna. The optimal policy is reached in about half the number of steps of experience and one-third the computation as Dyna required (and therefore about 20 times fewer steps and twice the computational effort of Q-learning).\\n\\n258\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n5.4 Other Model-Based Methods\\n\\nMethods proposed for based methods as well. RTDP (real-time\\n\\nmodel-based method t of the state-space that he agent is trying to By taking into accoun without necessarily vis’\\n\\nsolving MDPs given a model can be used in the context of model-\\n\\nynamic programming) (Barto, Bradtke, & Singh, 1995) is another at uses Q-learning to concentrate computational effort on the areas the agent is most likely to occupy. It is specific to problems in which\\n\\nachieve a particular goal state and the reward everywhere else is 0.\\n\\nthe start state, it can find a short path from the start to the goal, iting the rest of the state space.\\n\\nThe Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman, 994) exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent’s current state and the goal state, if there is one. States that are not in the envelope are summarized by a single “out” state. The planning process is an alternation between finding an optimal policy on the approximate MDP and to the envelope. Action may take place in parallel with planning, in states are also pruned out of the envelope.\\n\\nadding useful states which case irrelevan\\n\\n6. Generalization\\n\\nAll of the previous discussion has tacitly assumed that it is possible to enumerate the state and action spaces and store tables of values over them. Except in very small environments, his means impractical memory requirements. It also makes inefficient use of experience. In a large, smooth state space we generally expect similar states to have similar values and sim- actions. Surely, therefore, there should be some more compact representation han a table. Most problems will have continuous or large discrete state spaces; some wil have large or continuous action spaces. The problem of learning in large spaces is addresse: hrough generalization techniques, which allow compact storage of learned information an er of knowledge between “s:\\n\\nilar optima.\\n\\nrans imilar” states and actions. The large literature of genera. ues from inductive concept learning can be applied to reinforcement learning niques often need to be tailored to specific details of the problem. In the following sections, we explore the application of standar unction-approximation techniques, adaptive resolution models, and hierarchical methods o the problem of reinforcement learning.\\n\\nT\\n\\nhe s\\n\\nization techni\\n\\n. However, tec.\\n\\np D.\\n\\nhe reinforcement-learning architectures and algorithms discussed above have include orage of a variety of mappings, including S — A (policies), S > R (value functions), Sx A— * (Q functions and rewards), S x A > S (deterministic transitions), and S x Ax § => [0,1] (transition probabilities). Some of these mappings, such as transitions an immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervise learning that support noisy training examples. Popular techniques include various neural- network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods. Other mappings, especially the policy\\n\\n259\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nmapping, typically need specialized algorithms because training sets of input-output pairs are not available.\\n\\n6.1 Generalization over Input\\n\\nA reinforcement-learning agent’s current state plays a central role in its selection of reward- maximizing actions. Viewing the agent as a state-free black box, a description of the current state is its input. Depending on the agent architecture, its output is either an action selection, or an evaluation of the current state that can be used to select an action. The problem of deciding how the different aspects of an input affect the value of the output is sometimes called the “structural credit-assignment” problem. This section examines approaches to generating actions or evaluations as a function of a description of the agent’s current state. The first group of techniques covered here is specialized to the case when reward is not delayed; the second group is more generally applicable.\\n\\n6.1.1 IMMEDIATE REWARD\\n\\nWhen the agent’s actions do not influence state transitions, the resulting problem becomes one of choosing actions to maximize immediate reward as a function of the agent’s current state. These problems bear a resemblance to the bandit problems discussed in Section 2 except that the agent should condition its action selection on the current state. For this reason, this class of problems has been described as associative reinforcement learning. The algorithms in this section address the problem of learning from immediate boolean reinforcement where the state is vector valued and the action is a boolean vector. Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4.1. They can also be generalized to real-valued reward through reward comparison methods (Sutton, 1984).\\n\\nCRBP_ Thecomplementary reinforcement backpropagation algorithm (Ackley & Littman, 1990) (CRBP) consists of a feed-forward network mapping an encoding of the state to an encoding of the action. The action is determined probabilistically from the activation of the output units: if output unit ¢ has activation y;, then bit ¢ of the action vector has value 1 with probability y;, and 0 otherwise. Any neural-network supervised training procedure can be used to adapt the network as follows. If the result of generating action a is r = 1, then the network is trained with input-output pair (s,a). If the result is r = 0, then the network is trained with input-output pair (s,@), where @ = (1— a,...,1— ay).\\n\\nThe idea behind this training rule is that whenever an action fails to generate reward, CRBP will try to generate an action that is different from the current choice. Although it seems like the algorithm might oscillate between an action and its complement, that does not happen. One step of training a network will only change the action slightly and since the output probabilities will tend to move toward 0.5, this makes action selection more random and increases search. The hope is that the random distribution will generate an action that works better, and then that action will be reinforced.\\n\\nARC The associative reinforcement comparison (ARC) algorithm (Sutton, 1984) is an instance of the AHc architecture for the case of boolean actions, consisting of two feed-\\n\\n260\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nforward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units.\\n\\nIn the simplest case, the entire system learns only to optimize immediate reward. First, let us consider the behavior of the network that learns the policy, a mapping from a vector describing s to a Q or 1. If the output unit has activation y;, then a, the action generated, will be 1 if y+ v > 0, where v is normal noise, and 0 otherwise.\\n\\nThe adjustment for the output unit is, in the simplest case,\\n\\ne=r(a—1/2) ,\\n\\nwhere the first factor is the reward received for taking the most recent action and the second encodes which action was taken. The actions are encoded as 0 and 1, so a—1/2 always has the same magnitude; if the reward and the action have the same sign, then action 1 will be made more likely, otherwise action 0 will be.\\n\\nAs described, the network will tend to seek actions that given positive reward. To extend this approach to maximize reward, we can compare the reward to some baseline, b. This changes the adjustment to\\n\\n© =(r—d)a~1/2) |\\n\\nwhere 6 is the output of the second network. The second network is trained in a standard supervised mode to estimate r as a function of the input state s.\\n\\nVariations of this approach have been used in a variety of applications (Anderson, 1986; Barto et al., 1983; Lin, 1993b; Sutton, 1984).\\n\\nREINFORCE Algorithms Williams (1987, 1992) studied the problem of choosing ac- tions to maximize immedate reward. He identified a broad class of update rules that per- form gradient descent on the expected reward and showed how to integrate these rules with backpropagation. This class, called REINFORCE algorithms, includes linear reward-inaction (Section 2.1.3) as a special case.\\n\\nThe generic REINFORCE update for a parameter w;; can be written\\n\\nAw = a4j(r - badger Ina)\\n\\nwhere a;; is a non-negative factor, r the current reinforcement, 6;; a reinforcement baseline, and g; is the probability density function used to randomly generate actions based on unit activations. Both a;; and b;; can take on different values for each w;;, however, when a;; is constant throughout the system, the expected update is exactly in the direction of the expected reward gradient. Otherwise, the update is in the same half space as the gradient but not necessarily in the direction of steepest increase.\\n\\nWilliams points out that the choice of baseline, 6 convergence speed of the algorithm.\\n\\nij, can have a profound effect on the\\n\\nLogic-Based Methods Another strategy for generalization in reinforcement learning is to reduce the learning problem to an associative problem of learning boolean functions. A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive\\n\\n261\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nthe generalization process (Kaelbling, 1994b); the other searches the space of syntactic descriptions of functions using a simple generate-and-test method (Kaelbling, 1994a).\\n\\nThe restriction to a single boolean output makes these techniques difficult to apply. In very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output. In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The CASCADE method (Kaelbling, 1993b) allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort.\\n\\n6.1.2 DELAYED REWARD\\n\\nAnother method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning. Here, a function approximator is used o represent the value function by mapping a state description to a value. Many reseachers have experimented with this approach: Boyan and Moore (1995) used local memory-based methods in conjunction with value iteration; Lin (1991) used backprop- agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992, 995) used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich (1995) used backpropagation and T D(A) to learn good strategies for job-shop scheduling.\\n\\nAlthough there have been some positive examples, in general there are unfortunate in- eractions between function approximation and the learning rules. In discrete environments\\n\\nhere is a guarantee that any operation that updates the value function (according to the\\n\\nBellman equations) can only reduce the error between the current value function and the optimal value function. This guarantee no longer holds when generalization is used. These issues are discussed by Boyan and Moore (1995), who give some simple examples of value unction errors growing arbitrarily large when generalization is used with value iteration. Their solution to this, applicable only to certain classes of problems, discourages such diver-\\n\\ngence by only permitting updates whose estimated values can be shown to be near-optimal via a battery of Monte-Carlo experiments.\\n\\nThrun and Schwartz (1993) theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the “max” operator in the definition of the value function.\\n\\nSeveral recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show how the appro- priate choice of function approximator can guarantee convergence, though not necessarily to the optimal values. Baird’s residual gradient technique (Baird, 1995) provides guaranteed convergence to locally optimal solutions.\\n\\nPerhaps the gloominess of these counter-examples is misplaced. Boyan and Moore (1995) report that their counter-examples can be made to work with problem-specific hand-tuning despite the unreliability of untuned algorithms that provably converge in discrete domains. Sutton (1996) shows how modified versions of Boyan and Moore’s examples can converge successfully. An open question is whether general principles, ideally supported by theory, can help us understand when value function approximation will succeed. In Sutton’s com-\\n\\n262\\n\\nREINFORCEMENT LEARNING\\n\\n: A SURVEY\\n\\nparative experiments with Boyan and Moore’s counter-examples, he changes four aspects\\n\\nof the experiments:\\n\\n1. Small changes to\\n\\nthe task specifications.\\n\\n2. A very different kind of function approximator (CMAC (Albus, 1975)) that has weak\\n\\ngeneralization.\\n\\n3. A different learning algorithm: SARSA (Rummery & Niranjan, 1994) instead of value\\n\\niteration.\\n\\n4. A different training regime. Boyan and Moore sampled sta whereas Sutton’s method sampled along empirical traject\\n\\nThere are intuitive reasons to believe that the fourth factor is\\n\\nmore careful research is needed.\\n\\nAdaptive Resolution Models the environment into regions of states that can be considered t: learning and generating actions. Without detailed is very difficult to know what granularity or placement of par use adaptive resolution; during the course of learning, artition is constructed that is appropriate to the\\n\\nproblem is overcome in a\\n\\nDecision Trees va.\\n\\nIn environments ued variables, it is possible to learn compact decision trees for representing Q values. The\\n\\nmethods tha\\n\\nhat are charac\\n\\nes uniformly in state space, ories.\\n\\narticularly important, but\\n\\nIn many cases, what we would like to do is partition\\n\\ne same for the purposes of rior knowledge of the environment, it itions is appropriate. This\\n\\nenvironment.\\n\\nerized by a set of boolean or discrete-\\n\\nworks as fol\\n\\nG-learning algorithm (Chapman & Kk hat no partitioning is necessary an if it were one state. In parallel with input bits: it asks the question whe\\n\\nstates in which 6 = 0. If such a bit he process is repeated in each of t\\n\\naelbling, 1991), tries to learn this process, i her there is some bit 6 in\\n\\nhat the Q values for states in which 6 = 1 are significantly\\n\\nis found, it is used to spl\\n\\nQ values for gathers statistics based on individua\\n\\ne leaves. This method was able to learn very smal\\n\\nows. It starts by assuming the entire environment as\\n\\nthe state description such ifferent from Q values for it the decision tree. Then\\n\\n;\\n\\nre\\n\\ngame environment and or dealing with partial cannot, however, acqui (such as those needed\\n\\nVariable Resolution enables conventional\\n\\nresentations of the Q function in noisy state attributes. It outperformed Q-learning with backpro\\n\\nwas used by McCallum (1995 observability re partitions in which attribu o solve parity problems).\\n\\nDynamic Programming ynamic programming to be\\n\\nhe presence of an overwhel\\n\\nto learn behaviors in a complex driving-simulator. I\\n\\nming number of irrelevant, agation in a simple video- (in conjunction with other techniques\\n\\nes are only significant in combination\\n\\nThe VRDP algorithm (Moore, 1991 performed in real-valued multivariate\\n\\nstate-spaces where straightforward discretization would fall prey to the curse of dimension-\\n\\nality. A kd-tree (simi\\n\\nregions. The coarse regions are refined into detailed\\n\\nspace which are predic\\n\\nning “mental trajectories” through state space. This algorithm\\n\\ned to be important. This no\\n\\nof problems for which disadvantage of requiri\\n\\null high-resolution arrays wo ng a guess at an initially vali\\n\\n263\\n\\nar to a decision tree) is used to parti\\n\\ntion state space into coarse regions, but only in parts of the state ion of importance is obtained by run- proved effective on a number uld have been impractical. It has the trajectory through state-space.\\n\\n(a)\\n\\nKAELBLING, LITTMAN, & Moore\\n\\n(b)\\n\\n(c)\\n\\nStart\\n\\nFigure 7: (a) A two-dimensional maze pro start to goal without crossing an PartiGame during the entire first\\n\\nGoal\\n\\nNF\\n\\ni i\\n\\ninal fo EE\\n\\nBs\\n\\nThe point robot must find a path from y of the barrier lines. (b) The path taken by rial. It begins with intense exploration to find a\\n\\nblem.\\n\\nroute out of the almost entirely enclosed start region. Having eventually reached\\n\\na sufficiently high resolution, it discovers the gap and proceeds gree\\n\\nily towards\\n\\nthe goal, only to be temporarily blocked by the goal’s barrier region. (c) The\\n\\nsecond\\n\\ntrial.\\n\\nPartiGame Algorithm Moore’s PartiGame algorithm (Moore, 1994) is another solution to the problem of learning to achieve goal configurations in deterministic high-dimensional\\n\\ncontinuous s\\n\\naces by learning an adaptive-resolution model. It also divides the environment\\n\\ninto cells; but in each cell, the actions available consist of aiming at the neighboring cells\\n\\n(this aiming problem sta incremental\\n\\nis accomplished by a local controller, which must be provided as ement). The graph of cell transitions is solved for shortest paths in an online manner, but a minimax criterion is used to detect when a group of cells is\\n\\nart of the\\n\\ntoo coarse to prevent movement between obstacles or to avoid limit cycles. The offending\\n\\ncells are spli choose appropria An important fea it also struc the agent wi small local c\\n\\nures 1 ini ang\\n\\nto higher resolution. Eventually, the environment is divided up jus\\n\\ne actions for ach\\n\\nexploration of s ially try someth: es when all the\\n\\nFigure 7a shows a two-dimens\\'\\n\\nof a robot using second trial, star\\n\\nThis is a very than a minute. T limits its applica methods.\\n\\ned from a slight\\n\\nfast algorithm, | e restriction of bility, however.\\n\\nture is that, as well as reducing memory and computational re\\n\\nenough to ieving the goal, but no unnecessary distinctions are made. uirements, ate space in a multi-resolution manner. Given a failure, ing very different to rectify the failure, and only resort to ualitatively different strategies have been exhausted.\\n\\nional continuous maze. Figure 7b shows the performance\\n\\nhe PartiGame algorithm during the very first trial. Figure 7c shows the\\n\\ny different position.\\n\\nearning policies in spaces of up to nine dimensions in less he current implementation to deterministic environments McCallum (1995) suggests some related tree-structured\\n\\n264\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n6.2 Generalization over Actions\\n\\nThe networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well.\\n\\nIn cases such as this when actions are described combinatorially, it is important to generalize over actions to avoid keeping separate statistics for the huge number of actions that can be chosen. In continuous action spaces, the need for generalization is even more pronounced.\\n\\nWhen estimating Q values using a neural network, it is possible to use either a distinct network for each action, or a network with a distinct output for each action. When the action space is continuous, neither approach is possible. An alternative strategy is to use a single network with both the state and action as input and Q value as the output. Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value (Baird & Klopf, 1993).\\n\\nGullapalli (1990, 1992) has developed a “neural” reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts\\n\\nthe mean and variance based on previous experience. When the chosen actions are not performing well, the variance is high, resulting in exploration of the range of choices. When an action performs well, the mean is moved in that direction and the variance decreased, resulting in a tendency to generate more action values near the successful one. This method was successfully employed to learn to control a robot arm with many continuous degrees of\\n\\nfreedom.\\n\\n6.3 Hierarchical Methods\\n\\nAnother strategy for dealing with large state spaces is to treat them as a hierarchy of learning problems. In many cases, hierarchical solutions introduce slight sub-optimality in performance, but potentially gain a good deal of efficiency in execution time, learning time, and space. Hierarchical learners are commonly structured as gated behaviors, as shown in Figure 8. There is a collection of behaviors that map environment states into low-level actions and a gating function that decides, based on the state of the environment, which behavior’s actions should be switched through and actually executed. Maes and Brooks (1990) used a version of this architecture in which the individual behaviors were fixed a priori and the gating function was learned from reinforcement. Mahadevan and Connell (1991b) used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned. Lin (1993a) and Dorigo and Colombetti (1995, 1994) both used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework.\\n\\n6.3.1 FEUDAL Q-LEARNING\\n\\nFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external environment. Its actions consist of commands that\\n\\n265\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nFigure 8: A structure of gated behaviors.\\n\\nit can give to the low-level learner. When the master generates a particular command to the slave, it must reward the slave for taking actions that satisfy the command, even if they do not result in external reinforcement. The master, then, learns a mapping from states to commands. The slave learns a mapping from commands and states to external actions. The set of “commands” and their associated reinforcement functions are established in advance of the learning.\\n\\nThis is really an instance of the general “gated behaviors” approach, in which the slave can execute any of the behaviors depending on its command. The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels.\\n\\n6.3.2 COMPOSITIONAL Q-LEARNING\\n\\nSingh’s compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of condi- tions in sequential order. The achievement of the conditions provides reinforcement for the elemental tasks, which are trained first to achieve individual subgoals. Then, the gating function learns to switch the elemental tasks in order to achieve the appropriate high-level sequential goal. This method was used by Tham and Prager (1994) to learn to control a simulated multi-link robot arm.\\n\\n6.3.3 HIBRARCHICAL DISTANCE TO GOAL\\n\\nEspecially if we consider reinforcement learning modules to be part of larger agent archi- tectures, it is important to consider problems in which goals are dynamically input to the learner. Kaelbling’s HDG algorithm (1993a) uses a hierarchical approach to solving prob- lems when goals of achievement (the agent should get to a particular state as quickly as possible) are given to an agent dynamically.\\n\\nThe HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned (a priori, but more recent work (Ashar, 1994) addresses the case of learning the partition) into a set of regions whose centers are known as “landmarks.” If the agent is\\n\\n266\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\noffice\\n\\nFigure 9: An example of a partially observable environment.\\n\\ncurrently in the same region as the goal, then it uses low-level actions to move to the goal. If not, then high-level information is used to determine the next landmark on the shortest path from the agent’s closest landmark to the goal’s closest landmark. Then, the agent uses low-level information to aim toward that next landmark. If errors in action cause deviations in the path, there is no problem; the best aiming point is recomputed on every step.\\n\\n7. Partially Observable Environments\\n\\nIn many real-world environments, it will not be possible for the agent to have perfect and complete perception of the state of the environment. Unfortunately, complete observability is necessary for learning methods based on MDPs. In this section, we consider the case in which the agent makes observations of the state of the environment, but these observations may be noisy and provide incomplete information. In the case of a robot, for instance, it might observe whether it is in a corridor, an open room, a T-junction, etc., and those observations might be error-prone. This problem is also referred to as the problem of “incomplete perception,” “perceptual aliasing,” or “hidden state.”\\n\\nIn this section, we will consider extensions to the basic MDP framework for solving partially observable problems. The resulting formal model is called a partially observable Markov decision process or POMDP.\\n\\n7.1 State-Free Deterministic Policies\\n\\nThe most naive strategy for dealing with partial observability is to ignore it. That is, to reat the observations as if they were the states of the environment and try to learn to behave. Figure 9 shows a simple environment in which the agent is attempting to get to he printer from an office. If it moves from the office, there is a good chance that the agent will end up in one of two places that look like “hall”, but that require different actions for getting to the printer. If we consider these states to be the same, then the agent cannot ossibly behave optimally. But how well can it do?\\n\\nThe resulting problem is not Markovian, and Q-learning cannot be guaranteed to con- verge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate (Chrisman &\\n\\n267\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nLittman, 1993). It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations. Unfortunately, when the environment is not Markovian, the transition probabilities depend on the policy being executed, so this new policy will induce a new set of transition probabilities. This approach may yield plausible results in some cases, but again, there are no guarantees.\\n\\nIt is reasonable, though, to ask what the optimal policy (mapping from observations to actions, in this case) is. It is NP-hard (Littman, 1994b) to find this mapping, and even the best mapping can have very poor performance. In the case of our agent trying to get to the printer, for instance, any deterministic state-free policy takes an infinite number of steps to reach the goal on average.\\n\\n7.2 State-Free Stochastic Policies\\n\\nSome improvement can be gained by considering stochastic policies; these are mappings from observations to probability distributions over actions. If there is randomness in the agent’s actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan (1995) have developed an algorithm for finding locally-optimal stochastic policies, but finding a globally optimal policy is still NP hard.\\n\\nIn our example, it turns out that the optimal stochastic policy is for the agent, when in a state that looks like a hall, to go east with probability 2 — /2 % 0.6 and west with probability /2 —1 % 0.4. This policy can be found by solving a simple (in this case) quadratic program. The fact that such a simple example can produce irrational numbers gives some indication that it is a difficult problem to solve exactly.\\n\\n7.3 Policies with Internal State\\n\\nThe only way to behave truly effectively in a wide-range of environments is to use memory of previous actions and observations to disambiguate the current state. There are a variety of approaches to learning policies with internal state.\\n\\nRecurrent Q-learning One intuitively simple approach is to use a recurrent neural net- work to learn Q values. The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain “history features” to predict value. This approach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin & Mitchell, 1992; Schmidhuber, 1991b). It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems.\\n\\nClassifier Systems Classifier systems (Holland, 1975; Goldberg, 1989) were explicitly developed to solve problems with delayed reward, including those requiring short-term memory. The internal mechanism typically used to pass reward back through chains of decisions, called the bucket brigade algorithm, bears a close resemblance to Q-learning. In spite of some early successes, the original design does not appear to handle partially ob- served environments robustly.\\n\\nRecently, this approach has been reexamined using insights from the reinforcement- learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems (Dorigo & Bersini, 1994). Cliff and Ross (1994) start with Wilson’s zeroth-\\n\\n268\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nFigure 10: Structure of a POMDP agent.\\n\\nevel classifier system (Wilson, 1995) and add one and two-bit memory registers. They find hat, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments.\\n\\nDorigo and Colombetti applied classifier systems to a moderately complex problem of learning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti, 994).\\n\\nFinite-history-window Approach One way to restore the Markov property is to allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell (1992) used a fixed-width finite history window to learn a pole balancing task. McCallum (1995) describes the “utile suffix memory” which learns a variable-width window hat serves simultaneously as a model of the environment and a finite-memory policy. This system has had excellent results in a very complex driving-simulation domain (McCallum, 995). Ring (1994) has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations.\\n\\nPOMDP Approach Another strategy consists of using hidden Markov model (HMM) echniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Monahan, 1982). Chrisman (1992) showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum (1993), also gave heuristic state- splitting rules to attempt to learn the smallest possible model for a given environment. The resulting model can then be used to integrate information from the agent’s observations in order to make decisions.\\n\\nFigure 10 illustrates the basic structure for a perfect-memory controller. The component on the left is the state estimator, which computes the agent’s belief state, b as a function of the old belief state, the last action a, and the current observation 7. In this context, a belief state is a probability distribution over states of the environment, indicating the likelihood, given the agent’s past experience, that the environment is actually in each of those states. The state estimator can be constructed straightforwardly using the estimated world model and Bayes’ rule.\\n\\nNow we are left with the problem of finding a policy mapping belief states into action. This problem can be formulated as an MDP, but it is difficult to solve using the techniques described earlier, because the input space is continuous. Chrisman’s approach (1992) does not take into account future uncertainty, but yields a policy after a small amount of com- putation. A standard approach from the operations-research literature is to solve for the\\n\\n269\\n\\nKAELBLING, LITTMAN, & Moore\\n\\noptimal policy (or a close approximation thereof) based on its representation as a piecewise- linear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations (Cassandra et al., 1994; Littman, Cassandra, & Kaelbling, 1995a).\\n\\n8. Reinforcement Learning Applications\\n\\nOne reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act. But it is unsurprising that it has also been used by a number of researchers as a practical computational tool for constructing autonomous systems that improve themselves with experience. These applications have ranged from robotics, to industrial manufacturing, to combinatorial search problems such as computer game playing.\\n\\nPractical applications provide a test of the efficacy and usefulness of learning algorithms. They are also an inspiration for deciding which components of the reinforcement learning framework are of practical importance. For example, a researcher with a real robotic task can provide a data point to questions such as:\\n\\ne How important is optimal exploration? Can we break the learning period into explo- ration phases and exploitation phases?\\n\\ne What is the most useful model of long-term reward: Finite horizon? Discounted? Infinite horizon?\\n\\ne How much computation is available between agent decisions and how should it be used?\\n\\ne What prior knowledge can we build into the system, and which algorithms are capable of using that knowledge?\\n\\nLet us examine a set of practical applications of reinforcement learning, while bearing these questions in mind.\\n\\n8.1 Game Playing\\n\\nGame playing has dominated the Artificial Intelligence world as a problem domain ever since he field was born. Two-player games do not fit into the established reinforcement-learning ramework since the optimality criterion for games is not one of maximizing reward in the ace of a fixed environment, but one of maximizing reward against an optimal adversary (minimax). Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games (Littman, 1994a) and many researchers have used reinforcement earning in these environments. One application, spectacularly far ahead of its time, was Samuel’s checkers playing system (Samuel, 1959). This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning.\\n\\nMore recently, Tesauro (1992, 1994, 1995) applied the temporal difference algorithm o backgammon. Backgammon has approximately 107° states, making table-based rein- orcement learning impossible. Instead, Tesauro used a backpropagation-based three-layer\\n\\n270\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nTraining Hidden Results Games Units Basic Poor TD 1.0 300,000 80 Lost by 13 points in 51 games TD 2.0 800,000 40 Lost by 7 points in 38 games TD 2.1 1,500,000 80 Lost by 1 point in 40 games\\n\\nTable 2: TD-Gammon’s performance in\\n\\ngames against the top human professional players.\\n\\nA backgammon tournament involves playing a series of games for points until one player reaches a set target. TD-Gammon won none of these tournaments but came\\n\\nsufficiently close that it is now\\n\\nconsidered one of the best few players in the world.\\n\\nneural network as a function approximator for the value function\\n\\nBoard Position > Probability of victory for current player.\\n\\nTwo versions of the learning algorithm\\n\\nwere used. The first, which we will call Basic TD-\\n\\nGammon, used very little predefined knowledge of the game, and the representation of a board position was virtually a raw encoding, sufficiently powerful only to permit the neural network to distinguish between conceptually different positions. The second, TD-Gammon,\\n\\nwas provided with the same raw state crafted features of backgammon board\\n\\ninformation supplemented by a number of hand- positions. Providing hand-crafted features in this\\n\\nmanner is a good example of how inductive biases from human knowledge of the task can\\n\\nbe supplied to a learning algorithm. The training of both learning algorit\\n\\nwas achieved by constant self-play. No\\n\\ngreedily chose the move with the larges\\n\\nms required several months of computer time, and exploration strategy was used—the system always expected probability of victory. This naive explo-\\n\\nration strategy proved entirely adequate for this environment, which is perhaps surprising given the considerable work in the reinforcement-learning literature which has produced numerous counter-examples to show that greedy exploration can lead to poor learning per-\\n\\nformance. Backgammon, however, has is followed, every game is guaranteed information is obtained fairly frequent’\\n\\nwo important properties. Firstly, whatever policy o end in finite time, meaning that useful reward\\n\\ny. Secondly, the state transitions are sufficiently\\n\\nstochastic that independent of the policy, all states will occasionally be visited—a wrong initial value function has little danger of starving us from visiting a critical part of state space from which important information could be obtained.\\n\\nThe results (Table 2) of TD-Gammon are impressive. It has competed at the very top level of international human play. Basic TD-Gammon played respectably, but not at a\\n\\nprofessional standard.\\n\\n271\\n\\nFigure 11: Schaal and Atkeson’s devil-sticking robot. The tapered stick is hit alternately by each of the two hand sticks. The task is to keep the devil stick from falling for as many hits as possible. The robot has three motors indicated by torque vectors 71,72, 73.\\n\\nAlthough experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess (Thrun, 1995). It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains.\\n\\n8.2 Robotics and Control\\n\\nIn recent years there have been many robotics and control applications that have used reinforcement learning. Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway.\\n\\n1. Schaal and Atkeson (1994) constructed a two-armed robot, shown in Figure 11, tha earns to juggle a device known as a devil-stick. This is a complex non-linear contro ask involving a six-dimensional state space and less than 200 msecs per control deci- sion. After about 40 initial attempts the robot learns to keep juggling for hundreds o hits. A typical human learning the task requires an order of magnitude more practice o achieve proficiency at mere tens of hits.\\n\\nThe juggling robot learned a world model from experience, which was generalize o unvisited states by a function approximation scheme known as locally weighte regression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). Between each trial, a form of dynamic programming specific to linear control policies and locally linear ransitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design (Sage & White, 1977).\\n\\n272\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\n2. Mahadevan and Connell (1991a) discuss a task in which a mobile robot pushes large\\n\\nearned.\\n\\n3. Mataric (1994) describes a robotics experiment with, from t\\n\\nical reinforcement learning, an unthinka\\n\\nnals called progress estimators were use This was achieved in a robust manner he estimators, but had the freedom to Secondly, control was decentralized. Eac.\\n\\nwithout explicit communication with the ot\\n\\nrofi\\n\\nquantized into a small number of discre\\n\\nof the Q-learned policies were almost as he job.\\n\\n4. Q-learning has been used in an elevator dispa’ problem, which has been implemented in simulation only at this stage, involved four elevators servicing ten floors. The objective was to minimize the average squared wait time for passengers, discounted into future time. The problem can be posed as a discrete Markov system, but there are 10?? states even in the most simplified version of he problem. Crites and Barto used neural ne provided an excellent comparison study of their Q-learning approach against the most popular and the most sophisticated elevator dispatching algorithms. The squared wait ime of their controller was approximately 7% (“Empty the System” heuristic with a receding horizon controller) and less than half he squared wait time of the controller most frequently used in real elevator systems.\\n\\n5. The final example concerns an application o authors of this survey to a packaging task from a food processing industry. The roblem involves filling containers with varia The product characteristics also vary with time, but can be sensed. Depending on he task, various constraints are placed on the container-filling procedure. Here are\\n\\nhree examples:\\n\\ngoo\\n\\nbly high dimensional\\n\\nin which the robots\\n\\nfrom the induc\\n\\nive bias\\n\\nh robot learned its own policy\\n\\ners. Thirdly, s\\n\\nas a simple hand-crafte\\n\\nching task (Crites & Bar\\n\\nboxes for extended periods of time. Box-pushing is a well-known difficult robotics roblem, characterized by immense uncertainty in the results of actions. Q-learning was used in conjunction with some novel clustering techniques designed to enable a higher-dimensional input than a tabular approach would have permitted. The robot earned to perform competitively with the performance of a human-programmed so- ution. Another aspect of this work, mentioned in Section 6.3, was a breakdown of the monolithic task description into a set of lower level tasks to be\\n\\npre-programmed\\n\\ne viewpoint of theoret- state space, containing many dozens of degrees of freedom. Four mobile robots traveled within an enclo- sure collecting small disks and transporting them to a destination region. There were hree enhancements to the basic Q-learning algorithm. Firstly, pre-programmed sig- to break the monolithic task into subtasks.\\n\\nwere not forced to use\\n\\nhey provided. independently\\n\\nate space was brutally e states according to values o ber of pre-programmed boolean features of the underlying sensors. The performance\\n\\na small num-\\n\\ncontroller for\\n\\n0, 1996). The\\n\\nworks for function approximation and\\n\\ness than the best alternative algorithm\\n\\nreinforcement learning by one of the\\n\\nble numbers of non-identical products.\\n\\ne The mean weight of all containers produced by a shift must not be below the\\n\\nmanufacturer’s declared weight W.\\n\\n273\\n\\nKAELBLING, LI\\n\\nTTMAN, & Moore\\n\\ne The number of containers below the declared weight must be less than P%.\\n\\ne No containers may be produced below weight W’.\\n\\nSuch tasks are controlled by machinery which operates according to various setpoints. Conventional practice is that setpoints are chosen by human operators, but this choice\\n\\nis not easy as it is task constraints. The task was posed\\n\\nependent on the\\n\\nThe dependency is\\n\\ncurrent product characteristics and the current often difficult to model and highly non-linear.\\n\\nas a finite-horizon Markov decision task in which the state of the\\n\\nsystem is a function of the product characteristics, the amount of time remaining in\\n\\nthe production\\n\\nso far. The system\\n\\nregression was\\n\\ning was used to ma information was ob typically with wast. deployed successful\\n\\nSome interesting aspects of practical rein\\n\\nexamples. The mos\\n\\nnecessary to supplement plying extra knowledge comes a\\n\\nSup the system is subse\\n\\nthese, a knowledge-fr the finite lifetime of What forms did\\n\\nlinearity for the jugg\\n\\nshif was discretized i use intain an optimal\\n\\nage reduced by a y in several factor:\\n\\nstriking is that in al\\n\\nhe fundamental al\\n\\nuently less aw ee approach woul. he robots. his pre-programmed ing robot’s policy,\\n\\nno\\n\\nthe two mobile-robo\\n\\nthe @ values which assumed loca ionally used a manual dimensions and so required correspon\\n\\naddi\\n\\nsumption of local pie in the amoun\\n\\nT\\n\\nysis\\n\\nof lear e exploration s o judge were able to plora T strategies mir yet all prove Finally, it They\\n\\nwhere\\n\\nion.\\n\\nwere al\\n\\nThe\\n\\nearn well wit e packaging task use rors theoretically op adequate.\\n\\nis also worth considering the compu very different, which indicates that t various reinforcement learning algorithms do indeed hav juggler needed to make very fast decisions with had long periods (30 seconds and more) between each\\n\\nhe\\n\\nconsis\\n\\nexamples, while\\n\\nly €\\n\\ny discr ing\\n\\nand the mean was\\n\\nained. In simulate\\n\\na price: onomous.\\n\\nized state space. T\\n\\nage and percent below declared in the shift nto 200,000 discrete states and local weighted\\n\\nto learn and generalize a transition model. Prioritized sweep-\\n\\nvalue function as each new piece of transition experiments the savings were considerable, actor of ten. Since then the system has been\\n\\nies within the United States.\\n\\norcement learning come to light from these cases, to make a real system work it proved gorithm with extra pre-programmed knowledge. more human effort and insight is required and But it is also clear that for tasks such as have achieved worthwhile performance within\\n\\nknowledge take? It included an assumption of\\n\\na manual breaking up of the task into subtasks for\\n\\nbox-pusher also used a clustering technique for tent @ values. The four disk-collecting robots e packaging example had far fewer y weaker assumptions, but there, too, the as-\\n\\ncewise continui\\n\\nning\\n\\nyin t\\n\\nata required.\\n\\ntrategies are inter\\n\\nto profitably gree\\n\\nex te) imal (bu\\n\\ny\\n\\nesting too. T experiment. loration—alway: ptimism in t\\n\\ne transition model enabled massive reductions\\n\\ne juggler used careful statistical anal- However, both mobile robot applications 8 exploiting without deliberate ex- e face of uncertainty. None of these t computationally intractable) exploration, and of these experiments. utational demands of\\n\\national regimes iffering com ean array of differing applications. ow latency between each hit, but rial to consolidate the experiences\\n\\ne\\n\\ncollected on the previous trial and to perform the more aggressive computation necessary\\n\\nto produce a new reactive controller on the next trial. T\\n\\ne box-pushing robot was meant to\\n\\n274\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\noperate autonomously for hours and so had to make decisions with a uniform length control cycle. The cycle was sufficiently long for quite substantial computations beyond simple Q- learning backups. The four disk-collecting robots were particularly interesting. Each robot had a short life of less than 20 minutes (due to battery constraints) meaning that substantial number crunching was impractical, and any significant combinatorial search would have used a significant fraction of the robot’s learning lifetime. The packaging task had easy constraints. One decision was needed every few minutes. This provided opportunities for fully computing the optimal value function for the 200,000-state system between every control cycle, in addition to performing massive cross-validation-based optimization of the transition model being learned.\\n\\nA great deal of further work is currently in progress on practical implementations of reinforcement learning. The insights and task constraints that they produce will have an important effect on shaping the kind of algorithms that are developed in future.\\n\\n9. Conclusions\\n\\nThere are a variety of reinforcement-learning techniques that work effectively on a variety of small problems. But very few of these techniques scale well to larger problems. This is not because researchers have done a bad job of inventing learning techniques, but because it is very difficult to solve arbitrary problems in the general case. In order to solve highly complex problems, we must give up tabula rasa learning techniques and begin to incorporate bias that will give leverage to the learning process.\\n\\nThe necessary bias can come in a variety of forms, including the following:\\n\\nshaping: The technique of shaping is used in training animals (Hilgard & Bower, 1975); a teacher presents very simple problems to solve first, then gradually exposes the learner to more complex problems. Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).\\n\\nlocal reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local. In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly (Mataric, 1994).\\n\\nimitation: An agent can learn by “watching” another agent perform the task (Lin, 1991). For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel (Pomerleau, 1993).\\n\\nproblem decomposition: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very power- ful technique for biasing learning. Most interesting examples of robotic reinforcement learning employ this technique to some extent (Connell & Mahadevan, 1993).\\n\\nreflexes: One thing that keeps agents that know nothing from learning anything is that they have a hard time even finding the interesting parts of the space; they wander\\n\\n275\\n\\nlea:\\n\\nKAELBLING, LITTMAN, & Moore\\n\\naround at random never getting near the goal, or they are always “killed” immediately. These problems can be ameliorated by programming a set of “reflexes” that cause the agent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto, Grupen, & Connolly, 1994). These reflexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn. Recent work by Millan (1996) explores the use of reflexes to make robot learning safer and more efficient.\\n\\nWith appropriate biases, supplied by human programmers or teachers, complex reinforcement- rning problems will eventually be solvable. There is still much work to be done and many\\n\\ninteresting questions remaining for learning techniques and especially regarding methods for approximating, decomposing, and incorporating bias into problems.\\n\\nAcknowledgements\\n\\nT to\\n\\nanks to Marco Dorigo and three anonymous reviewers for comments that have helped improve this paper. Also thanks to our many colleagues in the reinforcement-learning\\n\\ncommunity who have done this work and explained it to us.\\n\\n93\\n\\nin\\n\\nLeslie Pack Kaelbling was supported in part by NSF grants IRI-9453383 and IRI 2395. Michael Littman was supported in part by Bellcore. Andrew Moore was supported art by an NSF Research Initiation Award and by 3M Corporation.\\n\\nReferences\\n\\nAc\\n\\nkley, D. H., & Littman, M. L. (1990). Generalization and scaling in reinforcement learn- ing. In Touretzky, D. S. (Ed.), Advances in Neural Information Processing Systems 2, pp. 550-557 San Mateo, CA. Morgan Kaufmann.\\n\\nAlbus, J. S. (1975). A new approach to manipulator control: Cerebellar model articulation\\n\\ncontroller (emac). Journal of Dynamic Systems, Measurement and Control, 97, 220- 227.\\n\\nAlbus, J. S. (1981). Brains, Behavior, and Robotics. BYTE Books, Subsidiary of McGraw-\\n\\nHill, Peterborough, New Hampshire.\\n\\nAnderson, C. W. (1986). Learning and Problem Solving with Multilayer Connectionist\\n\\nSystems. Ph.D. thesis, University of Massachusetts, Amherst, MA.\\n\\nAshar, R. R. (1994). Hierarchical learning in stochastic domains. Master’s thesis, Brown\\n\\nUniversity, Providence, Rhode Island.\\n\\nBaird, L. (1995). Residual algorithms: Reinforcement learning with function approxima-\\n\\ntion. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 30-37 San Francisco, CA. Morgan Kaufmann.\\n\\nBaird, L. C., & Klopf, A. H. (1993). Reinforcement learning with high-dimensional, con-\\n\\ntinuous actions. Tech. rep. WL-TR-93-1147, Wright-Patterson Air Force Base Ohio: Wright Laboratory.\\n\\n276\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nBarto, A. G., Bradtke, 8. J., & Singh, 8. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1), 81-138.\\n\\nBarto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5), 834-846.\\n\\nBellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.\\n\\nBerenji, H. R. (1991). Artificial neural networks and approximate reasoning for intelligent control in space. In American Control Conference, pp. 1075-1080.\\n\\nBerry, D. A., & Fristedt, B. (1985). Bandit Problems: Sequential Allocation of Experiments. Chapman and Hall, London, UK.\\n\\nBertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic Models. Prentice-Hall, Englewood Cliffs, NJ.\\n\\nBertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena Scientific, Belmont, Massachusetts. Volumes 1 and 2.\\n\\nBertsekas, D. P., & Castafion, D. A. (1989). Adaptive aggregation for infinite horizon dynamic programming. IEEE Transactions on Automatic Control, 34 (6), 589-598.\\n\\nBertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel and Distributed Computation: Numer- ical Methods. Prentice-Hall, Englewood Cliffs, NJ.\\n\\nBox, G. E. P., & Draper, N. R. (1987). Empirical Model-Building and Response Surfaces. Wiley.\\n\\nBoyan, J. A., & Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximating the value function. In Tesauro, G., Touretzky, D. S., & Leen, T. K. (Eds.), Advances in Neural Information Processing Systems 7 Cambridge, MA. The MIT Press.\\n\\nBurghes, D., & Graham, A. (1980). Introduction to Control Theory including Optimal Control. Ellis Horwood.\\n\\nCassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially observable stochastic domains. In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, WA.\\n\\nChapman, D., & Kaelbling, L. P. (1991). Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. In Proceedings of the Interna- tional Joint Conference on Artificial Intelligence Sydney, Australia.\\n\\nChrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. In Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 183-188 San Jose, CA. AAAT Press.\\n\\n277\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nChrisman, L., & Littman, M. (1993). Hidden state and short-term memory.. Presentation at Reinforcement Learning Workshop, Machine Learning Conference.\\n\\nCichosz, P., & Mulawka, J. J. (1995). Fast and efficient reinforcement learning with trun- cated temporal differences. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 99-107 San Francisco, CA. Morgan Kaufmann.\\n\\nCleveland, W. S., & Delvin, S. J. (1988). Locally weighted regression: An approach to regression analysis by local fitting. Journal of the American Statistical Association, 83(403), 596-610.\\n\\nCliff, D., & Ross, S. (1994). Adding temporary memory to ZCS. Adaptive Behavior, 3(2), 101-150.\\n\\nCondon, A. (1992). The complexity of stochastic games. Information and Computation, 96 (2), 203-224.\\n\\nConnell, J., & Mahadevan, S. (1993). Rapid task learning for real robots. In Robot Learning. Kluwer Academic Publishers.\\n\\nCrites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcement learning. In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural Information Processing Systems 8.\\n\\nDayan, P. (1992). The convergence of TD(A) for general \\\\. Machine Learning, 8(3), 341- 362.\\n\\nDayan, P., & Hinton, G. E. (1993). Feudal reinforcement learning. In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), Advances in Neural Information Processing Systems 5 San Mateo, CA. Morgan Kaufmann.\\n\\nDayan, P., & Sejnowski, T. J. (1994). TD(A) converges with probability 1. Machine Learn- ing, 14(3).\\n\\nDean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning with deadlines in stochastic domains. In Proceedings of the Eleventh National Conference on Artificial Intelligence Washington, DC.\\n\\nD’Epenoux, F. (1963). A probabilistic production and inventory problem. Management Science, 10, 98-108.\\n\\nDerman, C. (1970). Finite State Markovian Decision Processes. Academic Press, New York.\\n\\nDorigo, M., & Bersini, H. (1994). A comparison of q-learning and classifier systems. In From Animals to Animats: Proceedings of the Third International Conference on the Simulation of Adaptive Behamor Brighton, UK.\\n\\nDorigo, M., & Colombetti, M. (1994). Robot shaping: Developing autonomous agents through learning. Arteficial Intelligence, 71(2), 321-370.\\n\\n278\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nDorigo, M. (1995). Alecsys and the AutonoMouse: Learning to control a real robot by distributed classifier systems. Machine Learning, 19.\\n\\nFiechter, C.-N. (1994). Efficient reinforcement learning. In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, pp. 88-97. Association of Computing Machinery.\\n\\nGittins, J. C. (1989). Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. Wiley, Chichester, NY.\\n\\nGoldberg, D. (1989). Genetic algorithms in search, optimization, and machine learning. Addison-Wesley, MA.\\n\\nGordon, G. J. (1995). Stable function approximation in dynamic programming. In Priedi- tis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 261-268 San Francisco, CA. Morgan Kaufmann.\\n\\nGullapalli, V. (1990). A stochastic reinforcement learning algorithm for learning real-valued functions. Neural Networks, 3, 671-692.\\n\\nGullapalli, V. (1992). Reinforcement learning and its application to control. Ph.D. thesis, University of Massachusetts, Amherst, MA.\\n\\nHilgard, E. R., & Bower, G. H. (1975). Theories of Learning (fourth edition). Prentice-Hall, Englewood Cliffs, NJ.\\n\\nHoffman, A. J., & Karp, R. M. (1966). On nonterminating stochastic games. Management Science, 12, 359-370.\\n\\nHolland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor, MI.\\n\\nHoward, R. A. (1960). Dynamic Programming and Markov Processes. The MIT Press, Cambridge, MA.\\n\\nJaakkola, T., Jordan, M.1., & Singh, S. P. (1994). On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6(6).\\n\\nJaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Monte-carlo reinforcement learning in non-Markovian decision problems. In Tesauro, G., Touretzky, D. S., & Leen, T. kK. (Eds.), Advances in Neural Information Processing Systems 7 Cambridge, MA. The MIT Press.\\n\\nKaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Preliminary results. In Proceedings of the Tenth International Conference on Machine Learning Amherst, MA. Morgan Kaufmann.\\n\\nKaelbling, L. P. (1993b). Learning in Embedded Systems. The MIT Press, Cambridge, MA.\\n\\nKaelbling, L. P. (1994a). Associative reinforcement learning: A generate and test algorithm. Machine Learning, 15 (3).\\n\\n279\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nKaelbling, L. P. (1994b). Associative reinforcement learning: Functions in k-DNF. Machine Learning, 15(3).\\n\\nKirman, J. (1994). Predicting Real-Time Planner Performance by Domain Characterization. Ph.D. thesis, Department of Computer Science, Brown University.\\n\\nKoenig, S., & Simmons, R. G. (1993). Complexity analysis of real-time reinforcement learning. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pp. 99-105 Menlo Park, California. AAAI Press/MIT Press.\\n\\nKumar, P. R., & Varaiya, P. P. (1986). Stochastic Systems: Estimation, Identification, and Adaptive Control. Prentice Hall, Englewood Cliffs, New Jersey.\\n\\nLee, C. C. (1991). A self learning rule-based controller employing approximate reasoning and neural net concepts. International Journal of Intelligent Systems, 6(1), 71-93.\\n\\nLin, L.-J. (1991). Programming robots using reinforcement learning and teaching. In Proceedings of the Ninth National Conference on Artificial Intelligence.\\n\\nLin, L.-J. (1993a). Hierachical learning of robot skills by reinforcement. In Proceedings of the International Conference on Neural Networks.\\n\\nLin, L.-J. (1993b). Reinforcement Learning for Robots Using Neural Networks. Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\\n\\nLin, L.-J., & Mitchell, T. M. (1992). Memory approaches to reinforcement learning in non- Markovian domains. Tech. rep. CMU-CS-92-138, Carnegie Mellon University, School of Computer Science.\\n\\nLittman, M. L. (1994a). Markov games as a framework for multi-agent reinforcement learn- ing. In Proceedings of the Eleventh International Conference on Machine Learning, pp. 157-163 San Francisco, CA. Morgan Kaufmann.\\n\\nLittman, M. L. (1994b). Memoryless policies: Theoretical limitations and practical results. In Cliff, D., Husbands, P., Meyer, J-A., & Wilson, S. W. (Eds.), From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior Cambridge, MA. The MIT Press.\\n\\nLittman, M. L., Cassandra, A., & Kaelbling, L. P. (1995a). Learning policies for partially observable environments: Scaling up. In Prieditis, A., & Russell, S. (Eds.), Proceed- ings of the Twelfth International Conference on Machine Learning, pp. 362-370 San Francisco, CA. Morgan Kaufmann.\\n\\nLittman, M. L., Dean, T. L., & Kaelbling, L. P. (1995b). On the complexity of solving Markov decision problems. In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI-95) Montreal, Québec, Canada.\\n\\nLovejoy, W. S. (1991). A survey of algorithmic methods for partially observable Markov decision processes. Annals of Operations Research, 28, 47-66.\\n\\n280\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nMaes, P., & Brooks, R. A. (1990). Learning to coordinate behaviors. In Proceedings Eighth National Conference on Artificial Intelligence, pp. 796-802. Morgan Kaufmann.\\n\\nMahadevan, S. (1994). To discount or not to discount in reinforcement learning: A case study comparing R learning and Q learning. In Proceedings of the Eleventh Inter- national Conference on Machine Learning, pp. 164-172 San Francisco, CA. Morgan Kaufmann.\\n\\nMahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empirical results. Machine Learning, 22(1).\\n\\nMahadevan, S., & Connell, J. (1991a). Automatic programming of behavior-based robots using reinforcement learning. In Proceedings of the Ninth National Conference on Artificial Intelligence Anaheim, CA.\\n\\nMahadevan, S., & Connell, J. (1991b). Scaling reinforcement learning to robotics by ex- ploiting the subsumption architecture. In Proceedings of the Eighth International Workshop on Machine Learning, pp. 328-332.\\n\\nMataric, M. J. (1994). Reward functions for accelerated learning. In Cohen, W. W., & Hirsh, H. (Eds.), Proceedings of the Eleventh International Conference on Machine Learning. Morgan Kaufmann.\\n\\nMcCallum, A. K. (1995). Reinforcement Learning with Selective Perception and Hidden State. Ph.D. thesis, Department of Computer Science, University of Rochester.\\n\\nMcCallum, R. A. (1993). Overcoming incomplete perception with utile distinction memory. In Proceedings of the Tenth International Conference on Machine Learning, pp. 190- 196 Amherst, Massachusetts. Morgan Kaufmann.\\n\\nMcCallum, R. A. (1995). Instance-based utile distinctions for reinforcement learning with hidden state. In Proceedings of the Twelfth International Conference Machine Learn- ing, pp. 387-395 San Francisco, CA. Morgan Kaufmann.\\n\\nMeeden, L., McGraw, G., & Blank, D. (1993). Emergent control and planning in an au- tonomous vehicle. In Touretsky, D. (Ed.), Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, pp. 735-740. Lawerence Erlbaum Associates, Hills- dale, NJ.\\n\\nMillan, J. d. R. (1996). Rapid, safe, and incremental learning of navigation strategies. [EEE Transactions on Systems, Man, and Cybernetics, 26 (3).\\n\\nMonahan, G. E. (1982). A survey of partially observable Markov decision processes: Theory, models, and algorithms. Management Science, 28, 1-16.\\n\\nMoore, A. W. (1991). Variable resolution dynamic programming: Efficiently learning ac- tion maps in multivariate real-valued spaces. In Proc. Eighth International Machine Learning Workshop.\\n\\n281\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nMoore, A. W. (1994). The parti-game algorithm for variable resolution reinforcement learn- ing in multidimensional state-spaces. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural Information Processing Systems 6, pp. 711-718 San Mateo, CA. Morgan Kaufmann.\\n\\nMoore, A. W., & Atkeson, C. G. (1992). An investigation of memory-based function ap- proximators for learning control. Tech. rep., MIT Artifical Intelligence Laboratory, Cambridge, MA.\\n\\nMoore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less data and less real time. Machine Learning, 13.\\n\\nMoore, A. W., Atkeson, C. G., & Schaal, S. (1995). Memory-based learning for control. Tech. rep. CMU-RI-TR-95-18, CMU Robotics Institute.\\n\\nNarendra, K., & Thathachar, M. A. L. (1989). Learning Automata: An Introduction. Prentice-Hall, Englewood Cliffs, NJ.\\n\\nNarendra, K. §., & Thathachar, M. A. L. (1974). Learning automata—a survey. IEEE Transactions on Systems, Man, and Cybernetics, 4 (4), 323-334.\\n\\nPeng, J., & Williams, R. J. (1993). Efficient learning and planning within the Dyna frame- work. Adaptive Behavior, 1(4), 437-454.\\n\\nPeng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. In Proceedings of the Eleventh International Conference on Machine Learning, pp. 226-232 San Francisco, CA. Morgan Kaufmann.\\n\\nPomerleau, D. A. (1993). Neural network perception for mobile robot guidance. Kluwer Academic Publishing.\\n\\nPuterman, M. L. (1994). Markov Decision Processes—Discrete Stochastic Dynamic Pro- gramming. John Wiley & Sons, Inc., New York, NY.\\n\\nPuterman, M. L., & Shin, M. C. (1978). Modified policy iteration algorithms for discounted Markov decision processes. Management Science, 24, 1127-1137.\\n\\nRing, M. B. (1994). Continual Learning in Reinforcement Environments. Ph.D. thesis, University of Texas at Austin, Austin, Texas.\\n\\nRiide, U. (1993). Mathematical and computational techniques for multilevel adaptive meth- ods. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania.\\n\\nRumelhart, D. E., & McClelland, J. L. (Eds.). (1986). Parallel Distributed Processing: Explorations in the microstructures of cognition. Volume 1: Foundations. The MIT Press, Cambridge, MA.\\n\\nRummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems. Tech. rep. CUED/F-INFENG/TR166, Cambridge University.\\n\\n282\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nRust, J. (1996). Numerical dynamic programming in economics. In Handbook of Computa- tional Economics. Elsevier, North Holland.\\n\\nSage, A. P., & White, C. C. (1977). Optimum Systems Control. Prentice Hall.\\n\\nSalganicoff, M., & Ungar, L. H. (1995). Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 480-487 San Francisco, CA. Morgan Kaufmann.\\n\\nSamuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal of Research and Development, 8, 211-229. Reprinted in E. A. Feigenbaum and J. Feldman, editors, Computers and Thought, McGraw-Hill, New York 1963.\\n\\nSchaal, S., & Atkeson, C. (1994). Robot juggling: An implementation of memory-based learning. Control Systems Magazine, 14.\\n\\nSchmidhuber, J. (1996). A general method for multi-agent learning and incremental self- improvement in unrestricted environments. In Yao, X. (Ed.), Evolutionary Computa- tion: Theory and Applications. Scientific Publ. Co., Singapore.\\n\\nSchmidhuber, J. H. (1991a). Curious model-building control systems. In Proc. International Joint Conference on Neural Networks, Singapore, Vol. 2, pp. 1458-1463. IEEE.\\n\\nSchmidhuber, J. H. (1991b). Reinforcement learning in Markovian and non-Markovian environments. In Lippman, D. S., Moody, J. E., & Touretzky, D. S. (Eds.), Advances in Neural Information Processing Systems 3, pp. 500-506 San Mateo, CA. Morgan Kaufmann.\\n\\nSchraudolph, N. N., Dayan, P., & Sejnowski, T. J. (1994). Temporal difference learning of position evaluation in the game of Go. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural Information Processing Systems 6, pp. 817-824 San Mateo, CA. Morgan Kaufmann.\\n\\nSchrijver, A. (1986). Theory of Linear and Integer Programming. Wiley-Interscience, New York, NY.\\n\\nSchwartz, A. (1993). A reinforcement learning method for maximizing undiscounted re- wards. In Proceedings of the Tenth International Conference on Machine Learning, pp. 298-305 Amherst, Massachusetts. Morgan Kaufmann.\\n\\nSingh, S. P., Barto, A. G., Grupen, R., & Connolly, C. (1994). Robust reinforcement learning in motion planning. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural Information Processing Systems 6, pp. 655-662 San Mateo, CA. Morgan Kaufmann.\\n\\nSingh, 5. P., & Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces. Machine Learning, 22(1).\\n\\n283\\n\\nKAELBLING, LITTMAN, & Moore\\n\\nSingh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract models. In Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 202-207 San Jose, CA. AAAT Press.\\n\\nSingh, S. P. (1992b). Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3), 323-340.\\n\\nSingh, S. P. (1993). Learning to Solve Markovian Decision Processes. Ph.D. thesis, Depart- ment of Computer Science, University of Massachusetts. Also, CMPSCI Technical Report 93-77.\\n\\nStengel, R. F. (1986). Stochastic Optimal Control. John Wiley and Sons.\\n\\nSutton, R. S. (1996). Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. In Touretzky, D., Mozer, M., Hasselmo, M. (Eds.), Neural Information Processing Systems 8. &\\n\\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis, University of Massachusetts, Amherst, MA.\\n\\nSutton, R. S. (1988). Learning to predict by the method of temporal differences. Machine Learning, 3(1), 9-44.\\n\\nSutton, R. 8. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the Seventh International Conference on Machine Learning Austin, TX. Morgan Kaufmann.\\n\\nSutton, R. S$. (1991). Planning by incremental dynamic programming. In Proceedings of the Eighth International Workshop on Machine Learning, pp. 353-357. Morgan Kaufmann.\\n\\nTesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8, 257-277.\\n\\nTesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master- level play. Neural Computation, 6(2), 215-219.\\n\\nTesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of the ACM, 38(3), 58-67.\\n\\nTham, C.-K., Prager, R. W. (1994). A modular q-learning architecture for manipula- tor task decomposition. In Proceedings of the Eleventh International Conference on Machine Learning San Francisco, CA. Morgan Kaufmann. &\\n\\nThrun, S. (1995). Learning to play the game of chess. In Tesauro, G., Touretzky, D. S., Leen, T. K. (Eds.), Advances in Neural Information Processing Systems 7 Cambridge, MA. The MIT Press.\\n\\n284\\n\\nREINFORCEMENT LEARNING: A SURVEY\\n\\nThrun, S., & Schwartz, A. (1993). Issues in using function approximation for reinforcement learning. In Mozer, M., Smolensky, P., Touretzky, D., Elman, J., & Weigend, A. (Eds.), Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum.\\n\\nThrun, S. B. (1992). The role of exploration in learning control. In White, D. A., & Sofge, D. A. (Eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. Van Nostrand Reinhold, New York, NY.\\n\\nTsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine Learning, 16(3).\\n\\nTsitsiklis, J. N., & Van Roy, B. (1996). Feature-based methods for large scale dynamic programming. Machine Learning, 22(1).\\n\\nValiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11), 1134-1142.\\n\\nWatkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, King’s College, Cambridge, UK.\\n\\nWatkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8(3), 279-292.\\n\\nWhitehead, S. D. (1991). Complexity and cooperation in Q-learning. In Proceedings of the Eighth International Workshop on Machine Learning Evanston, IL. Morgan Kauf- mann.\\n\\nWilliams, R. J. (1987). A class of gradient-estimating algorithms for reinforcement learning in neural networks. In Proceedings of the IEEE First International Conference on Neural Networks San Diego, CA.\\n\\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3), 229-256.\\n\\nWilliams, R. J., & Baird, III, L. C. (1993a). Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems. Tech. rep. NU-CCS-93-11, Northeastern University, College of Computer Science, Boston, MA.\\n\\nWilliams, R. J., & Baird, III, L. C. (1993b). Tight performance bounds on greedy policies based on imperfect value functions. Tech. rep. NU-CCS-93-14, Northeastern Univer- sity, College of Computer Science, Boston, MA.\\n\\nWilson, 8. (1995). Classifier fitness based on accuracy. Evolutionary Computation, 3(2), 147-173.\\n\\nZhang, W., & Dietterich, T. G. (1995). A reinforcement learning approach to job-shop scheduling. In Proceedings of the International Joint Conference on Artificial Intel- lience.\\n\\n285'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:04:52.845831Z",
     "start_time": "2025-09-08T20:53:19.008818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step: Ingesting data from Medium blogs and PDF papers on Reinforcement Learning\n",
    "blog_docs = []\n",
    "for url in blog_urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    blog_docs.extend(loader.load())\n",
    "\n",
    "pdf_docs = []\n",
    "for url in pdf_urls:\n",
    "    loader = OnlinePDFLoader(url)\n",
    "    pdf_docs.extend(loader.load())"
   ],
   "id": "f174dd71f2c137ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:04:58.474699Z",
     "start_time": "2025-09-08T21:04:58.444467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess medium blogs\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "def clean_medium_content(doc: Document) -> Document:\n",
    "    text = doc.page_content\n",
    "    # Remove common Medium UI/boilerplate\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    skip_patterns = [\n",
    "        r'Sitemap|Open in app|Sign up|Sign in|Medium Logo|Write|Listen|Share',\n",
    "        r'Press enter or click to view image in full size',\n",
    "        r'followers|following|Responses \\(\\d+\\)|See all responses|Help|Status|About|Careers|Press|Blog|Privacy|Rules|Terms|Text to speech',\n",
    "        r'Written by .*?Medium',  # Author footer\n",
    "        r'^\\s*$'  # Empty lines\n",
    "    ]\n",
    "    for line in lines:\n",
    "        for pattern in skip_patterns:\n",
    "            line = re.sub(pattern, '', line, flags=re.IGNORECASE)\n",
    "        # Then apply the length check and append if it passes\n",
    "        if len(line.strip()) > 20 and not re.match(r'^--?\\d+$', line.strip()):\n",
    "            cleaned_lines.append(line.strip())\n",
    "\n",
    "    cleaned = ' '.join(cleaned_lines)\n",
    "    # Remove extra spaces/multiple newlines\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "\n",
    "    return Document(page_content=cleaned.strip(), metadata=doc.metadata)\n",
    "\n",
    "cleaned_blog_docs = []\n",
    "for doc in blog_docs:\n",
    "    cleaned_blog_docs.append(clean_medium_content(doc))"
   ],
   "id": "15029b0c7affb9f3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:05:01.238506Z",
     "start_time": "2025-09-08T21:05:01.233034Z"
    }
   },
   "cell_type": "code",
   "source": "cleaned_blog_docs[0]",
   "id": "8f34a5efefbbb5b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1', 'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium', 'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …', 'language': 'en'}, page_content='Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | MediumReinforcement Learning: An introduction (Part 1/4)Cédric Vandelaer10 min read·Aug 20, 2022--2Hi and welcome to the first part of a series on Reinforcement Learning.If you somehow ended up here without having heard of Reinforcement Learning (RL) before, then let me summarize it as follows: “RL is a general framework for training an artificial intelligence model to solve a certain task or goal” … or in layman’s , we make AI do cool things!The goal of this series is to learn RL and simultaneously explore some of the more recent research later on. We will start from the very basics and work our way towards more advanced topics. Even if you have almost no prior programming and/or mathematics knowledge, you should be able to follow along pretty smoothly.The first mini-series will be split into four parts:Part 1: What is Reinforcement learning?Part 2: RL terminology and formal conceptsPart 3: The REINFORCE algorithmPart 4: Implementing the REINFORCE algorithmAt the same time, this mini-series will be the introduction to future posts with increasing complexity. Feel free to skip to the next part if you are already familiar with the content.Note: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.That sounds cool! … but what can I do with RL?Reinforcement learning is a framework to learn any task. In theory, RL can solve any problem that is phrased as a Markov Decision Process. We will explain what that means later on. For now, let’s have a look at some successful applications. If you enjoy these examples, please be sure to also check out the work from the original authors.GeneralOne of the videos I love showing is the hide and seek video from OpenAI. The video is a nice example of how RL can us with finding novel solutions to problems, without explicitly programming tactics or solution methods.GamesOne of the early successes of Deep RL (which is combining RL with neural networks), was the ability to learn how to play Atari games, straight from pixels. Later on, researchers took it upon themselves to not only evaluate RL on the (relatively) simple Atari games, but rather evaluate it on the hardest competitive games out there. The assumption here is, that if RL can solve these complex games, it can also generalize to challenging real-world settings. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.RoboticsSolving tasks in simulations and video games is one thing, but what real life? Another popular field where RL is often applied (or at least holds great promise), is robotics. Robotics are significantly harder than simulations for various reasons. Think for example the time it takes to repeatedly make a robot try out a certain action. Or think the safety requirements involved for robotics. In the example below, you can see how the ANYmal robot from the Robotics System Lab in Zürich learned to recover from a fall.Real world examplesRL can be applied to many other domains than the ones I just mentioned. Advertising, finance, healthcare, … just to name a few. As a final example, I present a goal-oriented chatbot, trained to negotiate sales (source: https://siddharthverma314.github.io/research/chai-acl-2022/ ).RL: The basicsA description of the RL framework is as follows: We have an agent that tries to solve a task in a certain environment. The concept of agent should be taken very broadly here, an agent can be a robot, a chatbot, a virtual character, etc. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in.The RL problem is trying to maximize the cumulative reward the agent gets over time.Imagine our agent is a monkey and the task we want the monkey to solve, is to pick up as many bananas as possible. At every timestep, the monkey needs to decide to take an action. The actions could be to step towards the tree, grab something, climb, … Perhaps the reward at every timestep can be defined as the number of bananas the monkey got at that timestep. After every action, the monkey will also be in a new state. Maybe we define the state of the monkey as its position in the world. So when the monkey takes a step, the state at the next timestep would be the coordinates of the monkey at the next timestep. We are now searching for the optimal behavior, the best sequence of actions the monkey can take, to maximize the cumulative number of bananas it will get.How does RL fit in the bigger picture?You might look at this framework and think: “Hey, isn’t that exactly what people are already studying in the field of …?”. And in fact, you might be right.In other domains like engineering or mathematics, people have often been studying the same problems with different names and methods. Or in fields like neuroscience and psychology, some similarities can be found in the way our brain “rewards” us by releasing dopamine.The likely reason for this intersection of domains, is that reinforcement learning is the study of a fundamental problem. It is essentially the science of decision taking. In these series, we will be looking at it from the umbrella of computer science and machine learning.This general applicability is also what makes RL so interesting to me personally. RL is one of the potential technologies that could get us closer to general AI: an AI system that can solve any task, in contrast to a narrow set of tasks. There are also other technologies (e.g. big language models, graph neural networks, …) that are making some strides in this regard, but the problem statement of RL in particular seems to be the most ambitious.Another way of situating RL, is by looking at how it compares to other learning paradigms. Within the field of machine learning, people often distinguish between Supervised learning, Unsupervised learning and Reinforcement learning.When it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to. Our AI model is successful when it correctly predicts a label y given an (unseen) sample x.As an example, say we have a dataset consisting of cat and dog images. Each sample (image) has a label, stating whether the image contains a “dog” or a “cat”. The goal of our supervised model, is now to learn how to map a dog-image to the label “dog” and a cat-image to the label “cat”. After it has learned this mapping, the hope is that this AI model can repeat this process for new images that it has not seen before.In an Unsupervised learning context, we no longer provide any labels Y. So the task for the AI system now becomes learning some general statistics the dataset. We could for example give an AI the task to generate a new sample, similar to the ones seen in the dataset. In this case the model would be considered successful if it manages to correctly learn the interesting characteristics of a dataset.Reinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in. The “dataset” we need to consider here, are the actions our agent took and the accumulated rewards it got by taking those actions. An added difficulty here is that our dataset is non-static. Say that our agent acts in a certain way, we can then collect some data of the actions our agent took and we can try to optimize these (e.g. do more of the actions that led to a successful result). But as a result of this optimization, we have now changed the behavior of this agent, and thus we will need to collect new data to see how well our agent fares now.If RL is so great, then why isn’t everyone using RL?After reading all this, you might be wondering why people aren’t using RL to solve all imaginable problems. The truth is that even though the field has made a lot of advancements in the last few years, there are still a few fundamental problems to be solved. Progress is being made all the time, but to give you an idea of what you might encounter, I’ll list a few common ones.Sample (in-)efficiencyIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task. In this sense, RL is very inefficient compared to humans, for example, it doesn’t take a human dozens of hours to learn how to play an Atari game.This sample-efficiency can in part be explained by the fact that humans can leverage a lot of their previous knowledge (priors) when they encounter a new task. A human can for example reuse some of the knowledge and skills of previous games and/or concepts they already acquired from other experiences throughout their life. An RL-agent in contrast, starts the learning process without any assumptions.Another thing to mention is that leveraging knowledge from previous tasks is also an active research topic. I’ll just put one example (out of many) here to give you an idea.Deepmind GatoGoogle Jump-Start RLThe exploration-exploitation trade-offWhile the previous problem sounds more like an engineering effort (it’s not), the exploration-exploitation trade-off seems more fundamental. Whenever we train an RL-agent, the agent will need some time to explore, it needs to take some actions that it hasn’t taken before, in order to discover how to solve the problem. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing. Sometimes we want the agent to leverage what it has already learned to try and optimize further. This is the exploration-exploitation trade-off, we want an automated way to strike a good balance between letting the agent explore and taking actions for which it already knows what they will lead to.For a lot problems, it is quite possible that the agent gets stuck in a local optimum.The exploration-exploitation trade-off sounds very much tractable at first, but it turns out to be one of the hardest problems for RL to solve. To give you an idea how hard this problem is: The problem was originally considered by the scientists of the allied forces, but was suggested to be dropped over to Germany, because it was deemed so intractable that they wanted the German scientists to also waste their time on it.No silver bullet for this problem has been found and there are many people working on various solutions. I will leave a link here to a previously proposed solution called “curiosity-driven exploration” which I found particularly interesting.Curiosity-driven exploration by Self-supervised PredictionThe sparse-reward problemAnother rather fundamental problem, is the so called Sparse-reward problem. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve.Imagine for example this mountain car. The agent needs to move the car left and right, such that it gets enough momentum to reach the top. Initially though, the agent doesn’t know that it needs to move the car back and forth to reach the top. If we only give our agent a reward (a positive feedback signal) when we have reached the flag, it might not ever get a positive feedback signal, simply because it might never reach the flag by taking random actions (exploration).Something commonly done to counteract this problem, is by “reward shaping”. We will modify the reward such that the agent gets more feedback signals to learn from. In case of the mountain car, we could for example also give the agent a positive reward based on the speed or altitude it achieves. However, reward shaping is not a scalable solution. Luckily other solutions are being sought after.I’m leaving another example here which I think is an interesting (but not generally applicable approach) for counteracting sparse rewards.Hindsight Experience ReplayThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made.ConclusionPhew, that was a lot of information in a very short period, but you made it through! This first introduction should give you a good idea of what RL is all .In the next article, we will start formalizing some of the ideas and concepts which we have briefly touched upon in this post, as a preparation for getting our hands dirty and implementing these ideas ourselves.Artificial IntelligenceReinforcement LearningDeep LearningPolicy GradientData Science----2Written by Cédric Vandelaer192 ·8')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:05:03.170756Z",
     "start_time": "2025-09-08T21:05:03.142351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing function\n",
    "def clean_pdf_content(doc: Document) -> Document:\n",
    "    text = doc.page_content\n",
    "\n",
    "    # Remove metadata (journal, authors, copyright)\n",
    "    text = re.sub(r\"Journal of Artificial Intelligence Research.*?\\n\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"Leslie Pack Kaelbling.*?(?=\\nAbstract)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"©1996 AI Access Foundation.*?\\n\", \"\", text)\n",
    "\n",
    "    # Remove references section\n",
    "    text = re.sub(r\"References\\n.*\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove figure and table captions\n",
    "    text = re.sub(r\"Figure \\d+:.*?\\n\", \"\", text)\n",
    "    text = re.sub(r\"Table \\d+:.*?\\n\", \"\", text)\n",
    "\n",
    "    # Remove inline citations\n",
    "    text = re.sub(r\"\\(\\w+ et al., \\d{4}\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\(\\w+, \\d{4}\\)\", \"\", text)\n",
    "\n",
    "    # Remove footnotes\n",
    "    text = re.sub(r\"\\d+\\.\\s.*?\\n\", \"\", text)\n",
    "\n",
    "    # Normalize special characters and line breaks\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Return a new Document object with the cleaned text and original metadata\n",
    "    return Document(page_content=text, metadata=doc.metadata)\n",
    "\n",
    "cleaned_pdf_docs = []\n",
    "for doc in pdf_docs:\n",
    "    cleaned_pdf_docs.append(clean_pdf_content(doc))"
   ],
   "id": "4b0a64021d07fe61",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:05:05.499065Z",
     "start_time": "2025-09-08T21:05:05.492812Z"
    }
   },
   "cell_type": "code",
   "source": "cleaned_pdf_docs[0]",
   "id": "7454f8493c357dac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/tmp/tmp5wv6xw6u/tmp.pdf'}, page_content='Reinforcement Learning: A Survey Abstract This paper surveys the field of reinforcement learning from a computer-science per- spective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning. Reinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last five to ten years, it has attracted rapidly increasing interest in the machine learning and artificial intelligence communities. Its promise is beguiling—a way of programming agents by reward and punishment without needing to specify how the task is to be achieved. But there are formidable computational obstacles to fulfilling the promise. This paper surveys the historical basis of reinforcement learning and some of the current work from a computer science perspective. We give a high-level overview of the field and a taste of some specific approaches. It is, of course, impossible to mention all of the important work in the field; this should not be taken to be an exhaustive account. Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment. The work described here has a strong family resemblance to eponymous work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” It is appropriately thought of as a class of problems, rather than as a set of techniques. There are two main strategies for solving reinforcement-learning problems. The first is to search in the space of behaviors in order to find one that performs well in the environment. This approach has been taken by work in genetic algorithms and genetic programming, ©1996 AT Access Foundation and Morgan Kaufmann Publishers. All rights reserved. KAELBLING, LITTMAN, & Moore as well as some more novel search techniques . The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world. This paper is devoted almost entirely to the second set of techniques because they take advantage of the special structure of reinforcement-learning problems that is not available in optimization problems in general. It is not yet clear which set of approaches is best in which circumstances. The rest of this section is devoted to establishing notation and describing the basic reinforcement-learning model. Section 2 explains the trade-off between exploration and exploitation and presents some solutions to the most basic case of reinforcement-learning problems, in which we want to maximize the immediate reward. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them. Section 4 considers some classic model-free algorithms for reinforcement learning from delayed reward: adaptive heuristic critic, TD(A) and Q-learning. Section 5 demonstrates a continuum of algorithms that are sensitive to the amount of computation an agent can perform between actual steps of action in the environment. Generalization—the cornerstone of mainstream machine learning research—has the potential of considerably aiding reinforcement learning, as described in Section 1.1 Reinforcement-Learning Model In the standard reinforcement-learning model, an agent is connected to its environment via perception and action, as depicted in Figure 238 REINFORCEMENT LEARNING: A SURVEY Formally, the model consists of e a discrete set of environment states, S; e adiscrete set of agent actions, A; and e aset of scalar reinforcement signals; typically {0,1}, or the real numbers. The figure also includes an input function J, which determines how the agent views the environment state; we will assume that it is the identity function (that is, the agent perceives the exact state of the environment) until we consider partial observability in Section An intuitive way to understand the relation between the agent and its environment is with the following example dialogue. Environment: You are in state Agent: T\\'ll take action Environment: You received a reinforcement of 7 units. You are now in state Agent: T\\'ll take action Environment: You received a reinforcement of -4 units. You are now in state Agent: T\\'ll take action The agent’s job is to find a policy 7, mapping states to actions, that maximizes some long-run measure of reinforcement. We expect, in general, that the environment will be non-deterministic; that is, that taking the same action in the same state on two different occasions may result in different next states and/or different reinforcement values. This happens in our example above: from state 65, applying action 2 produces differing rein- forcements and differing states on two occasions. However, we assume the environment is stationary; that is, that the probabilities of making state transitions or receiving specific reinforcement signals do not change over time.! Reinforcement learning differs from the more widely studied problem of supervised learn- ing in several ways. The most important difference is that there is no presentation of in- put/output pairs. Instead, after choosing an action the agent is told the immediate reward and the subsequent state, but is not told which action would have been in its best long-term interests. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally. Another difference from supervised learning is that on-line performance is important: the evaluation of the system is often concurrent with learning. 239 KAELBLING, LITTMAN, & Moore Some aspects of reinforcement learning are closely related to search and planning issues in artificial intelligence. AI search algorithms generate a satisfactory trajectory through a graph of states. Planning operates in a similar manner, but typically within a construct with more complexity than a graph, in which states are represented by compositions of logical expressions instead of atomic symbols. These A] algorithms are less general than the reinforcement-learning methods, in that they require a predefined model of state transitions, and with a few exceptions assume determinism. On the other hand, reinforcement learning, at least in the kind of discrete cases for which theory has been developed, assumes that the entire state space can be enumerated and stored in memory—an assumption to which conventional search algorithms are not tied. 1.2 Models of Optimal Behavior Before we can start thinking about algorithms for learning to behave optimally, we have to decide what our model of optimality will be. In particular, we have to specify how the agent should take the future into account in the decisions it makes about how to behave now. There are three models that have been the subject of the majority of work in this area. The finite-horizon model is the easiest to think about; at a given moment in time, the agent should optimize its expected reward for the next h steps: h EQ ri) : t=0 it need not worry about what will happen after that. In this and subsequent expressions, r, represents the scalar reward received t steps into the future. This model can be used in two ways. In the first, the agent will have a non-stationary policy; that is, one that changes over time. On its first step it will take what is termed a h-step optimal action. This is defined to be the best action available given that it has h steps remaining in which to act and gain reinforcement. On the next step it will take a (h — 1)-step optimal action, and so on, until it finally takes a 1-step optimal action and terminates. In the second, the agent does receding-horizon control, in which it always takes the h-step optimal action. The agent always acts according to the same policy, but the value of h limits how far ahead it looks in choosing its actions. The finite-horizon model is not always appropriate. In many cases we may not know the precise length of the agent’s life in advance. The infinite-horizon discounted model takes the long-run reward of the agent into ac- count, but rewards that are received in the future are geometrically discounted according to discount factor +, (where 0 < 7 < 1): oo BD yr t=0 We can interpret y in several ways. It can be seen as an interest rate, a probability of living another step, or as a mathematical trick to bound the infinite sum. The model is conceptu- ally similar to receding-horizon control, but the discounted model is more mathematically tractable than the finite-horizon model. This is a dominant reason for the wide attention this model has received. 240 REINFORCEMENT LEARNING: A SURVEY Another optimality criterion is the average-reward model, in which the agent is supposed to take actions that optimize its long-run average reward: h lim BEY n) . hoo +=0 Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1 . One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount of reward in the initial phases and the other of whic does not. Reward gained on any initial prefix of the agent’s life is overshadowed by the long-run average performance. It is possible to generalize this model so that it takes into account both the long run average and the amount of initial reward than can be gained. In the generalized, bias optimal model, a policy is preferred if it maximizes the long-run average and ties are broken by the initial extra reward. Figure 2 contrasts these models of optimality by providing an environment in whic changing the model of optimality changes the optimal policy. In this example, circles represent the states of the environment and arrows are state transitions. There is only a single action choice from every state except the start state, which is in the upper left and marked with an incoming arrow. All rewards are zero except where marked. Under a finite-horizon model with h = 5, the three actions yield rewards of +6.0, +0.0, and +0.0, so the first action should be chosen; under an infinite-horizon discounted model with 7 = 0.9, the three choices yield +16.2, +59.0, and +58.5 so the second action should be chosen; and under the average reward model, the third action should be chosen since it leads to an average reward of + carefully in any application. The finite-horizon model is appropriate when the agent’s lifetime is known; one im- portant aspect of this model is that as the length of the remaining lifetime decreases, the agent’s policy may change. A system with a hard deadline would be appropriately modeled this way. The relative usefulness of infinite-horizon discounted and bias-optimal models is still under debate. Bias-optimality has the advantage of not requiring a discount parameter; however, algorithms for finding bias-optimal policies are not yet as well-understood as those for finding optimal infinite-horizon discounted policies. 1.38 Measuring Learning Performance The criteria given in the previous section can be used to assess the policies learned by a given algorithm. We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use. e Eventual convergence to optimal. Many algorithms come with a provable guar- antee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992). This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau 241 KAELBLING, LITTMAN, & Moore +2 Finite horizon, h=4 +10 Infinite horizon, y=0.9 OOO0-0-00\" Average reward at 99% of optimality may, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate. e Speed of convergence to optimality. Optimality is usually an asymptotic result, and so convergence speed is an ill-defined measure. More practical is the speed of convergence to near-optimality. This measure begs the definition of how near to optimality is sufficient. A related measure is level of performance after a given time, which similarly requires that someone define the given time. It should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accu- racy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework , there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance. That is usually an inappropriate view for an agent with a long existence in a complex environment. In spite of the mismatch between embedded reinforcement learning and the train/test perspective, Fiechter (1994) provides a PAC analysis for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views. Measures related to speed of learning have an additional weakness. An algorithm that merely tries to achieve optimality as fast as possible may incur unnecessarily large penalties during the learning period. A less aggressive strategy taking longer to achieve optimality, but gaining greater total reinforcement during its learning might be preferable. e Regret. A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret (Berry & Fristedt, 1985). It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain. 242 REINFORCEMENT LEARNING: A SURVEY 1.4 Reinforcement Learning and Adaptive Control Adaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algo- rithms for improving a sequence of decisions from experience. Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and ac- tions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory. A very common formulation of cost functions in adaptive control are quadratic penalties on deviation from desired state and action vectors. Most importantly, although the dynamic model of the system is not known in advance, and must be esti- mated from data, the structure of the dynamic model is fixed, leaving model estimation as a parameter estimation problem. These assumptions permit deep, elegant and powerful mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive control algorithms. One major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the roblems of exploration, we treat a very simple case in this section. The fundamental issues and approaches described here will, in many cases, transfer to the more complex instances of reinforcement learning discussed later in the paper. The simplest possible reinforcement-learning problem is known as the k-armed bandit problem, which has been the subject of a great deal of study in the statistics and applied mathematics literature (Berry & Fristedt, 1985). The agent is in a room with a collection of k gambling machines (each called a “one-armed bandit” in colloquial English). The agent is ermitted a fixed number of pulls, h. Any arm may be pulled on each turn. The machines do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal machine. When arm ? is pulled, machine 7 pays off 1 or 0, according to some underlying probability parameter p;, where payoffs are independent events and the p;s are unknown. What should the agent’s strategy be? This problem illustrates the fundamental tradeoff between exploitation and exploration. The agent might believe that a particular arm has a fairly high payoff probability; should it choose that arm all the time, or should it choose another one that it has less information about, but seems to be worse? Answers to these questions depend on how long the agent is expected to play the game; the longer the game lasts, the worse the consequences of prematurely converging on a sub-optimal arm, and the more the agent should explore. a There is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt (1985). We use the term “action” to indicate the agent’s choice of arm to pull. This eases the transition into delayed reinforcement models in Section Section 2.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case. 243 KAELBLING, LITTMAN, & Moore Section 2.2 presents three techniques that are not formally justified, but that have had wide use in practice, and can be applied (with similar lack of guarantee) to the general case. 2.1 Formally Justified Techniques There is a fairly well-developed formal theory of exploration for very simple problems. Although it is instructive, the methods it provides do not scale well to more complex problems. 2.1.1 DYNAMIC-PROGRAMMING APPROACH If the agent is going to be acting for a total of A steps, it can use basic Bayesian reasoning to solve for an optimal strategy (Berry & Fristedt, 1985). This requires an assumed prior joint distribution for the parameters {p;}, the most natural of which is that each p; is independently uniformly distributed between 0 and can be represented as a tabulation of action choices and payoffs: {n1, wi, na, W2,..., Mk, We} denotes a state of play in which each arm 7 has been pulled n; times with w; payoffs. We write V*(n1, wi,-.-,2k, WE) as the expected payoff remaining, given that a total of h pulls are available, and we use the remaining pulls optimally. If 0; nj; = h, then there are no remaining pulls, and V*(nj, wy,..., nz, we) = Ve (ny.wp..-.,npewp) = max; B Future payoff if agent takes action a, then acts optimally for remaining pulls = max; piV™ (ny, W;,---,2i +1, wie+1,---, MK. We)+ (1 = pi) V*(m1, Wi, ee ME 1, Wi, +. Me, WE) where p; is the posterior subjective probability of action 7 paying off given n;, w; and our prior probability. For the uniform priors, which result in a beta distribution, p; = The expense of filling in the table of V* values in this way for all attainable belief states is linear in the number of belief states times actions, and thus exponential in the horizon. 2.1.2 GITTINS ALLOCATION INDICES Gittins gives an “allocation index” method for finding the optimal choice of action at each step in k-armed bandit problems . The technique only applies under the discounted expected reward criterion. For each action, consider the number of times it has been chosen, n, versus the number of times it has paid off, w. For certain discount factors, there are published tables of “index values,” I(n,w) for each pair of n and w. Look up the index value for each action 7, I(nj,w;). It represents a comparative measure of the combined value of the expected payoff of action i (given its history of payoffs) and the value of the information that we would get by choosing it. Gittins has shown that choosing the action with the largest index value guarantees the optimal balance between exploration and exploitation. 244 REINFORCEMENT LEARNING: A SURVEY a=0 a=1 KL0+—O+—0 +++ O40 O90 + 0 0-00 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=1 a=0 a=1 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=0 Because of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward (Salganicoff & Ungar, 1995). Unfortunately, no one has yet been able to find an analog of index values for delayed reinforcement problems. 2.1.3 LEARNING AUTOMATA A branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar (1989), which were originally described explicitly as finite state automata. The Tsetlin automaton shown in Figure 3 provides an example that solves a 2-armed bandit arbitrarily near optimally as N approaches infinity. It is inconvenient to describe algorithms as finite-state automata, so a move was made to describe the internal state of the agent as a probability distribution according to which actions would be chosen. The probabilities of taking different actions would be adjusted according to their previous successes and failures. An example, which stands among a set of algorithms independently developed in the mathematical psychology literature (Hilgard & Bower, 1975), is the linear reward-inaction algorithm. Let p; be the agent’s probability of taking action e When action a; succeeds, Di t= pita(l—p) Pj (= py— ap; for 7 #2 e When action a; fails, p; remains unchanged (for all j). This algorithm converges with probability 1 to a vector containing a single 1 and the rest 0’s (choosing a particular action with probability 1). Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making a small (Narendra & Thathachar, 1974). There is no literature on the regret of this algorithm. 245 KAELBLING, LITTMAN, & Moore 2.2 Ad-Hoc Techniques In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun (1992) has surveyed a variety of these techniques. 2.2.1 GREEDY STRATEGIES The first strategy that comes to mind is to always choose the action with the highest esti- mated payoff. The flaw is that early unlucky sampling might indicate that the best action’s reward is less than the reward obtained from a suboptimal action. The suboptimal action will always be picked, leaving the true optimal action starved of data and its superiority never discovered. An agent must explore to ameliorate this outcome. A useful heuristic is optimism in the face of uncertainty in which actions are selected greedily, but strongly optimistic prior beliefs are put on their payoffs so that strong negative evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrar- ily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the ez- ploration bonus in Dyna , curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993). 2.2.2 RANDOMIZED STRATEGIES Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random. Some versions of this strategy start with a large value of p to encourage initial exploration, which is slowly decreased. An objection to the simple strategy is that when it experiments with a non-greedy action it is no more likely to try a promising alternative than a clearly hopeless alternative. A slightly more sophisticated strategy is Boltzmann exploration. In this case, the expected reward for taking action a, E.R(a) is used to choose an action probabilistically according to the distribution (ER(a)/T PO Sea PROT The temperature parameter T can be decreased over time to decrease exploration. This method works well if the best action is well separated from the others, but suffers somewhat when the values of the actions are close. It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care. 2.2.3 INTERVAL-BASED TECHNIQUES Exploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling’s interval estimation algorithm (1993b) stores statistics for each action a;: w; is the number of successes and n; the number of trials. An action is chosen by computing the upper bound of a 100-(1—a)% 246 REINFORCEMENT LEARNING: A SURVEY confidence interval on the success probability of each action and choosing the action with the highest upper bound. Smaller values of the a parameter encourage greater exploration. When payoffs are boolean, the normal approximation to the binomial distribution can be used to construct the confidence interval (though the binomial should be used for small n). Other payoff distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods (Box & Draper, 1987), which are used for comparing multiple treatments (for example, fertilizers or drugs) to determine which treatment (if any) is best in as small a set of experiments as possible. 2.3 More General Problems When there are multiple states, but reinforcement is still immediate, then any of the above solutions can be replicated, once for each state. However, when generalization is required, these solutions must be integrated with generalization methods (see section 6); this is straightforward for the simple ad-hoc methods, but it is not understood how to maintain theoretical guarantees. Many of these techniques focus on converging to some regime in which exploratory actions are taken rarely or never; this is appropriate when the environment is stationary. However, when the environment is non-stationary, exploration must continue to take place, in order to notice changes in the world. Again, the more ad-hoc techniques can be modified to deal with this in a plausible manner (keep temperature parameters from going to 0; decay the statistics in interval estimation), but none of the theoretically guaranteed methods can be applied. In the general case of the reinforcement learning problem, the agent’s actions determine not only its immediate reward, but also (at least probabilistically) the next state of the environment. Such environments can be thought of as networks of bandit problems, but the agent must take into account the next state as well as the immediate reward when it decides which action to take. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account. The agent will have to be able to learn from delayed reinforcement: it may take a long sequence of actions, receiving insignificant reinforcement, then finally arrive at a state with high reinforcement. The agent must be able to learn which of its actions are desirable based on reward that can take place arbitrarily far in the future. 3.1 Markov Decision Processes Problems with delayed reinforcement are well modeled as Markov decision processes (MDPs). An MDP consists of e@ aset of states S, e a set of actions A, 247 KAELBLING, LITTMAN, & Moore e a reward function R:S x A> ®R, and e astate transition function T : S x A — TI(S), where a member of II(S) is a probability istribution over the set S (i.e. it maps states to probabilities). We write T(s, a, s’) for the probability of making a transition from state s to state s’ using action a. The state transition function probabilistically specifies the next state of the environment as a function of its current state and the agent’s action. The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994). Although general MDPs may have infinite (even uncountable) state and action spaces, we will only discuss methods for solving finite-state and finite-action problems. In section 6, we discuss methods for solving problems with continuous input and output spaces. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in MDP environments, we will ex- plore techniques for determining the optimal policy given a correct model. These dynamic programming techniques will serve as the foundation and inspiration for the learning al- gorithms to follow. We restrict our attention mainly to finding optimal policies for the infinite-horizon discounted model, but most of these algorithms have analogs for the finite- horizon and average-case models as well. We rely on the result that, for the infinite-horizon discounted model, there exists an optimal deterministic stationary policy . We will speak of the optimal value of a state—it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy. Using 7 as a complete decision policy, it is written V*(s)= max E (>: on) t=0 This optimal value function is unique and can be defined as the solution to the simultaneous equations sles V*(s) = max (n a+7>> renner) WseS, (1) which assert that the value of a state s is the expected instantaneous reward plus the expected discounted value of the next state, using the best available action. Given the optimal value function, we can specify the optimal policy as x*(s) = argmax | R(s,a) +7 Ss T(s,a,8\\')V*(s\") “ ES 3.2.1 VALUE ITERATION One way, then, to find an optimal policy is to find the optimal value function. It can be determined by a simple iterative algorithm called value iteration that can be shown to converge to the correct V* values (Bellman, 1957; Bertsekas, 1987). 248 REINFORCEMENT LEARNING: A SURVEY initialize V(s) arbitrarily loop until policy good enough loop for s€S loop for aE A Q(s,4) = R(s,a) +7 Dyes Ts, 4, 8)V (6% V(s) := max, Q(s, a) end loop end loop It is not obvious when to stop the value iteration algorithm. One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function (Williams & Baird, 1993b). It says that if the maximum difference between two successive value functions is less than ¢, then the value of the greedy policy, he policy obtained by choosing, in every state, the action that maximizes the estimated discounted reward, using the current estimate of the value function) differs from the value function of the optimal policy by no more than 2ey/(1— 7) at any state. This provides an effective stopping criterion for the algorithm. Puterman (1994) discusses another stopping criterion, based on the span semi-norm, which may result in earlier termination. Another mportant result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged . And in practice, the greedy policy is often optimal long before the value function has converged. = Value iteration is very flexible. The assignments to V need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value of every state gets updated infinitely often on an infinite run. These issues are treated extensively by Bertsekas (1989), who also proves convergence results. Updates based on Equation 1 are known as full backups since they make use of infor- mation from all possible successor states. It can be shown that updates of the form Qs, a) = Qls,a) +a(r +7 maxQ(s!,a) — Q(s,a)) can also be used as long as each pairing of a and s is updated infinitely often, s’ is sampled from the distribution T(s, a, s’), r is sampled with mean R(s,a) and bounded variance, and the learning rate a is decreased slowly. This type of sample backup is critical to the operation of the model-free methods discussed in the next section. The computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions. Com- monly, the transition probabilities T(s, a, s’) are sparse. If there are on average a constant number of next states with non-zero probability then the cost per iteration is linear in the number of states and linear in the number of actions. The number of iterations required to reach the optimal value function is polynomial in the number of states and the magnitude of the largest reward if the discount factor is held constant. However, in the worst case the number of iterations grows polynomially in 1/(1— y), so the convergence rate slows considerably as the discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b). 249 KAELBLING, LITTMAN, & Moore 3.2.2 Poticy ITERATION The policy iteration algorithm manipulates the policy directly, rather than finding it indi- rectly via the optimal value function. It operates as follows: choose an arbitrary policy 7’ loop wisn compute the value function of policy 7: solve the linear equations V,(s) = R(s,7(s)) + ¥ Nores T(s, 7(8), 8)Vi(s’) improve the policy at each state: n\\'(s) := argmax, (R(s, a) + 7 Dees T(s, a, 8’) Vz(s\\')) until t= 7’ The value function of a policy is just the expected infinite discounted reward that will be gained, at each state, by executing that policy. It can be determined by solving a set of linear equations. Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. If it can, we change the policy to take the new action whenever it is in that situation. This step is guaranteed to strictly improve the performance of the policy. When no improvements are possible, then the policy is guaranteed to be optimal. Since there are at most |A||s| distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations (Puter- man, 1994). However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP (Littman et al., 1995b). 3.2.3 ENHANCEMENT TO VALUE ITERATION AND POLicy ITERATION In practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems. Puterman’s modified policy iteration algorithm (Puterman & Shin, 1978) provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of V,. Instead of finding an exact value for V,, we can perform a few steps of a modified value-iteration step where the policy is held fixed over successive iterations. This can be shown to produce an approximation to V, that converges linearly in Several standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution . State aggre- gation works by collapsing groups of states to a single meta-state solving the abstracted problem (Bertsekas & Castafion, 1989). 250 REINFORCEMENT LEARNING: A SURVEY 3.2.4 COMPUTATIONAL COMPLEXITY Value iteration works by producing successive approximations of the optimal value function. Each iteration can be performed in O(|A||S|?) steps, or faster if there is sparsity in the ransition function. However, the number of iterations required can grow exponentially in he discount factor ; as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of O(|A]|.$|?+|S|?) can be prohibitive. There is no known tight worst-case bound available or policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some ractictioners . Linear programming is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D’Epenoux, 1963; Hoffman & Karp, 1966). An advantage of this approach is that commercial-quality inear-programming packages are available, although the time and space requirements can still be quite high. From a theoretic perspective, linear programming is the only known algorithm that can solve MDPs in polynomial time, although the theoretically efficient algorithms have not been shown to be efficient in practice. In the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model. The model consists of knowledge of the state tran- sition probability function T(s, a, s’) and the reinforcement function R(s,a). Reinforcement learning is primarily concerned with how to obtain the optimal policy when such a model is not known in advance. The agent must interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy. At this point, there are two ways to proceed. e Model-free: Learn a controller without learning a model. e Model-based: Learn a model, and use it to derive a controller. Which approach is better? This is a matter of some debate in the reinforcement-learning community. A number of algorithms have been proposed on both sides. This question also appears in other fields, such as adaptive control, where the dichotomy is between direct and indirect adaptive control. This section examines model-free learning, and Section 5 examines model-based meth- ods. The biggest problem facing a reinforcement-learning agent is temporal credit assignment. How do we know whether the action just taken is a good one, when it might have far- reaching effects? One strategy is to wait until the “end” and reward the actions taken if the result was good and punish them if the result was bad. In ongoing tasks, it is difficult to know what the “end” is, and this might require a great deal of memory. Instead, we will use insights from value iteration to adjust the estimated value of a state based on 251 KAELBLING, LITTM. — 4 AN, & Moore T’4 the immediate reward and the estimated value is known as temporal difference methods (Sutt: temporal-difference learning strategies for the d 4.1 Adaptive Heuristic Critic and TD(\\\\) The adaptive heuristic critic algorithm is an ada Sutton, & Anderson, 1983) in which the value mented by solving a set of linear equations, but TD(0). A block nents: a critic (la reinforcement-lear rithms, modified acting to maximiz v, that is computed by the critic. The critic us learn to map states to their expected discounted is the one currently instantia We can see the analogy wi o deal with multiple states a: value function V, i new policy 7’ tha however, both components op: can be guaranteed to converge and Baird explored the convergence properties for that po maximizes the new value fun o the optimal pol call “incremental variants of iagram for this approach is gi beled AHC), and a reinforcemen ning component can be an instance of any of the k-armed bandit algo- e instantaneous reward, it will ed in the RL comp h modified policy i working in alternation. The policy 7 implemented by RL is fixed and t cy. Now we fix the critic and let the RL com olicy iteration” (Williams & Baird, 1993a). he adaptive heuristic critic. of the next state. This class of algorithms ‘on, 1988). We will consider two different iscounted infinite-horizon model. tive version of policy iteration (Barto, -function computation is no longer imple- is instead computed by an algorithm called iven in Figure be acting to maximize es val being executed eration if we imagine these components e critic learns the onent learn a plementations, ction, and so on. In most im erate simultaneously. Only the alternating implementation icy, under appropriate conditions. Williams of a class of AHC-related algorithms they It remains to explain how the critic can learn the value of a policy. We define (s, a, r,s’) to be an experience tuple summarizing a single t ransition in the environment. Here s is the agent’s state before the transition, a is its choice of action, r the instantaneous reward it receives, and s’ its resulting state. The value o algorithm which uses the update V(s): a policy is learned using Sutton’s TD(0) rule V(s) Whenever a state s is visited, its estimated val since r is the instantaneous reward received and occurring next state. This is analogous to the sa: VV (s\\')—V(s)) ue is updated to be closer to r+ yV(s‘), V(s’) is the estimated value of the actually mple-backup rule from value iteration—the only difference is that the sample is drawn from the real world rather than by simulating a known model. The key idea is that r + yV(s’ 252 is a sample of the value of V(s), and it is REINFORCEMENT LEARNING: A SURVEY more likely to be correct because it incorporates the real r. If the learning rate a is adjusted properly (it must be slowly decreased) and the policy is held fixed, TD(0) is guaranteed to converge to the optimal value function. The TD(0) rule as presented above is really an instance of a more general class of algorithms called TD(X), with 4 = V(u) = V(u) ta(rt+yV(s\\') —V(s)je(u) , but it is applied to every state according to its eligibility e(u), rather than just to the immediately previous state, s. One version of the eligibility trace is defined to be t : _ lifs=s e(s) = Say! *S sq , where 35,5, = { 0 otherwise k=1 The eligibility of a state s is the degree to which it has been visited in the recent past; when a reinforcement is received, it is used to update all the states that have been recently visited, according to their eligibility. When \\\\ = 0 this is equivalent to TD(0). When \\\\ = 1, it is roughly equivalent to updating all the states according to the number of times they were visited by the end of a run. Note that we can update the eligibility online as follows: e(s) i= yAe(s)+1 if s= current state yAe(s) otherwise It is computationally more expensive to execute the general TD(X), though it often converges considerably faster for large \\\\ (Dayan, 1992; Dayan & Sejnowski, 1994). There has been some recent work on making the updates more efficient (Cichosz & Mulawka, 1995) and on changing the definition to make T D(A) more consistent with the certainty-equivalent method (Singh & Sutton, 1996), which is discussed in Section 5.4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins’ Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation. Let Q*(s,a) be the expected discounted reinforcement of taking action ain state s, then continuing by choosing actions optimally. Note that V*(s) is the value of s assuming the best action is taken initially, and so V*(s) = max, Q*(s, a). Q*(s, a) can hence be written recursively as Q*(s,a) = R(s,a) +7 Ss T(s,a, 8’) max Q*(s’, a’). ES e Note also that, since V*(s) = max, Q*(s,a), we have x*(s) = argmax, Q*(s,a) as an optimal policy. Because the Q function makes the action explicit, we can estimate the Q values on- line using a method essentially the same as TD(0), but also use them to define the policy, 253 KAELBLING, LITTMAN, & Moore because an action can be chosen just by taking the one wit current state. The Q-learning rule is Qs.) = Qls,a) + alr +7 maxQ(s\\'a! where (s,a,r,s’) is an experie each state an infinite number of Q values will converge with probability 1 to Q* (Watkins, Jordan, & Singh, 1994). Q-le more than one step previously, When the Q values are ne the agent to act greedily, the maximum Q value for the — Q(s.4)) nce tuple as described earlier. If each action is executed in a is decayed appropriately, the 989; Tsitsiklis, 1994; Jaakkola, arning can also be extended to update states that occurred . as in TD(A) (Peng & Williams, 1994). y converged to their optimal values, it is appropriate for imes on an infinite run and ar. aking, in each situation, the action with the highest @ value. During learning, however, there is a difficult exploitation versus exploration trade-off to be made. There are no good, forma pt one of the standard practice is to ado AHC architectures seem to It can be hard to get components converge togethe is, that the Q values will con level. ly justified approaches to this problem in the general case; ad hoc methods discussed in section 2.ifficult to work with than Q-learning on a practical e relative learning rates right in AHC so that the two In addition, Q-learning is exploration insensitive: that verge to the optimal values, independent of how the agent be more t Tr. behaves while the data is being collected (as long as all state-action pairs are tried often enough). This means that, in Q-learning, the details learning algorithm. For t most effective model-free however, address any of oO ese go a. although the exploration-exploitation issue must be addressed the explora ion strategy will not affect the convergence of the -learning is the most popular and seems to be the earning from delayed reinforcement. It does not, reasons, Q rithm for he issues involved in generalizing over large state and/or action spaces. In addition, it may converge qui e slowly to a good policy. 4.3 Model-free Learning With Average Reward As described, Q-learning can be applied to discounted infinite-horizon MDPs. It can also be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset. Schwartz (1993) examine framework. Although his R-le some MDPs, severa problem they wish to solve th Q-learning (Mahade reward policies. Mahadevan ( a reinforcement-learning persp In particu (and some dynamic cies. Jaakkola, Jor researchers have found the average-reward van, 1994). With that in mind, researchers have studied the problem o ar, he showed that existing reinforcement-learning alg programming algorithms) do not always an and Singh (1995) described an average-reward learning algorithm the problem of adapting Q-learning to an average-reward arning algorithm seems to exhibit convergence problems for criterion closer to the true an a discounted criterion and therefore prefer R-learning to learning optimal average- 996) surveyed model-based average-reward algorithms from ective and found several difficulties with existing algorithms. orithms for average reward roduce bias-optimal poli- with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the expected uture reward for each state as the agent moves through the environment. In 254 REINFORCEMENT LEARNING: A SURVEY addition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new textbook (1995). Although this recent work provides a much needed theoretical foundation to this area of reinforcement learning, many important problems remain unsolved. The previous section showed how it is possible to learn an optimal policy without knowing the models T(s, a, s’) or R(s,a) and without even learning those models en route. Although many of these methods are guaranteed to find optimal policies eventually and use very little computation time per experience, they make extremely inefficient use of the data they gather and therefore often require a great deal of experience to achieve good performance. In this section we still begin by assuming that we don’t know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are especially important in applications in which computation is considered to be cheap and real-world experience costly. 5.1 Certainty Equivalent Methods We begin with the most conceptually straightforward method: first, learn the T and R functions by exploring the environment and keeping statistics about the results of each action; next, compute an optimal policy using one of the methods of Section There are some serious objections to this method: e It makes an arbitrary division between the learning phase and the acting phase. e How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data than a system that interleaves experience gathering with policy-building more tightly (Koenig & Simmons, 1993). See Figure 5 for an example. e The possibility of changes in the environment is also problematic. Breaking up an agent’s life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment changes. A variation on this idea is certainty equivalence, in which the model is learned continually through the agent’s lifetime and, at each step, the current model is used to compute an optimal policy and value function. This method makes very effective use of available data, but still ignores the question of exploration and is extremely computationally demanding, even for fairly small state spaces. Fortunately, there are a number of other model-based algorithms that are more practical. 5.2 Dyna Sutton’s Dyna architecture exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than 255 KAELBLING, LITTMAN, & Moore the certainty-equivalence approach. It simultaneously uses experience to build a model (T and R), uses experience to adjust the policy, and uses the model to adjust the policy. Dyna operates in a loop of interaction with the environment. Given an experience tuple (s,a,s\\',r), it behaves as follows: e Update the model, incrementing statistics for the transition from s to s’ on action a and for receiving reward r for taking action a in state s. The updated models are T and R. e Update the policy at state s based on the newly updated model using the rule Qls.a) = Risa) +7 Do F(s,0,8!) maxQls\\'.a’) . which is a version of the value-iteration update for Q values. e Perform k additional updates: choose k state-action pairs at random and update them according to the same rule as before: Q(sp, ag) :=R(sp, ag) + + oT (sn, ap, 8’) max Q(s\\', a’). e Choose an action a’ to perform in state s’, based on the Q values but perhaps modified by an exploration strategy. The Dyna algorithm requires about f times the computation of Q-learning per instance, but this is typically vastly less than for the naive model-based method. A reasonable value of & can be determined based on the relative speeds of computation and of taking action. Figure 6 shows a grid world in which in each cell the agent has four actions (N, S, E, W) and transitions are made deterministically to an adjacent cell, unless there is a block, in which case no movement occurs. As we will see in Table 1, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational effort, however. 256 REINFORCEMENT LEARNING: A SURVEY Steps before Backups before convergence convergence Q-learning 531,000 531,000 Dyna 62,000 3,055,000 prioritized sweeping 28,000 1,010,000 257 KAELBLING, LITTMAN, & Moore 5.3 Prioritized Sweeping / Queue-Dyna Although Dyna is a great improvement on previous methods, it suffers from being relatively undirected. It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the “interesting” parts of the state space. These problems are addressed by prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams, 1993), which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail. The algorithm is similar to Dyna, except that updates are no longer chosen at random and values are now associated with states (as in value iteration) instead of state-action pairs (as in Q-learning). To make appropriate choices, we must store additional information in the model. Each state remembers its predecessors: the states that have a non-zero transition probability to it under some action. In addition, each state has a priority, initially set to zero. Instead of updating & random state-action pairs, prioritized sweeping updates k states with the highest priority. For each high-priority state s, it works as follows: e Remember the current value of the state: Vou = V(s). e Update the state’s value V(s) = max (i. al+y>oT(s, 4, vie) 3! e Set the state’s priority back to e Compute the value change A = |V,7aq — V(s)|. Use A to modify the priorities of the predecessors of s. If we have updated the V value for state s’ and it has changed by amount A, then the immediate predecessors of s’ are informed of this event. Any state s for which there exists an action a such that T(s,a,s’) # 0 has its priority promoted to A - T(s,a,s’), unless its priority already exceeded that value. The global behavior of this algorithm is that when a real-world transition is “surprising” (the agent happens upon a goal state, for instance), then lots of computation is directed to propagate this new information back to relevant predecessor states. When the real- world transition is “boring” (the actual result is very similar to the predicted result), then computation continues in the most deserving part of the space. Running prioritized sweeping on the problem in Figure 6, we see a large improvement over Dyna. The optimal policy is reached in about half the number of steps of experience and one-third the computation as Dyna required (and therefore about 20 times fewer steps and twice the computational effort of Q-learning). 258 REINFORCEMENT LEARNING: A SURVEY 5.4 Other Model-Based Methods Methods proposed for based methods as well. RTDP (real-time model-based method t of the state-space that he agent is trying to By taking into accoun without necessarily vis’ solving MDPs given a model can be used in the context of model- ynamic programming) (Barto, Bradtke, & Singh, 1995) is another at uses Q-learning to concentrate computational effort on the areas the agent is most likely to occupy. It is specific to problems in which achieve a particular goal state and the reward everywhere else is the start state, it can find a short path from the start to the goal, iting the rest of the state space. The Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman, 994) exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent’s current state and the goal state, if there is one. States that are not in the envelope are summarized by a single “out” state. The planning process is an alternation between finding an optimal policy on the approximate MDP and to the envelope. Action may take place in parallel with planning, in states are also pruned out of the envelope. adding useful states which case irrelevan All of the previous discussion has tacitly assumed that it is possible to enumerate the state and action spaces and store tables of values over them. Except in very small environments, his means impractical memory requirements. It also makes inefficient use of experience. In a large, smooth state space we generally expect similar states to have similar values and sim- actions. Surely, therefore, there should be some more compact representation han a table. Most problems will have continuous or large discrete state spaces; some wil have large or continuous action spaces. The problem of learning in large spaces is addresse: hrough generalization techniques, which allow compact storage of learned information an er of knowledge between “s: ilar optima. rans imilar” states and actions. The large literature of genera. ues from inductive concept learning can be applied to reinforcement learning niques often need to be tailored to specific details of the problem. In the following sections, we explore the application of standar unction-approximation techniques, adaptive resolution models, and hierarchical methods o the problem of reinforcement learning. T he s ization techni . However, tec. p D. he reinforcement-learning architectures and algorithms discussed above have include orage of a variety of mappings, including S — A (policies), S > R (value functions), Sx A— * (Q functions and rewards), S x A > S (deterministic transitions), and S x Ax § => [0,1] (transition probabilities). Some of these mappings, such as transitions an immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervise learning that support noisy training examples. Popular techniques include various neural- network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMAC , and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods. Other mappings, especially the policy 259 KAELBLING, LITTMAN, & Moore mapping, typically need specialized algorithms because training sets of input-output pairs are not available. 6.1 Generalization over Input A reinforcement-learning agent’s current state plays a central role in its selection of reward- maximizing actions. Viewing the agent as a state-free black box, a description of the current state is its input. Depending on the agent architecture, its output is either an action selection, or an evaluation of the current state that can be used to select an action. The problem of deciding how the different aspects of an input affect the value of the output is sometimes called the “structural credit-assignment” problem. This section examines approaches to generating actions or evaluations as a function of a description of the agent’s current state. The first group of techniques covered here is specialized to the case when reward is not delayed; the second group is more generally applicable. 6.1.1 IMMEDIATE REWARD When the agent’s actions do not influence state transitions, the resulting problem becomes one of choosing actions to maximize immediate reward as a function of the agent’s current state. These problems bear a resemblance to the bandit problems discussed in Section 2 except that the agent should condition its action selection on the current state. For this reason, this class of problems has been described as associative reinforcement learning. The algorithms in this section address the problem of learning from immediate boolean reinforcement where the state is vector valued and the action is a boolean vector. Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4. CRBP_ Thecomplementary reinforcement backpropagation algorithm (Ackley & Littman, 1990) (CRBP) consists of a feed-forward network mapping an encoding of the state to an encoding of the action. The action is determined probabilistically from the activation of the output units: if output unit ¢ has activation y;, then bit ¢ of the action vector has value 1 with probability y;, and 0 otherwise. Any neural-network supervised training procedure can be used to adapt the network as follows. If the result of generating action a is r = 1, then the network is trained with input-output pair (s,a). If the result is r = 0, then the network is trained with input-output pair (s,@), where @ = (1— a,...,1— ay). The idea behind this training rule is that whenever an action fails to generate reward, CRBP will try to generate an action that is different from the current choice. Although it seems like the algorithm might oscillate between an action and its complement, that does not happen. One step of training a network will only change the action slightly and since the output probabilities will tend to move toward 0.5, this makes action selection more random and increases search. The hope is that the random distribution will generate an action that works better, and then that action will be reinforced. ARC The associative reinforcement comparison (ARC) algorithm is an instance of the AHc architecture for the case of boolean actions, consisting of two feed- 260 REINFORCEMENT LEARNING: A SURVEY forward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units. In the simplest case, the entire system learns only to optimize immediate reward. First, let us consider the behavior of the network that learns the policy, a mapping from a vector describing s to a Q or The adjustment for the output unit is, in the simplest case, e=r(a—1/2) , where the first factor is the reward received for taking the most recent action and the second encodes which action was taken. The actions are encoded as 0 and 1, so a—1/2 always has the same magnitude; if the reward and the action have the same sign, then action 1 will be made more likely, otherwise action 0 will be. As described, the network will tend to seek actions that given positive reward. To extend this approach to maximize reward, we can compare the reward to some baseline, b. This changes the adjustment to © =(r—d)a~1/2) | where 6 is the output of the second network. The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications (Anderson, 1986; Barto et al., 1983; Lin, 1993b; Sutton, 1984). REINFORCE Algorithms Williams studied the problem of choosing ac- tions to maximize immedate reward. He identified a broad class of update rules that per- form gradient descent on the expected reward and showed how to integrate these rules with backpropagation. This class, called REINFORCE algorithms, includes linear reward-inaction (Section 2.1.3) as a special case. The generic REINFORCE update for a parameter w;; can be written Aw = a4j(r - badger Ina) where a;; is a non-negative factor, r the current reinforcement, 6;; a reinforcement baseline, and g; is the probability density function used to randomly generate actions based on unit activations. Both a;; and b;; can take on different values for each w;;, however, when a;; is constant throughout the system, the expected update is exactly in the direction of the expected reward gradient. Otherwise, the update is in the same half space as the gradient but not necessarily in the direction of steepest increase. Williams points out that the choice of baseline, 6 convergence speed of the algorithm. ij, can have a profound effect on the Logic-Based Methods Another strategy for generalization in reinforcement learning is to reduce the learning problem to an associative problem of learning boolean functions. A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive 261 KAELBLING, LITTMAN, & Moore the generalization process (Kaelbling, 1994b); the other searches the space of syntactic descriptions of functions using a simple generate-and-test method (Kaelbling, 1994a). The restriction to a single boolean output makes these techniques difficult to apply. In very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output. In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The CASCADE method (Kaelbling, 1993b) allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort. 6.1.2 DELAYED REWARD Another method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning. Here, a function approximator is used o represent the value function by mapping a state description to a value. Many reseachers have experimented with this approach: Boyan and Moore (1995) used local memory-based methods in conjunction with value iteration; Lin (1991) used backprop- agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992, 995) used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich (1995) used backpropagation and T D(A) to learn good strategies for job-shop scheduling. Although there have been some positive examples, in general there are unfortunate in- eractions between function approximation and the learning rules. In discrete environments here is a guarantee that any operation that updates the value function (according to the Bellman equations) can only reduce the error between the current value function and the optimal value function. This guarantee no longer holds when generalization is used. These issues are discussed by Boyan and Moore (1995), who give some simple examples of value unction errors growing arbitrarily large when generalization is used with value iteration. Their solution to this, applicable only to certain classes of problems, discourages such diver- gence by only permitting updates whose estimated values can be shown to be near-optimal via a battery of Monte-Carlo experiments. Thrun and Schwartz (1993) theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the “max” operator in the definition of the value function. Several recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show how the appro- priate choice of function approximator can guarantee convergence, though not necessarily to the optimal values. Baird’s residual gradient technique provides guaranteed convergence to locally optimal solutions. Perhaps the gloominess of these counter-examples is misplaced. Boyan and Moore (1995) report that their counter-examples can be made to work with problem-specific hand-tuning despite the unreliability of untuned algorithms that provably converge in discrete domains. Sutton (1996) shows how modified versions of Boyan and Moore’s examples can converge successfully. An open question is whether general principles, ideally supported by theory, can help us understand when value function approximation will succeed. In Sutton’s com- 262 REINFORCEMENT LEARNING : A SURVEY parative experiments with Boyan and Moore’s counter-examples, he changes four aspects of the experiments: the task specifications. generalization. iteration. There are intuitive reasons to believe that the fourth factor is more careful research is needed. Adaptive Resolution Models the environment into regions of states that can be considered t: learning and generating actions. Without detailed is very difficult to know what granularity or placement of par use adaptive resolution; during the course of learning, artition is constructed that is appropriate to the problem is overcome in a Decision Trees va. In environments ued variables, it is possible to learn compact decision trees for representing Q values. The methods tha hat are charac es uniformly in state space, ories. articularly important, but In many cases, what we would like to do is partition e same for the purposes of rior knowledge of the environment, it itions is appropriate. This environment. erized by a set of boolean or discrete- works as fol G-learning algorithm (Chapman & Kk hat no partitioning is necessary an if it were one state. In parallel with input bits: it asks the question whe states in which 6 = aelbling, 1991), tries to learn this process, i her there is some bit 6 in hat the Q values for states in which 6 = 1 are significantly is found, it is used to spl Q values for gathers statistics based on individua e leaves. This method was able to learn very smal ows. It starts by assuming the entire environment as the state description such ifferent from Q values for it the decision tree. Then ; re game environment and or dealing with partial cannot, however, acqui (such as those needed Variable Resolution enables conventional resentations of the Q function in noisy state attributes. It outperformed Q-learning with backpro was used by McCallum (1995 observability re partitions in which attribu o solve parity problems). Dynamic Programming ynamic programming to be he presence of an overwhel to learn behaviors in a complex driving-simulator. I ming number of irrelevant, agation in a simple video- (in conjunction with other techniques es are only significant in combination The VRDP algorithm (Moore, 1991 performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimension- ality. A kd-tree (simi regions. The coarse regions are refined into detailed space which are predic ning “mental trajectories” through state space. This algorithm ed to be important. This no of problems for which disadvantage of requiri ull high-resolution arrays wo ng a guess at an initially vali 263 ar to a decision tree) is used to parti tion state space into coarse regions, but only in parts of the state ion of importance is obtained by run- proved effective on a number uld have been impractical. It has the trajectory through state-space. (a) KAELBLING, LITTMAN, & Moore (b) (c) Start Goal NF i i inal fo EE Bs The point robot must find a path from y of the barrier lines. (b) The path taken by rial. It begins with intense exploration to find a blem. route out of the almost entirely enclosed start region. Having eventually reached a sufficiently high resolution, it discovers the gap and proceeds gree ily towards the goal, only to be temporarily blocked by the goal’s barrier region. (c) The second trial. PartiGame Algorithm Moore’s PartiGame algorithm is another solution to the problem of learning to achieve goal configurations in deterministic high-dimensional continuous s aces by learning an adaptive-resolution model. It also divides the environment into cells; but in each cell, the actions available consist of aiming at the neighboring cells (this aiming problem sta incremental is accomplished by a local controller, which must be provided as ement). The graph of cell transitions is solved for shortest paths in an online manner, but a minimax criterion is used to detect when a group of cells is art of the too coarse to prevent movement between obstacles or to avoid limit cycles. The offending cells are spli choose appropria An important fea it also struc the agent wi small local c ures 1 ini ang to higher resolution. Eventually, the environment is divided up jus e actions for ach exploration of s ially try someth: es when all the Figure 7a shows a two-dimens\\' of a robot using second trial, star This is a very than a minute. T limits its applica methods. ed from a slight fast algorithm, | e restriction of bility, however. ture is that, as well as reducing memory and computational re enough to ieving the goal, but no unnecessary distinctions are made. uirements, ate space in a multi-resolution manner. Given a failure, ing very different to rectify the failure, and only resort to ualitatively different strategies have been exhausted. ional continuous maze. Figure 7b shows the performance he PartiGame algorithm during the very first trial. Figure 7c shows the y different position. earning policies in spaces of up to nine dimensions in less he current implementation to deterministic environments McCallum (1995) suggests some related tree-structured 264 REINFORCEMENT LEARNING: A SURVEY 6.2 Generalization over Actions The networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well. In cases such as this when actions are described combinatorially, it is important to generalize over actions to avoid keeping separate statistics for the huge number of actions that can be chosen. In continuous action spaces, the need for generalization is even more pronounced. When estimating Q values using a neural network, it is possible to use either a distinct network for each action, or a network with a distinct output for each action. When the action space is continuous, neither approach is possible. An alternative strategy is to use a single network with both the state and action as input and Q value as the output. Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value (Baird & Klopf, 1993). Gullapalli has developed a “neural” reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience. When the chosen actions are not performing well, the variance is high, resulting in exploration of the range of choices. When an action performs well, the mean is moved in that direction and the variance decreased, resulting in a tendency to generate more action values near the successful one. This method was successfully employed to learn to control a robot arm with many continuous degrees of freedom. 6.3 Hierarchical Methods Another strategy for dealing with large state spaces is to treat them as a hierarchy of learning problems. In many cases, hierarchical solutions introduce slight sub-optimality in performance, but potentially gain a good deal of efficiency in execution time, learning time, and space. Hierarchical learners are commonly structured as gated behaviors, as shown in Figure 6.3.1 FEUDAL Q-LEARNING Feudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external environment. Its actions consist of commands that 265 KAELBLING, LITTMAN, & Moore it can give to the low-level learner. When the master generates a particular command to the slave, it must reward the slave for taking actions that satisfy the command, even if they do not result in external reinforcement. The master, then, learns a mapping from states to commands. The slave learns a mapping from commands and states to external actions. The set of “commands” and their associated reinforcement functions are established in advance of the learning. This is really an instance of the general “gated behaviors” approach, in which the slave can execute any of the behaviors depending on its command. The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels. 6.3.2 COMPOSITIONAL Q-LEARNING Singh’s compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of condi- tions in sequential order. The achievement of the conditions provides reinforcement for the elemental tasks, which are trained first to achieve individual subgoals. Then, the gating function learns to switch the elemental tasks in order to achieve the appropriate high-level sequential goal. This method was used by Tham and Prager (1994) to learn to control a simulated multi-link robot arm. 6.3.3 HIBRARCHICAL DISTANCE TO GOAL Especially if we consider reinforcement learning modules to be part of larger agent archi- tectures, it is important to consider problems in which goals are dynamically input to the learner. Kaelbling’s HDG algorithm (1993a) uses a hierarchical approach to solving prob- lems when goals of achievement (the agent should get to a particular state as quickly as possible) are given to an agent dynamically. The HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned (a priori, but more recent work addresses the case of learning the partition) into a set of regions whose centers are known as “landmarks.” If the agent is 266 REINFORCEMENT LEARNING: A SURVEY office currently in the same region as the goal, then it uses low-level actions to move to the goal. If not, then high-level information is used to determine the next landmark on the shortest path from the agent’s closest landmark to the goal’s closest landmark. Then, the agent uses low-level information to aim toward that next landmark. If errors in action cause deviations in the path, there is no problem; the best aiming point is recomputed on every step. In many real-world environments, it will not be possible for the agent to have perfect and complete perception of the state of the environment. Unfortunately, complete observability is necessary for learning methods based on MDPs. In this section, we consider the case in which the agent makes observations of the state of the environment, but these observations may be noisy and provide incomplete information. In the case of a robot, for instance, it might observe whether it is in a corridor, an open room, a T-junction, etc., and those observations might be error-prone. This problem is also referred to as the problem of “incomplete perception,” “perceptual aliasing,” or “hidden state.” In this section, we will consider extensions to the basic MDP framework for solving partially observable problems. The resulting formal model is called a partially observable Markov decision process or POMDP. 7.1 State-Free Deterministic Policies The most naive strategy for dealing with partial observability is to ignore it. That is, to reat the observations as if they were the states of the environment and try to learn to behave. Figure 9 shows a simple environment in which the agent is attempting to get to he printer from an office. If it moves from the office, there is a good chance that the agent will end up in one of two places that look like “hall”, but that require different actions for getting to the printer. If we consider these states to be the same, then the agent cannot ossibly behave optimally. But how well can it do? The resulting problem is not Markovian, and Q-learning cannot be guaranteed to con- verge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate (Chrisman & 267 KAELBLING, LITTMAN, & Moore Littman, 1993). It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations. Unfortunately, when the environment is not Markovian, the transition probabilities depend on the policy being executed, so this new policy will induce a new set of transition probabilities. This approach may yield plausible results in some cases, but again, there are no guarantees. It is reasonable, though, to ask what the optimal policy (mapping from observations to actions, in this case) is. It is NP-hard (Littman, 1994b) to find this mapping, and even the best mapping can have very poor performance. In the case of our agent trying to get to the printer, for instance, any deterministic state-free policy takes an infinite number of steps to reach the goal on average. 7.2 State-Free Stochastic Policies Some improvement can be gained by considering stochastic policies; these are mappings from observations to probability distributions over actions. If there is randomness in the agent’s actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan (1995) have developed an algorithm for finding locally-optimal stochastic policies, but finding a globally optimal policy is still NP hard. In our example, it turns out that the optimal stochastic policy is for the agent, when in a state that looks like a hall, to go east with probability 2 — /2 % 0.6 and west with probability /2 —1 % 0. 7.3 Policies with Internal State The only way to behave truly effectively in a wide-range of environments is to use memory of previous actions and observations to disambiguate the current state. There are a variety of approaches to learning policies with internal state. Recurrent Q-learning One intuitively simple approach is to use a recurrent neural net- work to learn Q values. The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain “history features” to predict value. This approach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin & Mitchell, 1992; Schmidhuber, 1991b). It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems. Classifier Systems Classifier systems (Holland, 1975; Goldberg, 1989) were explicitly developed to solve problems with delayed reward, including those requiring short-term memory. The internal mechanism typically used to pass reward back through chains of decisions, called the bucket brigade algorithm, bears a close resemblance to Q-learning. In spite of some early successes, the original design does not appear to handle partially ob- served environments robustly. Recently, this approach has been reexamined using insights from the reinforcement- learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems (Dorigo & Bersini, 1994). Cliff and Ross (1994) start with Wilson’s zeroth- 268 REINFORCEMENT LEARNING: A SURVEY evel classifier system and add one and two-bit memory registers. They find hat, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments. Dorigo and Colombetti applied classifier systems to a moderately complex problem of learning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti, 994). Finite-history-window Approach One way to restore the Markov property is to allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell (1992) used a fixed-width finite history window to learn a pole balancing task. McCallum (1995) describes the “utile suffix memory” which learns a variable-width window hat serves simultaneously as a model of the environment and a finite-memory policy. This system has had excellent results in a very complex driving-simulation domain (McCallum, 995). Ring (1994) has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations. POMDP Approach Another strategy consists of using hidden Markov model (HMM) echniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Monahan, 1982). Chrisman (1992) showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum (1993), also gave heuristic state- splitting rules to attempt to learn the smallest possible model for a given environment. The resulting model can then be used to integrate information from the agent’s observations in order to make decisions. Figure 10 illustrates the basic structure for a perfect-memory controller. The component on the left is the state estimator, which computes the agent’s belief state, b as a function of the old belief state, the last action a, and the current observation Now we are left with the problem of finding a policy mapping belief states into action. This problem can be formulated as an MDP, but it is difficult to solve using the techniques described earlier, because the input space is continuous. Chrisman’s approach (1992) does not take into account future uncertainty, but yields a policy after a small amount of com- putation. A standard approach from the operations-research literature is to solve for the 269 KAELBLING, LITTMAN, & Moore optimal policy (or a close approximation thereof) based on its representation as a piecewise- linear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations (Cassandra et al., 1994; Littman, Cassandra, & Kaelbling, 1995a). One reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act. But it is unsurprising that it has also been used by a number of researchers as a practical computational tool for constructing autonomous systems that improve themselves with experience. These applications have ranged from robotics, to industrial manufacturing, to combinatorial search problems such as computer game playing. Practical applications provide a test of the efficacy and usefulness of learning algorithms. They are also an inspiration for deciding which components of the reinforcement learning framework are of practical importance. For example, a researcher with a real robotic task can provide a data point to questions such as: e How important is optimal exploration? Can we break the learning period into explo- ration phases and exploitation phases? e What is the most useful model of long-term reward: Finite horizon? Discounted? Infinite horizon? e How much computation is available between agent decisions and how should it be used? e What prior knowledge can we build into the system, and which algorithms are capable of using that knowledge? Let us examine a set of practical applications of reinforcement learning, while bearing these questions in mind. 8.1 Game Playing Game playing has dominated the Artificial Intelligence world as a problem domain ever since he field was born. Two-player games do not fit into the established reinforcement-learning ramework since the optimality criterion for games is not one of maximizing reward in the ace of a fixed environment, but one of maximizing reward against an optimal adversary (minimax). Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games (Littman, 1994a) and many researchers have used reinforcement earning in these environments. One application, spectacularly far ahead of its time, was Samuel’s checkers playing system . This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning. More recently, Tesauro (1992, 1994, 1995) applied the temporal difference algorithm o backgammon. Backgammon has approximately 107° states, making table-based rein- orcement learning impossible. Instead, Tesauro used a backpropagation-based three-layer 270 REINFORCEMENT LEARNING: A SURVEY Training Hidden Results Games Units Basic Poor TD 1.0 300,000 80 Lost by 13 points in 51 games TD 2.0 800,000 40 Lost by 7 points in 38 games TD 2.1 1,500,000 80 Lost by 1 point in 40 games games against the top human professional players. A backgammon tournament involves playing a series of games for points until one player reaches a set target. TD-Gammon won none of these tournaments but came sufficiently close that it is now considered one of the best few players in the world. neural network as a function approximator for the value function Board Position > Probability of victory for current player. Two versions of the learning algorithm were used. The first, which we will call Basic TD- Gammon, used very little predefined knowledge of the game, and the representation of a board position was virtually a raw encoding, sufficiently powerful only to permit the neural network to distinguish between conceptually different positions. The second, TD-Gammon, was provided with the same raw state crafted features of backgammon board information supplemented by a number of hand- positions. Providing hand-crafted features in this manner is a good example of how inductive biases from human knowledge of the task can be supplied to a learning algorithm. The training of both learning algorit was achieved by constant self-play. No greedily chose the move with the larges ms required several months of computer time, and exploration strategy was used—the system always expected probability of victory. This naive explo- ration strategy proved entirely adequate for this environment, which is perhaps surprising given the considerable work in the reinforcement-learning literature which has produced numerous counter-examples to show that greedy exploration can lead to poor learning per- formance. Backgammon, however, has is followed, every game is guaranteed information is obtained fairly frequent’ wo important properties. Firstly, whatever policy o end in finite time, meaning that useful reward y. Secondly, the state transitions are sufficiently stochastic that independent of the policy, all states will occasionally be visited—a wrong initial value function has little danger of starving us from visiting a critical part of state space from which important information could be obtained. The results (Table 2) of TD-Gammon are impressive. It has competed at the very top level of international human play. Basic TD-Gammon played respectably, but not at a professional standard. 271 Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess . It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 8.2 Robotics and Control In recent years there have been many robotics and control applications that have used reinforcement learning. Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway. The juggling robot learned a world model from experience, which was generalize o unvisited states by a function approximation scheme known as locally weighte regression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). Between each trial, a form of dynamic programming specific to linear control policies and locally linear ransitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design (Sage & White, 1977). 272 REINFORCEMENT LEARNING: A SURVEY earned. ical reinforcement learning, an unthinka nals called progress estimators were use This was achieved in a robust manner he estimators, but had the freedom to Secondly, control was decentralized. Eac. without explicit communication with the ot rofi quantized into a small number of discre of the Q-learned policies were almost as he job. hree examples: goo bly high dimensional in which the robots from the induc ive bias h robot learned its own policy ers. Thirdly, s as a simple hand-crafte ching task (Crites & Bar boxes for extended periods of time. Box-pushing is a well-known difficult robotics roblem, characterized by immense uncertainty in the results of actions. Q-learning was used in conjunction with some novel clustering techniques designed to enable a higher-dimensional input than a tabular approach would have permitted. The robot earned to perform competitively with the performance of a human-programmed so- ution. Another aspect of this work, mentioned in Section 6.3, was a breakdown of the monolithic task description into a set of lower level tasks to be pre-programmed e viewpoint of theoret- state space, containing many dozens of degrees of freedom. Four mobile robots traveled within an enclo- sure collecting small disks and transporting them to a destination region. There were hree enhancements to the basic Q-learning algorithm. Firstly, pre-programmed sig- to break the monolithic task into subtasks. were not forced to use hey provided. independently ate space was brutally e states according to values o ber of pre-programmed boolean features of the underlying sensors. The performance a small num- controller for 0, 1996). The works for function approximation and ess than the best alternative algorithm reinforcement learning by one of the ble numbers of non-identical products. e The mean weight of all containers produced by a shift must not be below the manufacturer’s declared weight W. 273 KAELBLING, LI TTMAN, & Moore e The number of containers below the declared weight must be less than P%. e No containers may be produced below weight W’. Such tasks are controlled by machinery which operates according to various setpoints. Conventional practice is that setpoints are chosen by human operators, but this choice is not easy as it is task constraints. The task was posed ependent on the The dependency is current product characteristics and the current often difficult to model and highly non-linear. as a finite-horizon Markov decision task in which the state of the system is a function of the product characteristics, the amount of time remaining in the production so far. The system regression was ing was used to ma information was ob typically with wast. deployed successful Some interesting aspects of practical rein examples. The mos necessary to supplement plying extra knowledge comes a Sup the system is subse these, a knowledge-fr the finite lifetime of What forms did linearity for the jugg shif was discretized i use intain an optimal age reduced by a y in several factor: striking is that in al he fundamental al uently less aw ee approach woul. he robots. his pre-programmed ing robot’s policy, no the two mobile-robo the @ values which assumed loca ionally used a manual dimensions and so required correspon addi sumption of local pie in the amoun T ysis of lear e exploration s o judge were able to plora T strategies mir yet all prove Finally, it They where ion. were al The earn well wit e packaging task use rors theoretically op adequate. is also worth considering the compu very different, which indicates that t various reinforcement learning algorithms do indeed hav juggler needed to make very fast decisions with had long periods (30 seconds and more) between each he consis examples, while ly € y discr ing and the mean was ained. In simulate a price: onomous. ized state space. T age and percent below declared in the shift nto 200,000 discrete states and local weighted to learn and generalize a transition model. Prioritized sweep- value function as each new piece of transition experiments the savings were considerable, actor of ten. Since then the system has been ies within the United States. orcement learning come to light from these cases, to make a real system work it proved gorithm with extra pre-programmed knowledge. more human effort and insight is required and But it is also clear that for tasks such as have achieved worthwhile performance within knowledge take? It included an assumption of a manual breaking up of the task into subtasks for box-pusher also used a clustering technique for tent @ values. The four disk-collecting robots e packaging example had far fewer y weaker assumptions, but there, too, the as- cewise continui ning yin t ata required. trategies are inter to profitably gree ex te) imal (bu y esting too. T experiment. loration—alway: ptimism in t e transition model enabled massive reductions e juggler used careful statistical anal- However, both mobile robot applications 8 exploiting without deliberate ex- e face of uncertainty. None of these t computationally intractable) exploration, and of these experiments. utational demands of ational regimes iffering com ean array of differing applications. ow latency between each hit, but rial to consolidate the experiences e collected on the previous trial and to perform the more aggressive computation necessary to produce a new reactive controller on the next trial. T e box-pushing robot was meant to 274 REINFORCEMENT LEARNING: A SURVEY operate autonomously for hours and so had to make decisions with a uniform length control cycle. The cycle was sufficiently long for quite substantial computations beyond simple Q- learning backups. The four disk-collecting robots were particularly interesting. Each robot had a short life of less than 20 minutes (due to battery constraints) meaning that substantial number crunching was impractical, and any significant combinatorial search would have used a significant fraction of the robot’s learning lifetime. The packaging task had easy constraints. One decision was needed every few minutes. This provided opportunities for fully computing the optimal value function for the 200,000-state system between every control cycle, in addition to performing massive cross-validation-based optimization of the transition model being learned. A great deal of further work is currently in progress on practical implementations of reinforcement learning. The insights and task constraints that they produce will have an important effect on shaping the kind of algorithms that are developed in future. There are a variety of reinforcement-learning techniques that work effectively on a variety of small problems. But very few of these techniques scale well to larger problems. This is not because researchers have done a bad job of inventing learning techniques, but because it is very difficult to solve arbitrary problems in the general case. In order to solve highly complex problems, we must give up tabula rasa learning techniques and begin to incorporate bias that will give leverage to the learning process. The necessary bias can come in a variety of forms, including the following: shaping: The technique of shaping is used in training animals (Hilgard & Bower, 1975); a teacher presents very simple problems to solve first, then gradually exposes the learner to more complex problems. Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up , and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995). local reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local. In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly . imitation: An agent can learn by “watching” another agent perform the task . For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel . problem decomposition: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very power- ful technique for biasing learning. Most interesting examples of robotic reinforcement learning employ this technique to some extent (Connell & Mahadevan, 1993). reflexes: One thing that keeps agents that know nothing from learning anything is that they have a hard time even finding the interesting parts of the space; they wander 275 lea: KAELBLING, LITTMAN, & Moore around at random never getting near the goal, or they are always “killed” immediately. These problems can be ameliorated by programming a set of “reflexes” that cause the agent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto, Grupen, & Connolly, 1994). These reflexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn. Recent work by Millan (1996) explores the use of reflexes to make robot learning safer and more efficient. With appropriate biases, supplied by human programmers or teachers, complex reinforcement- rning problems will eventually be solvable. There is still much work to be done and many interesting questions remaining for learning techniques and especially regarding methods for approximating, decomposing, and incorporating bias into problems. Acknowledgements T to anks to Marco Dorigo and three anonymous reviewers for comments that have helped improve this paper. Also thanks to our many colleagues in the reinforcement-learning community who have done this work and explained it to us. 93 in Leslie Pack Kaelbling was supported in part by NSF grants IRI-9453383 and IRI')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:05:10.228825Z",
     "start_time": "2025-09-08T21:05:10.224210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine all documents\n",
    "all_documents = cleaned_blog_docs + cleaned_pdf_docs\n",
    "\n",
    "# Print summary to verify\n",
    "print(f\"Loaded {len(all_documents)} documents.\")"
   ],
   "id": "5a9cb8bb1a0bc5a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 documents.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:05:12.747624Z",
     "start_time": "2025-09-08T21:05:12.734203Z"
    }
   },
   "cell_type": "code",
   "source": "all_documents",
   "id": "348b47f1478958cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1', 'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium', 'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …', 'language': 'en'}, page_content='Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | MediumReinforcement Learning: An introduction (Part 1/4)Cédric Vandelaer10 min read·Aug 20, 2022--2Hi and welcome to the first part of a series on Reinforcement Learning.If you somehow ended up here without having heard of Reinforcement Learning (RL) before, then let me summarize it as follows: “RL is a general framework for training an artificial intelligence model to solve a certain task or goal” … or in layman’s , we make AI do cool things!The goal of this series is to learn RL and simultaneously explore some of the more recent research later on. We will start from the very basics and work our way towards more advanced topics. Even if you have almost no prior programming and/or mathematics knowledge, you should be able to follow along pretty smoothly.The first mini-series will be split into four parts:Part 1: What is Reinforcement learning?Part 2: RL terminology and formal conceptsPart 3: The REINFORCE algorithmPart 4: Implementing the REINFORCE algorithmAt the same time, this mini-series will be the introduction to future posts with increasing complexity. Feel free to skip to the next part if you are already familiar with the content.Note: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.That sounds cool! … but what can I do with RL?Reinforcement learning is a framework to learn any task. In theory, RL can solve any problem that is phrased as a Markov Decision Process. We will explain what that means later on. For now, let’s have a look at some successful applications. If you enjoy these examples, please be sure to also check out the work from the original authors.GeneralOne of the videos I love showing is the hide and seek video from OpenAI. The video is a nice example of how RL can us with finding novel solutions to problems, without explicitly programming tactics or solution methods.GamesOne of the early successes of Deep RL (which is combining RL with neural networks), was the ability to learn how to play Atari games, straight from pixels. Later on, researchers took it upon themselves to not only evaluate RL on the (relatively) simple Atari games, but rather evaluate it on the hardest competitive games out there. The assumption here is, that if RL can solve these complex games, it can also generalize to challenging real-world settings. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.RoboticsSolving tasks in simulations and video games is one thing, but what real life? Another popular field where RL is often applied (or at least holds great promise), is robotics. Robotics are significantly harder than simulations for various reasons. Think for example the time it takes to repeatedly make a robot try out a certain action. Or think the safety requirements involved for robotics. In the example below, you can see how the ANYmal robot from the Robotics System Lab in Zürich learned to recover from a fall.Real world examplesRL can be applied to many other domains than the ones I just mentioned. Advertising, finance, healthcare, … just to name a few. As a final example, I present a goal-oriented chatbot, trained to negotiate sales (source: https://siddharthverma314.github.io/research/chai-acl-2022/ ).RL: The basicsA description of the RL framework is as follows: We have an agent that tries to solve a task in a certain environment. The concept of agent should be taken very broadly here, an agent can be a robot, a chatbot, a virtual character, etc. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in.The RL problem is trying to maximize the cumulative reward the agent gets over time.Imagine our agent is a monkey and the task we want the monkey to solve, is to pick up as many bananas as possible. At every timestep, the monkey needs to decide to take an action. The actions could be to step towards the tree, grab something, climb, … Perhaps the reward at every timestep can be defined as the number of bananas the monkey got at that timestep. After every action, the monkey will also be in a new state. Maybe we define the state of the monkey as its position in the world. So when the monkey takes a step, the state at the next timestep would be the coordinates of the monkey at the next timestep. We are now searching for the optimal behavior, the best sequence of actions the monkey can take, to maximize the cumulative number of bananas it will get.How does RL fit in the bigger picture?You might look at this framework and think: “Hey, isn’t that exactly what people are already studying in the field of …?”. And in fact, you might be right.In other domains like engineering or mathematics, people have often been studying the same problems with different names and methods. Or in fields like neuroscience and psychology, some similarities can be found in the way our brain “rewards” us by releasing dopamine.The likely reason for this intersection of domains, is that reinforcement learning is the study of a fundamental problem. It is essentially the science of decision taking. In these series, we will be looking at it from the umbrella of computer science and machine learning.This general applicability is also what makes RL so interesting to me personally. RL is one of the potential technologies that could get us closer to general AI: an AI system that can solve any task, in contrast to a narrow set of tasks. There are also other technologies (e.g. big language models, graph neural networks, …) that are making some strides in this regard, but the problem statement of RL in particular seems to be the most ambitious.Another way of situating RL, is by looking at how it compares to other learning paradigms. Within the field of machine learning, people often distinguish between Supervised learning, Unsupervised learning and Reinforcement learning.When it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to. Our AI model is successful when it correctly predicts a label y given an (unseen) sample x.As an example, say we have a dataset consisting of cat and dog images. Each sample (image) has a label, stating whether the image contains a “dog” or a “cat”. The goal of our supervised model, is now to learn how to map a dog-image to the label “dog” and a cat-image to the label “cat”. After it has learned this mapping, the hope is that this AI model can repeat this process for new images that it has not seen before.In an Unsupervised learning context, we no longer provide any labels Y. So the task for the AI system now becomes learning some general statistics the dataset. We could for example give an AI the task to generate a new sample, similar to the ones seen in the dataset. In this case the model would be considered successful if it manages to correctly learn the interesting characteristics of a dataset.Reinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in. The “dataset” we need to consider here, are the actions our agent took and the accumulated rewards it got by taking those actions. An added difficulty here is that our dataset is non-static. Say that our agent acts in a certain way, we can then collect some data of the actions our agent took and we can try to optimize these (e.g. do more of the actions that led to a successful result). But as a result of this optimization, we have now changed the behavior of this agent, and thus we will need to collect new data to see how well our agent fares now.If RL is so great, then why isn’t everyone using RL?After reading all this, you might be wondering why people aren’t using RL to solve all imaginable problems. The truth is that even though the field has made a lot of advancements in the last few years, there are still a few fundamental problems to be solved. Progress is being made all the time, but to give you an idea of what you might encounter, I’ll list a few common ones.Sample (in-)efficiencyIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task. In this sense, RL is very inefficient compared to humans, for example, it doesn’t take a human dozens of hours to learn how to play an Atari game.This sample-efficiency can in part be explained by the fact that humans can leverage a lot of their previous knowledge (priors) when they encounter a new task. A human can for example reuse some of the knowledge and skills of previous games and/or concepts they already acquired from other experiences throughout their life. An RL-agent in contrast, starts the learning process without any assumptions.Another thing to mention is that leveraging knowledge from previous tasks is also an active research topic. I’ll just put one example (out of many) here to give you an idea.Deepmind GatoGoogle Jump-Start RLThe exploration-exploitation trade-offWhile the previous problem sounds more like an engineering effort (it’s not), the exploration-exploitation trade-off seems more fundamental. Whenever we train an RL-agent, the agent will need some time to explore, it needs to take some actions that it hasn’t taken before, in order to discover how to solve the problem. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing. Sometimes we want the agent to leverage what it has already learned to try and optimize further. This is the exploration-exploitation trade-off, we want an automated way to strike a good balance between letting the agent explore and taking actions for which it already knows what they will lead to.For a lot problems, it is quite possible that the agent gets stuck in a local optimum.The exploration-exploitation trade-off sounds very much tractable at first, but it turns out to be one of the hardest problems for RL to solve. To give you an idea how hard this problem is: The problem was originally considered by the scientists of the allied forces, but was suggested to be dropped over to Germany, because it was deemed so intractable that they wanted the German scientists to also waste their time on it.No silver bullet for this problem has been found and there are many people working on various solutions. I will leave a link here to a previously proposed solution called “curiosity-driven exploration” which I found particularly interesting.Curiosity-driven exploration by Self-supervised PredictionThe sparse-reward problemAnother rather fundamental problem, is the so called Sparse-reward problem. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve.Imagine for example this mountain car. The agent needs to move the car left and right, such that it gets enough momentum to reach the top. Initially though, the agent doesn’t know that it needs to move the car back and forth to reach the top. If we only give our agent a reward (a positive feedback signal) when we have reached the flag, it might not ever get a positive feedback signal, simply because it might never reach the flag by taking random actions (exploration).Something commonly done to counteract this problem, is by “reward shaping”. We will modify the reward such that the agent gets more feedback signals to learn from. In case of the mountain car, we could for example also give the agent a positive reward based on the speed or altitude it achieves. However, reward shaping is not a scalable solution. Luckily other solutions are being sought after.I’m leaving another example here which I think is an interesting (but not generally applicable approach) for counteracting sparse rewards.Hindsight Experience ReplayThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made.ConclusionPhew, that was a lot of information in a very short period, but you made it through! This first introduction should give you a good idea of what RL is all .In the next article, we will start formalizing some of the ideas and concepts which we have briefly touched upon in this post, as a preparation for getting our hands dirty and implementing these ideas ourselves.Artificial IntelligenceReinforcement LearningDeep LearningPolicy GradientData Science----2Written by Cédric Vandelaer192 ·8'),\n",
       " Document(metadata={'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92', 'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium', 'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …', 'language': 'en'}, page_content='Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | MediumReinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and ApplicationsAre you interested in learning reinforcement learning but don’t know where to start? Look no further! In this article, I’ll provide an introduction to reinforcement learning (RL), explain its key concepts, and highlight some of its applications.Arjun Sarkar14 min read·Mar 9, 2023--Table of ContentsWhat is Reinforcement Learning?2. Key Concepts of Reinforcement LearningAgentEnvironmentStateActionRewardPolicyValue FunctionQ-FunctionExploration vs Exploitation3. Applications of Reinforcement LearningGamingRoboticsFinanceHealthcare4. Reinforcement Learning AlgorithmsQ-LearningDeep Q-Network (DQN)Policy GradientActor-Critic5. Challenges6. Future Directions1. What is Reinforcement Learning?Reinforcement learning (RL) is a branch of machine learning that focuses on enabling an agent to learn and make decisions based on rewards and punishments received from its environment. RL is different from supervised and unsupervised learning, as it doesn’t rely on pre-existing labeled datasets. Instead, the agent interacts with its environment, receives feedback in the form of rewards or punishments, and learns to optimize its actions accordingly.RL has found applications in various fields such as gaming, robotics, finance, and healthcare. It has the potential to solve complex problems that are difficult to model mathematically or manually program.Photo by Lenin Estrada on Unsplash2. Key Concepts of Reinforcement LearningBefore diving into RL algorithms and applications, it’s essential to understand the key concepts that underpin RL.Main components of Reinforcement LearningAgentAn agent is an intelligent system or program that interacts with an environment in order to learn how to achieve a certain goal. The agent learns by receiving feedback from the environment in the form of rewards or punishments for its actions. The ultimate goal of the agent is to learn a policy, which is a mapping from states of the environment to actions, that maximizes its long-term expected reward.The agent typically has three main components: a policy, a value function, and a learning algorithm. The policy is the agent’s strategy for selecting actions based on the current state of the environment. The value function estimates the expected long-term reward that the agent will receive from a particular state and action. The learning algorithm updates the agent’s policy and value function based on the feedback it receives from the environment.The agent’s behavior is characterized by the exploration-exploitation tradeoff. The agent must explore the environment to discover the best policy, but it also needs to exploit its current knowledge to maximize its expected reward. This tradeoff is often addressed using an exploration strategy, such as epsilon-greedy or Thompson sampling, that balances exploration and exploitation.Overall, the agent is the central component of a reinforcement learning system and is responsible for learning how to interact with and navigate the environment to achieve its goals.EnvironmentIn reinforcement learning, the term “environment” refers to the external world or the system in which an agent operates. It is the environment that an agent interacts with and receives feedback from, and the agent’s goal is to learn from these interactions and optimize its behavior to achieve a specific objective.The environment in reinforcement learning can be anything from a virtual simulation to a physical system, such as a robot or a game. The environment provides the agent with a set of observations or states, which describe the current state of the system. These observations are used by the agent to make decisions the actions it should take, and the actions it takes to result in a change in the state of the environment.The environment also provides feedback to the agent in the form of rewards or penalties, based on the actions it takes. These rewards serve as signals to the agent to reinforce or discourage certain behaviors, and the agent’s objective is to maximize its cumulative reward over time.The environment plays a critical role in the success of reinforcement learning, as the agent’s ability to learn and optimize its behavior is highly dependent on the quality of the environment and the feedback it provides. Therefore, creating an accurate and effective environment is essential in developing successful reinforcement learning systems.StateA state refers to the current situation or configuration of the environment that the agent is in. It includes all the relevant information that the agent needs to make decisions and take actions to maximize its rewards.The state can be represented in various ways, such as a set of variables or features that describe the current situation, or a complete image or sensory input of the environment. The choice of representation depends on the complexity of the environment and the task at hand.The state is important because it determines the actions that the agent can take and the rewards that it will receive. The agent’s goal is to learn a policy, which is a mapping from states to actions that maximizes its cumulative reward over time. Therefore, the state is a crucial part of the reinforcement learning process, as it forms the basis of the agent’s decision-making process.ActionAn action refers to a decision made by the agent in response to the state of the environment. It is a specific move or behavior that the agent takes in a particular state to transition to the next state. An action can be a choice from a set of available options or a continuous value in a range.For example, in a game of chess, an action can be moving a piece to a particular square on the board. In a self-driving car, an action can be accelerating, braking, or turning the steering wheel. In a robotic arm, an action can be moving to a particular position or rotating in a specific direction.The choice of action by the agent is crucial because it determines the rewards received from the environment. The goal of the agent is to learn the optimal policy that maximizes the cumulative rewards by selecting the best possible action in each state.RewardA reward is a scalar feedback signal that an agent receives from the environment after taking an action. The purpose of a reward is to indicate how well the agent is doing at achieving its goal, which is typically to maximize a cumulative measure of reward over time.Rewards can be positive, negative, or zero, depending on whether the agent’s action led to a desirable, undesirable, or neutral outcome. The agent’s objective is to learn a policy that maximizes the expected sum of future rewards, or the expected return.Designing the reward function is an important aspect of reinforcement learning, as it directly affects the behavior of the agent. A well-designed reward function should incentivize the agent to achieve the desired goal while avoiding unintended behaviors. However, designing reward functions that accurately capture the desired behavior can be challenging, and improper reward functions can lead to suboptimal or even undesirable behavior.PolicyA policy is a function that maps an agent’s current state to an action to be taken in that state. The policy defines the agent’s behavior or strategy for choosing actions in the environment.A policy can be deterministic, meaning that it always chooses the same action for a given state, or stochastic, meaning that it chooses actions probabilistically. In a stochastic policy, the probabilities of taking each action in a given state are specified by the policy.The goal of reinforcement learning is often to learn an optimal policy that maximizes the expected cumulative reward over time. This is typically achieved through trial and error, where the agent interacts with the environment, observes the resulting rewards and transitions to new states, and updates its policy based on the observed outcomes.Value FunctionA value function is a function that estimates the value of a state or state-action pair. It represents how good a particular state or action is in of achieving the agent’s goal. The value function is a critical component of many reinforcement learning algorithms because it guides the agent’s decision-making process.There are two types of value functions in reinforcement learning: state-value function and action-value function.State-value function: It predicts how much reward an agent can expect to receive from a given state. The state-value function is denoted by V(s) and is defined as the expected cumulative reward that an agent can receive starting from a given state s and the current policy.Action-value function: It predicts how much reward an agent can expect to receive by taking a particular action in a given state. The action-value function is denoted by Q(s,a) and is defined as the expected cumulative reward that an agent can receive starting from state s, taking action a, and the current policy.The value function is estimated using past experiences and updated through iterative learning methods, such as temporal difference learning or Monte Carlo methods. Accurate estimation of the value function is essential for the agent to make optimal decisions and maximize its long-term rewards.Q-FunctionThe Q function (also known as the action-value function) is a mathematical function that takes in a state-action pair as input and outputs the expected long-term reward of taking that action in that state and a given policy thereafter.The Q function is a crucial component in many reinforcement learning algorithms, such as Q-learning and SARSA, as it allows the agent to estimate the quality of different actions in different states. By learning the Q function, the agent can then choose actions that maximize the expected long-term reward, which is the goal of many reinforcement learning tasks.The Q function is often represented as a table or a function approximator, such as a neural network, that is learned through experience and interaction with the environment. The process of learning the Q function involves iteratively updating the Q values based on the observed rewards and transitions until the estimates converge to the true values.Exploration vs ExploitationExploration and exploitation are two important concepts in reinforcement learning that deal with how an agent should choose actions to take in an environment.Exploration refers to the agent’s behavior of trying out new actions in order to learn more the environment and potentially find better actions that lead to higher rewards. Exploitation, on the other hand, refers to the agent’s behavior of choosing the actions that have already been tried and proven to lead to high rewards.The challenge in reinforcement learning is to balance exploration and exploitation in order to maximize the agent’s long-term reward. If the agent only exploits known good actions, it may miss out on better actions that it hasn’t tried yet. On the other hand, if the agent only explores new actions, it may not accumulate enough reward to perform well in the long run.Various exploration strategies have been proposed, such as ε-greedy, softmax, and Upper Confidence Bound (UCB), among others. These strategies use different ways to balance exploration and exploitation, and the choice of strategy depends on the specific problem at hand.3. Applications of Reinforcement LearningReinforcement learning has found numerous applications in various fields. Here are some examples:GamingReinforcement learning has been applied to various games, from classic board games like chess and Go to modern video games like Dota 2 and StarCraft II. In these games, an agent learns to make decisions and compete against human players or other agents.RoboticsReinforcement learning has been used in robotics to enable robots to learn to navigate environments, manipulate objects, and perform complex tasks. In these applications, an agent learns from feedback received through sensors and actuators.FinanceReinforcement learning has found applications in finance, including algorithmic trading, portfolio optimization, and fraud detection. In these applications, an agent learns to make decisions based on market data and financial indicators.HealthcareReinforcement learning has been used in healthcare to optimize treatment plans and drug dosages. In these applications, an agent learns from patient data and medical records to make decisions that maximize patient outcomes.4. Reinforcement Learning AlgorithmsThere are several algorithms used in reinforcement learning. Here are some of the most commonly used ones:Q-LearningQ-learning is a model-free, off-policy reinforcement learning algorithm used to find the optimal action-selection policy for any given Markov decision process (MDP). In Q-learning, the agent learns an action-value function Q(s,a) that gives the expected utility of taking action a, in state s, and the optimal policy thereafter.The algorithm uses an iterative process to update the Q-values of each state-action pair based on the observed rewards received from the environment. The updates are made using the Bellman equation, which exes the expected value of the current state as the sum of the immediate reward and the expected value of the next state. By iteratively updating the Q-values, Q-learning converges to the optimal Q-function and thus the optimal policy.One of the key advantages of Q-learning is that it can learn optimal policies in environments with large state spaces and stochastic rewards. However, it requires sufficient exploration of the state-action space to avoid getting stuck in suboptimal policies, which can be challenging in some environments. Additionally, Q-learning assumes that the state transition and reward functions are unknown, which may not always be the case in practice.Deep Q-Network (DQN)Deep Q-Network (DQN) is a popular deep reinforcement learning algorithm introduced by DeepMind in 2013. It extends the Q-learning algorithm to work with high-dimensional input spaces by using a deep neural network to approximate the Q-function.In DQN, the agent uses a neural network to estimate the Q-values of each possible action in a given state. The network takes the state as input and outputs the estimated Q-values for each possible action. The agent then selects the action with the highest estimated Q-value to take.To train the DQN, the agent uses experience replay and a target network. Experience replay is a technique where the agent stores transitions from the environment in a buffer and samples random batches of these transitions to train the neural network. This s to break the correlation between consecutive samples and improve the stability of the learning process. The target network is a separate network used to generate the Q-value targets for training. The weights of the target network are frozen and only updated periodically with the weights of the main network, which s to stabilize the learning process.DQN has been applied successfully to a variety of tasks, including playing Atari games and controlling robotic systems.Policy GradientPolicy gradient algorithms are a class of reinforcement learning methods that learn a policy function by directly optimizing the objective function that measures the expected cumulative reward obtained by the policy. The policy function maps states to actions, and the goal of the algorithm is to find the policy that maximizes the expected cumulative reward over a long-term horizon.Policy gradient methods use gradient descent to iteratively update the policy parameters to maximize the expected cumulative reward. The policy is typically represented as a neural network, where the input is the state and the output is a probability distribution over the possible actions. The gradient of the objective function is computed with respect to the policy parameters, and the parameters are updated to move in the direction of the gradient.One popular policy gradient algorithm is the REINFORCE algorithm, which is a Monte Carlo algorithm that estimates the gradient of the expected reward using samples from the current policy. Another popular algorithm is the Actor-Critic algorithm, which combines policy gradient with value function estimation. The Actor-Critic algorithm uses a neural network to represent the policy and another neural network to represent the value function. The policy network is updated using the policy gradient, while the value network is updated using the temporal difference (TD) error, which is the difference between the predicted and actual rewards.Policy gradient methods have several advantages over value-based methods like Q-learning, including the ability to learn stochastic policies, handling of continuous action spaces, and better convergence properties. However, they can also suffer from high variance and slow convergence, especially in high-dimensional state spaces.Actor-CriticActor-critic algorithms are a type of reinforcement learning algorithm that combine the advantages of both policy-based and value-based methods. The actor-critic algorithm involves two neural networks, an actor network, and a critic network.The actor-network is responsible for selecting actions based on the current state of the environment. It uses the policy gradient method to update its parameters and improve its performance. The critic network, on the other hand, evaluates the value of the current state and action pair. It uses the temporal difference learning method to update its parameters and learn from the feedback received from the environment.The actor-critic algorithm uses the critic network to estimate the value of an action in a given state and then uses this estimate to update the policy of the actor-network. This allows the algorithm to balance exploration and exploitation in a more efficient manner, and improve the overall performance of the agent.Actor-critic algorithms can be further divided into several subtypes, including advantage actor-critic (A2C), asynchronous advantage actor-critic (A3C), and deep deterministic policy gradient (DDPG). These algorithms have been successfully applied to a wide range of tasks, including robotics, game-playing, and natural language processing.5. ChallengesReinforcement learning faces several challenges, including:Exploration vs Exploitation: Finding the right balance between exploration and exploitation is one of the fundamental challenges in reinforcement learning. The agent must explore the environment to learn the optimal policy while also exploiting the current knowledge to maximize the expected reward.Credit assignment: In some cases, the reward received by the agent may not reflect the quality of the actions taken in the past. This makes it difficult to assign credit to the actions that led to the reward.Generalization: Reinforcement learning often requires generalizing learned policies across different environments or tasks. This can be challenging because the agent must be able to adapt to new situations without forgetting what it has already learned.Function approximation: In many reinforcement learning problems, the state and action spaces are too large to store in memory or represent exactly. Thus, function approximation techniques are used to estimate value functions or policies. However, these approximations can introduce errors that affect the quality of the learned policies.Delayed rewards: The rewards received in reinforcement learning are often delayed, meaning that the agent must learn to associate current actions with future rewards. This can make it challenging to determine the best action to take in the current state.Sample inefficiency: Reinforcement learning algorithms typically require a large number of interactions with the environment to learn an optimal policy. This can be time-consuming and costly in some real-world applications where interactions with the environment are limited.6. Future DirectionsReinforcement learning is a rapidly evolving field with many exciting developments and future directions. Some of the potential areas of focus for future research and development include:Multi-agent reinforcement learning: Many real-world applications of reinforcement learning involve multiple agents interacting with each other, such as in autonomous driving or game theory. Developing algorithms and techniques that can handle these complex interactions is a promising direction for future research.Sample efficiency: One of the biggest challenges in reinforcement learning is the amount of data required to train an agent. Developing algorithms that can learn effectively from fewer samples could significantly increase the applicability of reinforcement learning in real-world scenarios.Safe and ethical RL: As reinforcement learning is applied to increasingly complex and important tasks, ensuring that agents behave safely and ethically becomes more critical. Future research could focus on developing techniques to ensure that agents behave in a manner consistent with ethical and legal standards.Transfer learning: Transfer learning involves leveraging knowledge gained from one task to an agent learn more efficiently in a new, related task. Developing techniques that allow for effective transfer learning could greatly increase the efficiency and effectiveness of reinforcement learning algorithms.Explainability and interpretability: As reinforcement learning is applied in more critical domains, such as healthcare or finance, it becomes more important to understand why agents are making particular decisions. Developing techniques to explain and interpret the decisions of reinforcement learning agents is a promising direction for future research.Reinforcement LearningDeep LearningMachine LearningNeural NetworksAlgorithms----Written by Arjun Sarkar3.2K ·411 Ph.D. student — Deep Learning on Biomedical Images at the Leibniz Institute-HKI, Germany. LinkedIn-https://www.linkedin.com/in/arjun-sarkar-9a051777/No responses yet'),\n",
       " Document(metadata={'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c', 'title': 'An Introduction to Deep Reinforcement Learning | Medium', 'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\", 'language': 'en'}, page_content=\"An Introduction to Deep Reinforcement Learning | MediumAn Introduction to Deep Reinforcement LearningChapter 1 of the Deep Reinforcement Learning Course v2.0Thomas Simonini14 min read·Oct 9, 2020--6We launched a new free, updated, Deep Reinforcement Learning Course from beginner to expert, with Hugging Face 🤗👉 The new version of the course: https://huggingface.co/deep-rl-course/unit0/introductionThe chapter below is the former version, the new version is here 👉 https://huggingface.co/deep-rl-course/unit1/introductionWe launched a new free, updated, Deep Reinforcement Learning Course from beginner to expert, with Hugging Face 🤗👉 The new version of the course: https://huggingface.co/deep-rl-course/unit0/introductionThe chapter below is the former version, the new version is here 👉 https://huggingface.co/deep-rl-course/unit1/introductionWelcome to the most fascinating topic in Artificial Intelligence: Deep Reinforcement Learning.Deep RL is a type of Machine Learning where an agent learns how to behave in an environment by performing actions and seeing the results.Since 2013 and the Deep Q-Learning paper, we’ve seen a lot of breakthroughs. From OpenAI five that beat some of the best Dota2 players of the world, to the Dexterity project, we live in an exciting moment in Deep RL research.OpenAI Five, an AI that beat some of the best Dota2 players of the worldMoreover, since the first version of this course in 2018, a ton of new libraries (TF-Agents, Stable-Baseline 2.0…) and environments where launched: MineRL (Minecraft), Unity ML-Agents, OpenAI retro (NES, SNES, Genesis games…). You have now access to so many amazing games to build your agents.That’s why this is the best moment to start learning, and with this course you’re in the right place.Yes, because this article is the first chapter of Deep RL Course v2.0 a free course from beginner to expert where you’ll master the skills and architectures you need, to become a deep reinforcement learning expert.During this course, you’ll build a strong professional portfolio by implementing awesome agents with Tensorflow and PyTorch that learn to play Space invaders, Minecraft, Starcraft, Sonic the hedgehog and more!So in this first chapter, you’ll learn the foundations of deep reinforcement learning.It’s really important to master these elements before diving into implementing Deep Reinforcement Learning agents. The goal in this chapter is to give you solid foundations.If you prefer, you can watch the 📹 video version of this chapter:So let’s get started!What is Reinforcement Learning?In order to understand what is reinforcement learning, let’s start with the big picture.The big pictureThe idea behind Reinforcement Learning is that an agent (an AI) will learn from the environment by interacting with it (through trial and error) and receiving rewards (negative or positive) as feedback for performing actions.Learning from interaction with the environment comes from our natural experiences.For instance, imagine you put your little brother in front of a video game he never played, a controller in his hands, and let him alone.Your brother will interact with the environment (the video game) by ing the right button (action). He got a coin, that’s a +1 reward. It’s positive, he just understood that in this game he must get the coins.But then, he es right again and he touches an enemy, he just died -1 reward.By interacting with his environment through trial and error, your little brother just understood that in this environment, he needs to get coins, but avoid the enemies.Without any supervision, the child will get better and better at playing the game.That’s how humans and animals learn, through interaction. Reinforcement Learning is just a computational approach of learning from action.A formal definitionIf we take now a formal definition:Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.But how Reinforcement Learning works?The Reinforcement Learning FrameworkThe RL ProcessThe RL Process: a loop of state, action, reward and next stateTo understand the RL process, let’s imagine an agent learning to play a platform game:A big thanks to Felix for this updated illustrationOur Agent receives state S0 from the Environment — we receive the first frame of our game (environment).Based on that state S0, the agent takes an action A0 — our agent will move to the right.Environment transitions to a new state S1 — new frame.Environment gives some reward R1 to the agent — we’re not dead (Positive Reward +1).This RL loop outputs a sequence of state, action and reward and next state.The goal of the agent is to maximize its cumulative reward, called the expected return.The reward hypothesis: the central idea of Reinforcement LearningWhy the goal of the agent is to maximize the expected return?Because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).That’s why in Reinforcement Learning, to have the best behavior, we need to maximize the expected cumulative reward.(Optional) Markov PropertyYou’ll see in papers that the RL process is called the Markov Decision Process (MDP).We’ll talk again the Markov Property in the next chapters. But if you need to remember something today it is just that Markov Property implies that our agent needs only the current state to make its decision what action to take and not the history of all the states and actions he took before.Now let’s dive a little bit on all this new vocabulary:Observations/States SpaceObservations/States are the information our agent gets from the environment. In the case of a video game, it can be a frame (a screenshot), in the case of the trading agent, it can be the value of a certain stock etc.There is a differentiation to make between observation and state:State s: is a complete description of the state of the world (there is no hidden information). In a fully observed environment.In chess game, we receive a state from the environment since we have access to the whole check board information.With a chess game, we are in a fully observed environment, since we have access to the whole check board information.Observation o: is a partial description of the state. In a partially observed environment.In Super Mario Bros, we only see a part of the level close to the player, so we receive an observation.In Super Mario Bros, we are in a partially observed environment, we receive an observation since we only see a part of the level.In reality, we use the term state in this course but we will make the distinction in implementations.Action SpaceThe Action space is the set of all possible actions in an environment.The actions can come from a discrete or continuous space:Discrete space: the number of possible actions is finite.Again, in Super Mario Bros, we have only 4 directions and jump possibleIn Super Mario Bros, we have a finite set of actions since we have only 4 directions and jump.Continuous space: the number of possible actions is infinite.A Self Driving Car agent has an infinite number of possible actions since he can turn left 20°, 21°, 22°, honk, turn right 20°, 20,1°…SourceTaking this information into consideration is crucial because it will have importance when we will choose in the future the RL algorithm.Rewards and the discountingThe reward is fundamental in RL because it’s the only feedback for the agent. Thanks to it, our agent knows if the action taken was good or not.The cumulative reward at each time step t can be written as:The cumulative reward is equal to the sum of all rewards of the sequence.Which is equivalent to:However, in reality, we can’t just add them like that. The rewards that come sooner (at the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.Let say your agent is this small mouse that can move one tile each time step, and your opponent is the cat (that can move too). Your goal is to eat the maximum amount of cheese before being eaten by the cat.As we can see in the diagram, it’s more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).As a consequence, the reward near the cat, even if it is bigger (more cheese), will be more discounted since we’re not really sure we’ll be able to eat it.To discount the rewards, we proceed like this:We define a discount rate called gamma. It must be between 0 and 1.The larger the gamma, the smaller the discount. This means our agent cares more the long term reward.On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more the short term reward (the nearest cheese).2. Then, each reward will be discounted by gamma to the exponent of the time step.As the time step increases, the cat gets closer to us, so the future reward is less and less probable to happen.Our discounted cumulative expected rewards is:Type of tasksA task is an instance of a Reinforcement Learning problem. We can have two types of tasks: episodic and continuous.Episodic taskIn this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and New States.For instance think Super Mario Bros, an episode begin at the launch of a new Mario Level and ending when you’re killed or you’re reach the end of the level.Beginning of a new episodeContinuous tasksThese are tasks that continue forever (no terminal state). In this case, the agent has to learn how to choose the best actions and simultaneously interacts with the environment.For instance, an agent that do automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop him.Exploration/ Exploitation tradeoffFinally, before looking at the different methods to solve Reinforcement Learning problems, we must cover one more very important topic: the exploration/exploitation trade-off.Exploration is exploring the environment by trying random actions in order to find more information the environment.Exploitation is exploiting known information to maximize the reward.Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, we can fall into a common trap.Let’s take an example:In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation).But if our agent does a little bit of exploration, it can discover the big reward (the pile of big cheese).This is what we call the exploration/exploitation trade off. We need to balance how much we explore the environment and how much we exploit what we know the environment.Therefore, we must define a rule that s to handle this trade-off. We’ll see in future chapters different ways to handle it.If it’s still confusing think of a real problem: the choice of a restaurant:Source: Berkley AI CourseExploitation: You go everyday to the same one that you know is good and take the risk to miss another better restaurant.Exploration: Try restaurants you never went before, with the risk of having a bad experience but the probable opportunity of an amazing experience.The two main approaches for solving RL problemsNow that we learned the RL framework, how do we solve the RL problem?In other , how to build a RL agent that can select the actions that maximize its expected cumulative reward?The Policy π: the agent’s brainThe Policy π is the brain of our Agent, it’s the function that tell us what action to take given the state we are. So it defines the agent behavior at a given time.Think of policy as the brain of our agent, the function that will tells us the action to take given a stateThis Policy is the function we want to learn, our goal is to find the optimal policy π*, the policy that maximizes expected return when the agent acts according to it. We find this π* through training.There are two approaches to train our agent to find this optimal policy π*:Directly, by teaching the agent to learn which action to take, given the state is in: Policy-Based Methods.Indirectly, teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states: Value-Based Methods.Policy-Based MethodsIn Policy-Based Methods, we learn a policy function directly.This function will map from each state to the best corresponding action at that state. Or a probability distribution over the set of possible actions at that state.As we can see here, the policy (deterministic) directly indicates the action to take for each step.We have two types of policy:Deterministic: a policy at a given state will always return the same action.action = policy(state)Stochastic: output a probability distribution over actions.policy(actions | state) = probability distribution over the set of actions given the current stateGiven an initial state, our stochastic policy will output a probability distributions over the possible actions at that state.Value based methodsIn Value based methods, instead of training a policy function, we train a value function that maps a state to the expected value of being at that state.The value of a state is the expected discounted return the agent can get if it starts in that state, and then act according to our policy.“Act according to our policy” just means that our policy is “going to the state with the highest value”.Here we see that our value function defined value for each possible state.Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.The “Deep” in Reinforcement LearningWait… you spoke Reinforcement Learning, but why we speak Deep Reinforcement Learning?Deep Reinforcement Learning introduces deep neural networks to solve Reinforcement Learning problems — hence the name “deep.”For instance, in the next article, we’ll work on Q-Learning (classic Reinforcement Learning) and then Deep Q-Learning both are value-based RL algorithms.You’ll see the difference is that in the first approach, we use a traditional algorithm to create a Q table that s us find what action to take for each state.In the second approach, we will use a Neural Network (to approximate the q value).Schema inspired by the Q learning notebook by UdacityIf you are not familiar with Deep Learning you definitely should watch the MIT Intro Course on Deep Learning (Free)MIT Deep Learning 6.S191MIT's introductory course on deep learning methods with applications to computer vision, natural language processing…introtodeeplearning.comThat was a lot of information, if we summarize:Reinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment by interacting with it through trial and error and receiving rewards (negative or positive) as feedback.The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected cumulative reward.The RL process is a loop that outputs a sequence of state, action, reward and next state.To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long term future reward.To solve an RL problem, you want to find an optimal policy, the policy is the “brain” of your AI that will tell us what action to take given a state. The optimal one is the one who gives you the actions that max the expected return.There are two ways to find your optimal policy:By training your policy directly: policy-based methods.By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.Finally, we speak Deep RL because we introduces deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based) hence the name “deep.”Congrats on finishing this chapter! That was the biggest one, and there was a lot of information.That’s normal if you’re still feel confuse with all these elements. This was the same for me and for all people who studied RL.Take time to really grasp the material before continuing. It’s important to master these elements and having a solid foundations before entering the fun part: creating AI that plays video games.Naturally, during the course, we’re going to use and deeper explain again these but it’s better to have a good understanding of them now before diving into the next chapters.In the next chapter, we’re going to learn our first RL algorithm Q-Learning and dive deeper into the value-based methods.You’ll train your first RL agent: a taxi Q-Learning agent that will need to learn to navigate in a city to transport its passengers from a point A to a point B. This will be fun.If you liked my article, please click the 👏 below as many times as you liked the article so other people will see this here on Medium. And don’t forget to follow me on Medium, on Twitter, and on Youtube.See you next time,Keep learning, stay awesome,Deep Reinforcement Learning Course v2.0:Chapter 1: Introduction to Deep Reinforcement LearningChapter 2, Part 1: Q-Learning with Taxi-v3Chapter 2, Part 2: Q-Learning with Taxi-v3Machine LearningAIArtificial IntelligenceReinforcement LearningDeep Learning----6Written by Thomas Simonini5.3K ·220 Developer Advocate 🥑 at Hugging Face 🤗| Founder Deep Reinforcement Learning class 📚 https://bit.ly/3QADz2Q |\"),\n",
       " Document(metadata={'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e', 'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium', 'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …', 'language': 'en'}, page_content='Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | MediumAnalytics Vidhya·Analytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comIntroduction to Reinforcement Learning (RL) in PyTorchHarsh Panchal21 min read·Aug 26, 2021--6Photo by Alina Grubnyak on UnsplashRecap of Supervised LearningSo far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In supervised learning, we are given some sort of training data consisting of input/output pairs, with the goal being to be able to predict the output given some new inputs after learning the model. For example, we previously looked at a Convolutional Neural Network (CNN) classification model for MNIST; given a training set of 60000 digit images and corresponding digit labels (e.g. ‘5’), we learned a model that was capable of predicting the digit label of new MNIST images. In order words, something like (but not exactly) this:Image by authorWhat if we want to learn how to perform more complex behaviors, where data collection can be expensive? How do you teach a robot to walk? Self-driving cars? How do you defeat human champions in the game of Go?Reinforcement LearningEnter Reinforcement Learning. In Reinforcement Learning, our model (commonly referred to as an agent in this context) interacts with an environment by taking actions 𝑎a and receives some sort of feedback from the environment in the form of a reward 𝑟. In this sense, reinforcement learning algorithms learn by experience. We call the trajectory of going from start to finish of a task an episode, and often our agent will learn by undergoing many episodes.Image by authorMany reinforcement learning algorithms are modeled as Markov Decision Processes (MDPs). In these settings, we have a concept of a state 𝑠, which encapsulates the situation of the agent (e.g. location, velocity). From each state 𝑠𝑡, the agent takes an action 𝑎𝑡, which results in a transition from one state 𝑠𝑡 to another 𝑠𝑡+1. In many settings, there is stochasticity in this transition, meaning that there’s is a distribution over 𝑠𝑡+1 conditioned on 𝑠𝑡 and 𝑎𝑡. Often, several of these states are considered episode-ending, after which the agent can no longer make any transitions or collect any more reward. These correspond to states such as reaching the final goal, a game concluding, or falling of a cliff. In the end, our goal is to learn a policy 𝜋 or a mapping from states to actions.In an MDP, we assume that we can always tell which state 𝑠𝑡 our agents is in. However, this isn’t always the case. Sometimes, all we have access to are observations 𝑜𝑡 that provide information the state 𝑠𝑡st, but enough to precisely pinpoint the exact one. We call such settings Partially Observable Markov Decision Processes (POMDPs). Imagine for example a Roomba being trained to navigate a living room with RL. From its infrared and mechanical “bump” sensors, it receives partial information (𝑜𝑡ot) as to where it might be, but not a definitive location (𝑠𝑡). Operating as a POMDP adds a whole layer of complexity to RL algorithms. For the rest of day though, we’ll focus on MDPs, as their much simpler and easier to use to teach basic concepts.A simple MDP exampleImage by authorIn the above example, we can see the 3 possible states for the agent as 𝑠0, 𝑠1, and 𝑠2, with 2 actions 𝑎0 and 𝑎1 available from each state. We can see that each action doesn’t lead to a deterministic transition to the next stage, as shown by multiple paths from each action. Note that each of the outcomes of action is labeled with a small black number between 0 and 1. This denotes the probability of that outcome (which state we end up at) given the action; as these are probabilities, the sum of the probabilities of arriving at each of the next states 𝑠𝑡+1 given a previous state 𝑠𝑡st and selected action 𝑎𝑡 is 1.ObjectiveThe goal of the agent is to maximize the total reward 𝑅R it can receive over a number of steps. It is important to ensure the reward actually captures the true goal we want the agent to achieve. The agent will dutifully attempt to maximize the objective it is given, without any considerations to any implicit objectives that a human may desire. There are more than a few (amusing) anecdotes of RL agents learning undesirable behaviors by exploiting some aspect of the reward function. As such, defining this reward requires special care.One countermeasure commonly deployed by RL researchers is the concept of discounted rewards. This is done with a multiplicative term 𝛾: a reward 𝑇 step in the future is discounted as 𝛾𝑇𝑟𝑇. Using discounting encourages the agent to finish the task sooner rather than later, a common implicit criterion. With discounting then, the RL agent’s goal is to maximize:Image by authorThis is far from the complete solution to making our rewards accurately capture our desired objectives, but achieving higher rewards sooner rather than later is an almost universal preference, so we almost always add it. Designing a good reward function can be art is highly dependent on the task.Reinforcement Learning as Supervised Learning?At first, this doesn’t seem too different from the supervised methods we’ve looked at before, and some natural questions might arise:Why can’t we just treat RL as a supervised task? Why can’t we use the reward (or rather, the negative of the reward) as our supervised loss?Unlike in supervised learning, in reinforcement learning, we often don’t have a pre-apportioned dataset to learn from. In some problems set-ups, we may have examples of other agents (oftentimes humans) performing the desired task, but these aren’t necessarily optimal examples of how to maximize the reward, which is what we want to learn. In most RL settings, we don’t have any examples of state-action trajectories beyond what our agent experiences through trial-and-error, which are even more suboptimal.Open AI GymBefore we dive any deeper into implementing reinforcement learning models, first we need an environment. Remember, the goal is to learn an agent that can interact with an environment in the way we want, so we need something that our agent can interact with and receive rewards from. In robotics, this is often the real world (or some set-up in the real world). However, it is oftentimes cheaper and quicker to first test our algorithms in simulated settings. There are a number of tasks that are popular benchmarks for the reinforcement learning community, such as cart pole, mountain car, or Atari 2600 games. In the spirit of accelerating progress and promoting openness in the research community, Open AI has very nicely coded up Open AI Gym, which has implementations of many of these environments for public use. We will be using these environments, as it allows us to focus on the algorithms themselves, instead of worrying implementing each problem setting ourselves.To use it, we first need to download and install it. Make sure you’re in your PyTorch environment first!# If you environment isn\\'t currently active, activate it:# conda activate pytorchpip install gymOnce it’s installed, we can import it like any other Python module:import gymFrozenLake (a Grid World)Let’s start with a simple environment: FrozenLake. Here’s the official description from OpenAI gym:Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you’ll fall into the freezing water. At this time, there’s an international frisbee shortage, so it’s absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won’t always move in the direction you intend.A visualization of FrozenLake as a grid world:Image by authorAt the start of an episode, we begin in the upper left corner (S). Our goal is to move ourselves to the lower right corner (G), avoiding falling into the holes (H). Icy water is cold.In reinforcement learning , each of the 16 locations on the grid is a state, and action is attempting to move in one of four directions (left, down, right, up). Each move will result in the agent’s state changing from 𝑠𝑡 to 𝑠𝑡+1 as it changes location unless it attempts to move in the direction of a wall, which results in the agent’s state not changing (the agent doesn’t move). We receive a positive reward of “+1” for reaching the goal (G), discounted according to how long it took. While there is not a negative reward for falling into a hole (H), the agent still pays a penalty in the sense that falling into the hole is episode-ending and therefore prevents it from receiving any reward. We want to learn a policy 𝜋 that takes us from our starting location (S) to the goal (G) in as few steps as possible.To really establish what we are trying to accomplish here, it’s worth debunking a few common initial misconceptions:Knowledge of the states and transition probabilities: From the top-down view, your first thought might be to plot out a path from the start to the finish, just as you would with a maze. However, this view is provided to us the algorithm designers so we can visualize the problem at hand. The agent learning the task does not get this prior knowledge; all we are to tell it is that there are going to be 16 states and 4 possible actions from each state. A more proper analogy would be if I blindfolded you and dropped you in the middle of a frozen lake, and told you your state (location) every time you decided to take a step in one of four directions, then set off fireworks when you stepped on the frisbee.Knowledge of the goal (reward): In OpenAI’s official description of the environment, you (the agent) know what you’re hoping to accomplish: You want to retrieve the frisbee while avoiding falling through the ice. The agent does not know this. Rather, it learns the goal by experiencing rewards (or penalties), and the algorithm updates its policy such that it will be more (or less) likely to do those actions again. Note that this means that if an agent never experiences certain rewards, it won’t know they exist.Prior knowledge of pathfinding, physics, etc.: As a human, even if you haven’t solved this task before, you still bring a tremendous amount of prior knowledge to this problem. For example, you know the shortest path to a destination is a line. You know that North, South, East, and West, are directions and that going North and then South brings you back to where you already were. You know ice is slippery. You know icy water is cold. You know being in icy cold water is bad. It’s important to keep in mind that our agent will begin knowing none of these things; its initial policy is essentially picking actions completely at random. By the end of the training, it still won’t know what abstract concepts like “North/South,” “cold,” or “slippery” mean, but it will have (hopefully) learned a good policy that allows it to complete the goal.Interacting with FrozenLakeThis example is simple enough that we could code the environment and its interface ourselves fairly easily, but OpenAI has already done it, and we’d like to focus on the algorithm of solving it as much as possible. We can create an instantiation of FrozenLake in a single line of code:In [ ]:env = gym.make(\\'FrozenLake-v0\\')Open AI Gym environments provide a mechanism to observe the state of the environment, and since FrozenLake is an MDP (as opposed to POMDP), the observation is the state itself. For FrozenLake, there are 16 grid locations on the map, meaning we have 16 states. We can confirm this by looking at the size of the observation_space attribute for the environment we just created.In [ ]:env.observation_spaceOur agent will interact with this environment causing its state to change. For FrozenLake, we have 4 options, each corresponding to attempting to step in a particular direction: [Left, Down, Right, Up]. We can confirm this by looking at the size of action_space of our environment.In [ ]:env.action_spaceBefore interacting with the environment, we have to first reset it to initialize it. Resetting also returns an observation of the first state after reset. In FrozenLake, we always start in the upper left corner, which corresponds to state 0. As such, we see the reset() command returning 0.In [ ]:env.reset()We can visualize the FrozenLake environment by calling render(). In more complex tasks this will actually add frames to a video showing our agent\\'s progress, but for FrozenLake, it just prints out a text representation, with the highlighted character showing our agent\\'s current location. We can see that we started in the upper-left corner, on the \"S,\" as promised.In [ ]:env.render()Now, let’s try moving. One thing to keep in mind is that the original FrozenLake environment is “slippery.” Because of the ice, if you try to go in one direction, you end up with a 1/3 chance of going in the direction you meant and the two adjacent directions each. For example, if we try going right, we have equal probabilities of slipping and going up and down instead. This makes things a little more complicated, so for now, let’s first turn off the stochasticity and make this a deterministic transition instead. We do this by registering a new type of environment, and then instantiating a copy of the said environment, making sure to reset it first.In [ ]:# Non-slippery versionfrom gym.envs.registration import registerregister( id=\\'FrozenLakeNotSlippery-v0\\', entry_point=\\'gym.envs.toy_text:FrozenLakeEnv\\', kwargs={\\'map_name\\' : \\'4x4\\', \\'is_slippery\\': False},)env = gym.make(\\'FrozenLakeNotSlippery-v0\\')env.reset()We advance time in an OpenAI environment with the step() method, which takes as argument an action. Let\\'s trying moving right, which corresponds to the action 2. Notice that the output is a tuple of four elements: the next observation (object), the reward (float), whether or not the episode is done (boolean), and a dictionary of information (dict) that may be useful for debugging (this dict shouldn\\'t be used in the final algorithm itself).In [ ]:env.step(2)Next, let’s render() to visualize what happened. Observe that this particular environment prints out the action we took in parentheses up top, in this case \"(Right)\", and then shows the result of that action. Notice that while most of the time, we succeed in going in the direction we want to, occasionally we slip on the ice and go in a direction we didn\\'t intend.In [ ]:env.render()We can keep doing this as many times as we want. Since we’re in Jupyter, we can just keep running the same cell (making small edits to change our action).Notice that once we fall into a hole, the episode is over, and we can no longer do anything. The same is true after reaching the goal.In [ ]:env.step(0)env.render()Before we get into any RL, let’s see how random actions perform in this environment:In [ ]:env.reset()done = Falsewhile not done: env.render() action = env.action_space.sample() _, _, done, _ = env.step(action)Hm. Not great. Alright, so clearly picking random steps isn’t very likely to take us to the goal. It’s apparent just from looking at the map that there’re much better policies that we can learn. How are we going to do so?Q-learningThere are many algorithms that we can use, but let’s choose Q-learning, which we covered earlier today. Remember, in Q-learning (and SARSA, it turns out), we’re trying to learn the Q values for the states in our system.The Q value for a policy 𝜋 is a function of the state 𝑠s and action 𝑎a and is defined as the :Image by authorIntuitively, the Q value is the total reward (including discounting) that the agent will gain if it takes action 𝑎a from state 𝑠 and then follows policy 𝜋 for the rest of the episode. As one might expect, if Q is known exactly, the agent will attain the highest reward from 𝑠s if the policy 𝜋 is to pick the 𝑎a with the highest Q value.Okay, so if we know the Q values for the system, then we can trivially find the optimal policy. So what are the Q values of the system? Well, in the beginning, we don’t know, but we can try to learn them through experience. This is where Q-learning comes in. Q-learning iteratively updates the Q values in the way:Image by authorNotice that Q-learning is an off-policy method, in the sense that you don’t actually learn from the trajectory you actually took (otherwise it’d be SARSA). Instead, we learn from the greedy transition, i.e. the best action we know how to take.And that’s it! We run our agent through many episodes, experiencing many 𝑠𝑡→𝑎𝑡→𝑠𝑡+1 transitions and rewards, and just like that, we eventually learn a good Q function (and thus a good policy). Now, of course, there are a bunch of small details and tweaks to make this work in practice, but we’ll get to those later.Q-learning in FrozenLakeFrozenLake is a very simple setting, one that we would call a toy problem. With only 16 states and 4 actions, there are only 64 state-action pairs possible (16x4=64), less if we account for the goal and the holes being episode ending (for simplicity though, we won’t). With these few state-action pairs, we can actually solve this problem tabularly. Let’s set up a Q table, and initialize the Q-values for all state-action pairs to zeros. Note that while we could, we’re actually not going to need PyTorch in this example; PyTorch’s autograd and neural network libraries are unnecessary here, as we’re only going to be modifying a table of numbers. Instead, we’ll use a Numpy array to store the Q table.In [ ]:import numpy as np#Initialize table with all zeros to be uniformQ = np.zeros([env.observation_space.n, env.action_space.n])A few hyperparameters we’re going to set:alpha: learning rate for the Q functiongamma: discount rate for future rewardsnum_episodes: number of episodes (trajectories from start to goal/hole) our agent will learn fromWe’re also going to store our rewards in an array called rs.In [ ]:# Learning parametersalpha = 0.1gamma = 0.95num_episodes = 2000# array of reward for each episoders = np.zeros([num_episodes])Now for the bulk of the algorithm itself. Notice that we’re going to loop through the process num_episodes times, resetting the environment each time. At each step, we take the action with the highest Q value for our current state, with some randomness added in (especially at the beginning) to encourage exploration. After each action, we update our Q table greedily based on the reward experienced and the next best action. We also make sure to update our state, rinse, and repeat. We continue taking actions in an episode until it is, storing the final total reward for the episode.In [ ]:for i in range(num_episodes): # Set total reward and time to zero, done to False r_sum_i = 0 t = 0 done = False #Reset environment and get first new observation s = env.reset() while not done: # Choose an action by greedily (with noise) from Q table a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i/10+1))) # Get new state and reward from environment s1, r, done, _ = env.step(a) # Update Q-Table with new knowledge Q[s,a] = (1 - alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:])) # Add reward to episode total r_sum_i += r*gamma**t # Update state and time s = s1 t += 1 rs[i] = r_sum_iHow did we do? Let’s take a look at the rewards that we saved. We can plot the reward versus the episode number, and hopefully, we’ll see some sort of an increase over time. RL performance can be extremely noisy, so let’s instead plot a moving average.In [ ]:## Plot reward vs episodesimport matplotlib.pyplot as plt# Sliding window averager_cumsum = np.cumsum(np.insert(rs, 0, 0)) r_cumsum = (r_cumsum[50:] - r_cumsum[:-50]) / 50# Plotplt.plot(r_cumsum)plt.show()Pretty good. We might also be interested in how often our agent actually reached the goal. This won’t account for how quickly the agent got there (which might also be of interest), but let’s ignore that for now. To prevent us from being overwhelmed by data points, let’s bucket the values into 10 stages, printing out how many episodes of each stage resulted in finding the goal.In [ ]:# Print number of times the goal was reachedN = len(rs)//10num_Gs = np.zeros(10)for i in range(10): num_Gs[i] = np.sum(rs[i*N:(i+1)*N] > 0) print(\"Rewards: {0}\".format(num_Gs))Our RL agent does a really good job at navigating the FrozenLake when its moves are deterministic, but after all, this is supposed to be FrozenLake, so where’s the fun without the slipperiness? Let’s go back to the original environment and see how the agent does.In [ ]:env = gym.make(\\'FrozenLake-v0\\')#Initialize table with all zeros to be uniformQ = np.zeros([env.observation_space.n, env.action_space.n])# Learning parametersalpha = 0.1gamma = 0.95num_episodes = 2000# array of reward for each episoders = np.zeros([num_episodes])for i in range(num_episodes): # Set total reward and time to zero, done to False r_sum_i = 0 t = 0 done = False #Reset environment and get first new observation s = env.reset() while not done: # Choose an action by greedily (with noise) from Q table a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i/10+1))) # Get new state and reward from environment s1, r, done, _ = env.step(a) # Update Q-Table with new knowledge Q[s,a] = (1 - alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:])) # Add reward to episode total r_sum_i += r*gamma**t # Update state and time s = s1 t += 1 rs[i] = r_sum_i## Plot reward vs episodes# Sliding window averager_cumsum = np.cumsum(np.insert(rs, 0, 0)) r_cumsum = (r_cumsum[50:] - r_cumsum[:-50]) / 50# Plotplt.plot(r_cumsum)plt.show()# Print number of times the goal was reachedN = len(rs)//10num_Gs = np.zeros(10)for i in range(10): num_Gs[i] = np.sum(rs[i*N:(i+1)*N] > 0) print(\"Rewards: {0}\".format(num_Gs))Much harder. However, we can see that the model does eventually learn something.PyTorch in RLHey, not bad. However, while the previous example was fun and simple, it was noticeably lacking any hint of PyTorch.We could have used a PyTorch Tensor to store the Q table, but that\\'s not any better than using a NumPy array. PyTorch\\'s true utility comes from building neural networks and calculating/applying gradients automatically, which learning the Q table didn\\'t need.Continuous domainsIn our previous example, we mentioned that with only 16 discrete states and 4 actions/states, the Q table only needed to hold 64 values, which is very manageable. However, what if the state or action space is continuous? You could discretize it, but then you have to pick a resolution, and your state-action space could explode exponentially. Treating these binned states or actions as completely different states is also ignoring that two consecutive bins are likely very similar in the needed policy. You can learn these relationships, but doing so is horribly sample inefficient.Instead of learning a Q table then, perhaps a Q function would be more appropriate. This function would take in a state and action as an input and return a Q value as an output. The Q function may be very complex, but as we’ve learned over the past few days, neural networks are very flexible and good for approximating arbitrary functions. Deep Q Networks take such an approach.Cart PoleLet’s look at the cart pole problem next. In this setting, we have a pole attached to a hinge on a cart, with the goal being to keep the pole vertical as long as possible, without traveling too far along the rail. Because of gravity, the pole will fall unless the cart is exactly beneath the pole’s center of gravity. To prevent the pole from falling, the agent can apply a force of +1 or -1 to the cart to move it left and right along a track. The agent receives a reward of +1 for every timestamp the pole remains vertical; the game ends when the pole falls past 15 degrees from vertical or the cart moves more than 2.4 units away from the center. We’re going to somewhat arbitrarily call “success” achieving a reward of +200; alternatively, the agent needs to avoid the aforementioned failure conditions for 200 ticks.Image by authorFirst, let’s create an instance of the cart pole environment:In [ ]:env = gym.make(\\'CartPole-v0\\')Again, we can look at the observation_space for this environment. Also similar to FrozenLake, since this version of the cart pole is an MDP (as opposed to POMDP), the observation is the state itself. We can see that the states for cart pole have 4 dimensions, which correspond to [cart position, cart velocity, pole angle, pole angular velocity]. Importantly, notice these states are continuous values.In [ ]:env.observation_spaceWe can look at them action_space again too. In cart pole, there are two actions available to the agent: [apply force left, apply force right]. We can see this by examining the action_space attribute:In [ ]:env.action_spaceResetting the environment returns our first observations, which we can see has 4 values corresponding to the 4 previously mentioned state variables.In [ ]:env.reset()Before we get into any reinforcement learning, let’s see how we perform actions within the environment.In [ ]:done = Falsewhile not done: env.render() action = env.action_space.sample() _, _, done, _ = env.step(action)Okay, so clearly choosing a random action at every time step doesn’t really achieve our goal of keeping the pole vertical. We’re going to need something smarter.Let’s close that rendering window. We do this with close(). Note that gym renderings can be a little finicky, especially on Windows; either close() or restarting your Jupyter kernel may be necessary to close the rendered window.In [ ]:env.close()Cart pole is actually a fairly simple problem (it’s very low dimensional), and so there are simpler ways to do this, but since we’ve been having so much fun with deep learning, let’s use a neural network. Specifically, let’s build a DQN that uses Q-learning to learn how to balance the pole. We’re going to give our DQN agent 1000 episodes to try and reach the goal of 200 ticks.There are a lot of small details that go into making these models work well, so instead of going through it piece by piece, the full code:In [ ]:# Based on: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/from collections import dequeimport randomimport mathimport gymimport numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fclass DQN(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(4, 24) self.fc2 = nn.Linear(24, 48) self.fc3 = nn.Linear(48, 2)def forward(self, x): x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.relu(x) x = self.fc3(x) return xclass DQNCartPoleSolver: def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False): self.memory = deque(maxlen=100000) self.env = gym.make(\\'CartPole-v0\\') if monitor: self.env = gym.wrappers.Monitor(self.env, \\'../data/cartpole-1\\', force=True) self.gamma = gamma self.epsilon = epsilon self.epsilon_min = epsilon_min self.epsilon_decay = epsilon_log_decay self.alpha = alpha self.alpha_decay = alpha_decay self.n_episodes = n_episodes self.n_win_ticks = n_win_ticks self.batch_size = batch_size self.quiet = quiet if max_env_steps is not None: self.env._max_episode_steps = max_env_steps# Init model self.dqn = DQN() self.criterion = torch.nn.MSELoss() self.opt = torch.optim.Adam(self.dqn.parameters(), lr=0.01)def get_epsilon(self, t): return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))def preprocess_state(self, state): return torch.tensor(np.reshape(state, [1, 4]), dtype=torch.float32) def choose_action(self, state, epsilon): if (np.random.random() <= epsilon): return self.env.action_space.sample() else: with torch.no_grad(): return torch.argmax(self.dqn(state)).numpy()def remember(self, state, action, reward, next_state, done): reward = torch.tensor(reward) self.memory.append((state, action, reward, next_state, done)) def replay(self, batch_size): y_batch, y_target_batch = [], [] minibatch = random.sample(self.memory, min(len(self.memory), batch_size)) for state, action, reward, next_state, done in minibatch: y = self.dqn(state) y_target = y.clone().detach() with torch.no_grad(): y_target[0][action] = reward if done else reward + self.gamma * torch.max(self.dqn(next_state)[0]) y_batch.append(y[0]) y_target_batch.append(y_target[0]) y_batch = torch.cat(y_batch) y_target_batch = torch.cat(y_target_batch) self.opt.zero_grad() loss = self.criterion(y_batch, y_target_batch) loss.backward() self.opt.step() if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decaydef run(self): scores = deque(maxlen=100)for e in range(self.n_episodes): state = self.preprocess_state(self.env.reset()) done = False i = 0 while not done: if e % 100 == 0 and not self.quiet: self.env.render() action = self.choose_action(state, self.get_epsilon(e)) next_state, reward, done, _ = self.env.step(action) next_state = self.preprocess_state(next_state) self.remember(state, action, reward, next_state, done) state = next_state i += 1scores.append(i) mean_score = np.mean(scores) if mean_score >= self.n_win_ticks and e >= 100: if not self.quiet: print(\\'Ran {} episodes. Solved after {} trials ✔\\'.format(e, e - 100)) return e - 100 if e % 100 == 0 and not self.quiet: print(\\'[Episode {}] - Mean survival time over last 100 episodes was {} ticks.\\'.format(e, mean_score))self.replay(self.batch_size) if not self.quiet: print(\\'Did not solve after {} episodes 😞\\'.format(e)) return eif __name__ == \\'__main__\\': agent = DQNCartPoleSolver() agent.run() agent.env.close()Reinforcement learning can be kind of noisy. In some sense, it depends on your agent “lucking” into the right behavior so that it can learn from it, and occasionally one can get stuck in a bad rut. Even if your agent fails to “solve” the problem (i.e. reach 200 ticks), you should still see the mean survival time mostly climbing as the agent experiences more episodes. You may need to re-run learning a couple of times for the agent to reach 200 ticks.Once you have reached this sentence you have gone through all the steps for introduction to Reinforcement Learning (RL) in PyTorchHere is a summary of your accomplishment today:1. Supervised learning2. Reinforcement Learning3. Open AI Gym4. FrozenLake (a Grid World)5. PyTorch in RL6. Cart PoleMachine LearningReinforcement LearningPytorch----6Published in Analytics Vidhya76K ·Last published Sep 1, 2025Analytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comWritten by Harsh Panchal30 ·2 Python | Machine Learning | Data science enthusiast.'),\n",
       " Document(metadata={'source': '/tmp/tmp5wv6xw6u/tmp.pdf'}, page_content='Reinforcement Learning: A Survey Abstract This paper surveys the field of reinforcement learning from a computer-science per- spective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning. Reinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last five to ten years, it has attracted rapidly increasing interest in the machine learning and artificial intelligence communities. Its promise is beguiling—a way of programming agents by reward and punishment without needing to specify how the task is to be achieved. But there are formidable computational obstacles to fulfilling the promise. This paper surveys the historical basis of reinforcement learning and some of the current work from a computer science perspective. We give a high-level overview of the field and a taste of some specific approaches. It is, of course, impossible to mention all of the important work in the field; this should not be taken to be an exhaustive account. Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment. The work described here has a strong family resemblance to eponymous work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” It is appropriately thought of as a class of problems, rather than as a set of techniques. There are two main strategies for solving reinforcement-learning problems. The first is to search in the space of behaviors in order to find one that performs well in the environment. This approach has been taken by work in genetic algorithms and genetic programming, ©1996 AT Access Foundation and Morgan Kaufmann Publishers. All rights reserved. KAELBLING, LITTMAN, & Moore as well as some more novel search techniques . The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world. This paper is devoted almost entirely to the second set of techniques because they take advantage of the special structure of reinforcement-learning problems that is not available in optimization problems in general. It is not yet clear which set of approaches is best in which circumstances. The rest of this section is devoted to establishing notation and describing the basic reinforcement-learning model. Section 2 explains the trade-off between exploration and exploitation and presents some solutions to the most basic case of reinforcement-learning problems, in which we want to maximize the immediate reward. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them. Section 4 considers some classic model-free algorithms for reinforcement learning from delayed reward: adaptive heuristic critic, TD(A) and Q-learning. Section 5 demonstrates a continuum of algorithms that are sensitive to the amount of computation an agent can perform between actual steps of action in the environment. Generalization—the cornerstone of mainstream machine learning research—has the potential of considerably aiding reinforcement learning, as described in Section 1.1 Reinforcement-Learning Model In the standard reinforcement-learning model, an agent is connected to its environment via perception and action, as depicted in Figure 238 REINFORCEMENT LEARNING: A SURVEY Formally, the model consists of e a discrete set of environment states, S; e adiscrete set of agent actions, A; and e aset of scalar reinforcement signals; typically {0,1}, or the real numbers. The figure also includes an input function J, which determines how the agent views the environment state; we will assume that it is the identity function (that is, the agent perceives the exact state of the environment) until we consider partial observability in Section An intuitive way to understand the relation between the agent and its environment is with the following example dialogue. Environment: You are in state Agent: T\\'ll take action Environment: You received a reinforcement of 7 units. You are now in state Agent: T\\'ll take action Environment: You received a reinforcement of -4 units. You are now in state Agent: T\\'ll take action The agent’s job is to find a policy 7, mapping states to actions, that maximizes some long-run measure of reinforcement. We expect, in general, that the environment will be non-deterministic; that is, that taking the same action in the same state on two different occasions may result in different next states and/or different reinforcement values. This happens in our example above: from state 65, applying action 2 produces differing rein- forcements and differing states on two occasions. However, we assume the environment is stationary; that is, that the probabilities of making state transitions or receiving specific reinforcement signals do not change over time.! Reinforcement learning differs from the more widely studied problem of supervised learn- ing in several ways. The most important difference is that there is no presentation of in- put/output pairs. Instead, after choosing an action the agent is told the immediate reward and the subsequent state, but is not told which action would have been in its best long-term interests. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally. Another difference from supervised learning is that on-line performance is important: the evaluation of the system is often concurrent with learning. 239 KAELBLING, LITTMAN, & Moore Some aspects of reinforcement learning are closely related to search and planning issues in artificial intelligence. AI search algorithms generate a satisfactory trajectory through a graph of states. Planning operates in a similar manner, but typically within a construct with more complexity than a graph, in which states are represented by compositions of logical expressions instead of atomic symbols. These A] algorithms are less general than the reinforcement-learning methods, in that they require a predefined model of state transitions, and with a few exceptions assume determinism. On the other hand, reinforcement learning, at least in the kind of discrete cases for which theory has been developed, assumes that the entire state space can be enumerated and stored in memory—an assumption to which conventional search algorithms are not tied. 1.2 Models of Optimal Behavior Before we can start thinking about algorithms for learning to behave optimally, we have to decide what our model of optimality will be. In particular, we have to specify how the agent should take the future into account in the decisions it makes about how to behave now. There are three models that have been the subject of the majority of work in this area. The finite-horizon model is the easiest to think about; at a given moment in time, the agent should optimize its expected reward for the next h steps: h EQ ri) : t=0 it need not worry about what will happen after that. In this and subsequent expressions, r, represents the scalar reward received t steps into the future. This model can be used in two ways. In the first, the agent will have a non-stationary policy; that is, one that changes over time. On its first step it will take what is termed a h-step optimal action. This is defined to be the best action available given that it has h steps remaining in which to act and gain reinforcement. On the next step it will take a (h — 1)-step optimal action, and so on, until it finally takes a 1-step optimal action and terminates. In the second, the agent does receding-horizon control, in which it always takes the h-step optimal action. The agent always acts according to the same policy, but the value of h limits how far ahead it looks in choosing its actions. The finite-horizon model is not always appropriate. In many cases we may not know the precise length of the agent’s life in advance. The infinite-horizon discounted model takes the long-run reward of the agent into ac- count, but rewards that are received in the future are geometrically discounted according to discount factor +, (where 0 < 7 < 1): oo BD yr t=0 We can interpret y in several ways. It can be seen as an interest rate, a probability of living another step, or as a mathematical trick to bound the infinite sum. The model is conceptu- ally similar to receding-horizon control, but the discounted model is more mathematically tractable than the finite-horizon model. This is a dominant reason for the wide attention this model has received. 240 REINFORCEMENT LEARNING: A SURVEY Another optimality criterion is the average-reward model, in which the agent is supposed to take actions that optimize its long-run average reward: h lim BEY n) . hoo +=0 Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1 . One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount of reward in the initial phases and the other of whic does not. Reward gained on any initial prefix of the agent’s life is overshadowed by the long-run average performance. It is possible to generalize this model so that it takes into account both the long run average and the amount of initial reward than can be gained. In the generalized, bias optimal model, a policy is preferred if it maximizes the long-run average and ties are broken by the initial extra reward. Figure 2 contrasts these models of optimality by providing an environment in whic changing the model of optimality changes the optimal policy. In this example, circles represent the states of the environment and arrows are state transitions. There is only a single action choice from every state except the start state, which is in the upper left and marked with an incoming arrow. All rewards are zero except where marked. Under a finite-horizon model with h = 5, the three actions yield rewards of +6.0, +0.0, and +0.0, so the first action should be chosen; under an infinite-horizon discounted model with 7 = 0.9, the three choices yield +16.2, +59.0, and +58.5 so the second action should be chosen; and under the average reward model, the third action should be chosen since it leads to an average reward of + carefully in any application. The finite-horizon model is appropriate when the agent’s lifetime is known; one im- portant aspect of this model is that as the length of the remaining lifetime decreases, the agent’s policy may change. A system with a hard deadline would be appropriately modeled this way. The relative usefulness of infinite-horizon discounted and bias-optimal models is still under debate. Bias-optimality has the advantage of not requiring a discount parameter; however, algorithms for finding bias-optimal policies are not yet as well-understood as those for finding optimal infinite-horizon discounted policies. 1.38 Measuring Learning Performance The criteria given in the previous section can be used to assess the policies learned by a given algorithm. We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use. e Eventual convergence to optimal. Many algorithms come with a provable guar- antee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992). This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau 241 KAELBLING, LITTMAN, & Moore +2 Finite horizon, h=4 +10 Infinite horizon, y=0.9 OOO0-0-00\" Average reward at 99% of optimality may, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate. e Speed of convergence to optimality. Optimality is usually an asymptotic result, and so convergence speed is an ill-defined measure. More practical is the speed of convergence to near-optimality. This measure begs the definition of how near to optimality is sufficient. A related measure is level of performance after a given time, which similarly requires that someone define the given time. It should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accu- racy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework , there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance. That is usually an inappropriate view for an agent with a long existence in a complex environment. In spite of the mismatch between embedded reinforcement learning and the train/test perspective, Fiechter (1994) provides a PAC analysis for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views. Measures related to speed of learning have an additional weakness. An algorithm that merely tries to achieve optimality as fast as possible may incur unnecessarily large penalties during the learning period. A less aggressive strategy taking longer to achieve optimality, but gaining greater total reinforcement during its learning might be preferable. e Regret. A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret (Berry & Fristedt, 1985). It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain. 242 REINFORCEMENT LEARNING: A SURVEY 1.4 Reinforcement Learning and Adaptive Control Adaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algo- rithms for improving a sequence of decisions from experience. Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and ac- tions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory. A very common formulation of cost functions in adaptive control are quadratic penalties on deviation from desired state and action vectors. Most importantly, although the dynamic model of the system is not known in advance, and must be esti- mated from data, the structure of the dynamic model is fixed, leaving model estimation as a parameter estimation problem. These assumptions permit deep, elegant and powerful mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive control algorithms. One major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the roblems of exploration, we treat a very simple case in this section. The fundamental issues and approaches described here will, in many cases, transfer to the more complex instances of reinforcement learning discussed later in the paper. The simplest possible reinforcement-learning problem is known as the k-armed bandit problem, which has been the subject of a great deal of study in the statistics and applied mathematics literature (Berry & Fristedt, 1985). The agent is in a room with a collection of k gambling machines (each called a “one-armed bandit” in colloquial English). The agent is ermitted a fixed number of pulls, h. Any arm may be pulled on each turn. The machines do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal machine. When arm ? is pulled, machine 7 pays off 1 or 0, according to some underlying probability parameter p;, where payoffs are independent events and the p;s are unknown. What should the agent’s strategy be? This problem illustrates the fundamental tradeoff between exploitation and exploration. The agent might believe that a particular arm has a fairly high payoff probability; should it choose that arm all the time, or should it choose another one that it has less information about, but seems to be worse? Answers to these questions depend on how long the agent is expected to play the game; the longer the game lasts, the worse the consequences of prematurely converging on a sub-optimal arm, and the more the agent should explore. a There is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt (1985). We use the term “action” to indicate the agent’s choice of arm to pull. This eases the transition into delayed reinforcement models in Section Section 2.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case. 243 KAELBLING, LITTMAN, & Moore Section 2.2 presents three techniques that are not formally justified, but that have had wide use in practice, and can be applied (with similar lack of guarantee) to the general case. 2.1 Formally Justified Techniques There is a fairly well-developed formal theory of exploration for very simple problems. Although it is instructive, the methods it provides do not scale well to more complex problems. 2.1.1 DYNAMIC-PROGRAMMING APPROACH If the agent is going to be acting for a total of A steps, it can use basic Bayesian reasoning to solve for an optimal strategy (Berry & Fristedt, 1985). This requires an assumed prior joint distribution for the parameters {p;}, the most natural of which is that each p; is independently uniformly distributed between 0 and can be represented as a tabulation of action choices and payoffs: {n1, wi, na, W2,..., Mk, We} denotes a state of play in which each arm 7 has been pulled n; times with w; payoffs. We write V*(n1, wi,-.-,2k, WE) as the expected payoff remaining, given that a total of h pulls are available, and we use the remaining pulls optimally. If 0; nj; = h, then there are no remaining pulls, and V*(nj, wy,..., nz, we) = Ve (ny.wp..-.,npewp) = max; B Future payoff if agent takes action a, then acts optimally for remaining pulls = max; piV™ (ny, W;,---,2i +1, wie+1,---, MK. We)+ (1 = pi) V*(m1, Wi, ee ME 1, Wi, +. Me, WE) where p; is the posterior subjective probability of action 7 paying off given n;, w; and our prior probability. For the uniform priors, which result in a beta distribution, p; = The expense of filling in the table of V* values in this way for all attainable belief states is linear in the number of belief states times actions, and thus exponential in the horizon. 2.1.2 GITTINS ALLOCATION INDICES Gittins gives an “allocation index” method for finding the optimal choice of action at each step in k-armed bandit problems . The technique only applies under the discounted expected reward criterion. For each action, consider the number of times it has been chosen, n, versus the number of times it has paid off, w. For certain discount factors, there are published tables of “index values,” I(n,w) for each pair of n and w. Look up the index value for each action 7, I(nj,w;). It represents a comparative measure of the combined value of the expected payoff of action i (given its history of payoffs) and the value of the information that we would get by choosing it. Gittins has shown that choosing the action with the largest index value guarantees the optimal balance between exploration and exploitation. 244 REINFORCEMENT LEARNING: A SURVEY a=0 a=1 KL0+—O+—0 +++ O40 O90 + 0 0-00 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=1 a=0 a=1 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=0 Because of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward (Salganicoff & Ungar, 1995). Unfortunately, no one has yet been able to find an analog of index values for delayed reinforcement problems. 2.1.3 LEARNING AUTOMATA A branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar (1989), which were originally described explicitly as finite state automata. The Tsetlin automaton shown in Figure 3 provides an example that solves a 2-armed bandit arbitrarily near optimally as N approaches infinity. It is inconvenient to describe algorithms as finite-state automata, so a move was made to describe the internal state of the agent as a probability distribution according to which actions would be chosen. The probabilities of taking different actions would be adjusted according to their previous successes and failures. An example, which stands among a set of algorithms independently developed in the mathematical psychology literature (Hilgard & Bower, 1975), is the linear reward-inaction algorithm. Let p; be the agent’s probability of taking action e When action a; succeeds, Di t= pita(l—p) Pj (= py— ap; for 7 #2 e When action a; fails, p; remains unchanged (for all j). This algorithm converges with probability 1 to a vector containing a single 1 and the rest 0’s (choosing a particular action with probability 1). Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making a small (Narendra & Thathachar, 1974). There is no literature on the regret of this algorithm. 245 KAELBLING, LITTMAN, & Moore 2.2 Ad-Hoc Techniques In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun (1992) has surveyed a variety of these techniques. 2.2.1 GREEDY STRATEGIES The first strategy that comes to mind is to always choose the action with the highest esti- mated payoff. The flaw is that early unlucky sampling might indicate that the best action’s reward is less than the reward obtained from a suboptimal action. The suboptimal action will always be picked, leaving the true optimal action starved of data and its superiority never discovered. An agent must explore to ameliorate this outcome. A useful heuristic is optimism in the face of uncertainty in which actions are selected greedily, but strongly optimistic prior beliefs are put on their payoffs so that strong negative evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrar- ily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the ez- ploration bonus in Dyna , curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993). 2.2.2 RANDOMIZED STRATEGIES Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random. Some versions of this strategy start with a large value of p to encourage initial exploration, which is slowly decreased. An objection to the simple strategy is that when it experiments with a non-greedy action it is no more likely to try a promising alternative than a clearly hopeless alternative. A slightly more sophisticated strategy is Boltzmann exploration. In this case, the expected reward for taking action a, E.R(a) is used to choose an action probabilistically according to the distribution (ER(a)/T PO Sea PROT The temperature parameter T can be decreased over time to decrease exploration. This method works well if the best action is well separated from the others, but suffers somewhat when the values of the actions are close. It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care. 2.2.3 INTERVAL-BASED TECHNIQUES Exploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling’s interval estimation algorithm (1993b) stores statistics for each action a;: w; is the number of successes and n; the number of trials. An action is chosen by computing the upper bound of a 100-(1—a)% 246 REINFORCEMENT LEARNING: A SURVEY confidence interval on the success probability of each action and choosing the action with the highest upper bound. Smaller values of the a parameter encourage greater exploration. When payoffs are boolean, the normal approximation to the binomial distribution can be used to construct the confidence interval (though the binomial should be used for small n). Other payoff distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods (Box & Draper, 1987), which are used for comparing multiple treatments (for example, fertilizers or drugs) to determine which treatment (if any) is best in as small a set of experiments as possible. 2.3 More General Problems When there are multiple states, but reinforcement is still immediate, then any of the above solutions can be replicated, once for each state. However, when generalization is required, these solutions must be integrated with generalization methods (see section 6); this is straightforward for the simple ad-hoc methods, but it is not understood how to maintain theoretical guarantees. Many of these techniques focus on converging to some regime in which exploratory actions are taken rarely or never; this is appropriate when the environment is stationary. However, when the environment is non-stationary, exploration must continue to take place, in order to notice changes in the world. Again, the more ad-hoc techniques can be modified to deal with this in a plausible manner (keep temperature parameters from going to 0; decay the statistics in interval estimation), but none of the theoretically guaranteed methods can be applied. In the general case of the reinforcement learning problem, the agent’s actions determine not only its immediate reward, but also (at least probabilistically) the next state of the environment. Such environments can be thought of as networks of bandit problems, but the agent must take into account the next state as well as the immediate reward when it decides which action to take. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account. The agent will have to be able to learn from delayed reinforcement: it may take a long sequence of actions, receiving insignificant reinforcement, then finally arrive at a state with high reinforcement. The agent must be able to learn which of its actions are desirable based on reward that can take place arbitrarily far in the future. 3.1 Markov Decision Processes Problems with delayed reinforcement are well modeled as Markov decision processes (MDPs). An MDP consists of e@ aset of states S, e a set of actions A, 247 KAELBLING, LITTMAN, & Moore e a reward function R:S x A> ®R, and e astate transition function T : S x A — TI(S), where a member of II(S) is a probability istribution over the set S (i.e. it maps states to probabilities). We write T(s, a, s’) for the probability of making a transition from state s to state s’ using action a. The state transition function probabilistically specifies the next state of the environment as a function of its current state and the agent’s action. The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994). Although general MDPs may have infinite (even uncountable) state and action spaces, we will only discuss methods for solving finite-state and finite-action problems. In section 6, we discuss methods for solving problems with continuous input and output spaces. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in MDP environments, we will ex- plore techniques for determining the optimal policy given a correct model. These dynamic programming techniques will serve as the foundation and inspiration for the learning al- gorithms to follow. We restrict our attention mainly to finding optimal policies for the infinite-horizon discounted model, but most of these algorithms have analogs for the finite- horizon and average-case models as well. We rely on the result that, for the infinite-horizon discounted model, there exists an optimal deterministic stationary policy . We will speak of the optimal value of a state—it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy. Using 7 as a complete decision policy, it is written V*(s)= max E (>: on) t=0 This optimal value function is unique and can be defined as the solution to the simultaneous equations sles V*(s) = max (n a+7>> renner) WseS, (1) which assert that the value of a state s is the expected instantaneous reward plus the expected discounted value of the next state, using the best available action. Given the optimal value function, we can specify the optimal policy as x*(s) = argmax | R(s,a) +7 Ss T(s,a,8\\')V*(s\") “ ES 3.2.1 VALUE ITERATION One way, then, to find an optimal policy is to find the optimal value function. It can be determined by a simple iterative algorithm called value iteration that can be shown to converge to the correct V* values (Bellman, 1957; Bertsekas, 1987). 248 REINFORCEMENT LEARNING: A SURVEY initialize V(s) arbitrarily loop until policy good enough loop for s€S loop for aE A Q(s,4) = R(s,a) +7 Dyes Ts, 4, 8)V (6% V(s) := max, Q(s, a) end loop end loop It is not obvious when to stop the value iteration algorithm. One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function (Williams & Baird, 1993b). It says that if the maximum difference between two successive value functions is less than ¢, then the value of the greedy policy, he policy obtained by choosing, in every state, the action that maximizes the estimated discounted reward, using the current estimate of the value function) differs from the value function of the optimal policy by no more than 2ey/(1— 7) at any state. This provides an effective stopping criterion for the algorithm. Puterman (1994) discusses another stopping criterion, based on the span semi-norm, which may result in earlier termination. Another mportant result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged . And in practice, the greedy policy is often optimal long before the value function has converged. = Value iteration is very flexible. The assignments to V need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value of every state gets updated infinitely often on an infinite run. These issues are treated extensively by Bertsekas (1989), who also proves convergence results. Updates based on Equation 1 are known as full backups since they make use of infor- mation from all possible successor states. It can be shown that updates of the form Qs, a) = Qls,a) +a(r +7 maxQ(s!,a) — Q(s,a)) can also be used as long as each pairing of a and s is updated infinitely often, s’ is sampled from the distribution T(s, a, s’), r is sampled with mean R(s,a) and bounded variance, and the learning rate a is decreased slowly. This type of sample backup is critical to the operation of the model-free methods discussed in the next section. The computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions. Com- monly, the transition probabilities T(s, a, s’) are sparse. If there are on average a constant number of next states with non-zero probability then the cost per iteration is linear in the number of states and linear in the number of actions. The number of iterations required to reach the optimal value function is polynomial in the number of states and the magnitude of the largest reward if the discount factor is held constant. However, in the worst case the number of iterations grows polynomially in 1/(1— y), so the convergence rate slows considerably as the discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b). 249 KAELBLING, LITTMAN, & Moore 3.2.2 Poticy ITERATION The policy iteration algorithm manipulates the policy directly, rather than finding it indi- rectly via the optimal value function. It operates as follows: choose an arbitrary policy 7’ loop wisn compute the value function of policy 7: solve the linear equations V,(s) = R(s,7(s)) + ¥ Nores T(s, 7(8), 8)Vi(s’) improve the policy at each state: n\\'(s) := argmax, (R(s, a) + 7 Dees T(s, a, 8’) Vz(s\\')) until t= 7’ The value function of a policy is just the expected infinite discounted reward that will be gained, at each state, by executing that policy. It can be determined by solving a set of linear equations. Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. If it can, we change the policy to take the new action whenever it is in that situation. This step is guaranteed to strictly improve the performance of the policy. When no improvements are possible, then the policy is guaranteed to be optimal. Since there are at most |A||s| distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations (Puter- man, 1994). However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP (Littman et al., 1995b). 3.2.3 ENHANCEMENT TO VALUE ITERATION AND POLicy ITERATION In practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems. Puterman’s modified policy iteration algorithm (Puterman & Shin, 1978) provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of V,. Instead of finding an exact value for V,, we can perform a few steps of a modified value-iteration step where the policy is held fixed over successive iterations. This can be shown to produce an approximation to V, that converges linearly in Several standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution . State aggre- gation works by collapsing groups of states to a single meta-state solving the abstracted problem (Bertsekas & Castafion, 1989). 250 REINFORCEMENT LEARNING: A SURVEY 3.2.4 COMPUTATIONAL COMPLEXITY Value iteration works by producing successive approximations of the optimal value function. Each iteration can be performed in O(|A||S|?) steps, or faster if there is sparsity in the ransition function. However, the number of iterations required can grow exponentially in he discount factor ; as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of O(|A]|.$|?+|S|?) can be prohibitive. There is no known tight worst-case bound available or policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some ractictioners . Linear programming is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D’Epenoux, 1963; Hoffman & Karp, 1966). An advantage of this approach is that commercial-quality inear-programming packages are available, although the time and space requirements can still be quite high. From a theoretic perspective, linear programming is the only known algorithm that can solve MDPs in polynomial time, although the theoretically efficient algorithms have not been shown to be efficient in practice. In the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model. The model consists of knowledge of the state tran- sition probability function T(s, a, s’) and the reinforcement function R(s,a). Reinforcement learning is primarily concerned with how to obtain the optimal policy when such a model is not known in advance. The agent must interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy. At this point, there are two ways to proceed. e Model-free: Learn a controller without learning a model. e Model-based: Learn a model, and use it to derive a controller. Which approach is better? This is a matter of some debate in the reinforcement-learning community. A number of algorithms have been proposed on both sides. This question also appears in other fields, such as adaptive control, where the dichotomy is between direct and indirect adaptive control. This section examines model-free learning, and Section 5 examines model-based meth- ods. The biggest problem facing a reinforcement-learning agent is temporal credit assignment. How do we know whether the action just taken is a good one, when it might have far- reaching effects? One strategy is to wait until the “end” and reward the actions taken if the result was good and punish them if the result was bad. In ongoing tasks, it is difficult to know what the “end” is, and this might require a great deal of memory. Instead, we will use insights from value iteration to adjust the estimated value of a state based on 251 KAELBLING, LITTM. — 4 AN, & Moore T’4 the immediate reward and the estimated value is known as temporal difference methods (Sutt: temporal-difference learning strategies for the d 4.1 Adaptive Heuristic Critic and TD(\\\\) The adaptive heuristic critic algorithm is an ada Sutton, & Anderson, 1983) in which the value mented by solving a set of linear equations, but TD(0). A block nents: a critic (la reinforcement-lear rithms, modified acting to maximiz v, that is computed by the critic. The critic us learn to map states to their expected discounted is the one currently instantia We can see the analogy wi o deal with multiple states a: value function V, i new policy 7’ tha however, both components op: can be guaranteed to converge and Baird explored the convergence properties for that po maximizes the new value fun o the optimal pol call “incremental variants of iagram for this approach is gi beled AHC), and a reinforcemen ning component can be an instance of any of the k-armed bandit algo- e instantaneous reward, it will ed in the RL comp h modified policy i working in alternation. The policy 7 implemented by RL is fixed and t cy. Now we fix the critic and let the RL com olicy iteration” (Williams & Baird, 1993a). he adaptive heuristic critic. of the next state. This class of algorithms ‘on, 1988). We will consider two different iscounted infinite-horizon model. tive version of policy iteration (Barto, -function computation is no longer imple- is instead computed by an algorithm called iven in Figure be acting to maximize es val being executed eration if we imagine these components e critic learns the onent learn a plementations, ction, and so on. In most im erate simultaneously. Only the alternating implementation icy, under appropriate conditions. Williams of a class of AHC-related algorithms they It remains to explain how the critic can learn the value of a policy. We define (s, a, r,s’) to be an experience tuple summarizing a single t ransition in the environment. Here s is the agent’s state before the transition, a is its choice of action, r the instantaneous reward it receives, and s’ its resulting state. The value o algorithm which uses the update V(s): a policy is learned using Sutton’s TD(0) rule V(s) Whenever a state s is visited, its estimated val since r is the instantaneous reward received and occurring next state. This is analogous to the sa: VV (s\\')—V(s)) ue is updated to be closer to r+ yV(s‘), V(s’) is the estimated value of the actually mple-backup rule from value iteration—the only difference is that the sample is drawn from the real world rather than by simulating a known model. The key idea is that r + yV(s’ 252 is a sample of the value of V(s), and it is REINFORCEMENT LEARNING: A SURVEY more likely to be correct because it incorporates the real r. If the learning rate a is adjusted properly (it must be slowly decreased) and the policy is held fixed, TD(0) is guaranteed to converge to the optimal value function. The TD(0) rule as presented above is really an instance of a more general class of algorithms called TD(X), with 4 = V(u) = V(u) ta(rt+yV(s\\') —V(s)je(u) , but it is applied to every state according to its eligibility e(u), rather than just to the immediately previous state, s. One version of the eligibility trace is defined to be t : _ lifs=s e(s) = Say! *S sq , where 35,5, = { 0 otherwise k=1 The eligibility of a state s is the degree to which it has been visited in the recent past; when a reinforcement is received, it is used to update all the states that have been recently visited, according to their eligibility. When \\\\ = 0 this is equivalent to TD(0). When \\\\ = 1, it is roughly equivalent to updating all the states according to the number of times they were visited by the end of a run. Note that we can update the eligibility online as follows: e(s) i= yAe(s)+1 if s= current state yAe(s) otherwise It is computationally more expensive to execute the general TD(X), though it often converges considerably faster for large \\\\ (Dayan, 1992; Dayan & Sejnowski, 1994). There has been some recent work on making the updates more efficient (Cichosz & Mulawka, 1995) and on changing the definition to make T D(A) more consistent with the certainty-equivalent method (Singh & Sutton, 1996), which is discussed in Section 5.4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins’ Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation. Let Q*(s,a) be the expected discounted reinforcement of taking action ain state s, then continuing by choosing actions optimally. Note that V*(s) is the value of s assuming the best action is taken initially, and so V*(s) = max, Q*(s, a). Q*(s, a) can hence be written recursively as Q*(s,a) = R(s,a) +7 Ss T(s,a, 8’) max Q*(s’, a’). ES e Note also that, since V*(s) = max, Q*(s,a), we have x*(s) = argmax, Q*(s,a) as an optimal policy. Because the Q function makes the action explicit, we can estimate the Q values on- line using a method essentially the same as TD(0), but also use them to define the policy, 253 KAELBLING, LITTMAN, & Moore because an action can be chosen just by taking the one wit current state. The Q-learning rule is Qs.) = Qls,a) + alr +7 maxQ(s\\'a! where (s,a,r,s’) is an experie each state an infinite number of Q values will converge with probability 1 to Q* (Watkins, Jordan, & Singh, 1994). Q-le more than one step previously, When the Q values are ne the agent to act greedily, the maximum Q value for the — Q(s.4)) nce tuple as described earlier. If each action is executed in a is decayed appropriately, the 989; Tsitsiklis, 1994; Jaakkola, arning can also be extended to update states that occurred . as in TD(A) (Peng & Williams, 1994). y converged to their optimal values, it is appropriate for imes on an infinite run and ar. aking, in each situation, the action with the highest @ value. During learning, however, there is a difficult exploitation versus exploration trade-off to be made. There are no good, forma pt one of the standard practice is to ado AHC architectures seem to It can be hard to get components converge togethe is, that the Q values will con level. ly justified approaches to this problem in the general case; ad hoc methods discussed in section 2.ifficult to work with than Q-learning on a practical e relative learning rates right in AHC so that the two In addition, Q-learning is exploration insensitive: that verge to the optimal values, independent of how the agent be more t Tr. behaves while the data is being collected (as long as all state-action pairs are tried often enough). This means that, in Q-learning, the details learning algorithm. For t most effective model-free however, address any of oO ese go a. although the exploration-exploitation issue must be addressed the explora ion strategy will not affect the convergence of the -learning is the most popular and seems to be the earning from delayed reinforcement. It does not, reasons, Q rithm for he issues involved in generalizing over large state and/or action spaces. In addition, it may converge qui e slowly to a good policy. 4.3 Model-free Learning With Average Reward As described, Q-learning can be applied to discounted infinite-horizon MDPs. It can also be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset. Schwartz (1993) examine framework. Although his R-le some MDPs, severa problem they wish to solve th Q-learning (Mahade reward policies. Mahadevan ( a reinforcement-learning persp In particu (and some dynamic cies. Jaakkola, Jor researchers have found the average-reward van, 1994). With that in mind, researchers have studied the problem o ar, he showed that existing reinforcement-learning alg programming algorithms) do not always an and Singh (1995) described an average-reward learning algorithm the problem of adapting Q-learning to an average-reward arning algorithm seems to exhibit convergence problems for criterion closer to the true an a discounted criterion and therefore prefer R-learning to learning optimal average- 996) surveyed model-based average-reward algorithms from ective and found several difficulties with existing algorithms. orithms for average reward roduce bias-optimal poli- with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the expected uture reward for each state as the agent moves through the environment. In 254 REINFORCEMENT LEARNING: A SURVEY addition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new textbook (1995). Although this recent work provides a much needed theoretical foundation to this area of reinforcement learning, many important problems remain unsolved. The previous section showed how it is possible to learn an optimal policy without knowing the models T(s, a, s’) or R(s,a) and without even learning those models en route. Although many of these methods are guaranteed to find optimal policies eventually and use very little computation time per experience, they make extremely inefficient use of the data they gather and therefore often require a great deal of experience to achieve good performance. In this section we still begin by assuming that we don’t know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are especially important in applications in which computation is considered to be cheap and real-world experience costly. 5.1 Certainty Equivalent Methods We begin with the most conceptually straightforward method: first, learn the T and R functions by exploring the environment and keeping statistics about the results of each action; next, compute an optimal policy using one of the methods of Section There are some serious objections to this method: e It makes an arbitrary division between the learning phase and the acting phase. e How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data than a system that interleaves experience gathering with policy-building more tightly (Koenig & Simmons, 1993). See Figure 5 for an example. e The possibility of changes in the environment is also problematic. Breaking up an agent’s life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment changes. A variation on this idea is certainty equivalence, in which the model is learned continually through the agent’s lifetime and, at each step, the current model is used to compute an optimal policy and value function. This method makes very effective use of available data, but still ignores the question of exploration and is extremely computationally demanding, even for fairly small state spaces. Fortunately, there are a number of other model-based algorithms that are more practical. 5.2 Dyna Sutton’s Dyna architecture exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than 255 KAELBLING, LITTMAN, & Moore the certainty-equivalence approach. It simultaneously uses experience to build a model (T and R), uses experience to adjust the policy, and uses the model to adjust the policy. Dyna operates in a loop of interaction with the environment. Given an experience tuple (s,a,s\\',r), it behaves as follows: e Update the model, incrementing statistics for the transition from s to s’ on action a and for receiving reward r for taking action a in state s. The updated models are T and R. e Update the policy at state s based on the newly updated model using the rule Qls.a) = Risa) +7 Do F(s,0,8!) maxQls\\'.a’) . which is a version of the value-iteration update for Q values. e Perform k additional updates: choose k state-action pairs at random and update them according to the same rule as before: Q(sp, ag) :=R(sp, ag) + + oT (sn, ap, 8’) max Q(s\\', a’). e Choose an action a’ to perform in state s’, based on the Q values but perhaps modified by an exploration strategy. The Dyna algorithm requires about f times the computation of Q-learning per instance, but this is typically vastly less than for the naive model-based method. A reasonable value of & can be determined based on the relative speeds of computation and of taking action. Figure 6 shows a grid world in which in each cell the agent has four actions (N, S, E, W) and transitions are made deterministically to an adjacent cell, unless there is a block, in which case no movement occurs. As we will see in Table 1, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational effort, however. 256 REINFORCEMENT LEARNING: A SURVEY Steps before Backups before convergence convergence Q-learning 531,000 531,000 Dyna 62,000 3,055,000 prioritized sweeping 28,000 1,010,000 257 KAELBLING, LITTMAN, & Moore 5.3 Prioritized Sweeping / Queue-Dyna Although Dyna is a great improvement on previous methods, it suffers from being relatively undirected. It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the “interesting” parts of the state space. These problems are addressed by prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams, 1993), which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail. The algorithm is similar to Dyna, except that updates are no longer chosen at random and values are now associated with states (as in value iteration) instead of state-action pairs (as in Q-learning). To make appropriate choices, we must store additional information in the model. Each state remembers its predecessors: the states that have a non-zero transition probability to it under some action. In addition, each state has a priority, initially set to zero. Instead of updating & random state-action pairs, prioritized sweeping updates k states with the highest priority. For each high-priority state s, it works as follows: e Remember the current value of the state: Vou = V(s). e Update the state’s value V(s) = max (i. al+y>oT(s, 4, vie) 3! e Set the state’s priority back to e Compute the value change A = |V,7aq — V(s)|. Use A to modify the priorities of the predecessors of s. If we have updated the V value for state s’ and it has changed by amount A, then the immediate predecessors of s’ are informed of this event. Any state s for which there exists an action a such that T(s,a,s’) # 0 has its priority promoted to A - T(s,a,s’), unless its priority already exceeded that value. The global behavior of this algorithm is that when a real-world transition is “surprising” (the agent happens upon a goal state, for instance), then lots of computation is directed to propagate this new information back to relevant predecessor states. When the real- world transition is “boring” (the actual result is very similar to the predicted result), then computation continues in the most deserving part of the space. Running prioritized sweeping on the problem in Figure 6, we see a large improvement over Dyna. The optimal policy is reached in about half the number of steps of experience and one-third the computation as Dyna required (and therefore about 20 times fewer steps and twice the computational effort of Q-learning). 258 REINFORCEMENT LEARNING: A SURVEY 5.4 Other Model-Based Methods Methods proposed for based methods as well. RTDP (real-time model-based method t of the state-space that he agent is trying to By taking into accoun without necessarily vis’ solving MDPs given a model can be used in the context of model- ynamic programming) (Barto, Bradtke, & Singh, 1995) is another at uses Q-learning to concentrate computational effort on the areas the agent is most likely to occupy. It is specific to problems in which achieve a particular goal state and the reward everywhere else is the start state, it can find a short path from the start to the goal, iting the rest of the state space. The Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman, 994) exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent’s current state and the goal state, if there is one. States that are not in the envelope are summarized by a single “out” state. The planning process is an alternation between finding an optimal policy on the approximate MDP and to the envelope. Action may take place in parallel with planning, in states are also pruned out of the envelope. adding useful states which case irrelevan All of the previous discussion has tacitly assumed that it is possible to enumerate the state and action spaces and store tables of values over them. Except in very small environments, his means impractical memory requirements. It also makes inefficient use of experience. In a large, smooth state space we generally expect similar states to have similar values and sim- actions. Surely, therefore, there should be some more compact representation han a table. Most problems will have continuous or large discrete state spaces; some wil have large or continuous action spaces. The problem of learning in large spaces is addresse: hrough generalization techniques, which allow compact storage of learned information an er of knowledge between “s: ilar optima. rans imilar” states and actions. The large literature of genera. ues from inductive concept learning can be applied to reinforcement learning niques often need to be tailored to specific details of the problem. In the following sections, we explore the application of standar unction-approximation techniques, adaptive resolution models, and hierarchical methods o the problem of reinforcement learning. T he s ization techni . However, tec. p D. he reinforcement-learning architectures and algorithms discussed above have include orage of a variety of mappings, including S — A (policies), S > R (value functions), Sx A— * (Q functions and rewards), S x A > S (deterministic transitions), and S x Ax § => [0,1] (transition probabilities). Some of these mappings, such as transitions an immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervise learning that support noisy training examples. Popular techniques include various neural- network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMAC , and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods. Other mappings, especially the policy 259 KAELBLING, LITTMAN, & Moore mapping, typically need specialized algorithms because training sets of input-output pairs are not available. 6.1 Generalization over Input A reinforcement-learning agent’s current state plays a central role in its selection of reward- maximizing actions. Viewing the agent as a state-free black box, a description of the current state is its input. Depending on the agent architecture, its output is either an action selection, or an evaluation of the current state that can be used to select an action. The problem of deciding how the different aspects of an input affect the value of the output is sometimes called the “structural credit-assignment” problem. This section examines approaches to generating actions or evaluations as a function of a description of the agent’s current state. The first group of techniques covered here is specialized to the case when reward is not delayed; the second group is more generally applicable. 6.1.1 IMMEDIATE REWARD When the agent’s actions do not influence state transitions, the resulting problem becomes one of choosing actions to maximize immediate reward as a function of the agent’s current state. These problems bear a resemblance to the bandit problems discussed in Section 2 except that the agent should condition its action selection on the current state. For this reason, this class of problems has been described as associative reinforcement learning. The algorithms in this section address the problem of learning from immediate boolean reinforcement where the state is vector valued and the action is a boolean vector. Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4. CRBP_ Thecomplementary reinforcement backpropagation algorithm (Ackley & Littman, 1990) (CRBP) consists of a feed-forward network mapping an encoding of the state to an encoding of the action. The action is determined probabilistically from the activation of the output units: if output unit ¢ has activation y;, then bit ¢ of the action vector has value 1 with probability y;, and 0 otherwise. Any neural-network supervised training procedure can be used to adapt the network as follows. If the result of generating action a is r = 1, then the network is trained with input-output pair (s,a). If the result is r = 0, then the network is trained with input-output pair (s,@), where @ = (1— a,...,1— ay). The idea behind this training rule is that whenever an action fails to generate reward, CRBP will try to generate an action that is different from the current choice. Although it seems like the algorithm might oscillate between an action and its complement, that does not happen. One step of training a network will only change the action slightly and since the output probabilities will tend to move toward 0.5, this makes action selection more random and increases search. The hope is that the random distribution will generate an action that works better, and then that action will be reinforced. ARC The associative reinforcement comparison (ARC) algorithm is an instance of the AHc architecture for the case of boolean actions, consisting of two feed- 260 REINFORCEMENT LEARNING: A SURVEY forward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units. In the simplest case, the entire system learns only to optimize immediate reward. First, let us consider the behavior of the network that learns the policy, a mapping from a vector describing s to a Q or The adjustment for the output unit is, in the simplest case, e=r(a—1/2) , where the first factor is the reward received for taking the most recent action and the second encodes which action was taken. The actions are encoded as 0 and 1, so a—1/2 always has the same magnitude; if the reward and the action have the same sign, then action 1 will be made more likely, otherwise action 0 will be. As described, the network will tend to seek actions that given positive reward. To extend this approach to maximize reward, we can compare the reward to some baseline, b. This changes the adjustment to © =(r—d)a~1/2) | where 6 is the output of the second network. The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications (Anderson, 1986; Barto et al., 1983; Lin, 1993b; Sutton, 1984). REINFORCE Algorithms Williams studied the problem of choosing ac- tions to maximize immedate reward. He identified a broad class of update rules that per- form gradient descent on the expected reward and showed how to integrate these rules with backpropagation. This class, called REINFORCE algorithms, includes linear reward-inaction (Section 2.1.3) as a special case. The generic REINFORCE update for a parameter w;; can be written Aw = a4j(r - badger Ina) where a;; is a non-negative factor, r the current reinforcement, 6;; a reinforcement baseline, and g; is the probability density function used to randomly generate actions based on unit activations. Both a;; and b;; can take on different values for each w;;, however, when a;; is constant throughout the system, the expected update is exactly in the direction of the expected reward gradient. Otherwise, the update is in the same half space as the gradient but not necessarily in the direction of steepest increase. Williams points out that the choice of baseline, 6 convergence speed of the algorithm. ij, can have a profound effect on the Logic-Based Methods Another strategy for generalization in reinforcement learning is to reduce the learning problem to an associative problem of learning boolean functions. A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive 261 KAELBLING, LITTMAN, & Moore the generalization process (Kaelbling, 1994b); the other searches the space of syntactic descriptions of functions using a simple generate-and-test method (Kaelbling, 1994a). The restriction to a single boolean output makes these techniques difficult to apply. In very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output. In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The CASCADE method (Kaelbling, 1993b) allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort. 6.1.2 DELAYED REWARD Another method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning. Here, a function approximator is used o represent the value function by mapping a state description to a value. Many reseachers have experimented with this approach: Boyan and Moore (1995) used local memory-based methods in conjunction with value iteration; Lin (1991) used backprop- agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992, 995) used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich (1995) used backpropagation and T D(A) to learn good strategies for job-shop scheduling. Although there have been some positive examples, in general there are unfortunate in- eractions between function approximation and the learning rules. In discrete environments here is a guarantee that any operation that updates the value function (according to the Bellman equations) can only reduce the error between the current value function and the optimal value function. This guarantee no longer holds when generalization is used. These issues are discussed by Boyan and Moore (1995), who give some simple examples of value unction errors growing arbitrarily large when generalization is used with value iteration. Their solution to this, applicable only to certain classes of problems, discourages such diver- gence by only permitting updates whose estimated values can be shown to be near-optimal via a battery of Monte-Carlo experiments. Thrun and Schwartz (1993) theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the “max” operator in the definition of the value function. Several recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show how the appro- priate choice of function approximator can guarantee convergence, though not necessarily to the optimal values. Baird’s residual gradient technique provides guaranteed convergence to locally optimal solutions. Perhaps the gloominess of these counter-examples is misplaced. Boyan and Moore (1995) report that their counter-examples can be made to work with problem-specific hand-tuning despite the unreliability of untuned algorithms that provably converge in discrete domains. Sutton (1996) shows how modified versions of Boyan and Moore’s examples can converge successfully. An open question is whether general principles, ideally supported by theory, can help us understand when value function approximation will succeed. In Sutton’s com- 262 REINFORCEMENT LEARNING : A SURVEY parative experiments with Boyan and Moore’s counter-examples, he changes four aspects of the experiments: the task specifications. generalization. iteration. There are intuitive reasons to believe that the fourth factor is more careful research is needed. Adaptive Resolution Models the environment into regions of states that can be considered t: learning and generating actions. Without detailed is very difficult to know what granularity or placement of par use adaptive resolution; during the course of learning, artition is constructed that is appropriate to the problem is overcome in a Decision Trees va. In environments ued variables, it is possible to learn compact decision trees for representing Q values. The methods tha hat are charac es uniformly in state space, ories. articularly important, but In many cases, what we would like to do is partition e same for the purposes of rior knowledge of the environment, it itions is appropriate. This environment. erized by a set of boolean or discrete- works as fol G-learning algorithm (Chapman & Kk hat no partitioning is necessary an if it were one state. In parallel with input bits: it asks the question whe states in which 6 = aelbling, 1991), tries to learn this process, i her there is some bit 6 in hat the Q values for states in which 6 = 1 are significantly is found, it is used to spl Q values for gathers statistics based on individua e leaves. This method was able to learn very smal ows. It starts by assuming the entire environment as the state description such ifferent from Q values for it the decision tree. Then ; re game environment and or dealing with partial cannot, however, acqui (such as those needed Variable Resolution enables conventional resentations of the Q function in noisy state attributes. It outperformed Q-learning with backpro was used by McCallum (1995 observability re partitions in which attribu o solve parity problems). Dynamic Programming ynamic programming to be he presence of an overwhel to learn behaviors in a complex driving-simulator. I ming number of irrelevant, agation in a simple video- (in conjunction with other techniques es are only significant in combination The VRDP algorithm (Moore, 1991 performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimension- ality. A kd-tree (simi regions. The coarse regions are refined into detailed space which are predic ning “mental trajectories” through state space. This algorithm ed to be important. This no of problems for which disadvantage of requiri ull high-resolution arrays wo ng a guess at an initially vali 263 ar to a decision tree) is used to parti tion state space into coarse regions, but only in parts of the state ion of importance is obtained by run- proved effective on a number uld have been impractical. It has the trajectory through state-space. (a) KAELBLING, LITTMAN, & Moore (b) (c) Start Goal NF i i inal fo EE Bs The point robot must find a path from y of the barrier lines. (b) The path taken by rial. It begins with intense exploration to find a blem. route out of the almost entirely enclosed start region. Having eventually reached a sufficiently high resolution, it discovers the gap and proceeds gree ily towards the goal, only to be temporarily blocked by the goal’s barrier region. (c) The second trial. PartiGame Algorithm Moore’s PartiGame algorithm is another solution to the problem of learning to achieve goal configurations in deterministic high-dimensional continuous s aces by learning an adaptive-resolution model. It also divides the environment into cells; but in each cell, the actions available consist of aiming at the neighboring cells (this aiming problem sta incremental is accomplished by a local controller, which must be provided as ement). The graph of cell transitions is solved for shortest paths in an online manner, but a minimax criterion is used to detect when a group of cells is art of the too coarse to prevent movement between obstacles or to avoid limit cycles. The offending cells are spli choose appropria An important fea it also struc the agent wi small local c ures 1 ini ang to higher resolution. Eventually, the environment is divided up jus e actions for ach exploration of s ially try someth: es when all the Figure 7a shows a two-dimens\\' of a robot using second trial, star This is a very than a minute. T limits its applica methods. ed from a slight fast algorithm, | e restriction of bility, however. ture is that, as well as reducing memory and computational re enough to ieving the goal, but no unnecessary distinctions are made. uirements, ate space in a multi-resolution manner. Given a failure, ing very different to rectify the failure, and only resort to ualitatively different strategies have been exhausted. ional continuous maze. Figure 7b shows the performance he PartiGame algorithm during the very first trial. Figure 7c shows the y different position. earning policies in spaces of up to nine dimensions in less he current implementation to deterministic environments McCallum (1995) suggests some related tree-structured 264 REINFORCEMENT LEARNING: A SURVEY 6.2 Generalization over Actions The networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well. In cases such as this when actions are described combinatorially, it is important to generalize over actions to avoid keeping separate statistics for the huge number of actions that can be chosen. In continuous action spaces, the need for generalization is even more pronounced. When estimating Q values using a neural network, it is possible to use either a distinct network for each action, or a network with a distinct output for each action. When the action space is continuous, neither approach is possible. An alternative strategy is to use a single network with both the state and action as input and Q value as the output. Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value (Baird & Klopf, 1993). Gullapalli has developed a “neural” reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience. When the chosen actions are not performing well, the variance is high, resulting in exploration of the range of choices. When an action performs well, the mean is moved in that direction and the variance decreased, resulting in a tendency to generate more action values near the successful one. This method was successfully employed to learn to control a robot arm with many continuous degrees of freedom. 6.3 Hierarchical Methods Another strategy for dealing with large state spaces is to treat them as a hierarchy of learning problems. In many cases, hierarchical solutions introduce slight sub-optimality in performance, but potentially gain a good deal of efficiency in execution time, learning time, and space. Hierarchical learners are commonly structured as gated behaviors, as shown in Figure 6.3.1 FEUDAL Q-LEARNING Feudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external environment. Its actions consist of commands that 265 KAELBLING, LITTMAN, & Moore it can give to the low-level learner. When the master generates a particular command to the slave, it must reward the slave for taking actions that satisfy the command, even if they do not result in external reinforcement. The master, then, learns a mapping from states to commands. The slave learns a mapping from commands and states to external actions. The set of “commands” and their associated reinforcement functions are established in advance of the learning. This is really an instance of the general “gated behaviors” approach, in which the slave can execute any of the behaviors depending on its command. The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels. 6.3.2 COMPOSITIONAL Q-LEARNING Singh’s compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of condi- tions in sequential order. The achievement of the conditions provides reinforcement for the elemental tasks, which are trained first to achieve individual subgoals. Then, the gating function learns to switch the elemental tasks in order to achieve the appropriate high-level sequential goal. This method was used by Tham and Prager (1994) to learn to control a simulated multi-link robot arm. 6.3.3 HIBRARCHICAL DISTANCE TO GOAL Especially if we consider reinforcement learning modules to be part of larger agent archi- tectures, it is important to consider problems in which goals are dynamically input to the learner. Kaelbling’s HDG algorithm (1993a) uses a hierarchical approach to solving prob- lems when goals of achievement (the agent should get to a particular state as quickly as possible) are given to an agent dynamically. The HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned (a priori, but more recent work addresses the case of learning the partition) into a set of regions whose centers are known as “landmarks.” If the agent is 266 REINFORCEMENT LEARNING: A SURVEY office currently in the same region as the goal, then it uses low-level actions to move to the goal. If not, then high-level information is used to determine the next landmark on the shortest path from the agent’s closest landmark to the goal’s closest landmark. Then, the agent uses low-level information to aim toward that next landmark. If errors in action cause deviations in the path, there is no problem; the best aiming point is recomputed on every step. In many real-world environments, it will not be possible for the agent to have perfect and complete perception of the state of the environment. Unfortunately, complete observability is necessary for learning methods based on MDPs. In this section, we consider the case in which the agent makes observations of the state of the environment, but these observations may be noisy and provide incomplete information. In the case of a robot, for instance, it might observe whether it is in a corridor, an open room, a T-junction, etc., and those observations might be error-prone. This problem is also referred to as the problem of “incomplete perception,” “perceptual aliasing,” or “hidden state.” In this section, we will consider extensions to the basic MDP framework for solving partially observable problems. The resulting formal model is called a partially observable Markov decision process or POMDP. 7.1 State-Free Deterministic Policies The most naive strategy for dealing with partial observability is to ignore it. That is, to reat the observations as if they were the states of the environment and try to learn to behave. Figure 9 shows a simple environment in which the agent is attempting to get to he printer from an office. If it moves from the office, there is a good chance that the agent will end up in one of two places that look like “hall”, but that require different actions for getting to the printer. If we consider these states to be the same, then the agent cannot ossibly behave optimally. But how well can it do? The resulting problem is not Markovian, and Q-learning cannot be guaranteed to con- verge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate (Chrisman & 267 KAELBLING, LITTMAN, & Moore Littman, 1993). It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations. Unfortunately, when the environment is not Markovian, the transition probabilities depend on the policy being executed, so this new policy will induce a new set of transition probabilities. This approach may yield plausible results in some cases, but again, there are no guarantees. It is reasonable, though, to ask what the optimal policy (mapping from observations to actions, in this case) is. It is NP-hard (Littman, 1994b) to find this mapping, and even the best mapping can have very poor performance. In the case of our agent trying to get to the printer, for instance, any deterministic state-free policy takes an infinite number of steps to reach the goal on average. 7.2 State-Free Stochastic Policies Some improvement can be gained by considering stochastic policies; these are mappings from observations to probability distributions over actions. If there is randomness in the agent’s actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan (1995) have developed an algorithm for finding locally-optimal stochastic policies, but finding a globally optimal policy is still NP hard. In our example, it turns out that the optimal stochastic policy is for the agent, when in a state that looks like a hall, to go east with probability 2 — /2 % 0.6 and west with probability /2 —1 % 0. 7.3 Policies with Internal State The only way to behave truly effectively in a wide-range of environments is to use memory of previous actions and observations to disambiguate the current state. There are a variety of approaches to learning policies with internal state. Recurrent Q-learning One intuitively simple approach is to use a recurrent neural net- work to learn Q values. The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain “history features” to predict value. This approach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin & Mitchell, 1992; Schmidhuber, 1991b). It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems. Classifier Systems Classifier systems (Holland, 1975; Goldberg, 1989) were explicitly developed to solve problems with delayed reward, including those requiring short-term memory. The internal mechanism typically used to pass reward back through chains of decisions, called the bucket brigade algorithm, bears a close resemblance to Q-learning. In spite of some early successes, the original design does not appear to handle partially ob- served environments robustly. Recently, this approach has been reexamined using insights from the reinforcement- learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems (Dorigo & Bersini, 1994). Cliff and Ross (1994) start with Wilson’s zeroth- 268 REINFORCEMENT LEARNING: A SURVEY evel classifier system and add one and two-bit memory registers. They find hat, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments. Dorigo and Colombetti applied classifier systems to a moderately complex problem of learning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti, 994). Finite-history-window Approach One way to restore the Markov property is to allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell (1992) used a fixed-width finite history window to learn a pole balancing task. McCallum (1995) describes the “utile suffix memory” which learns a variable-width window hat serves simultaneously as a model of the environment and a finite-memory policy. This system has had excellent results in a very complex driving-simulation domain (McCallum, 995). Ring (1994) has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations. POMDP Approach Another strategy consists of using hidden Markov model (HMM) echniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Monahan, 1982). Chrisman (1992) showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum (1993), also gave heuristic state- splitting rules to attempt to learn the smallest possible model for a given environment. The resulting model can then be used to integrate information from the agent’s observations in order to make decisions. Figure 10 illustrates the basic structure for a perfect-memory controller. The component on the left is the state estimator, which computes the agent’s belief state, b as a function of the old belief state, the last action a, and the current observation Now we are left with the problem of finding a policy mapping belief states into action. This problem can be formulated as an MDP, but it is difficult to solve using the techniques described earlier, because the input space is continuous. Chrisman’s approach (1992) does not take into account future uncertainty, but yields a policy after a small amount of com- putation. A standard approach from the operations-research literature is to solve for the 269 KAELBLING, LITTMAN, & Moore optimal policy (or a close approximation thereof) based on its representation as a piecewise- linear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations (Cassandra et al., 1994; Littman, Cassandra, & Kaelbling, 1995a). One reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act. But it is unsurprising that it has also been used by a number of researchers as a practical computational tool for constructing autonomous systems that improve themselves with experience. These applications have ranged from robotics, to industrial manufacturing, to combinatorial search problems such as computer game playing. Practical applications provide a test of the efficacy and usefulness of learning algorithms. They are also an inspiration for deciding which components of the reinforcement learning framework are of practical importance. For example, a researcher with a real robotic task can provide a data point to questions such as: e How important is optimal exploration? Can we break the learning period into explo- ration phases and exploitation phases? e What is the most useful model of long-term reward: Finite horizon? Discounted? Infinite horizon? e How much computation is available between agent decisions and how should it be used? e What prior knowledge can we build into the system, and which algorithms are capable of using that knowledge? Let us examine a set of practical applications of reinforcement learning, while bearing these questions in mind. 8.1 Game Playing Game playing has dominated the Artificial Intelligence world as a problem domain ever since he field was born. Two-player games do not fit into the established reinforcement-learning ramework since the optimality criterion for games is not one of maximizing reward in the ace of a fixed environment, but one of maximizing reward against an optimal adversary (minimax). Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games (Littman, 1994a) and many researchers have used reinforcement earning in these environments. One application, spectacularly far ahead of its time, was Samuel’s checkers playing system . This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning. More recently, Tesauro (1992, 1994, 1995) applied the temporal difference algorithm o backgammon. Backgammon has approximately 107° states, making table-based rein- orcement learning impossible. Instead, Tesauro used a backpropagation-based three-layer 270 REINFORCEMENT LEARNING: A SURVEY Training Hidden Results Games Units Basic Poor TD 1.0 300,000 80 Lost by 13 points in 51 games TD 2.0 800,000 40 Lost by 7 points in 38 games TD 2.1 1,500,000 80 Lost by 1 point in 40 games games against the top human professional players. A backgammon tournament involves playing a series of games for points until one player reaches a set target. TD-Gammon won none of these tournaments but came sufficiently close that it is now considered one of the best few players in the world. neural network as a function approximator for the value function Board Position > Probability of victory for current player. Two versions of the learning algorithm were used. The first, which we will call Basic TD- Gammon, used very little predefined knowledge of the game, and the representation of a board position was virtually a raw encoding, sufficiently powerful only to permit the neural network to distinguish between conceptually different positions. The second, TD-Gammon, was provided with the same raw state crafted features of backgammon board information supplemented by a number of hand- positions. Providing hand-crafted features in this manner is a good example of how inductive biases from human knowledge of the task can be supplied to a learning algorithm. The training of both learning algorit was achieved by constant self-play. No greedily chose the move with the larges ms required several months of computer time, and exploration strategy was used—the system always expected probability of victory. This naive explo- ration strategy proved entirely adequate for this environment, which is perhaps surprising given the considerable work in the reinforcement-learning literature which has produced numerous counter-examples to show that greedy exploration can lead to poor learning per- formance. Backgammon, however, has is followed, every game is guaranteed information is obtained fairly frequent’ wo important properties. Firstly, whatever policy o end in finite time, meaning that useful reward y. Secondly, the state transitions are sufficiently stochastic that independent of the policy, all states will occasionally be visited—a wrong initial value function has little danger of starving us from visiting a critical part of state space from which important information could be obtained. The results (Table 2) of TD-Gammon are impressive. It has competed at the very top level of international human play. Basic TD-Gammon played respectably, but not at a professional standard. 271 Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess . It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 8.2 Robotics and Control In recent years there have been many robotics and control applications that have used reinforcement learning. Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway. The juggling robot learned a world model from experience, which was generalize o unvisited states by a function approximation scheme known as locally weighte regression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). Between each trial, a form of dynamic programming specific to linear control policies and locally linear ransitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design (Sage & White, 1977). 272 REINFORCEMENT LEARNING: A SURVEY earned. ical reinforcement learning, an unthinka nals called progress estimators were use This was achieved in a robust manner he estimators, but had the freedom to Secondly, control was decentralized. Eac. without explicit communication with the ot rofi quantized into a small number of discre of the Q-learned policies were almost as he job. hree examples: goo bly high dimensional in which the robots from the induc ive bias h robot learned its own policy ers. Thirdly, s as a simple hand-crafte ching task (Crites & Bar boxes for extended periods of time. Box-pushing is a well-known difficult robotics roblem, characterized by immense uncertainty in the results of actions. Q-learning was used in conjunction with some novel clustering techniques designed to enable a higher-dimensional input than a tabular approach would have permitted. The robot earned to perform competitively with the performance of a human-programmed so- ution. Another aspect of this work, mentioned in Section 6.3, was a breakdown of the monolithic task description into a set of lower level tasks to be pre-programmed e viewpoint of theoret- state space, containing many dozens of degrees of freedom. Four mobile robots traveled within an enclo- sure collecting small disks and transporting them to a destination region. There were hree enhancements to the basic Q-learning algorithm. Firstly, pre-programmed sig- to break the monolithic task into subtasks. were not forced to use hey provided. independently ate space was brutally e states according to values o ber of pre-programmed boolean features of the underlying sensors. The performance a small num- controller for 0, 1996). The works for function approximation and ess than the best alternative algorithm reinforcement learning by one of the ble numbers of non-identical products. e The mean weight of all containers produced by a shift must not be below the manufacturer’s declared weight W. 273 KAELBLING, LI TTMAN, & Moore e The number of containers below the declared weight must be less than P%. e No containers may be produced below weight W’. Such tasks are controlled by machinery which operates according to various setpoints. Conventional practice is that setpoints are chosen by human operators, but this choice is not easy as it is task constraints. The task was posed ependent on the The dependency is current product characteristics and the current often difficult to model and highly non-linear. as a finite-horizon Markov decision task in which the state of the system is a function of the product characteristics, the amount of time remaining in the production so far. The system regression was ing was used to ma information was ob typically with wast. deployed successful Some interesting aspects of practical rein examples. The mos necessary to supplement plying extra knowledge comes a Sup the system is subse these, a knowledge-fr the finite lifetime of What forms did linearity for the jugg shif was discretized i use intain an optimal age reduced by a y in several factor: striking is that in al he fundamental al uently less aw ee approach woul. he robots. his pre-programmed ing robot’s policy, no the two mobile-robo the @ values which assumed loca ionally used a manual dimensions and so required correspon addi sumption of local pie in the amoun T ysis of lear e exploration s o judge were able to plora T strategies mir yet all prove Finally, it They where ion. were al The earn well wit e packaging task use rors theoretically op adequate. is also worth considering the compu very different, which indicates that t various reinforcement learning algorithms do indeed hav juggler needed to make very fast decisions with had long periods (30 seconds and more) between each he consis examples, while ly € y discr ing and the mean was ained. In simulate a price: onomous. ized state space. T age and percent below declared in the shift nto 200,000 discrete states and local weighted to learn and generalize a transition model. Prioritized sweep- value function as each new piece of transition experiments the savings were considerable, actor of ten. Since then the system has been ies within the United States. orcement learning come to light from these cases, to make a real system work it proved gorithm with extra pre-programmed knowledge. more human effort and insight is required and But it is also clear that for tasks such as have achieved worthwhile performance within knowledge take? It included an assumption of a manual breaking up of the task into subtasks for box-pusher also used a clustering technique for tent @ values. The four disk-collecting robots e packaging example had far fewer y weaker assumptions, but there, too, the as- cewise continui ning yin t ata required. trategies are inter to profitably gree ex te) imal (bu y esting too. T experiment. loration—alway: ptimism in t e transition model enabled massive reductions e juggler used careful statistical anal- However, both mobile robot applications 8 exploiting without deliberate ex- e face of uncertainty. None of these t computationally intractable) exploration, and of these experiments. utational demands of ational regimes iffering com ean array of differing applications. ow latency between each hit, but rial to consolidate the experiences e collected on the previous trial and to perform the more aggressive computation necessary to produce a new reactive controller on the next trial. T e box-pushing robot was meant to 274 REINFORCEMENT LEARNING: A SURVEY operate autonomously for hours and so had to make decisions with a uniform length control cycle. The cycle was sufficiently long for quite substantial computations beyond simple Q- learning backups. The four disk-collecting robots were particularly interesting. Each robot had a short life of less than 20 minutes (due to battery constraints) meaning that substantial number crunching was impractical, and any significant combinatorial search would have used a significant fraction of the robot’s learning lifetime. The packaging task had easy constraints. One decision was needed every few minutes. This provided opportunities for fully computing the optimal value function for the 200,000-state system between every control cycle, in addition to performing massive cross-validation-based optimization of the transition model being learned. A great deal of further work is currently in progress on practical implementations of reinforcement learning. The insights and task constraints that they produce will have an important effect on shaping the kind of algorithms that are developed in future. There are a variety of reinforcement-learning techniques that work effectively on a variety of small problems. But very few of these techniques scale well to larger problems. This is not because researchers have done a bad job of inventing learning techniques, but because it is very difficult to solve arbitrary problems in the general case. In order to solve highly complex problems, we must give up tabula rasa learning techniques and begin to incorporate bias that will give leverage to the learning process. The necessary bias can come in a variety of forms, including the following: shaping: The technique of shaping is used in training animals (Hilgard & Bower, 1975); a teacher presents very simple problems to solve first, then gradually exposes the learner to more complex problems. Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up , and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995). local reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local. In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly . imitation: An agent can learn by “watching” another agent perform the task . For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel . problem decomposition: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very power- ful technique for biasing learning. Most interesting examples of robotic reinforcement learning employ this technique to some extent (Connell & Mahadevan, 1993). reflexes: One thing that keeps agents that know nothing from learning anything is that they have a hard time even finding the interesting parts of the space; they wander 275 lea: KAELBLING, LITTMAN, & Moore around at random never getting near the goal, or they are always “killed” immediately. These problems can be ameliorated by programming a set of “reflexes” that cause the agent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto, Grupen, & Connolly, 1994). These reflexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn. Recent work by Millan (1996) explores the use of reflexes to make robot learning safer and more efficient. With appropriate biases, supplied by human programmers or teachers, complex reinforcement- rning problems will eventually be solvable. There is still much work to be done and many interesting questions remaining for learning techniques and especially regarding methods for approximating, decomposing, and incorporating bias into problems. Acknowledgements T to anks to Marco Dorigo and three anonymous reviewers for comments that have helped improve this paper. Also thanks to our many colleagues in the reinforcement-learning community who have done this work and explained it to us. 93 in Leslie Pack Kaelbling was supported in part by NSF grants IRI-9453383 and IRI'),\n",
       " Document(metadata={'source': '/tmp/tmpg6zq5rhs/tmp.pdf'}, page_content='arXiv:2312.14925v2 [cs.LG] 30 Apr 2024 A Survey of Reinforcement Learning from Human Feedback Timo Kaufmann timo.kaufmann@ifi.lmu. de IMU Munich, MCML Munich Paul Weng paul.weng@duke. edu Duke Kunshan University Viktor Bengs viktor. bengs@ifi.lmu.de IMU Munich, MCML Munich Eyke Hiillermeier eyke@ifi.lmu.de IMU Munich, MCML Munich Abstract Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interac- tion. This positioning offers a promising avenue to enhance the performance and adaptabil- ity of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model’s ca- pabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader per- spective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic rela- tionship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to pro- vide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research. Contents 1 Introduction 3 1.1 Why Human Feedback... 2.4 Reinforcement Learning from Human Feedback .. 2... 20. 2.5 Active Learning... 2... ee 14 Feedback 15 3.1 Attributes of Feedback Types... 1 Introduction In reinforcement learning (RL), an agent traditionally navigates through an environment and attempts to make optimal decisions (i.e., action choices) through a process of trial and error. Whether a decision is optimal or not is determined solely by reward signals. These signals have to be defined by a system designer based on measurements of the agent’s performance, ensuring that the learning agent receives the nec feedback to learn the correct behavior. Designing a reward function, however, is challenging. Indeed, suc is hard to formally define and measure in many applications. Beyond that, a sparse signal of success may not be well suited for agent learning — resulting in the need for reward shaping , where the reward signal is transformed into one that is more suitable for learning. This often makes the reward signal more susceptible to spurious correlations, however — behaviors that are rewarded because they are usually correlated with the true objective but are not valuable in themselves. This ultimately cumulates in the issue of reward hacking , where learning agents exploit reward-specific loopholes to achieve undesired outcomes while still generating high rewards. n response to these challenges, reinforcement learning from human feedback (RLHF) has emerged as a ractically meaningful alternative that introduces a critical human-in-the-loop component to the standard RL learning paradigm. In a nutshell, RLHF differs from RL in that the objective is defined and iteratively refined by the human in the loop instead of being specified ahead of time. This approach not only has the otential to overcome the limitations and issues of classical RL methods but also has potential benefits for agent alignment, where the agent’s learning goals are more closely aligned with human values, promoting ethically sound and socially responsible AI systems. RLHF has seen a number of successful applications, advances in methodology, and theoretical insights since he last comparable survey . The applications span various domains, including large anguage model (LLM) fine-tuning , image generation , continuous con- rol , games , and robotics (Hejna & Sadigh, 2022). At the same ime, there have been a lot of developments in the methodology since the last comparable survey . Examples of methodological developments include fusing multiple feedback types to leverage their relative strengths (see Section 3.5), enhancing query efficiency through active learning and active query syn- hesis (see Section 4.1.1), incorporating psychological insights to improve the quality of human feedback (see Section 4.2.1), using techniques such as meta-learning to quickly adapt learned preferences to new tasks using prior data (see Section 5.5.1), and using available preference data more efficiently through approaches such as data augmentation and semi-supervised learning (see Section 5.5.2). Finally, there have been some achievements with regard to theoretical results for the field of RLHF (Section 7), providing new insights but also new questions for the fundamental mathematical problems underlying the modeling of the learning scenario in RLHF. In this survey, we, therefore, discuss the current state of affairs with regard to the ongoing research in RLHF, classify the current approaches as well as concisely describe their main characteristics, and give a brief overview of the application areas. In the remainder of this section, we will start by discussing the motivation (Section 1.1) and origins (Section 1.2) of RLHF as well as the scope of this survey (Section 1.3) and conclude by outlining the contents of the following sections (Section 1.5). 1. In conventional RL, the agent’s objective is defined by a reward function that it aims to maxi- mize (Sutton & Barto, 2018). Specifying this reward function can be challenging, particularly in complex domains: What would be a suitable reward function for a robot assisting humans in a household environ- ment or for autonomous vehicles navigating through a busy urban environment? Moreover, even reward functions that initially seem well-defined can lead to unexpected behaviors due to distributional shifts or over-optimization, raising practical and safety concerns. Learning the agent’s objective from human feedback circumvents reward engineering challenges and fosters robust training, with the reward function dynamically refined and adjusted to distributional shifts as the agent learns. Interactive Feedback vs. Demonstrations The field of inverse RL aims to infer reward functions rom human demonstrations (Arora & Doshi, 2021). While this can partially resolve reward engineering challenges, it faces inherent difficulties: (i) It is generally not possible to robustly identify rewards from demonstrations (Cao et al., 2021a; Mindermann & Armstrong, 2018), (ii) it is only applicable in scenarios where good demonstrations can be obtained, (iii) it struggles to outperform the demonstrator, and (iv) humans often do not demonstrate the behavior they would prefer a machine to adopt . nteractive feedback, by contrast, can use active queries to differentiate between human preferences and irrelevant noise, is much easier to provide than demonstrations, does not require near-optimal performance rom the human evaluators, and elicits preferences on the behavior that a human would prefer from the machine. Interactive feedback can also be used to complement demonstrations, in which case it can be used o shape and refine capabilities learned through initial training, like behavioral cloning, thereby preventing overfitting to demonstrated behavior . Avoiding Reward Engineering Reward engineering in RL presents significant challenges, as accurately specifying reward functions is notoriously difficult (Amodei et al., 2016; Knox et al., 2023). These challenges can be mitigated by utilizing human feedback, which enables training agents for tasks that are hard to define manually and helps avoid safety issues arising from misaligned rewards . Safety issues related to a misalignment between the agent’s and the human’s objectives are studied as the AI alignment roblem , in particular agent alignment and value alignment . Excessive optimization for poorly specified rewards often leads to unintended behaviors. Examples of such ehaviors include exploiting flaws in the environment simulation for higher rewards (Lehman et al., 2020; Baker et al., 2020) or engaging in more general reward hacking , where the behavior maximizes the specified reward but deviates from the intended objective. This is evident in cases where agents ‘ocus on intermediate rewards without achieving the actual goal (Clark & Amodei, 2016) or prematurely exit a game to avoid negative rewards . The root of these issues is that the reward ‘unction does not properly reflect the actual learning task. While these issues may seem trivial in game- ike environments, their implications are far more serious in safety-critical contexts such as healthcare an autonomous driving. In these settings, it is crucial to prevent misaligned reward functions from leading to harmful outcomes, like a care robot causing injury or a self-driving car jeopardizing road safety. RLHF presents a promising approach to enhance alignment by enabling agents to learn rom human feedback, which is often more closely aligned with the true objective than manually specifie rewards. Nonetheless, the effectiveness of RLHF in resolving these alignment issues is debated . Examples of possible pitfalls raised in this debate are that the agent may be incentivized to manipulate he human teacher to provide feedback that is easier to optimize (Armstrong et al., 2020; Carroll et al., 2023) or that the agent may learn to exploit errors in human judgement . We refer the intereste reader to the survey by Casper et al. (2023) for a more detailed discussion of these issues. Despite this debate, RLHF represents an important early step towards aligning agents with human values and serves as a foundation to build on to further improve agent alignment. 1.2 The Origins of RLHF Learning behavior from human feedback has long been studied as a subfield of RL, but methods and termi- nology have evolved over time. Early methods focused on learning directly from human rewards (Knox, 012; Isbell et al., 2001; Knox & Stone, 2009), action advice , or action critique udah et al., 2010). Notable approaches in this area include TAMER (Knox & Stone, 2009; Warnell et al., 018), which interprets human feedback as samples of the optimal action-value function, and the later iS) ya COACH (MacGlashan et al., 2017; Arumugam et al., 2019), which interprets human feedback in a policy- dependent way, i.e., as samples of the advantage function. This survey, however, focuses on more indirect approaches to inferring the objective from human feedback. Reinforcement learning from human feedback (RLHF) in its modern guise has its origin in the setting of preference-based reinforcement learning (PbRL) as introduced independently by Akrour et al. (2011) and Cheng et al. (2011). The original idea of preference-based reinforcement learning (PbRL) is to infer the objective from qualitative feedback, such as pairwise preferences between behaviors or between actions given Feedback Type PbRL SSRL RLHF Binary trajectory comparisons Trajectory rankings State preferences Action preferences Binary critique Scalar feedback Corrections Action advice mplicit feedback Natural language ww KK RRQ LS ww RK KKK KO NNN N NANA states, instead of quantitative feedback in the form of numerical rewards. The term RLHF was coined as an alternative later on (Askell et al., 2021; Ouyang et al., 2022; OpenAI, 2022), though initially referring to he same concept of learning behavior from relative feedback. Disentangling PbRL and RLHF is challenging due to their overlapping use in the literature. For instance, Christiano et al. (2017) themselves are using the term PbRL, yet are often cited as a seminal reference for RLHF (Daniels-Koch & Freedman, 2022; Ouyang et al., 2022). This indicates the interchangeability of hese terms. Practically, RLHF is often associated with reward modeling and deep RL, while PbRL is often inked to direct policy optimization in traditional RL settings. This is underlined by Jeon et al. (2020), who characterize PbRL as limited to direct policy learning from preferences. This is in contrast with other sources, however, who include reward learning within the scope of PbRL (Christiano et al., 2017; Wirth et al., 2017). Despite the overlapping and sometimes conflicting usage, RLHF is increasingly viewed as a generalization of PbRL. While both involve human feedback to define RL objectives, PbRL primarily focuses on relative feedback, such as binary comparisons and rankings. RLHF not only includes these aspects but also extends to a wider range of feedback types (Metz et al., 2023; Yuan et al., 2024). Table 1 gives an exemplary overview of our interpretation of these terms. Another concept, semi-supervised reinforcement learning (SSRL), introduced by Christiano (2016) and dis- cussed by Amodei et al. (2016), refers to an RL setting where an agent receives feedback on a subset of its experiences. The initial discussions of SSRL focused on absolute feedback on subsets of the agent’s experi- ences, making the concept complementary to PbRL. In contrast to PbRL and RLHF, the term SSRL seems to be used less in the recent literature. In our work, we adopt the viewpoint that RLHF is a broader category that encompasses various approaches where human feedback is used to define the objective of an RL agent. In this definition, RLHF encompasses both PbRL and SSRL. As the definitions and distinctions between these terms are not universally agreed upon, these distinctions are based on our interpretation of the current predominant usage of these terms in the literature. 1. This section outlines the criteria guiding our selection of approaches in the realm of RLHF. We focus on works that rely on a reward model as the sole source of information about the objective. This reward model should then be learned in an interactive, online, scalable, and asynchronous manner. The following will describe each of these criteria in more detail. Reward Modeling We focus on approaches that learn a reward model from human feedback and then use this model to train a policy. Although it is possible to directly optimize a policy from human feedback , thereby performing RLHF without reward learning, this approach was almost not practiced for a long time and has only recently gained renewed interest especially in the on domain of language model fine-tuning (see Section 6.3). The decomposition into reward learning and policy training offers many conceptual and practical benefits. Among those benefits are the direct applicability of supervised learning techniques for the reward model and the possibility of evaluating the reward model in isolation. In addition to that, the decomposition naturally leads to a form of semi-supervised learning, enabling the agent to use labeled episodes for reward model training while leveraging unlabelled episodes to refine its behavior and explore the environment. Human Defined While there are many approaches that include humans in the RL loop, in this survey, we focus on approaches where human feedback is the only source of truth about the objective. This excludes approaches to reward shaping, feature engineering, and other forms of human guidance that are supplementary to a given objective. Interactive and Online We also put an emphasis on providing feedback in an interactive, online manner. This excludes imitation learning, learning from demonstration, and pure inverse RL. While we do not directly cover inverse RL in this survey, combinations of inverse RL methods with interactive improvements of the reward function are in scope and employed by some of the surveyed methods. See Sections 3.3 and 5.5.1 for a discussion of those approaches. Scalable and Asynchronous We focus on works in which the human is included in the loop, but the agent is not blocked by the human’s feedback, and the human does not need to be present continuously. This distinguishes RLHF from more direct methods of incorporating a human into the RL loop, and we believe that this is key for practicality and efficiency. In addition to these criteria, we mainly focus on works published after 2017 since earlier works are surveyed by Wirth et al. (2017). Nevertheless, some works from this period are revisited from time to time in order to elaborate on certain concepts that are still state of the art or have significantly shaped it. Note that while there has been a recent forus on RLHF for LLMs, LLMs are not the primary focus of this work. Instead, we cover RLHF in a broader context, focusing on control applications as well as discussing the implications of RLHF for fine-tuning generative models such as LLMs. 1.4 Prior Surveys Based on the criteria mentioned in the previous section, we will first differentiate our survey from other surveys in marginally related subject areas sharing the common theme of human-in-the-loop RL. Then, we will describe the differences between our survey and previous surveys or survey-like articles that exist within the RLHF field. 1.4. Human participation in machine learning (ML), particularly in guiding machine learners, is a much-studied scenario. This field, commonly referred to as human-in-the-loop ML, can be further divided into subfields based on various criteria, e.g., the ones detailed in Section 1. Human-in-the-Loop Learning from human feedback falls into the domain of human-in-the-loop ML. Wu et al. (2022) survey human-in-the-loop ML in general. They also cover some applications of RLHF (for LLMs in particular) but do not give a detailed overview. Retzlaff et al. (2024) provide a similar overview over human-in-the-loop RL in particular, focusing on human involvement in RL on a more abstract level than our work and not covering RLHF in detail. Similarly broad in scope, Najar & Chetouani (2021) study the setting of RL with human advice, which they define as ‘teaching signals that can be communicated by the teacher to the learning system without executing the task’ While this setting subsumes RLHF, the broad generality limits the depth in which their survey can cover RLHF approaches. Interactive RL RLHF can be considered a sub-field of interactive RL, which studies RL algorithms that learn in interaction with humans. This interaction can take the form of feedback defining an objec- Reference Topic Reward Human Interactive Sealab le Modelling Defined and Online Async. Wu et al. (2022) Human-in-the-loop ML x x x x Retzlaff et al. (2024) Human-in-the-loop RL (v) (Vv) v (Vv) cet) Chetouani RL with human advice (v) (Vv) v (Vv) Lin et al. (2020a) Social feedback x (Vv) v x Poole & Lee (2024) RL from brain signals x v v x Cruz & Igarashi (2020) nteractive RL for HCI x x v x Osa et al. (2018) mitation learning x v x x Arora & Doshi (2021) nverse RL v v x v Bignold et al. (2021) Assisted RL x x (Vv) x Luketina et al. (2019) Language-informed RL x x x x Zhang et al. (2021) Human guidance x v v x Ji et al. (2023b) AI Alignment v x v v Liu et al. (2023c) LLM applications x x x x Ours RLHF v v v v tive, resulting in the RLHF setting, but can also, e.g., be used to drive exploration or speed up the agent’s learning process. Cruz & Igarashi (2020) survey interactive RL from an human-computer interaction (HCI) viewpoint, exploring various ways humans can influence RL agents, with a particular focus on reward definition based on human feedback, without a predefined environmental reward function. Due to the breadth of their survey, they do not cover many works in this area. The survey by Lin et al. (2020a) centers on interactive RL using human social cues, like gestures and spoken language, but does not cover the reward modeling aspect. Similarly, the study by Poole & Lee (2024) examines RL with direct feedback from human brain signals, such as through brain-computer interfaces, also not focusing on reward modeling. Demonstrations Learning from demonstrations, in the form of behavior cloning and inverse RL (Arora & Doshi, 2021), shares the goal of RLHF to learn behavior from human input. In contrast to RLHF, however, it requires demonstrations of the desired behavior instead of feed- back, and these demonstrations are usually not provided interactively and online. This limits their applications and also their final performance due to the availability of near-optimal demonstrations. Nonetheless, imitation and demonstration can be a useful component of an RLHF system but are not the main focus of this survey. However, we will discuss the intersection between these fields in some parts whenever necessary. Assisted RL Bignold et al. (2021) review the field of assisted RL, where an agent may receive external information (for example, from a human) that aids it in action selection. While updates to the reward function are one of the possible effects of advice in this setting (in addition to action selection or modifications of the agent’s internal state), it is usually assumed that an initial reward function is given and the extent of the updates is limited to reward shaping or supplementary reward signals. In contrast to RLHF, the external information does not define the task but only helps the agent in achieving it. Closely related to this, Luketina et al. (2019) survey RL assisted by natural language. In addition to this assistance setting, they also discuss approaches that infer a language-conditioned that the aspect is not covered. Reference (Focus) Beyond Label Wirth et al. (2017) (preference-based RL) x v) Abdelkareem et al. (2022) x x (recent advances of PbRL) Jeon et al. (2020) V x (feedback modelling) Casper et al. (2023) V x (open issues in RLHF) Fernandes et al. (2023). ; v v (language generation) Metz et al. (2023) (feedback types) v v Yuan et al. (2024) (feedback types) v v Ours (fundamentals, recent V V advances, and trends) RM Comparisons Collection Training Theory Bevdhamarks x (V) Vv) (Vv) x K x (V) K (V) K K K v v v reward function. However, they discuss this setting rather briefly and use techniques from inverse RL and not RLHF. Guidance In their survey on human guidance, Zhang et al. (2021) delve into various aspects related to RLHF. Although they touch on aspects such as r of their work. Instead, their main focus lies on ex, involve the learning of a reward model. eward learning, i AI Alignment Ji et al. (2023b) provide a general overview of AI alignment the objectives of an intelligent system with those o in some detail. As AI alignment is a very broad fie! into as much depth on the topic of RLHF as we do its human opera: here. Applications Liu et al. (2023c) give an overview of current applications of as ChatGPT and GPT- ly enjoys a lot o: s a broader pers is not the primary emphasis loring more immediate approaches that do not , Le., the challenge of aligning ors. This survey covers RLHF Id, however, the article nevertheless does not go RLHF methods for LLMs such f attention, it is only one spe- pective, examining the diverse applications and impact of RLHF encompassing application areas beyond LLMs. 1.4. There have been previous surveys or survey-like articles that are closely related to RLHF. Table 3 gives a brief overview of how these articles differ from ours, which we will explain in more detail below. Preference-Based RL Previous surveys in the domain of RLHF often focus on PbRL, where feedback is limited to binary preferences (see Section 1.2). An illustrative example of this is the survey by Wirth et al. (2017), which is a direct precursor to our work. In contrast to our work, they concentrate on binary preferences for trajectories and primarily survey methods that learn policies without deriving a reward model. Since then, the reward-modeling approach has become dominant in the field, and other approaches have extended RLHF to new feedback types. Abdelkareem et al. (2022) give another more recent literature review of PbRL. While this review focuses on reward modeling and includes some recent work, it is far less comprehensive than our review, as many aspects are only touched upon and partly overlap with those of Wirth et al. (2017). Feedback Types Although not a survey per se, Jeon et al. (2020) propose reward-rational implicit choice as a unifying framework to comprehend many previous studies in PbRL and RLHF. To illustrate its generality, they overview different feedback types used in previous work and explain how they fit into their framework. The concurrent works by Metz et al. (2023) and Yuan et al. (2024), which are also not strictly surveys, propose frameworks for studying user interaction and interface design for multiple feedback types. As part of their work, they provide a classification of feedback types and a brief overview of RLHF approaches. Metz et al. (2023) have a stronger focus on the feed- back interface and on learning from multiple feedback types simultaneously, discussing properties o: feedback types and proposing a standard encoding for them. On the other hand, Yuan et al. (2024) also include an offline RLHF benchmark and have a stronger focus on the reward learning aspect, focusing on the entire learning pipeline. Nevertheless, many facets of RLHF are not dealt with a all in those studies, as they are not primarily survey articles. Our survey has a broader scope and, therefore, has more extensive coverage, going beyond the study of feedback types and discussing more recent work. Domain-Specific Fernandes et al. (2023) focuses on human feedback for langauge generation. As a resul of their focus, their survey is less comprehensive than this work but discusses some language-specific aspects that do not fall into our scope, such as using feedback models at generation time. Open Problems Casper et al. (2023) provide a detailed overview of the open questions and limitations o: RLHF with a particular focus on aspects of security, governance, and transparency. In their article, reward modeling is also covered, as is human feedback, which goes beyond preference comparisons, but other aspects, such as theoretical approaches or an overview of existing benchmarks, are no included. Thus, it can be seen as a supplementary article that is ideal for further reading once being familiarized with the topic through our survey. All in all, our survey can be seen as the canonical continuation of Wirth et al. (2017), which examines the evolution of the field of PbRL to the more modern and general field of RLHF. This includes a thorough description of the basics as well as an in-depth discussion of current advances and trends in the field. 1.5 Outline In the next section, we begin with an introduction to the basics by revisiting the most important concepts from the standard RL setting, which are also naturally important in RLHF (Section 2). We then dive into the RLHF topic by outlining the most studied scenario of reward model learning from pairwise preferences. Using this introductory and illustrative example scenario, we explain the basic framework of RLHF alongside its three main components of (human) feedback, label collection (feedback acquisition), and reward model learning. These three main components will essentially form the structure of our survey. In Section 3, we turn our attention to the human feedback component and provide an overview of the different types of feedback as well as their key attributes. The important concepts in terms of label collection are then explained in Section 4, followed by learning the reward model in Section 2 Preliminaries In this section, we recall the basic setting and the most important concepts of RL and RLHF. In the course of this review, we will fix the notation that will be used throughout the survey. We first introduce what is probably the most studied RLHF scenario, i.e., learning a reward model from binary trajectory comparisons. Based on this introductory and illustrative example scenario, we explain the basic framework of RLHF with its main components and briefly discuss the respective roles of these components in the learning process. We will also briefly touch on active learning, which strongly connects to the feedback collection component. Notations For any integer n € N, we denote by [n] the set {1,2,...,n}. For any set S, A(S) denotes the set of probability distributions over S. We use P(£) for denoting the probability of some event E, while E[X] is used to denote the expected value of a random variable X. In some cases, we will write Ep|-] or similar variants to emphasize that the distribution for the expected value is governed by the probability distribution P € A(S). Moreover, we will write X ~ P if a random variable X is distributed according to a probability distribution P. 2.1 Reinforcement Learning Reinforcement learning (RL) (Sutton & Barto, 2018) is the setting of learning behavior from rewarded in- teraction with an environment. Such a learning environment is formalized as an Markov decision process (MDP), which is a model for sequential decision-making. In an MDP, an agent iteratively observes its current state, takes an action that causes the transition to a new state, and finally receives a reward that depends on the action’s effectiveness. Formally, an MDP is defined as a tuple (S,.A, P, R,do,) where « S isa set of states (the state space), « Aisa set of actions (the action space), « P: Sx A- A(S) is a transition function (the transition dynamics), « R: Sx A-— Risa reward function, « do € A(S) is a distribution over initial states, « and 7 € [0,1] is a discount factor. The transition function P defines the dynamics of the environment: For any state s and action a, the value P(s,a)(s’), also sometimes denoted P(s’ | s, a), is the probability of reaching the state s’ after executing a in s. In light of this, we will also refer to the transition function sometimes simply as the transition dynamics. For a given state and action, the transition is conditionally independent of all previous states and actions, which is known as the Markov property and the reason for the naming as an MDP. The value R(s,a) € R provides an immediate evaluation after performing action a in state s, which is also called the (instantaneous) reward. It is also quite possible that the instantaneous reward is 0 for some states, and one only receives a reward in specific states, for example, in so-called terminal states for which the transition function is zero. When both the state space S and the action space A are finite, we call the MDP a tabular MDP. In an MDP, an H-step trajectory 7 is a sequence of H € N\\\\{0} pairs of state-action ending in a terminal state. Formally, it is given by 7 = (80, a0, $1,41,---, 8H). Given tp > 0 and H’ < H, we can define a segment F = (Sty; Uo, $1941) to 4+1,-++, 8H\"), Which refers to a continuous sequence of steps within a larger trajectory. A trajectory 7’s return R(r) is the accumulated (discounted) rewards collected along this trajectory: A-1 R(r) = YF 7\" R(sn,an)- ” h=0 Note that we here use the same notation for the return and the reward function, but both have different signatures (trajectory vs. state-action pair). We can also define the return R(c) of a segment o in a similar manner. The return is well defined even if the horizon H is infinite as long as y < 10 Environment Query qi Reward Model me | abeler Label |; Agent HEGscia Environment Dynamics Sate wo bjective Reward ron Agent Action az (b) RLHF with reward modeling (a) The standard RL setting. A policy specifies how to select actions in a state, either deterministically or stochastically. In the former case, a policy is simply a mapping 7 : S + A from states to actions, while in the latter, it is a mapping a: S& —» A(A) from states to probability distributions over actions. Since the deterministic case is a special case of the stochastic one, we assume the latter case in the following. The basic interaction loop is depicted in Fig. la: The agent chooses an action a; ~ 7(s;) based on its policy and the current state. As a consequence, the environment transitions into the new state si,1 ~ P(s:, at), governed by the transition dynamics. The agent observes this new state and the reward rz41 ~ R(s, a), after which this interaction cycle is restarted. In this setting, the RL agent aims at learning a policy that maximizes the expected return I (a) = Edo,p.x[R(7)], where the expectation is with respect to policy 7, transition function P, and initial distribution do. To solve this problem, two families of RL approaches have been considered: model-based RL and model-free RL. The methods in the first family learn a model (i-e., P, R) of the underlying MDP to help solve the RL problem, while the methods in the second directly try to obtain a good policy without learning an MDP model. The second family can be further decomposed into two main categories: value-based methods and policy search methods. In deep RL, both value functions and policies are approximated with neural networks. Value-based methods (e.g., DQN and its variants (Mnih et al., 2015; Hessel et al., 2018)) aim at learning the Q-function Q* of an optimal policy. The Q-function of a policy 7 is defined by: H-1 Qx(8,@) = Ep bs a) , h=0 where so = s, and aj = a and in the expectation, an ~ 7(- | sn) as well as s, ~ P(- | 8n—1,an—1) for h € [H — 1]. A policy can be naturally designed from a Q-function by choosing an action in a greedy manner in each state: m(s) = argmax, Q(s,a). Note that for a deterministic optimal policy 7* it holds that I(x*) = Ea[Q*(s,7*(s))]- Similar to the action-value function Q, we can also define the state-value function H-1 V,(s) = Ep. bs 7\"R(8n; Gn) | so = | . h=0 11 ts value for some state s is the expected return when starting in that state and then always using the policy a. It is related to the Q-function by means of Vi(s) = Eawn(s) (Qx(s,4)] for any state s € S. In contrast, policy search methods directly aim at finding a good policy in some parametrized policy space. The most data-efficient algorithms in this class of methods follow an actor-critic scheme where both an actor (i.e., a policy) and a critic (i-e., usually its Q-value function) are learned at the same time. Typical represen- ative methods here are PPO , TD3 , or SAC . RL algorithms can further be classified as either on-policy or off-policy. In an on-policy algorithm, such as PPO, only the recently generated transitions are used for training. In contrast, in an off-policy algorithm, such as DQN (or its variants), TD3, or SAC, the agent can be updated with transitions not necessarily generated by its current policy. While on-policy training is usually more stable, off-policy training enables more data-efficient learning by reusing samples from a replay buffer that stores past transitions. 2.2 Preference-Based MDPs In contrast to standard RL as described in the previous section, RLHF does not assume that a reward signal is available. It instead assumes the existence of an oracle (e.g., human labeler) that can provide information about the reward in a specific indirect manner. More precisely, in the RLHF, the agent can make queries qi to the oracle, which in practice means asking for human feedback, and in response, the agent receives a abel l;, which in general gives a hint about the reward. In principle, the query can be made asynchronously o the actual conventional RL cycle. See Fig. 1b for an illustration. In the most common setting, the oracle can compare two (segments of) trajectories, but various other cases have been considered, as we shall see later on. For the former case, RLHF is based on the setting of preference-based MDPs (Gilbert et al., 2017; Wirth et al., 2017), which can be defined as an MDP model without reward function, but where comparisons of trajectories are available. 2.3 Reward Learning RLHF approaches can be divided into two categories, depending on whether a utility-based approach is used ‘or reward modeling or an alternative criterion that is detached from a utility concept is used (Gilbert et al., 2016; Gilbert & Weng, 2016; Wirth et al., 2017). Most works fall into the first category, on which this overview focuses. Such approaches assume a human-dependent utility function that can be used as a reward ‘unction in order to apply standard RL methods. Next, we will describe the commonly used approach for reward learning for the common setting of binary trajectory comparisons. The prevalent approach to learning a utility function from observations of pairwise comparisons is based on the Bradley-Terry model (Bradley & Terry, 1952), which stipulates a probabilistic model for the oracle (human labeler): 1 1+ exp(R(72) — R(11))’ P(T1 + m2) = where > means “preferred to” and R(r) corresponds to the utility (i-e., return in the context of RL) of N 1 max] 1+ exp(Ru(73) — Ry(t})) ° 12 In the context of RL, since Ry(r) = jZg 7! Ru(sn,an), (2) can then directly be used to train a function approximator (e.g., single or ensemble of neural network) to approximate R. This entire modeling approach accommodates the case of a noisy or unreliable oracle, in which case the Bradley-Terry model can be understood as the generative model of the answers from the oracle (or labels provided by the human labeler). When the oracle is reliable, more direct methods based on preference elici- tation to recover the reward function have been studied (Regan & Boutilier, 2009; 2011; Weng & Zanuttini, 2013; Gilbert et al., 2015; Sadigh et al., 2017; Wilde et al., 2018). In this survey, we will focus on the general case where the oracle may be noisy. Note that in contrast to the typical way of preference learning, the learned reward function is used to train an RL agent and not directly to compare trajectories. This discrepancy in the objective function in the reward learning part and how the learned rewards are used may lead to suboptimal policies (Lindner et al., 2021b). 2.4 Reinforcement Learning from Human Feedback In the RLHF setting as illustrated in Fig. 1b, the learning agent needs to solve an RL task without having access to a reward function. To this end, the agent usually simultaneously learns an approximation of the reward function (via the assumed utility function) and an RL policy. Therefore, a generic RLHF algorithm consists of repeating two phases: (1) reward learning and (2) RL training. The first phase can itself be decomposed into two main steps: (i) generate queries to ask the oracle, (ii) train a reward function approximator with the answers provided by the oracle. The RL training part is more conventional and is usually directly based on running a deep RL algorithm using the currently trained reward function approximator. Algorithm 1 Generic RLHF Algorithm in an Actor-Critic Scheme 1: Initialize parameters 6 (policy), @ (critic), and y (reward) 2: Initialize replay buffer B with randomly-generated trajectories 3: fori =1,...,N do 4: // Reward learning Generate queries from B Update D with answers to queries from the oracle Update ~ using D (e.g., to maximize Eq. (2)) // RL training Update B with new trajectories generated with 7 10: Update 6 (actor) using B and Ry ll: Update ¢ (critic) using B and Ry 12: end for on This basic generic algorithm is summarized in Algorithm 1, where an off-policy actor-critic scheme is assumed to be used for the RL training part, but other RL policy learning approaches can, of course, also be used here. For an on-policy algorithm, such as PPO , only the recently generated transitions are used for training. For a DQN-like algorithm, lines 9 to 11 would be replaced by a loop where transitions are generated by a behavior policy based on the current estimate of the Q-function (e.g., e-greedy algorithm) and the Q network is updated using mini-batches of transitions sampled from the replay buffer D. An efficient RLHF algorithm needs to overcome several difficulties which are specific to this setting: « The oracle may provide various types of feedback (see Section 3). The questions of what information some given feedback provides and how observed feedback can be exploited need to be answered (see Section 5). ¢ Informative queries need to be generated to minimize the efforts of the oracle, which is crucial when it is a human (see Section 4). Active learning techniques (see next subsection) can be adapted to face this challenge. 13 ¢ The RL agent is actually trained in a non-stationary environment since the reward approximator is concurrently updated. The RL training part needs, therefore, to account for this factor, e.g., using non-vanishing learning rates (see Section 6). ¢ There is also the question of how the agent’s performance can be meaningfully evaluated, especially if the reward function is not known (see Section 8). ¢ Collecting feedback directly from humans introduces its own challenges, such as the question of a suitable user interface and the associated issues of delay between query and feedback observation, or the feedback variability and reliability (see Section 4). This may explain why many studies evaluate novel RLHF algorithms with simulated feedback. A standard RL algorithm can be run in the RL training part, as done in most previous work in RLHF (although this may not be the best approach). This suggests that any improvements in a standard deep RL method (e.g., auxiliary losses , planning in learned model , curriculum learning , or data augmentation (Laskin et al., 2020; Lee et al., 2020; Lin et al., 2020b)) may potentially be transferred to the RLHF setting. In addition, most previous work in RLHF directly uses trajectories stored in replay buffer D to synthesize queries. An interesting research direction to explore in RLHF would be to specifically generate trajectories in order to be able to synthesize more informative queries (instead of only generating trajectories that are beneficial for RL training). This would lead to tackle a novel exploration-exploitation dilemma: Shall we visit state-action pairs that may be bad but may help better learn the reward function, or shall we visit state-action pairs that we currently think are good? This is further discussed in Section 5.5.In RLHF, since the oracle is a human or a group of humans, reducing the number of queries is crucial to limit the labeling cost. Therefore, the reward learning part requires techniques similar to those proposed in active learning, which we recall next. 2.5 Active Learning In active learning , the task is to strategically select data points for labeling to minimize the amount of labeled data required to achieve a desired level of performance, particularly valuable in scenarios like RLHF where labeling is costly. Unlike batch learning, where labeled data is predetermined, active learning empowers the learner to actively select the most informative unlabeled instances for labeling, maximizing the learning process with limited labeled data. We will only briefly introduce the active learning task here and then discuss the strategies for creating informative queries considered thus far in Section For RLHF with pairwise comparisons, this setting can be formally described as follows. Suppose there is a set of N pairs of trajectories {(7{,73) | 7 = 1,...,.N}, where each pair (r/,75) can be interpreted as an unlabeled instance. To efficiently learn a reward function to explain observed pairwise comparisons, an agent can select a set of unlabeled pairs (possibly a singleton) to query an oracle to obtain their labels. At a high level, the main idea in active learning is to query data points to quickly reduce the epistemic (i.e., reducible) uncertainty about the predictions of the learned model, although other aspects can be important, such as the representativeness of the queried data points (see Wang et al. (2023a) for a survey). Two main representations are considered to describe this epistemic uncertainty: either using an ensemble of models or using a Bayesian representation. In both cases, a first basic approach selects queries using uncertainty- based criteria in order to focus on instances with high prediction uncertainty as measured by, e.g., variance or entropy computed over predictions. In contrast to the first approach, where the selection criteria are instance-based, a second approach considers criteria that may depend on all instances. Possible options are, for instance, expected model change, expected error reduction, or density-weighted uncertainty-based criteria. Here, the weights in the expectation or density allow us to take into account the distributional information about the instances and, therefore, to focus on the higher-density regions. 14 3 Feedback Agent A St Reward Model Feedback mechanisms are fundamental to the success o: Environment Dynamics ction ag State s¢41) Query gi _—___ Ee oton Label |; the RLHF loop discussed in this section. f any RL system. In the standard setting as described in Section 2.1, the RL agents expect feedback in the form of scalar immediate rewards. These rewards are most commonly determined by a hand-engineered reward function, which can be used to evaluate any state-action combination. As discussed in Section 1.1, it is desirable to allow humans to refine behavior interactively through feedback instead of requiring the While a human could, in principle, assign rewards to e role of the reward function, this is usually impractical effort required to provide rewards on a sufficiently reg to that, directly integrating human rewards into the which impedes the learning pace while waiting for hum: numeric state-action rewards, which is challenging to In contrast to directly rewarding each of the agent m to pre-specify a reward function. ach of the agent’s actions directly, thereby taking the ‘or multiple reasons. The main challenge is the human ular basis, i.e., at least once per episode. In addition RL loop would require these rewards immediately, an feedback. Finally, the standard RL setting expects rovide in a consistent manner. ’s actions, RLHF as discussed in this survey (see Section 1.3) harnesses indirect and asynchronous feedback methods. Such methods avoid the challenges of immediate numeric rewards and are also better al improved learning progress and human user experience. igned with human interaction patterns, resulting in A feedback type is a kind of interaction in which a human conveys some information about their preferences. Examples include pairwise comparisons and direct cri iques. This section is concerned with the many ways in which human feedback can be expressed and used. Several previous works have already studied and sorted feedback types by listing the most common ones (Jeon et al., 2020; Yuan et al., 2024) and discussing their attributes and dimensions (Metz et al., 2023; Lindner & El-Assady, 2022). The attributes and classes described in this section build upon this prior work and can be considered a synthesis and extension of it. The remainder of this section will start by discussing relevant attributes of feedback types that can be used to classify them (Section 3.1). We will then discuss c common classes and examples of interactive feedback types (Section 3.2) as well as some non-interactive types that can serve as initializations (Section 3.3). 3.1 Attributes of Feedback Types Feedback types may differ on many dimensions, some of which relate to the way feedback is given (arity, involvement), others to the form of the query instance i it is given on (granularity, abstraction), and yet others to features of the human interaction (intent, explicitness). The attributes we discuss in this section are based on the framework proposed by Metz et al. (2023). We have adjusted and added terminology where i clarity, generalized the distinction between relative and of co-generative involvement and literal intent. Furthermore, we systematically analyze a set of exemp feedback types in the next section, expanding on the ini by Metz et al. (2023). In the following, we will introduce each of the six attributes in more detail. Arity This attribute describes whether a single instan aids absolute feedback to ‘arity’, and added the categories ary itial examination of a smaller range of abstract classes ce is evaluated in isolation (unary) or relative to other instances (binary, n-ary). Unary feedback is often convenient for detailed and descriptive feedback but lacks any grounding and therefore puts feedback. Non-unary feedback always has an a great burden on the human to provide consistent implicit grounding but relates to the instances being comparable. While n-ary feedback, such as rankings, can provide more information than binary fee back, it also puts a higher cognitive burden on the labeler. Involvement The labeler may either passively observe an instance, actively generate it, or coactively par- Granularity Feed tici labelers since it does not require the ability to demonstra at the most informative examples wi h active learning tec! ate in its generation (co-generation). Passive involvement poses the smallest challenge to the e the task. It can also easily be directed hniques. Unfortunately, passive feedback often cannot match the information density of active feedback. It is, therefore, common to combine bot. then refine it from passive a human can share contro: makes it possible to direc with the the human’s attention to the h types to first initialize the reward model from (possi eedback. Between these two extremes is co-generative feedback, in which ess demanding than active feedback and most informative samples, but it is still agent. This more taxing than purely passive involvement. differ back may also from whole episode recor ings over on t. artial segments to bly very suboptimal) can be active feedback and he granularity of the instances being evaluated. This ranges eedback on individual steps (i.e., states, actions, or state-action pairs). A more coarse-grained granularity has the advantage of giving humans more context and getting problems. is often impractica as demonstrated by Guan queries within an episode that we only classi: is compatible with paper uses entire e or te pisodes. Finer-grained feedback is much easier to learn ious for humans to provide. I et al. (2021) who propose to use queries on step granularity, but batch 0 provide additional context and make it easier to give feedback. Note fy a type of feedback as “episode” granularity if it requires entire episodes. If it partial segments as well, we classify it as “segment” even if is also possible to s Abstraction This describes whether feedback is given directly on raw instances, e.g., (see granu easier to for a human to ma that this features used as in (see invo Explicitness Humans may communica Intent of actions directed at other pur information is often much more erences. The assumed human intent can uts for the reward model. Ty poses. While exp readi e important for instructive, or descriptive in their explicit feedbac! feedback. Evaluative, instructive, an descriptive a reward function, whereas literal feedback is a by] reward function directly. Descriptive a partial reward function. The context of a particular instance The distinction between litera They argue that humans with arity) or on abstract features of the instances. While feature-level learn from, extracting useful features is challenging. In some contexts, i ke abstract judgments rather than more intuitive instance-level judgments. Note always refers to the level of abstraction that the user sees, icit information is easiest to eedback for larger sections of behavior but also poses credit assignment rom and simplifies credit assignment, but rike a compromise, he discussed source behavior recordings information can be may also be harder which may differ from the pes of feedback that depend on active generation lvement), such as improvements, generally work on a raw instance level. e explicitly for the purposes of feedback or implicitly as a side-effect learn from, implicit y available and can possibly communicate more detailed pref- eedback processing. A human may be evaluative, , while they are generally literal in their implicit eedback is pedagogical in nature, aiming to teach product of a human actor’s efforts to optimize the feedback is often given on the level of the task, e.g., through other types of intent, in contrast, are generally given within the of be havior. and pedagogical feedback was introduced by Milli & Dragan (2020). edagogical intent (i.e., evaluative, instructive, or descriptive) may act differently compared to humans with literal intent. Even though they find that assuming the (wrong) literal intent can still lead to know this intent to choose the right human model (see Section 5.1.5). 3. better reward inference, it still indicates that it can be important to Even though the concrete types of feedback used in the literature are rarely exactly the same, they can generally be sorted into a set of common classes. We will describe a selection of those classes, their defining 16 be used to learn a representation prior to learning a reward model. When * is specified for an attribute, this indicates that it is not a defining feature of the class and may vary in different instantiations. Class Granularity Involvement Arity Abstr. Intent Expl. Critique + Observe Unary + Evaluative — Explici Comparisons + Observe 2+ + Evaluative — Explici e Inter-Temporal Segment: Observe Unary + Evaluative — Explici = 4 Proxy Rewards Episode Observe + Feature Descriptive Explici & | Social Behavior Segment: Observe Unary Instance — Litera Implici Improvements Episode Co-generative Unary Instance + Natural Language Observe Unary + Descriptive Explici a, J E-Stops Episode Observe Unary Instance — Litera Implici BR Importance + Observe + + Descriptive Explici a Feature Traces Segment: Active Unary Instance Descriptive Explici cc | Similarity Queries + Observe Ternary Descriptive Explici attributes, and examples of concrete instances in the literature in the following. Table 4 gives an overview of the classes and their attributes as described in Section 3.3.2.1 Primary Feedback Classes This section introduces common feedback classes which can be used on their own to learn a reward model. The classes are critique, comparisons, inter-temporal feedback, proxy rewards, social behavior, improvements, and natural language. Critique Critique is arguably the most direct type of feedback. In this setting, the human expresses their preference by directly critiquing an instance of agent behavior, often in the form of binary feedback. Note that critique, as considered in this survey, is distinct from directly supplying a reward signal since it is given in an asynchronous and indirect manner (see Section 1.3). The defining features of critique are that the human passively observes the behavior (involvement), gives feedback on a single instance (arity), and does so explicitly (explicitness) with an evaluative intent. The feedback may be given for any granularity and on any level of abstraction. There are many examples of critique feedback in the literature. Xiao et al. (2020) employ binary feedback on individual state and action pairs. Although they learn a shaping reward that complements an environment reward signal, the same technique could be used without environment reward. Huang et al. (2023) extend his to multi-label feedback, allowing the user to distinguish between a regular good or bad action and a erminal action that achieves the goal or fails to do so. They map these classes to scalar values and then learn a reward model by regression. Wang et al. (2020) present an approach to learning a reward model from noisy critiques in the form of human physiological signals (brain signals) using active querying. In contrast to this action-level feedback, Fu et al. (2018b); Singh et al. (2019) rely on binary outcome success labels. Fu et al. (2018b) introduce the basic approach, which Singh et al. (2019) extend by moving to an off-policy setting and including online queries, thereby reducing the reliance on many positive examples by interactively correcting false positives. n addition to learning the main reward function, critique can also be used for safety evaluation. Cosner et al. (2022) train a secondary reward model focused on safety from binary action critiques. This is in addition o the main reward model, which is trained from comparisons in their approach. Note that this secondary 17 safety model could, in principle, be trained with any of the feedback types discussed here, using methods identical to the ones used for reward learning. Comparisons Binary comparisons and rankings are among the most common types of feedback. The defining features of comparisons are that the human passively observes the behavior (involvement), gives relative feedback on multiple instances (arity), and does so explicitly (explicitness) with an evaluative in- tent. It is most commonly given on a segment (granularity), but other granularities are possible in princi- ple . Similarly, comparisons are commonly requested on an instance level (abstraction), but this is not a requirement. Comparisons were first used for direct policy learning (Akrour et al., 2011; Cheng et al., 2011), but were later extended to the reward-learning setting (Wirth et al., 2016; Christiano et al., 2017). The most com- mon setting relies on pairwise comparisons of trajectory segments, but comparisons of individual states or actions were also considered in early PbRL works and com- parisons can even be extended to more abstract trajectory features . This basic setting has been extended and modified in various ways. To reduce noise in the labels, it is common to extend the binary choice by giving the labelers the option to avoid the hard choice and in- stead indicate incomparability, uncertainty, or perceived similarity . This option is c commonly interpreted as “equally preferable”, e.g., as if each trajectory had an equal probability of being preferred in the preference predictor. It is alco common, however, to additionally provide an “incomparable” option which results in simply omitting the query from the data set (Christiano et al., 2017; Ibarz et al., 2018). In contrast to this, Verma et al. (2023) explicitly state that they do not allow for the equally preferred option, arguing that these cases are rare enough not to matter very much. Another line of research suggests that more precision in the expression of pairwise preferences, such as softening the hard binary choice to scalar feedback indicating the strength of a preference , can be beneficial for preference learning. Other extensions change the pairwise setting to choices among larger choice sets or even full rankings (Myers et al., 2021; Brown et al., 2020; Ouyang et al., 2022). Jain et al. (2015) compare different kinds of re-ranking feedback (select first which is better than top, choose best from top-5, choose best from random set). Their study suggests that the first two are roughly equivalent, while the latter is much less informative. Askell et al. (2021) evaluate binary comparisons as well as ranking, but finds that ranking performs better. Ziegler et al. (2020) note that for language tasks, a larger choice set can amortize he time needed for a labeler to get acquainted with the context necessary to understand a query. Basu et al. (2019) propose to use hierarchical queries, i.e., a sequence of pairwise comparisons that build up on each other. Pairwise comparisons are generally easier to supply than ratings, even when the feedback is given by a ‘oundation model (Wang et al., 2024b). While absolute feedback is generally dependent on a policy serving as an implicit baseline , the explicit baseline in pairwise comparisons can be more easily controlled. Therefore, comparisons provide many benefits over absolute ratings, such as reduced bias, inconsistencies, and subjectivity (Yannakakis & Martinez, 2015). On the flip-side, comparisons convey relatively little information per label. Tien et al. (2023) study the weaknesses of pairwise-comparison-based reward learning. Since the amount of information provided for each label is small, these models are prone ‘o causal confusion, i.e., misattributing reward to noise and misidentifying the reward function. Inter-Temporal Feedback One limitation of trajectory comparisons is that they require a set of roughly comparable trajectories. In many real-world environments, starting conditions or even the agent’s current task may vary between episodes. In these cases, it is hard for human labelers to compare trajectories from these episodes, limiting the usefulness of comparison feedback. One way to remedy this limitation is to provide feedback within a single trajectory. Instead of comparing a set of instances with each other as in regular comparative feedback, inter-temporal feedback conveys relative judgments over different states in time within a single instance. The defining features of inter-temporal feedback are that it is given explicitly (explicitness) on segment (granularity) while passively observing (involvement) a single instance (arity) of the agent’s behavior with evaluative intent. It is most commonly given on raw instances, but any level 18 of abstraction is possible in principle. There are two main ways to convey this feedback: Reward sketching and inter-temporal preferences. Reward sketching, as introduced by Cabi et al. (2020), involves users sketching a visual representation of the reward function over time. This type of feedback, which can be given by sketching a graph with the mouse while watching a behavior recording, provides intuitive, per-timestep reward annotations. Rahtz et al. (2022) also adopted this approach, referring to it as “one of the highest-bandwidth feedback mechanisms currently available”. Inter-temporal preferences were introduced by Abramson et al. (2022). In this setting, humans give feedback on multiple points of a trajectory, indicating whether an agent makes progress towards or regresses from a goal. This is then interpreted as preferences relative to the other labeled and unlabelled points. The authors note that one potential downside of this feedback type is that labelers may tend to give preferences on short-term actions that are easy to judge, failing to communicate long-horizon preferences. Cui & Niekum (2018) propose a similar type of feedback, in which humans segment a trajectory into good and bad parts. This makes it possible to derive many state-action labels from a few segmentation points. Proxy Rewards Proxy rewards are partial or inaccurate reward functions that convey information about the task the agent is supposed to complete but may not induce optimal behavior. This form of feedback does not generally refer to any particular behavior instance but instead gives global direction for the entire task. However, in line with our selection criteria (Section 1.3), we only consider proxy reward feedback that is interactive and online within the context of one or multiple observations to fill holes in the initial description. The defining features of proxy reward feedback is that the labeler passively observes the agent’s behavior (involvement) and gives feedback explicitly (explicitness) on a feature-level (abstraction) with descriptive intent (intent). Proxy reward feedback may be given with respect to a single or multiple instances (arity), although it generally refers to multiple instances. It is most commonly given on an pisode (granularity), but other granularities are possible in principle. @ The work by He & Dragan (2021) exemplifies this form of feedback by using proxy reward functions, i.e., preliminary reward functions that might not cover all edge cases, to guide the agent toward learning the actual reward function. Alternatively, Mindermann et al. (2018); Hadfield-Menell et al. (2017b) suggest querying about the reward function. They allow users to choose from a set of understandable, linear proxy rewards or to specify which features are more critical in the linear reward structure. In a related setting, Guan et al. 2023) lets the user specify changes to a current symbolic reward function in the form of changes to target attributes (e.g., walking speed). Social Behavior Humans give rich implicit social feedback in the form of facial reactions and gestures when interacting with agents. The defining attributes of this type of feedback are that it is given implic- itly (explicitness) on passively observed (involvement) segments (granularity) with respect to a single instance (arity) and literal intent. Cui et al. (2021) propose a framework to learn reward functions from such social behavior. They suggest a two-phase training setup. In the first phase, they ground the implicit feedback by use of incentives, i.e., they incentivize humans to have a known objective. After learning a mapping from feedback to reward, they use regular RL techniques to learn a policy. Note that the learned reward function can be seen as conditional on human implicit feedback and, therefore, they require a human in the loop throughout training. Improvements Improvements are a form of feedback in which the human improves on the agent’s behavior, either by intervening as the agent acts or by providing a corrected behavior after the agent acts. To improve an episode, it is usually necessary to observe the entire episode (granularity) at the instance level (abstraction). In this type of feedback, the human both observes and demonstrates behavior, resulting in co-generative involvement. Improvements generally relate to a single reference trajectory being improved (unary arity), although an improvement could also be interpreted as a binary comparison between the improved and the non-improved trajectory. Improvements are most commonly provided explicitly with instructive intent. 19 We distinguish between post-fac o improvements, calling them corrections, and improvements made while the agent is acting, calling them interventions. The key difference is that the uncorrected trajectory is available in the case of corrections, while it can only be estimated in the case of interventions. Interventions can be considered to user share autonomy to reach a common goal learn form the occurrence of an int Luo et al. (2024) by directly inferring nega‘ and the other is to learn from the data is then used for behavior c not directly used as feedback. In formats: Either by overtaking cont agent’s movements. Losey et al. (2 method that was later extended (Losey & O’Malley, 2018) risk-sensitive deployment. Li et al. physical corrections without needing to wait until the trajectory is comp! be an ins ervention itself that the prior ive rewards from inter corrected trajectory. The la‘ ntify challenging situations reward model training. T addition rol from t (022) proposes to learn o the above distinction, a reward mo to incorpora his setting t (2021) further extend ance of the shared autonomy setting since the agent and the . There are two main ways to leverage interventions: One is to behavior was suboptimal, as done, e.g., by ventions without learning a reward model, ter set: (2022), who ask humans to intercede on agent failure. They use this where the agent is the weakest and to ide: oning an ing is studied by Abramson et al. o collect targeted demonstrations for their evaluations. The gathered he fact that a correction occurred is interventions can come in two main he agent to correct its behavior or by physically correcting the del from such physical corrections, a te uncertainty for active learning and o learn from a sequence of correlated eted. The correction case is closely related to the setting of coactive learning (Shivaswamy & Joachims, 2015), in which a learning system repeatedly proposes a solution which a user may correct to reach a common goal. Jain et al. (2015) treat corrections as a demonstration whi le Jeon et al. (2020) propose (but do not evaluate) an alternative interpretation of inferring implicit preferences from comparisons by assuming the corrected trajectory is preferred over the original one. Natural Language Natural language is a versatile form of feedback that can be used to convey a wide range of information. It cannot on! ly be used to express preferences, but also to suggest concrete changes. Natural language feedback may be given on any granularity, at any level of abstraction. Its defining features are that it is given explicit While much wor! instructions in a non-interactive se language is used as feedback, not natural language. As an example o: where one agen’ Language may be interpreted as a policy (involvement) with descrip learning a language-conditioned policy, regardless of whether or not (responsible for solving problems) is controlled by t (responsible for setting tasks) is controlled by the human. ly (explicitness) in the context of tive intent. ting (Hermann et al., 2017; Nair e or task definition. Note, however, this, Abramson et al. (2022) use R. orm of feedback in the reward-ratio where an utterance implies a set o: can choose an w human to provide 2022b). An alternative way rom the language. rom language feed Language feedback is demonstrated in language feedback terance that maximizes the probability of a desired act specific which leads to pragmatic reasoning). This can be used to infer he utterances they did provide. A similar approac! f trajectories compatible with the w a single (arity) observed behavior or k in language for RL focuses on the problem of learning a policy that follows natural-language al., 2021), we focus on works where that RLHF can be a useful tool for the feedback itself is in the form of LHF in an interactive agents setting, he learned policy and another agent nal choice setting , terance (grounding) and the human ion (ie., the human is assumed to be a reward which may have caused the 20 h is followed, e.g., by (Sumers et al., of incorporating language feedback is by extracting more structured forms of feedback An example is to use sentiment analysis to extract information about the reward function ack, as proposed by Sumers et al. (2021). can also be interpreted by an LLM directly, without learning a reward model. This the concurrent works by Ma et al. (2024) and Xie et al. (2024) who propose to learn a symbolic reward model in the form of a language-model written piece of (Python) code. They use natural o improve the reward model based on observations of the agent’s behavior induced by he previous version of the reward model. In addition to the way language feedback is used, the way it is elicited can also have an impact on the learning process. Sumers et al. (2022a) study the relative merits of natural language instructions and descriptions of the desired outcome. They find that instructions tend to work better for low-autonomy settings, while descriptions are more effective in high-autonomy settings. 3.2.2 Supplementary Classes Merge this with representation-specific, highlight distinction direct feedback about preferences vs feedback about representation in section introduction This section introduces two feedback classes, e-stops and importance, that can be used in conjunction with a primary class to learn a reward model. These classes are not sufficient to learn a reward model on their own, but can supplement a primary feedback type. E-Stops Emergency stops (e-stops) are an active type of feedback. In this type of feedback, the human may intervene with the agent’s behavior by stopping it, i.e., they may choose to stop the agent’s current trajectory at any point. This is closely related to interventions but, in contrast o those, e-stops do not suggest an alternative action. The defining features of e-stops are that the human oth passively observes the agent’s behavior (involvement), gives absolute feedback on a single instance (arity) on the instance level (abstraction) and does so implicitly (explicitness) as a side-effect of regular interaction. The intent is literal due to the implicit nature. For the purposes of intervention, the human usually observes the full episode (granularity). Due to the small amount of infrequent information they rovide, e-stops should only be considered as a supplementary feedback type. E-stops are primarily intended to prevent bad behavior and only implicitly convey information about the correct behavior. This interaction and the arising incentives have been formalized in the form of the “off switch game” by Hadfield-Menell et al. (2017a). Jeon et al. (2020) propose to interpret this as a form of reward-rational feedback, where the ‘off’ choice maps to the trajectory with the robot remaining still after he off switch has been triggered (see Section 5.1.1). Kahn et al. (2021) demonstrate that a robot can learn o navigate using such feedback. Importance Another form of supplementary feedback may come in the form of importance labels, com- municating which parts of the observation are important for the objective. Its defining features are that he importance information itself does not contribute towards generating behavior samples (observed in- volvement), is of descriptive intent, and is given explicitly (explicitness). Granularity, arity, and abstraction may vary depending on the primary feedback type. Since importance feedback needs a base ask with respect to which the importance is defined, it cannot be used on its own but is rather a supple- mentary type of feedback. One way to convey this information is by labeling salient parts of a visual input. This is explored by Guan et al. (2021), who augment pairwise comparisons with manually annotated visual saliency maps, in- forming the algorithm which parts of the visual input contributed to the decision. They leverage these an- notations for data augmentation by assuming that random perturbations to irrelevant (non-salient) regions do not impact the human preferences. Basu et al. (2018) take an even more direct approach by combining comparative feedback with direct feature queries, i.e., asking the user which feature is important for inferring he reward. 3.2.3 Representation-Specific Classes While the previous classes of feedback types are all aimed at directly learning a reward function, there are also classes of feedback types that do not directly learn a reward function but rather help to learn a better representation. Feature Traces While many approaches either rely on hard-coded features or learn a model entirely end- to-end, it is also possible to actively elicit new features from human feedback. Feature traces were proposed by Bobu et al. (2022) as an approach to actively learn new relevant features. This type of feedback relies on 21 a human operator to demonstrate a behavior in which a certain feature of interest, such as the distance to a sensitive object, monotonically increases or decreases. They make it possible to extend the set of features once the current set can no longer adequately explain the human feedback supplied through another type of feedback. The defining characteristics of feature traces are that they are of descriptive (intent) and explicitly (explicitness) given in an active manner (involvement) for a single (arity) segment (granularity) on an instance-level abstraction. Feature traces are strongly related to inter-temporal preferences (Section 3.2.1) since both types rely on changes in feature or reward values in the course of a single trajectory. Bobu et al. (2022) propose to learn from feature traces by leveraging a Bradley-Terry model to learn the feature values, similar to other approaches that use such a model to learn reward values. Similar to importance feedback, feature traces rely on another type of feedback to actually make use of the learned features and is, therefore, a purely supplementary form of feedback. For instance, Bobu et al. (2022) use intervention feedback to train a reward model on the set of features derived using feature traces. Similarity Queries Similarity queries are a feedback type aimed at learning a representation conforming to a notion of similarity and difference in the trajectory space. That aim is closely aligned with that of feature queries, though the actual queries are more similar to comparisons. The queries consist of triples of trajectories, with one anchor and two alternatives, for which the human has to decide which pair is more similar. Responses to similarity queries are given on observed behavior (involvement) with ternary arity, descriptive intent, and explicit feedback, while the granularity and abstraction may vary. This type of feedback was first introduced by Bobu et al. (2023), who used it to learn representations for reward learning. 3.3 Initializations Some modes of communicating reward functions are not interactive nor online and, therefore, do not directly fit within the scope of this survey (Section 1.3). Since these are often used to initialize a reward function for later refinement with some of the previously discussed interactive feedback types, they are still worth mentioning. Initializations are most commonly given by examples of successful task completions, either in the form of terminal or goal states , expert demonstrations (Ibarz et al., 2018; Fu et al., 2018a; Palan et al., 2019; Lee et al., 2021b; Buyik et al., 2022a; Abramson et al., 2022; Huang et al., 2023), labeled demonstrations , or ranked demonstrations . It is even possible to infer some human preferences by the state of the environment alone, e.g., assuming the parts of the initial environment that are under the user’s control are largely set up in accordance with the user’s preferences (Shah et al., 2019; Lindner et al., 2021a). Since offline initialization is not the main focus of our survey, we do not cover these works in detail. We refer the interested reader to literature on inverse RL (Arora & Doshi, 2021) for further details on learning from demonstrations in particular. 3.4 Choice of Feedback Type The best feedback type is not always clear and may depend on the task, the user, or the agent. It may also change over time as the agent learns more about the task. Table 4 may serve as a starting point to select a set of feedback types which may be applicable based on the possible user interaction and expertise, e.g., y the desired granularity, level of abstraction or involvement. This choice may be informed, among other eatures, by the task complexity or time horizon: Jain et al. (2015) compare purely passive (re-ranking based) with active (slight trajectory improvements) feedback and conclude that the former is usually preferred by humans when the action space is large while the latter is preferred for complex tasks. Similarly, Sumers et al. (2023) empirically demonstrate that language is a more effective teaching modality than demonstrations for complex concepts. The relevance of the time horizon is highlighted in the work by Sumers et al. (2022b), which suggests that instructive feedback is more efficient for short time horizons and descriptive feedback or longer horizons. n addition to these static choices, Section 4.1.2 will discuss how to choose a feedback type adaptively. It is also possible to let the user choose the feedback type, as demonstrated by Jain et al. (2015) who let the user 22 choose between re-ranking and improvement. Jeon et al. (2020) propose to use this choice of feedback type itself as a source of information (“meta-choice”). 3.5 Combination of Feedback Types In addition to using any of the previously described feedback types in isolation, combining multiple feedback types is both possible and often advantageous. There are three main ways to combine feedback types: (a) a two-phase setup, consisting of initialization and refinement, (b) integrating a primary feedback type with a supplementary one; and (c) merging multiple primary feedback types. The two-phase setup can be used to either initialize the reward model from offline data or to learn a representation that improves later reward learning. A common approach involves using one feedback type, typically demonstrations (see Section 3.3), to initialize the reward model, subsequently fine-tuning this model with another type, such as comparisons. This method is exemplified by Ibarz et al. (2018), who combined demonstrations and pairwise comparisons. For a more detailed discussion on feedback types suitable for initialization, refer to Section 3. Combining a primary feedback type with a supplementary one can be beneficial to make the most out of the available user-interactions. Supplementary feedback, while not sufficient for reward model training by itself, can often be collected cheaply or as a side-effect of other interactions, making it a valuable addition to the rimary feedback type. Refer to Section 3.2.2 for a discussion on supplementary feedback types. Finally, combining multiple primary feedback types can be beneficial to capitalize on the strengths of each. For instance, Koppol et al. (2020) combine informative and demanding queries with less-informative and less-demanding ones to achieve a balance between cognitive load and informativeness. Similarly, to en- hance expressivity, Mehta & Losey (2023) combines demonstrations, pairwise comparisons, and corrections, allowing users to select their preferred type of feedback. 4 Label Collection Environment Agent Reward Model Action az Dynamics $4 State sei Query gi Labeler Label |; In this section, we explain how preference data can be collected for training a reward model, independent of the specific type of feedback used. We start by overviewing the active learning problem posed by query selection, e.g., how queries can be generated for a given query type, and how even the type of feedback itself can be selected. Following this, we discuss issues arising in such human-computer interaction. 4.1 Active Learning We first discuss query generation and selection for a given feedback type, then the extension of these tech- niques to the choice of feedback types. 23 References Factors Uncertainty On-policy Query Trajectory Query Query Data Simplicity Quality Diversity — Cost Daniel et al. (2014) Probability of improvement x x x x x Expected improvement x x x x x Upper confidence bound x x x x x Christiano et al. (2017) Ensemble variance x x x x x Sadigh et al. (2017) Volume removal x x x x x Biyik et al. (2024) Volume removal x x x v v Wilde et al. (2018) Feasible space reduction x x v x x Ibarz et al. (2018) Random x x x x x Mindermann et al. (2018) Information gain x x x x x Cui & Niekum (2018) nformation gain v x v x x Racca et al. (2019) Entropy x v x x x Biyik et al. (2019) Mutual information x v x x x Biyik et al. (2020) nformation gain x x x x x Reddy et al. (2020) Ensemble averaged KL- x x v v x divergence to mean output x x x x x Novoseller et al. (2020) Dueling posterior sampling x x v x x Wilde et al. (2020; 2021) Vlaximum regret x x x x x Lee et al. (2021b) Ensemble-averaged entropy x x x x x Lindner et al. (2021b) nformation gain v x x x x Katz et al. (2021) Posterior sampling x x v x x Myers et al. (2023) Expected value of information x x x x x Biyik et al. (2024) Mutual information x x x v x Dwaracherla et al. (2024) Double Thompson sampling x x v x x Hu et al. (2024) Random v x x x x 4.1.1 Query Generation and Selection One core problem that needs to be tackled in RLHF is that of learning about the human’s preferences. This problem shares some simi arities with the active learning setting since the agent can actively query a human teacher about those preferences. However, in contrast to standard active learning, which usually assumes a supervised learning set ing, in RLHF, the agent needs to solve this pro lem in the context of RL. This means that it can both influence the distribution of the data (i.e., the transitions) and decide which data should be labeled. As the RL agent is trained and its learned policy changes, the trajectories it generates will naturally evolve. Most work directly uses natively, the agent can a! RL training), possib! Liu et al. (2023b)). This Algorithm 1) so tha via a criterion, usual. acquisition function y with a learned transi ind of active generation of queries can possibly | n order to efficiently learn a suitable reward it can quickly learn a existing work in RLHF uses an acquisition he trajectories ob ained during RL training for rajectories, candidate queries are often randomly generated. However, efficient approach is to generate queries by lso generate trajectories specifically to be used ion model to limit the sampling cost (e.g., Reddy et al. (2020); model, the agent must genera good strategy using those queries. This selection is performed y called acquisition function, which allows the queries to be compared. Although most ‘unction that provides some measure of uncertainty (e.g., about he learned rewards), which is arguably one of the most important factors in active learning, an efficient (see Table 5 for works dedicated to improving query selection) may need to include references learning. Using such or pairwise comparison, a more exploiting preference transitivity . Alter- ‘or querying (not necessarily for ead to more informative ones. e and select queries (Line 5 from various additional other aspects, such as: on-policy data, query simplicity, trajectory quality, query diversity, 1 We follow the terminology used in Bayesian active learning. 24 or query cost, which will be discussed one by one in the following. As a side note, interestingly, as highlighted by Habibian et al. (2022), the queries asked by an RL agent may also reveal its current reward learning stage. Uncertainty This factor usually corresponds to epistemic uncertainty (Hiillermeier & Waegeman, 2021), which represents how uncertain the agent is about the ground-truth reward function. Epistemic uncertainty can be contrasted to aleatoric uncertainty, which describes inherent stochasticity in the system. While the latter cannot be fully eliminated, the former can be reduced with addi- tional queries. Uncertainty is usually one of the most important aspects to consider when deciding which query to ask. It is usually represented either as a probability distribution (i-e., belief) in Bayesian approaches or using an ensemble of reward networks to approximate this belief. However, other representations are also possible, e.g., using the recently-proposed epistemic neural network . With a belief representation, various classic acquisition functions have been considered in the Bayesian framework, such as the probability of improvement, the expected improvement, or the upper confidence bound. For instance, Daniel et al. (2014) compare those three criteria in a robotic domain and observe that the latter yield the best performance, but at a high cost in terms of number of queries, while the first asks the fewest number of queries, but with a slightly lower asymptotic per- formance. Other alternative criteria have been considered, such as volume removal (Sadigh et al., 2017; Basu et al., 2018; 2019) or information gain (Mindermann et al., 2018; Biyik et al., 2019; 2020). The volume removal criterion uses the minimum volume of the hypothesis set removed by an answer to a query as the acquisition function and has been shown to be effective in practice. However, Biyik et al. (2019) show that volume removal has uninformative global optima and argue that the practical effectiveness is due to non-convexity leading to local optima that are informative. They also show that optimizing for information gain has the same computational complexity while avoiding this flaw. One drawback of these Bayesian approaches is that they require maintaining a distribution over reward functions (e.g., using Gaussian processes or simpler probability distributions, such as Gaussian distribution, but with a linear reward model) and, there- fore, may not be suitable for more complex domains due to the computational complexity or the strong assumption about the reward model. When using an ensemble instead of a direct belief representation, these criteria for epistemic un- certainty reduction correspond to measures of disagreement within the ensemble. Previous cri- teria could possibly be applied, but one popular candidate is the variance of the ensemble out- puts (Lee et al., 2021b; Metcalf et al., 2023; Gleave & Irving, 2022) or equivalently its standard deviation . The average entropy of the ensemble outputs (e.g., when assuming a Bradley-Terry model for the human answers) has also been used (Lee et al., 2021b;a; Park et al., 2022). However, note that it does not quantify epistemic uncertainty but rather the aleatoric uncertainty in the human’s answers, as provided by the response model of the human. Therefore, this criterion may not be suitable in the RLHF setting since it amounts to focusing on the queries for which an answer is expected to be the most random (according to the Bradley-Terry model). By definition of this model, the segments in the pairwise comparisons are the most similar in terms of returns and are, therefore, the hardest to answer for the human. In contrast to those acquisition functions that lead to deterministic query selection, sampling-based approaches have also been studied, from pure random selection to Thompson sampling and its variants (Novoseller et al., 2020; Dwaracherla et al., 2024). Re- cently, in the context of fine-tuning LLMs with pairwise comparison queries, Dwaracherla et al. (2024) shows that using the relatively novel epistemic neural network , dou- ble Thompson sampling (Wu & Liu, 2016), which naturally favors better elements to be compared, performs well experimentally. In addition to epistemic uncertainty, one may also take the outcomes into account to select queries, that is consider utilities (i.e., returns or expected returns in RL). In a Bayesian setting, this leads to acquisition functions such as expected value of information or information gain over return differences (Lindner et al., 2021b), while in a non-Bayesian setting, the notion of regret, which measures the difference of performance between a policy optimal for the ground-truth reward function and a policy optimal for a learned reward function, can be used . Finally, it should be mentioned that naturally approaches with use uncertainty as the main criterion for label collection (e.g., No for more details). heoretical guarantees usually also voseller et al. (2023), see Section 7 On-Policy Data Only focusing on uncertainty is likely insufficient or inefficient in RL because the previous methods may focus on choosing queries to identify the reward function as precisely as possible uniformly on the whole state-action space. However, it may be important to favor more on-policy trajectories to guarantee the relevance of the generated queries for the current policy. Indeed, improving reward learning in state-action regions that may never be visited with the current policy would lead to wasteful queries (Lindner et al., 2021b). One simple approach to ensure that the data is more on-policy is by simply sampling from the current policy (Cui & Niekum, 2018) or favoring more recently-generated trajectories (Eberhard et al., 2022; Hu et al., 2024). Query Simplicity Selecting queries only based on their informativeness may lead to queries that are hard for a human to answer, which is, for example, the case for the average entropy. The ease of answering a query is important to alleviate the cognitive load of the human oracle. Some work specifically takes this aspect into account, for instance, by considering the similarity of consecutive queries or the information gain. For this latter criterion, Buyik et al. (2019) show that in contrast to volume removal, it naturally leads to queries that are easier to answer for a human because information gain can be increased when the uncertainty in the human answer is lower. Trajectory Quality Most approaches directly use the trajectories generated during RL training. Especially early in training, these can be very bad with respect to the ground-truth reward function. In addition to that, they can be irrelevant or even contradictory for a given task . Building queries on such trajectories may lead to unnatural queries for a human to respond to, such as comparing a very bad trajectory with an irrelevant one. Katz et al. (2021) measure trajectory quality by optimizing over sampled reward functions. Similarly, Cui & Niekum (2018) generate trajectories using op imal policies for reward functions sampled from the current Bayesian belief. Query Diversity When asking many queries (in batch, in sequence), the diversity o the queries becomes especially crucial to avoid asking redundant queries. Most work (Christiano e al., 2017; Lee et al., 2021b; Verma & Me calf, 2024) follows a very myopic approach: Queries are often selected from a Query Cost The cost usually randomly-generated set of potential queries, and sequences of queries are not really coordi- nated. While some work exists that specifically tackles the selection of a batch of diverse queries (Biyik & Sadigh, 2018; Bryik et al., 2024), the latter is rarely considered due to its computational intractability. Indeed, planning ahead a sequence of queries would amount to solving a sequential decision-making problem under uncertainty over a combinatorial action space (i.e., the set of pos- sible queries). For diverse batch querying, previous work considered using clustering methods such as k-medoids (Biyik & Sadigh, 2018) or more recently determinantal point processes, which define probability distributions that promote diversity . of generating queries may also be an important factor if the interaction of the human is live since it may not be practical to let the human wait before showing any queries . In that case, it may be more important to quickly show some relatively good queries instead of computing the most informative ones. Although this factor may not translate directly into an acquisition function, it may influence the choice of the acquisition function and its implementation in a given problem. Since various different acquisition functions have been considered, some effort (Lee et al., 2021b;a) has been made to compare them. Generally speaking, uncertainty-based criteria (e.g., variance or average entropy) seem to often perform better empirically compared to random selection, a query diversity-based 26 criterion alone or combined with an uncertainty-based criterion. In addition, Eberhard et al. (2022) em- pirically observe that a variance-based criterion performs better than a selection method only based on trajectory recency. Surprisingly, random selection has been shown to perform competitively in some cases (Christiano et al., 2017; Ibarz et al., 2018). Thus, a better understanding of which acquisition function should be preferred in which situation or domain is still an open question. In addition, combinations of different criteria have naturally also been evaluated. For instance, Reddy et al. (2020) use four acquisition functions (high uncertainty, high novelty, high reward, low reward) in parallel. This approach has also been validated in a 3D environment . A more sophisticated approach consists of considering a portfolio of acquisition functions and learning to select them using a multi-armed bandit approach . Various extensions to the basic setting have also been investigated. In the context of multiple human labelers, the issue of selecting reliable teachers to query arises (Daniels-Koch & Freedman, 2022). Assuming all teachers have the same preferences, this can be modeled for pairwise comparisons by incorporating a rationality coefficient 6 into a Bradley-Terry model and estimating this factor: N 1 . max] T+ exp(3(Ru (td) — Ru)’ ®) where a higher 6 corresponds to a more reliable human (see Section 5.1.2). The setting in which this assumption does not hold, i.e., the labeler’s reward functions differ (a setting already considered in in- verse RL (Choi & Kim, 2012)), has also been studied recently (Siththaranjan et al., 2024; Xue et al., 2023a; Dong et al., 2024; Myers et al., 2021; Bakker et al., 2022). Interestingly, a noisy oracle may sometimes pro- vide more information than a completely reliable oracle. For instance, in Eq. (3), the frequency of erroneous answers given by the noisy oracle is related to how much a segment is preferred to the other one (Xu et al., 2020; Chan et al., 2021). In contrast, only a binary preorder over segments can be inferred from the answers of a reliable and deterministic oracle, which may not be enough to recover the true reward function. Another notable recent work is that by Ellis et al. (2024) who raise the issue of identifiability of the ground- truth reward function: many reward functions result in the same optimal behaviors. The authors propose a framework that enables the generation of acquisition function for various definitions of reward similarity, such as the one discussed in Section 5.4.1.2 Adaptive Choice of Feedback Type In addition to selecting queries within a given feedback type, it is also possible to actively select the feedback type itself . The best choice of feedback type can depend on many factors, such as human rationality as well as task-dependent factors, some of which may change during the labeling process. Ghosal et al. (2023) formalize this setting as one in which we try to select a feedback design (or feedback type) x out of the space of possible designs 4 such that the expected information gain over the distribution of reward functions is maximized for the next human response. Concretely, the goal is to choose a feedback design by means of r= argmax Eo, ~P(en|2) [Dx (P(O | cn, x) | P())], rE where c;, is the human response to a query defined by x and P(6) is the prior distribution over reward functions. Ghosal et al. (2023) find that the most informative feedback type depends on the (type-dependent) rationality of the human labeler (see Section 4.2.1). More precisely, it is shown that the most informative feedback depends on the rationality factor, e.g., while demonstrations are more informative than comparisons when the human is highly rational, comparisons should be preferred in less-rational settings. Given that this rationality might change due to factors such as fatigue or an individual labeler’s capabilities, this suggests that adaptively adjusting the feedback type during the labeling process may be worthwhile. Further study of this relationship is a promising area for future work. 27 4. This section explores the label collection process, which follows after query selection. This task intersects with several related disciplines, especially within the social sciences, as it encompasses the design of interactions to facilitate informative query responses. A prominent field in this area is psychometrics , which focuses on measuring psychological attributes, including preferences. Similarly, survey research is dedicated to developing techniques for gathering information from individuals via surveys. Human- computer interaction plays a significant role as well, investigating the design of user interfaces tailored for preference elicitation . Moreover, preference label collection is also necessary for discrete choice experiments within health economics , where it is used for the assessment of service values. 4.2.1 Psychology-Aware Preference Elicitation Understanding human psychology is essential for effective preference elicitation in RLHF systems. Human decision-making is complex, often diverging from traditional rational choice models due to cognitive, social, and emotional factors. This complexity is exemplified by phenomena like fatigue, which can affect the reliability of choices based on the order of queries. This section overviews these phenomena, exploring how constructive preferences, biases, framing effects, and social interactions shape the observed choices. Recognizing and addressing these psychological underpinnings is key to developing more accurate and reliable systems. In this section, we will discuss various psychological phenomena, such as cognitive biases and response biases, and related effects (fallacies, biases, heuristics, psychological phenomena impacting decision- making processes), which may falsify labels by adding systematic bias or noise. Preference learning methods typically assume the existence of inherent, stable preferences that can be elicited through querying. Contrary to this assumption, psychological research, such as the work by Lichtenstein & Slovic (2006), indicates that preferences are often constructed during the elicitation pro- cess and may vary with the method of elicitation or over time. This suggests that the feedback type not only affects elicitation’s effectiveness but also shapes preferences. Systematic biases, noise, and other psy- chological factors may influence observed choices, challenging the traditional models of human choice used o infer latent utilities (see Section 5.1). The elicitation method, query presentation, and context thus play a critical role in shaping measured preferences, compounded by cognitive biases and irrationalities. The influence of psychological phenomena on preference learning has been well-documented in the litera- ure, especially within the context of explicit preference elicitation for recommender systems. For instance, Tran et al. (2021) provide a thorough discussion of the relationship between psychology and recommender systems. Similarly, Atas et al. (2021) review how preference construction is influenced by cognitive biases, personality traits, and emotional states in recommender systems, discussing effects like serial position, fram- ing, anchoring, choice overload, and preference visibility. In a more specialized discussion, Mandl et al. (2011) focus on cognitive biases in the context of consumer decision-making and its interaction with rec- ommender systems. Mandl et al. (2011) specifically address cognitive biases in consumer decision-making in interaction with recommender systems. Finally, Kaufmann et al. (2023) link these psychological aspects to RLHF, discussing the common practice of using synthetic instead of real human feedback for algorithm evaluation and highlighting the limitations of that approach. They further discuss challenges posed by real human feedback, many of which are related to the concepts discussed in the following paragraphs, as well as the opportunities provided by integrating psychological insights into RLHF systems. Constructive preferences are closely related to framing effects, which refer to changes in elicited preferences based on how tasks or alternatives are described, even when these descriptions are essentially equivalent. For example, presenting a choice as a loss versus a gain can lead to different decisions despite identical outcomes. Moreover, serial position effects, commonly known as primacy and recency effects, also play a significant role. These effects describe the tendency for the beginning and end of an experience to influence subjective experience disproportionately. This phenomenon becomes particularly relevant in scenarios like video choices, where the initial or concluding segments might disproportionately affect preferences. Atas et al. (2021) discuss both of these effects in the context of recommender systems. 28 Ordering effects pose another challenge in preference elicitation, where the sequence of queries can affect responses. Day et al. (2012) outline several factors contributing to these effects: institutional learning, changing preferences, and varying levels of cognitive effort. Institutional learning involves gaining familiarity with the task and feedback type, which can enhance labeler’s expertise and, consequently, the accuracy of their responses. However, due to the constructive nature of preferences, their preferences may evolve during the elicitation process, leading to changing preferences. This evolution might also be influenced by anchoring effects, where previously seen instances bias responses. Furthermore, cognitive effort levels can fluctuate due to factors like fatigue or boredom. This is closely related to choice overload, a form of fatigue from excessive choices, as discussed by Atas et al. (2021) and bounded rationality, as explored by Chen et al. (2013). In such scenarios, labelers might opt out of making a choice when overwhelmed by options. Bounded rationality refers to the limitations in human decision-making capabilities, particularly when processing large amounts of information, which aligns with the concept of choice overload. To address these challenges, studies like Biyik et al. (2019) and Zhang et al. (2022) propose methods to reduce cognitive effort in responding to queries. Buyik et al. (2019) focus on posing queries that are straightforward for humans to answer, while Zhang et al. (2022) enhance the human evaluation process by presenting queries in a user-friendly format. When multiple labelers collaborate on the same task in preference elicitation, as is studied, e.g., by Barnett et al. (2023) and Daniels-Koch & Freedman (2022), this may lead to another set of biases if they have the opportunity to exchange information. This exchange may either be direct or indirect through observing the system’s predictions, which are based on the other labeler’s feedback. Such interactions can affect their preferences through several mechanisms, as identified by Atas et al. (2021): anchoring effects, ransfer of emotional states, and conflict avoidance. Anchoring effects, for instance, occur when a labeler’s choices are influenced by the knowledge of others’ preferences or system predictions, a phenomenon also discussed under the term preference visibility. This bias can lead labelers to align their preferences with the anchors they are exposed to, which is a significant consideration in recommender systems. Understanding hese biases is crucial for designing RLHF systems that mitigate the influence of labeler interactions on reference construction. The effects previously discussed stem from systemic biases in preference expression. In addition to these iases, choices may also be affected by noise. This is commonly discussed under the term stochastic rational- ity, where an agent’s behavior is rational with respect to an unobserved random state. The reward-rational implicit choice framework, as introduced by Jeon et al. (2020), addresses this by integrating a rationality ‘actor 3 into the human choice model (see Eq. (3)). This factor’s impact has been further examined by Ghosal et al. (2023) through synthetic experiments and user studies, demonstrating that accurately estimat- ing this type-dependent rationality coefficient can enhance learning performance and guide feedback type selection (see Section 4.1.2). However, a practical method for estimating this factor remains a challenge. While Ghosal et al. (2023) use calibration feedback with a known latent utility function for estimation, such an approach is not feasible for most tasks. In a related study, Daniels-Koch & Freedman (2022) investigate a scenario with multiple teachers, focusing on the agent’s ability to select the most knowledgeable or rational eacher. Therefore, developing more advanced methods to estimate this factor, along with understanding its variability due to factors like fatigue or other ordering effects, presents a vital area for future research in reference elicitation Finally, the quality of human feedback is biased towards factors that are easy to judge. Hosking et al. (2024) demonstrates that in the case of LLM fine-tuning, humans tend to favor assertiveness over factuality, since he latter is hard to judge without external assistance or resources. A similar phenomenon was observed in he control setting by Amodei et al. (2017), where the agent learned a behavior that looked good only from he camera angle that the human labelers had access to. ncorporating psychological insights into the preference-learning components of RLHF systems is essential for optimizing their efficacy. A key area of focus should be research aimed at mitigating biases and harnessing cognitive aspects of preference formation. For instance, designing user interfaces that minimize framing effects and developing algorithms that account for ordering and serial positioning are crucial steps. In this realm, Metz et al. (2023) and Yuan et al. (2024) each propose a configurable user interface for studying various feedback types and their combinations. Additionally, the study by Krening & Feigh (2018) on the impact of feedback type, such as binary critiques versus action advice, on task performance and labeler 29 satisfaction highlights the significant role of feedback type in preference elicitation. Furthermore, the work of Pommeranz et al. (2012) in user-interaction design underlines the importance of having an expressive feedback type to increase user engagement. The integration of these research findings into RLHF systems points to a clear need for a more multidisci- plinary approach. Drawing insights from related fields like behavioral economics and psychology can provide valuable methodologies and perspectives. Addressing irrational choice patterns and enhancing the quality of human feedback remain critical challenges. As we continue to develop and refine these systems, the fo- cus should be on creating robust frameworks that align learning processes with human behavior, effectively managing the inherent complexity and variability of human feedback. 4.2.2 Importance of Researcher-Labeler Agreement High-quality labels are important for the final policy in an RLHF process. Early work on fine-tuning language models using RLHF noticed a mismatch between the researcher’s goals and the (paid) labeler’s actual labels (researcher-labeler disagreement). Ziegler et al. (2020) note that researchers agreed with each other about 60% of the time (on 4-way comparisons, where random choice would result in 25% agreement), while agreeing with labelers only 38% or 46% of the time (depending on the task). Stiennon et al. (2020) attempt to reduce these disagreements by maintaining a hands-on relationship with the labelers and thereby ensuring high researcher-labeler agreement. Concretely, they provide on-boarding with detailed instructions, keep an open channel of communication between researchers and labelers, and give feedback to the labelers. They evaluate the researcher-labeler agreement and reach an agreement rate of 77% + 2%. Perfect labels are often impossible due to the inherently subjective nature of the task. Returning to the example given by Stiennon et al. (2020), different researchers agreed with each other in only 73% + 4% of the cases. Ouyang et al. (2022) also report the agreement rates on a different task (instruction fine-tuning instead of summarization) and find that labelers agree with each other in 72.6 + 1.5% of the time, after a screening procedure that, amongst others, selects labelers that agree with researcher labels. Preferences can additionally be inconsistent between feedback types, as demonstrated by the finding of Bansal et al. (2024), which shows that preferences inferred from ratings and rankings significantly disagree for both human and AI annotators. The importance of quality does not trump the importance of quantity, however. Indeed, Stiennon et al. (2020) note that excluding low-confidence samples from the data set generally did not help with reward model training. This indicates that even though quality is important, a larger quantity is still generally better. o The scale of the labeled data set required for effective training and refinement varies widely, impacting the quality of the resulting models. Studies have shown a broad range in data set sizes, from tens of labels in smaller studies to hundreds in more complex scenarios . Larger- scale applications may require thousands or even millions of labels , each bringing its own challenges in ensuring label accuracy and consistency. This variability in data set size underscores the need for rigorous label quality control measures across different scales. In smaller data sets, each label carries more weight, making accuracy and precision critical. Conversely, in larger data sets, the challenge lies in maintaining consistency and mitigating systematic biases that might emerge from he sheer volume of data. Similarly, the labeling setting varies in the surveyed works, from author-provided feedback , over small in-person studies , to larger remote studies . Each setting rovides unique challenges to ensure high-quality labels. Various works have suggested measures to improve label quality. Hagendorff & Fabi (2022) discuss the ossible failure modes of the labeling task in more detail, for example, discussing systematic biases and conflicting motivation, and propose concrete changes to the training and evaluation methodology to alleviate hese. Glaese et al. (2022) suggest providing labelers with multiple natural language rules and collecting reference labels for each rule individually to improve label quality. This is related to Bai et al. (2022b), who propose to generate feedback automatically based on such a set of rules and a language model. 30 5 Reward Model Training Environment Dynamics In this section, we delve deeper into the process of reward model learning, which we briefly touched on in Section 2. 5.1 Human Feedback Model The basic premise underlying the majority of approaches in RLHF is that human feedback is directly related to the reward function to be learned. To this end, the human feedback must first be captured in a sound mathematical framework that establishes the connection to the reward function. On a high level, one can break down (almost) all feedback types in Section 3.2 to a choice scenario: The human chooses one specific feedback option (label) from an available (possibly infinite) pool of possible feedback options (choice sets). Here, the query that is made specifies the explicit contents of the choice set, e.g., if the query is to compare two trajectories, then the choice set consists of all possible outcomes for these two trajectories. Assuming that human choices are not always optimal, one obtains a fruitful mathematical framework when focusing on the probability P(c is chosen|C) , (4) where C is the set of possible choices and c € C one explicit choice. For the RLHF scenario, where the agent asks queries g; and the human gives labels 1; as feedback (see Section 2.4), the choice set is specified by a function of the query. Formally, C = m(q) for some mapping m that maps a query gq to the set of all possible candidate labels extractable from q for the specific feedback type. For example, if the query is to rank a finite number of trajectories, then the choice set is the set of all possible rankings that can occur for the trajectories involved. With this view, we can therefore place (4) in the RLHF context and write P (label 1 is provided | m(q)) (5) for the probability that a human labeler returns a label | from all possible candidate labels that can be extracted from a given query g. We explain next how this probability can be modeled and discuss various related modeling questions (e.g., human rationality, multiple humans, or Markov assumption). One could also recover the noiseless scenario if the latter probability distribution is degenerated for all possible candidate label sets. 5.1.1 Boltzmann Distribution Human choice models as in (4) have been studied for a long time in various scientific fields such as psychol- ogy , economics , or behavioral science . Accordingly, there are many different choice models to resort to for (5), which, in some cases, are the same models, just under ?This point of view goes back to the work of Jeon et al. (2020). 31 different names. A popular class of such human choice models assumes every choice option c to be equipped with a (latent) utility u., which the human perceives in a perturbed way. This perturbation is modeled by means of perturbation random variables €, that perturb the utility in an additive way, so that (4) becomes P(c is chosen|C) = P (« = argmax Uc + «) . (6) ceC The translation for the RLHF setting for (5) is then accordingly P (label 1 is provided | m(q)) = P ( =argmax w+ «) ; (7) lem(q) and we shall now stick to the RLHF translation from now on. These probabilities depend on the specific distributional assumptions that are made on the perturbation variables that only for specific cases lead to a closed-form of the right-hand sides of the latter equations. When stipulating a standard Gumbel distribution for the perturbations, one always obtains a closed form that is proportional to the exponential utility of the provided label: P (label / is provided | m(q)) « exp(ur) . (8) This is known as the Boltzmann distribution that also appears in a perhaps slightly modified version in various different subfields of ML and statistics. When restricting to discrete (choice) sets for m(q), this distribution is also known as the multinomial logit model or Gibbs measure , and as the Bradley-Terry model (Bradley & Terry, 1952) when the choice sets consist of pairs. All of these also have a link to the Plackett-Luce model (Luce, 1959; Plackett, 1975), which is a probability distribution on the space of total orders or rankings (see Alvo & Yu (2014) for details). This model is often used for various reasons. A particularly compelling reason is the closed analytic form, which in turn makes it possible to obtain a closed form for the gradient with respect to the utilities. Another reason is that this model satisfies Luce’s axiom of choice , which requires the probability of choosing an option from a pool of choice options not being affected by the presence or absence of other options in the pool. In this way, coherent decision-making is ensured, which, however, might be challenged as humans are likely not making fully rational decisions (see Section 4.2.1). Jeon et al. (2020) show that the usage of the Boltzmann distribution is justified by the principle of maximum entropy. More precisely, they show that it is the maximum entropy distribution over choices for a so-called satisficing human decision maker, i.e., one who is making in expectation a choice with an optimal reward up to some slack € > To build the bridge between reward learning and the modeling of human feedback, the Boltzmann distribution can be used by assuming that the utilities can be represented as a function of the reward function, usually featuring the return of a trajectory. More specifically, one assumes a grounding function G that maps choice options (or labels) to the set of distributions over trajectories and sets the utility of a label / as w= E,raw[R(7)) - ®) Note that uj depends on the return R, so that we also may use u;(R) to emphasize this dependency. For the common case of pairwise trajectory comparisons, where for two trajectories 7,,72 we obtain for the possible labels | € {71 > 72,71 ~ T2} the respective utility by using the projection onto the preferred trajectory as the grounding function G. Accordingly, the utility of the label represents essentially the utility of the preferred trajectory of that label, i.e., 71 or 72 in this case. As another example consider the case of e-stops feedback (see Section 3.2.2). Here, the possible labels | provided by the user are STOP; and CONT encoding the stopping at time ¢ or the continuation of a trajectory. For the grounding function, one can define aw = { 1 = CONT, ToTe---T, |= STOP;, where 32 feedback type. It is worth noting that one can also easily find a grounding function for the feedback type of a (partial) order over trajectories as considered, for instance, by Myers et al. (2021). Moreover, one can generalize this modeling approach by using (partial) segments instead of trajectories. Although this general human feedback model has been much in use and shown to be useful for the sake of hu- man alignment, it is not without its critics (see Lindner & El-Assady (2022) or Section 3.2.1 in Casper et al. (2023)). This has led to different adaptions of the general model based on the Boltzmann distribution that will be discussed in the following. Moreover, we will also concisely review other human feedback models that have been in use besides the Boltzmann distribution, discuss relevant work on the consequences or robustness of human feedback model misspecification, and highlight contributions on varying the standard assumptions on the nature of the human feedback. 5.1. The Boltzmann distribution in (8) can be extended by a rationality coefficient 3 € [0,00) that reflects the precision of the human labeler?: P (label 1 is provided | m(q)) = P ( =argmax 6 uy + «) x exp(3- uw). (10) lem(q) The higher 6, the more (10) resembles a pointmass distribution modeling a highly rational human labeler (decision-maker) that is always able to identify the option the with highest utility, while the lower 8, the more (10) resembles a uniform distribution modeling a highly irrational human labeler (decision-maker) acting purely at random. Without this extension, the commonly considered Boltzmann distribution (or Bradley-Terry model in the common case of pairwise comparisons) in (8) assumes a rationality coefficient of . Ghosal et al. (2023) show in their experiments that the estimation of this coefficient can indeed positively influence reward learning. For the estimation, however, a calibration reward function is needed, as the rationality coefficient is otherwise not identifiable (Bengs & Hiillermeier, 2020). Similar findings are shown y Daniels-Koch & Freedman (2022), who model the rationality coefficient as a query-dependent function hat might differ for the human labelers (see Section 5.1.6). Another alternative to the rationality coefficient for representing irrational humans is achieved by introducing a query-independent error probability . To be more precise, it is assumed that the human labeler only adheres to the Boltzmann distribution in (8) in 90% of cases and otherwise makes completely random decisions. This formulation is similar to Huber’s contaminated model (Mu & Xiong, 2023). 5.1.3 Alternative Utility Notions Knox et al. (2024) show that the Boltzmann model does not generally lead to an identifiable reward function using (9) by presenting three concrete scenarios for which identification is not possible. The root cause of he non-identifiability is the usage of a trajectory’s return as the utility in (9). They, therefore, suggest using a trajectory’s regret as an alternative, which provably leads to identifiable rewards. A trajectory’s regret is the negated sum of the optimal policy’s advantage over each state-action pair in he trajectory. Empirically, it has been shown that this modification improves the alignment of the learned strategy with human preferences. The downside of this alternative is that regret depends on the unknown optimal policy. Recently, it has also been suggested to consider @Q-values of a human policy as the utili- ies , while Holladay et al. (2016) used differences of cost functions that depend on the available choice set and the human’s uncertainty. 5.1.4 Human Feedback Models Beyond Boltzmann While the human feedback model based on the Boltzmann distribution is the most widely used model nowadays, other models have also been considered in the literature. In particular, for the probability in (4) ’This can be achieved by multiplying the utilities u, with 33 other models such as the Thurstone model (Wilson et al., 2012; Kupcsik et al., 2018; Biyik et al., 2020), the ridge-noise model , the binary model or mixed forms thereof have been considered. Of these models, only the Thurstone model has a similar interpretation as the Boltzmann distribution based on perturbed utilities, only differing in the distribution of the perturbance random variables. Link functions Another possibility, which is particularly popular in theoretical work on RLHF (see Section 7), is the use of other functions on the right-hand sides of Eq. (8) than the exponential function. The concept is primarily used for pairwise comparisons of trajectories. It essentially states that the probability of the result of a pairwise comparison between two trajectories is the difference of their utility values under a so-called link function. More specifically, let ¢ = {1,72} be the query to compare the trajectories 7, and 72, then, assuming a link function @ : R — [0,1], one models the probability in (5) for J representing a preference for 7, as P (label / is provided | m(q)) = P (71 > 72 |m({71, 72})) = ®(ur, — ury) - (11) For | representing a preference for 72, one proceeds similarly. The minimal assumptions on the link functions are that (i) it is (strictly) monotonically increasing to take into account that trajectories with higher utilities will also have a higher chance to be picked; (ii) (x) = 1 — &(—z) to ensure that P (7 > 72|m({t1,72})) = 1—P (m1 ~ 72| m({71, 72})). Note that the second property implies ®(x) = 1/2 so that trajectories with the same utility also have the same chance of being selected. Any cumulative distribution function of a symmetric continuous random variable fulfills these two conditions. The two most common link functions that both fulfill the conditions are the linear link function given by ®(x) = max{0,min{1,1/2- (1+ 2)}} and the logistic link function given by 1 (x) = 1+ exp(—2) * Both are cumulative distribution functions: The linear link function is the cumulative distribution function of a continuous uniform distribution on [0,1]. In contrast, the logistic link function is the cumulative distribution function of a logistic distribution with location parameter 0 and scale parameter Two-Staged Choice Model Bobu et al. (2020b) propose the Limiting Errors due to Similar Selections (LESS) model that is inspired by the attribute rule model suggested by Gul et al. (2014). It assumes a feature map for trajectories and a (similarity) function mapping trajectory features and trajectories to integers and uses a two-stage process for modeling the human feedback (or choice): First, choosing a trajectory feature according to the Boltzmann distribution and then a trajectory with the (logarithmic) similarity functions as the utilities within the Boltzmann distribution. Their experiments show that this model can capture human feedback more appropriately than the standard Boltzmann distribution. Generative Model Abramson et al. (2022) evaluate the usage of a generative model for learning from human preferences. More specifically, instead of assuming some underlying utility as in the Bradley-Terry model, they attempt to train a model to generate the human feedback (inter-temporal preferences in this case, see Section 3.2.1) and directly interpret this feedback as reward. However, they found that this empirically does not perform as well as the inter-temporal Bradley-Terry model. 34 5.1.5 Misspecification The human feedback model may be misspecified in various ways. Milli & Dragan (2020) investigate the roblem of misspecifying the nature of human feedback that can be either literal or pedagogical. The former means that the human gives targeted feedback for solving the actual RL problem, while the latter means hat the human gives targeted feedback that is deemed helpful for the learner. They show theoretically and empirically that the case of a learner assuming a pedagogical feedback with an actual literal human always erforms worse than the reversed case, i.e., a learner assuming a literal feedback with an actual pedagogical human. A related question is studied by Freedman et al. (2021), namely, what if the learner makes incorrect assump- ions about the choices from which the human selects its feedback? They consider different types of such choice set misspecification and show that depending on the type of misspecification, the performances might vary drastically, even leading to no losses at all in some specific cases. n the field of inverse RL, the general question of the robustness of reward learning in terms of a misspec- ified human feedback model is theoretically investigated by Skalse & Abate (2023). It turns out that the optimality model is not robust with respect to any misspecification, the Boltzmann model is robust for quite a range of specific misspecifications, and the degree of robustness of the maximal causal entropy model lies etween the latter two. Even though these results are primarily derived for inverse RL, they also have similar immediate implications for RLHF. 5.1.6 Diverse Preferences One potential issue with the RLHF framework is that it does not specify whose preferences to align to. It is common to request feedback from multiple labelers in a crowd-sourcing setting, in which case the different labelers may disagree. There are two main ways to deal with this challenge: Either trying to learn each labeler’s preference separately, or trying to learn a model of the group’s mean preference. Bakker et al. (2022) investigate the first option by proposing to learn multiple reward functions, which can then be aggregated in arbitrary manners and even be utilized to find consensus among people with different preferences. The second is more commonly used, however: Xue et al. (2023a) learn a single reward ‘unction from multiple humans who may give diverse and inconsistent feedback, aiming to stabilize learning in spite of these inconsistencies using regularization, a consistency constraint, and ensembling. Similarly, Chhan et al. (2024) try to estimate the correct preference for pairwise trajectories directly by combining he users’ expressed preference labels instead of assuming individual reward functions. As a middle-ground etween the single reward function and multiple ones, Myers et al. (2021) propose to learn a multimodal reward function that captures multiple people’s preferences and use a mixture model of Plackett-Luce models ‘0 represent the feedback more accurately. With a stronger focus on the active retrieval of human feedback, Freedman et al. (2023) model the problem of selecting a suitable human labeler as a variant of the multi-armed bandit problem (Lattimore & Szepesvari, 2020), which they call hidden-utility bandit. In this variant, the agent has in each decision round the choice between two options: (i) drawing a bandit arm, then receiving a hidden arm-dependent utility, and finally observing an item, or (ii) querying a human to observe a preference between two sampled items and incurring a human-specific query cost. The feedback mechanism of all human teachers is modeled via a same Boltzmann distribution, differing only in their known individual rationality coefficients. The same modeling of human feedback is also considered by Barnett et al. (2023), who, however, use a Bayesian approach to determine which person should be queried in order to obtain the most informative feedback in expectation. aniels-KochDANIELS-KOCH & Freedman (2022) investigate the rationality coefficient already considered in the previously mentioned work and model it as a query-dependent function that might differ for the human labelers. 5.1.7 Relaxation of the Markov Assumption Most works assume that the human feedback is given based on a latent Markovian reward model, ice., the return of a trajectory tT decomposes into a sum of independent rewards over state-action pairs (see (1)). Early et al. (2022) relax this assumption by dropping the need for the Markov property, such that the instan- taneous reward might depend on hidden states. Similarly, Kim et al. (2023) avoid the Markov assumption by utilizing a transformer as the preference model. A similar effect may be achieved by learning a state representation with a recurrent network in which the rewards are Markov, similar to the approach taken by Hafner et al. (2023), but we are not aware of any work exploring this. Abramson et al. (2022) work in a non-Markovian setting as well by utilizing memory-augmented networks for both the policy and the reward model. 5.2 Utility Learning After choosing a human model to relate feedback to utilities, we can use the observed feedback to recover the latent utilities. This utility learning can be reduced to a standard supervised learning problem and, therefore, is commonly solved with the techniques of empirical risk minimization or Bayesian approaches, both of which will be discussed in the following. 5.2.1 Empirical Risk Minimization The most prevalent variant for learning the reward function, already been presented in Section 2.3, is a special case of empirical risk minimization. The general approach of empirical risk minimization for reward function learning, assuming an underlying human feedback model with utilities as in (9) is to find the minimizer of £(R:D) => €(w(R)- ti, m(qy)) (12) where D = {(l;, awn, is the given data set of observed label and query pairs, f: R x Q x C is a suitable loss function with Q being the set of all possible label sets, and u.(R) denoting the utility (depending on the return R) of the possible labels for the given label-query pair 1;,m(q;). As an illustration, consider the common case of pairwise trajectory comparisons where queries are pairs of trajectories q; = {ri rit, and labels l; € {7} > 73,7} < 73} = m(qi) are the human’s preference over the two trajectories. For a given query qi, we then obtain (2) as a special case of (12) by using the loss function: (u.(R), li, m(qi)) 1 ~ [es (; + exp(Um(q:)\\\\: (R) _ Tm) 1 ~ es (; + exp(E,nG(m(q:)\\\\t) R()] — =n) , where in the case of pairwise comparison, u.(R) = (wi;(R), Um(q,)\\\\1,(R)) and the grounding function G is the projection onto the preferred trajectory. This is the negative log-likelihood for the Boltzmann distribution for the observational pair (1;, m(qi)). For the entire learning process, a model class R is assumed for the reward function R. This model class is usu- ally a parameterized class of functions, such as, for example, the class of linear reward functions R= {Ry(s,a) = ¥d(s,a) | (s,a) €S x AW ERY, where @ : S x A > R?@ is some state-action feature mapping. This entails that good features are known in advance such that rewards can be expressed as a linear combination of those features. Using such linear models may lead to reward model misspecification. Studying this setting, Bobu et al. (2020a) propose to adapt the hyperparameter 6 in (3) to account for this issue. Since the assumption of a linear reward model may be too strong in practice, most recent work is based on non-linear models, especially using differentiable models, but other cases have been investigated as well. In the latter case, for instance, decision trees have been considered to learn an interpretable reward model (see Section 5.2.4). In the former case, simple multilayer perceptron (MLP) has naturally been considered, but more recent deep learning architectures are more commonly used in the recent literature. Thus, especially with partially observable domains, the reward network may be composed of a state-action encoder followed 36 by fully connected layers. For instance, Abramson et al. (2022) combine ResNet blocks for image processing, a learnable embedding table, a multi-modal transformer, LSTMs, and MLPs. Besides, Kim et al. (2023) utilize a Transformer-based architecture , motivated by the observation that rewards are often non-Markovian. In addition to the usual empirical risk in (12), it is also typical, as in supervised ML, to add a regularization function to prevent overfitting: N L(Ry;D) = S° e(w, (Ray). m(qi)) + AvP), (13) i=l where \\\\,.: V > Rx is a regularization function defined on the parameter space V of the underlying reward model class. For instance, Christiano et al. (2017) simply use L2 regularization and also consider dropout in some domains. Recently, Verma & Metcalf (2024) propose to define a more complex regularization term, which consists in biasing the learned rewards to be proportional to an approximate state importance provided by a trained Transformer-based forward model. The main supervised loss to train the reward model can also be augmented with additional losses corre- sponding to auxiliary tasks to avoid overfitting and improve generalizability. For instance, Abramson et al. (2022) use a behavior cloning loss and add a policy head to the reward network, thereby preventing that the reward model drifts too far from its initialization from the policy. Metcalf et al. (2023) design a reward model using state-action representations trained to be temporally consistent via self-supervised learning. On a related note, the scalar preference optimization problem has been extended to a multidimensional one by Zhong et al. (2024) to represent diverse human preferences and Marta et al. (2023) for query efficiency. 5.2.2 Bayesian Approach As is generally the case in supervised ML, there is also the variant of using Bayesian modeling for learning a target object instead of the (empirical) minimization of a loss function. To this end, one starts with a prior distribution p over the parameter space of the reward function that is updated in light of the data set D by means of Bayes theorem: P(b|D) x Ly(D) - oh), where Ly(D) = [][, P(t; is provided | m(q;), %) is the likelihood of the data under the assumed human feedback model with reward function Ry. Such an approach is used for pairwise trajectory comparisons, for instance, by Schoenauer et al. (2014) for the noisy-ridge model or by Sadigh et al. (2017) for the Boltzmann distribution as the human feedback model. In inverse RL, such Bayesian approaches have been considered as well (see Section 4.3 in Arora & Doshi (2021)). Instead of assuming that the reward functions are parameterized, one can use the reward functions directly as a parameter class and use a prior distribution over them. This could, for example, be a Gaussian process as initially considered by Kupcsik et al. (2018) for pairwise trajectory comparisons and adapted in later works (Buyik et al., 2020; Cosner et al., 2022). Here, again, it is worth mentioning that such considerations have also been made in inverse RL before (see Section 4.3 in Arora & Doshi (2021)). 5.2.3 Partial Identifiability A crucial question when it comes to learning the reward function is whether the reward function can be identified at all. If two reward functions induce exactly the same human feedback model, the reward func- tion is called partially identifiable or ambiguous. Skalse et al. (2023) study this topic for the Boltzmann distribution as the underlying human feedback model when demonstrations (inverse RL) or pairwise tra- jectory preferences are given as feedback. For demonstrations, this question has also been examined in other works (Ng & Russell, 2000; Dvijotham & Todorov, 2010; Kim et al., 2021; Cao et al., 2021a). Ona related note, Ellis et al. (2024) tackle this identifiability issue by considering suitable acquisition functions (see Section 4.1.1). 37 5.2.4 Interpretability The field of explainable artificial intelligence (XAI) has emerged in recent years to improve the transparency and explainability of models or even to enable them in the first place. Roughly speaking, the aim is to resort to more interpretable methods or provide explanations for both experts and non-experts, shedding light on why a certain input in a (black box) model leads to a certain result. Explanations can take different forms, as can he ways to ensure the transparency of models, and for a detailed overview, we refer to Barredo Arrieta et al. (2020). It is worth noting that the field has grown so extensively over the years that even dedicated overviews ‘or the field of interpretable and explainable RL are by now available (Puiutta & Veith, 2020; Glanois et al., 2022; Qing et al., 2023; Milani et al., 2023). For the branch of RLHF, the existing works are quite sparse and mostly limited to using tree models as ransparent and explainable models for learning the reward function (Bewley & Lécué, 2022; Bewley et al., 2022; Kalra & Brown, 2022; Bewley et al., 2024; Kalra & Brown, 2023). Another way to realize explainabil- ity within RLHF suggested by Zhang & Kashima (2023) is to learn simultaneously the reward function and he importance of states using a weight network. Assuming that for (long) trajectories, only a few states are important for the preference outcome, their framework can be used to select samples for explainability urposes. Moreover, a perturbation analysis is suggested to evaluate explanations in a quantitative manner using the learned state importance weights. 5.2.5 Online Improvements Christiano et al. (2017) demonstrate that it is important to improve the reward model online, a finding that has been confirmed by subsequent works such as the one by Gao et al. (2023), which empirically demonstrates that overoptimization of a reward model trained offline leads to performance degradation. Without online improvements, issues of overoptimization of an imperfect reward model may occur. Abramson et al. (2022) give an example of this: They attempt to fine-tune an agent initialized with behavioral cloning with an engineered reward function and find that it fails to generalize and actually worsens the performance. They also compare RLHF with a reward model trained offline with iterative improvement and find that iterative improvement leads to better performance, even sometimes exceeding human performance. This is related to issues posed by the approximate nature of the reward model in general, discussed in further detail in Section 6.1, but improving reward model accuracy, in general, is not sufficient: McKinney et al. (2022) further show the interdependence of the reward model and the policy, demonstrating that reward models trained online together with a policy may not be effective when a completely new policy is trained. Solutions to the problems of overoptimization and interdependence can take different forms: One is to update the reward model online with sufficient frequency using notably more on-policy queries (see Section 4.1.1), another is to improve the reward model, e.g., by leveraging ensembles or by modifying the training procedure to place additional emphasis on challenging examples , and a third, discussed in Section 6.1, is to add constraints to the policy training. 5.2.6 Learning from Multiple Feedback Types As discussed in Section 3.5, it is often desirable to combine several feedback types. This requires extensions of the learning process to incorporate different sources of feedback. Learning from multiple feedback types can be achieved by pre-processing the feedback, assuming common latent factors, or by using the feedback types for distinct purposes. The first approach is demonstrated by , who infer preferences from demonstrations, allowing them to treat both types of feedback equally in the learning pipeline. In the style of the sec- ond approach, Jeon et al. (2020) proposes the unified framework of reward-rational choice, which allows for interpreting many forms of human feedback as Boltzmann-rational choices and, through this common framework, enables combination and adaptive selection of feedback types. Finally, different types of feed- back can be used for entirely different purposes, such as one for objective learning and another for safety- constraints or for representation learning (see Section 3.2.3). 38 Since multiple sources of reward information may conflict, it is important to consider how to combine them. Krasheninnikov et al. (2021) study several possible strategies of combining several reward functions in this setting, relating it to multi-task inverse RL. Note that this challenge of conflicting sources of reward relates tightly to challenges posed when receiving diverse preferences from different labelers, as discussed in Section 5.1.5.2.7 Offline Reward Learning There is a recent trend towards offline RLHF, where both the reward model and the policy are trained offline. The offline setting is also frequently considered in RLHF theory (Section 7). Early approaches in this area (Kim et al., 2023; Shin et al., 2023) first generate queries from an offline dataset of behaviors, gather human responses, train a reward model from the resulting preference data, and then leverage offline RL algorithms to derive a policy. We do not cover these works in detail, since this survey primarily focuses on the interactive and online setting (see Section 1.3). Nonetheless, the offline setting is particularly useful for evaluating novel approaches, e.g., for active query selection, using offline datasets. We refer the interested readers to Section 8.4 for a discussion of available datasets. 5.3 Evaluating Learned Reward Functions A central question when it comes to learning the reward function is how to evaluate the learned reward function and how reward functions can be compared with each other. For this purpose, different approaches are available, e.g.: Rollout Method In inverse RL, a common method for evaluation is the rollout method (Ng et al., 1999; Fu et al., 2018a; Ibarz et al., 2018; Brown et al., 2019). In this approach, one first learns an optimal policy for the learned reward function and then estimates the value of this policy for online trajectory rollouts using the known ground-truth reward function. This approach can be transferred to RLHF as well. In many cases, however, especially in safety-critical areas such as medicine or autonomous driving, such online rollouts cannot be executed. Off-policy Evaluations When online rollouts are not possible, so-called off-policy evaluations, which esti- mate the value of the optimal policy on the basis of an available data set, may be considered. For coping with biases or large variances due to policy mismatch, approaches using importance sam- pling , regression- or classification-based methods (Paduraru, 2013; Le et al., 2019; Irpan et al., 2019), or combinations of these (Jiang & Li, 2016) have been proposed. The problem with these approaches, however, is that the traces of the explicit sources of error through policy learning or reward learning are blurred, and that they require access to the ground-truth rewards. Distance Functions Yet another alternative, which has been advanced in the seminal paper by Gleave et al. (2022a), is using a suitable distance function for reward functions. Suitable here means that two reward functions, which differ only by certain transformations such as potential shaping (Ng & Russell, 2000) or positive scaling, should have zero distance if these transformations do not change the policy ranking with regard to the expected return. For this purpose, Gleave et al. (2022a) present a pseudometric, called Equivalent-Policy Invariant Comparison (EPIC) distance, that is determined in three steps: First, mapping two reward functions to a so-called canonical- ization form that is invariant to transformations of the latter kind. Second, normalizing these canonicalization forms by means of a specific weighted L2-norm whose weights are determined by a distribution over the transitions. Finally, the EPIC distance is the weighted L2-norm distance of the normalized canonicalization forms. Even if some attractive properties, above all a Lipschitz continuity in terms of the EPIC distance of two reward functions for the difference of the value functions of the induced optimal policies is shown, this distance has its shortcomings. One of these is that the canonicalization form used by EPIC distance does not encode sufficient knowledge about the underlying transition dynamics, which might lead to unreliable distances when evaluating reward functions on physically non-realizable 39 transitions. To this end, Wulfe et al. (2022) propose the Dynamics-Aware Reward Distance (DARD), which uses a slightly different form of canonicalization but restricts the evaluation of the reward ‘unctions to transitions that are approximately physically feasible. unctional parameters fulfill certain requirements, then the result properties, e.g., being a pseudometric that is zero if and only if particular, these metrics retain the flexibility of DARD (in terms o Visual and Human Inspection For an evaluation by visual inspection, . Recently, EPIC-like distances and STAndardised Reward Comparison (STARC) metrics , which are entire classes of pseudometrics on the space of all reward unctions were proposed that generalize the three-step approach underlying the EPIC distance (and DARD) by parameterizing each of the steps. Specifically, the canonicalization function in the first step, the normalization in the second, and the metric in the third step are kept variable. If these three ing distance has some appealing he two reward functions induce the same ordering of policies or imply upper and lower bounds on value function differences. In specifying transition dynamics), while at the same time preserving the theoretical justification of EPIC. enner & Gleave (2021) propose a method for preprocessing reward functions by transforming them into simpler but equivalent reward unctions for better interpretability. Related to this and the ro reward function learned can also be evaluated by a human (or ex of the agent on the target task. Besides, in the context of LLM lout method, the quality of the pert) by examining the behavior Is, datasets have been proposed and specifically designed to evaluate the (in)consistency of learned reward models with respect to semantic changes of prompts . 5.4 Reward Model Inputs Besides the feedback type, another factor is the modality of the reward consists of the agent’s observations and actions. Observations can range from true state to high dimensional inputs (e.g., images), while actions can range from discrete finite actions to continuous actions. For instance, many typical RL benchmarks are in the continuous control domain (e.g., MuJoCo simulate robotics tasks) with true state representations and simple discrete actions. In such problems, Christiano et al. (2017) train reward models from these inputs. When no compact state representation is available, raw images are often used in control tasks, which makes the learning of rewards more challenging since the setting becomes partially observable and the reward function is generally not Markov with respect to the observations. In sv of approximating a true state with a sequence of frames is often employed. This approach is used, for instance, by Christiano et al. (2017) to train reward models on the Atari only observations as inputs, one can resort to recurrent models or Transformer-base models . More recently, many applications of RLHF are in the natural language processing (NLP) domain. In these settings, the policy takes natural language as both input and output while the reward model takes it as inpw model input data. This usually uch cases, the conventional trick benchmark suite. When taking (see, e.g., the work by Ouyang et al. (2022)). Naturally, more complex scenarios (e.g., with both language and vision inputs ) have also been studied. 5.5 Increasing Feedback Efficiency Maximizing feedback efficiency is vital in RLHF due to the high cost of human feedback. This section delves into methods that enhance learning from limited human feedback. We discuss methods that leverage prior offline data, methods that use (partially unlabelled) data more efficiently, more informative data. 5.5.1 Using Prior Data and methods that aim to gather There are often large amounts of prior data available at little or no additional cost. While this data generally was generated for other tasks, many basic human preferences are the same for various tasks and can often 40 even be extracted from completely unsupervised data such as text corpora. By leveraging this prior data, we can greatly reduce the amount of feedback necessary to learn the current task’s objective. We explore various methods, including meta- and transfer learning, leveraging foundation models, reward model initialization, preference model pretaining, and supervised representation learning. Meta- and Transfer Learning Meta- and transfer learning techniques in reward model training exploit the commonalities across different objectives, facilitating quick adaptation to new tasks. Ren et al. (2022) develop a broadly applicable meta-reward model, pre-trained on a diverse set of tasks to capture a wide range of preference patterns, enabling efficient adaptation to new tasks with fewer examples. Xie et al. (2018) use a similar meta-learning approach to build a goal classifier across multiple visuomotor tasks. Closely related to these meta-learning approaches, Hejna & Sadigh (2022) integrate few-shot learning principles, optimizing their approach for scenarios where only a few examples are available for adapting to new tasks. In the domain of transfer learning, Liu et al. (2023a) explore zero-shot transfer of preferences, a method that enables adapting preferences without additional data from the new task. In a different vein, but closely related to meta- and transfer learning, Mendez et al. (2018) tackle the lifelong inverse RL problem, focusing on inferring reward functions for multiple tasks over time, which involves knowledge transfer between tasks. Collectively, these studies underscore the potential of meta- and transfer learning in enhancing the efficiency and applicability of reward models in RLHF. Leveraging Foundation Models Foundation models, i.e., large models trained on large amounts of often unlabeled data, can acquire significant knowledge about basic human preferences. A language model trained to predict the next token in a text corpus, for example, may learn to complete the sentence ‘Frank was mad that his vacuum robot broke the vase,’ thereby learning that humans prefer non-destructive behavior. These learned preferences can then be leveraged in RLHF approaches. For instance, Kwon et al. (2023) propose to use LLM as a source of rewards. Du et al. (2023) is another example, where a success detector is trained using a pre-trained vision-language model (Flamingo). Their approach utilizes a data set of trajectories with binary success labels, employing a non-interactive training method. Reward Model Initialization It is often beneficial to initialize the reward model with parameters from a model trained on a related task. This strategy is particularly common in language model fine-tuning, where self-supervised pretraining is a common practice. In such scenarios, it becomes logical to use these pre-trained models for initializing not just the policy but also the reward model. This methodology is adopted by Askell et al. (2021) and Ouyang et al. (2022). Specifically, Ouyang et al. (2022) use a pretrained language model for the reward model, opting for a smaller model relative to the policy to mitigate unstable learning. Notably, while they apply supervised fine-tuning to the policy before the RLHF phase, the reward model is initialized directly from the language model without any preliminary fine-tuning. This approach’s applicability extends beyond language models to other areas. A notable example is Abramson et al. (2022), who, in the control domain, begin by training a policy through contrastive self-supervised learning and ehavioral cloning. They then add an MLP head to the policy for the prediction of cumulative rewards. Reward Model Pretraining Reward model pretraining (Askell et al., 2021; Bai et al., 2022a) leverages rior offline data to pretrain the preference model before training it on policy samples. Askell et al. (2021) note that in the case of language models, noisy preference data can be readily obtained from sources such as rated Reddit comments, preferred Stack Overflow answers, and reverted Wikipedia edits. They leverage this as a pretraining step to increase data efficiency. This is in addition to regular language model pretraining, as discussed in the previous paragraph. A similar approach could be applied to control in case prior data and some means of inferring preferences, such as human corrections, are available. Even if no inferred preferences are available, Verma & Kambhampati (2023a) show that it can be beneficial to pre-train the preference model to predict close to constant reward on an initial set of trajectories. This avoids excessive fitting of the policy to random initialization differences in the reward function. Supervised Representation Learning A compact representation that captures all relevant information for human preferences while minimizing noise can greatly enhance preference learning efficiency. It may also generalize better than a representation learned end-to-end as part of the preference learning task, which 41 may contain spurious correlations. Bobu et al. (2022) address this by proposing the learning of features hrough explicit human feedback using feature traces. Feature traces (see Section 3.2.3) involve human labelers explicitly teaching relevant features one by one by demonstrating behavior in which the feature monotonically increases or decreases. This method directly aligns the learned representation with human- identified features, enhancing preference learning efficiency but requiring detailed human input. However, eature traces require labelers to be able to identify and articulate relevant features, which can be challenging. Bobu et al. (2023) offer an alternative approach with their Similarity-based Implicit Representation Learn- ing (SIRL) method. SIRL learns representations from similarity queries (see Section 3.2.3), where human labelers provide feedback on whether behaviors are similar or different concerning the features that matter o them. This method captures a broader range of human notions of similarity without needing explicit eature knowledge, thus reducing the cognitive load on human labelers. In summary, while both approaches emphasize human feedback’s centrality in representation learning, they differ in their methods of gathering his feedback. The feature traces used by Bobu et al. (2022) require specific feature knowledge, whereas SIRL used by Bobu et al. (2023) utilizes more intuitive similarity assessments, potentially offering a more user-friendly way to capture human preferences. These diverse methods of utilizing prior data demonstrate the potential for enhancing data efficiency in RLHF, enabling more effective learning from limited human feedback. 5.5.2 Using Data More Efficiently Beyond the application of prior data, several techniques can enhance the efficiency of data utilization in training processes. This section will discuss a range of such methods, including self-supervised and semi- supervised training, as well as the integration of inductive biases and data augmentation strategies. These approaches are designed to make the most of the available human interactions and improve the final perfor- mance of RLHF models. Self-Supervised Auxiliary Tasks Self-supervised training enhances data efficiency in reward model training by using unannotated data to capture information about the task. This technique extends beyond the scope of pretraining methods, as discussed in the prior section, to incorporating concurrent auxiliary tasks to maximize the utility of available data. A prevalent technique, as applied by Abramson et al. (2022), Brown et al. (2020), and Metcalf et al. (2023), involves adding self-supervised losses to enhance represen- tation learning for rewards. Abramson et al. (2022) implement a contrastive task where the reward net- work differentiates between observations that are consistent between multiple modalities and those that are not, blending this with preference learning loss and behavioral cloning. Brown et al. (2020) add mul- tiple auxiliary tasks such as inverse and forward dynamics modeling, temporal distance prediction, and variational autoencoder training. Similarly, Metcalf et al. (2023) use the self-predictive representations tech- nique to learn state representations that encode environmental dynamics, enabling a linear model to anticipate successor states, thereby forming an efficient basis for preference learning and significantly boosting sample efficiency. However, auxiliary losses for better representation learning are not the only approach to leverage self-supervised training. An alternate approach by Verma & Metcalf (2024) involves identifying important states using attention weights from a world model transformer and state im- portance estimates based on a preference predicting transformer. These estimates can aid credit assignment for observed preferences, further optimizing the training process. Semi-Supervised Training Semi-supervised training, blending labeled and unlabeled data, can leverage the unlabeled data to glean information about the task and the environment. This is most commonly done by generating pseudo-labels for the unlabeled data, either by leveraging model predictions or by making assumptions. The first approach is utilized by Cao et al. (2021b) and Zhan et al. (2021), which use generative models and GAN-based methods to mimic human preferences. Similarly, Park et al. (2022) expand their data set with high-confidence unlabeled samples based on the preference predictor’s evaluations. The second strategy, making assumptions to augment data, is showcased by Zhou & Xu (2020). They generate preference data by assuming that (i) human-written examples are better than model-written examples, (ii) human- written and model-written examples are indistinguishable amongst themselves, and (iii) generations of later model iterations are better than those of earlier ones. 42 Data Augmentation Data augmentation focuses on creating additional examples from existing labeled data. Temporal augmentation is particularly effective in RLHF, involving trajectory data. This is exemplified y Brown et al. (2019) and Park et al. (2022) who base their augmentation on the premise that preferences ‘or complete trajectories can be extrapolated to cropped segments, allowing the generation of multiple deriva- tive pairs from a single labeled trajectory pair. Park et al. (2022) additionally explore state modifications, such as random re-scaling and Gaussian noise addition, finding temporal cropping to be the most effective, with noise sometimes negatively impacting performance. In a similar vein, Verma & Kambhampati (2023b) ‘ocus on augmenting trajectories by concentrating on changing elements in observations and perturbing the other parts, based on the premise that movement indicates importance in image-based observations. Com- plementing these methods, Abramson et al. (2022) employ augmentation by randomly altering instructions and language responses, thus creating artificial examples of non-preferred behavior. These diverse data aug- mentation methods collectively enhance the training data set, contributing to the increased robustness and efficacy of RLHF models. Relatedly, Meta-Reward-Net optimizes not only for the preference prediction accuracy of he learned reward function but also of the learned Q function in an actor-critic RL algorithm. This is beneficial since it avoids the phenomenon of confirmation bias, where one learned model (in this case the Q function) overfits to targets predicted by another model (the reward model). It is not strictly a data augmentation technique, but closely related in practice. 5.5.3 Gathering Better Data n addition to leveraging unlabeled data and using labels more efficiently, sample efficiency can be further increased by collecting more informative samples in the first place. This can either be achieved by selecting more informative samples from the experience buffer or by generating more informative experiences. While selecting informative samples from the experience buffer is addressed under active learning (see Section 4.1.1), his section focuses on generating more informative experiences. While we are not aware of many works in this area, one possible approach involves steering the agent’s exploration towards regions of the state space where human feedback would be most beneficial. Liang et al. 2022) implement this by employing intrinsic motivation, driven by the estimated uncertainty of the reward model, to guide the agent’s exploration. This highlights the potential of not just using data more efficiently ut also generating data in a more targeted manner. 6 Policy Learning Agent Environment Dynamics Action at After learning a reward model, or, more commonly, interleaved with reward model learning, the next step is to train a policy that maximizes the expected accumulated reward. This section will algorithms for policy learning, which can be categorized into two main techniques: adaptation of conventional RL algorithms and direct policy optimization (DPO). 6.1 Adaptation of RL Algorithms Using the learned reward model, any standard RL algorithm (e.g., DQN, A3C, PPO, SAC) could potentially be applied to train a policy. However, in the setting of RLHF, this direct application may suffer from two 43 issues: The non-stationary nature of the learned reward function in RLHF and the inaccuracy of intermediate reward models. We will discuss these issues and possible adaptations of RL algorithms to address them in the following. Non-Stationary Rewards RL algorithms are designed to learn a policy that maximizes the expected accumulated reward in an MDP framework, which assumes a stationary reward function. The RLHF setting violates that assumption by periodically updating the reward model, leading to a non-stationary reward ‘unction. Various works have empirically demonstrated that conventional RL algorithms can be applied nonetheless, with little to no modification. Christiano et al. (2017) argue that policy-gradient methods are better suited for non-stationary reward functions compared to value-based methods. They and various follow-up works successfully apply policy-gradient methods without any modifications in this setting. This approach has been picked up for language-model fine-tuning as well . Later works have shown that value-based methods (possibly in an actor-critic scheme) can also be effective in RLHF (Ibarz et al., 2018; Lee et al., 2021b; Park et al., 2022; Liu et al., 2022; Xue et al., 2023b). One rick to make value-based methods work is to use the reward model to relabel the experiences in the replay uffer whenever it is updated (Ibarz et al., 2018; Lee et al., 2021b). Similar to conventional RL, the use of such a replay buffer can greatly decrease the amount of environment interactions necessary for successful learning. As demonstrated by Gulcehre et al. (2023), the sample efficiency can be increased even further by using offline-RL techniques in a growing-batch RL setting, an offline-RL technique that iteratively increases he size of the dataset by policy rollouts while still being more sample-efficient than online RL. n addition to the basic RL approaches, there are also some policy learning approaches tailored specifically for RLHF. Wu et al. (2023) propose a policy gradient algorithm, called Pairwise Proximal Policy Optimization (P30), as an alternative to PPO, which avoids estimating the value function and at the same time is provably invariant with respect to equivalent rewards (unlike PPO). In a similar vein, Zhu et al. (2023b) replace the KL-regularization of PPO by means of a squared error term of the logarithmic probabilities resulting in a seemingly more stable RL learner. Overoptimization of Approximate Rewards Since the learned reward model, which is only an ap- roximation of the true reward function, is used to train a policy, overoptimization or reward hacking can happen. Section 5.2.5 discusses the interdependence of the reward model and the policy in more detail as well as possible improvements from the reward model side, while here we focus on how to adapt policy training to cope with possibly inaccurate rewards in general. One approach is to regularize the policy so as not to diverge too much from human-given demonstrations. This is particularly common for language-model fine-tuning (Ouyang et al., 2022; Abramson et al., 2022), but Abramson et al. (2022) explores this for control as well. They found that this was important for some cases, in particular for deciding when to output language, but not for all. Going beyond KL-regularization, Moskovitz et al. (2024) investigate several techniques of constrained RL to only maximize rewards up to a threshold while avoiding excessive deviation from a pre-trained policy. 6.2 Framing RLHF for Generative Models as a Bandit Problem So far, we have assumed that we ultimately want to solve a reinforcement learning problem represented by an MDP. However, especially with regard to the application of RLHF for the area of LLMs, there is now another simplified way of looking at the problem. Namely, as an instantiation of a (contextual) preference- based bandits problem , which can of course be modeled by the more general case of a Markoy decision process (MDP). In both cases, the focus is on the concept of tokens or rather sequences of tokens. However, in the MDP point of view, the state space S consists of all previous tokens and the prompt (represented as a sequence of tokens), while the action space A consists of all potential next tokens. A terminal state is often indicated here by the special token <eos> and trajectories are filled with this token until the maximum length H is reached. Moreover, the transition function P is degenerated (or deterministic) 44 with being one only for the state that is the concatenation of the current state and the taken action. A (latent) reward is only received at the end of the trajectory giving rise to a sparse feedback scenario. The (contextual) preference-based bandits view, on the other hand, naturally considers no state space and no transition function, but an action space consisting of all possible responses to a prompt (both represented as a sequence of tokens). Here, the prompt specifies the context for which at least two actions are executed and for which a qualitative comparison is observed as feedback. In bandit literature, this is also referred to as a “(multi-)duel”, coining the term dueling bandits. Thus, this point of view takes a trajectory-wise perspective, while the MDP point of view takes a token-wise perspective. Note that the bandit formulation considers an entire episode (response in the LLM context) as an action with a single associated reward, resulting in sparse feedback. This is in contrast to the standard RLHF formulation as it is often used in control settings, where it is assumed that the reward of a trajectory is composed of the sum of the rewards of individual steps, which allows the optimizer to distribute rewards densely as best fits the data. On an intuitive level, this leads to state-action pairs that often occur in preferred trajectories to be highly rewarded, without necessarily putting all reward on the terminal actions. In practice, this can lead to nicely-shaped reward functions , which cannot directly be achieved in the bandit setting. However, Chan et al. (2024) show how to take advantage of the predominantly used transformer architecture for the reward model in order to obtain a denser reward, even when assuming the bandit setting: More specifically, since the transformer architecture maintains attention weights in the last layer for each token, these can be used to attribute the overall reward signal to individual tokens. The main appeal of the bandit formulation is that it does not require exploration of the environment’s dynamics, since they are deterministic. It therefore enables simpler policy learning approaches, such as DPO or VPO , discussed in the following section. 6.3 Direct Policy Optimization The two-phase approach involving utility learning and policy optimization is not the only viable path to earning a policy from human feedback. While we have previously discussed the case in which we learn a reward function from observed preferences by assuming a human feedback model, an emerging branch of the iterature is concerned with circumventing the reward-learning step and using preferences directly to address he actual RL problem. Concrete approaches are DPO , SLiC-HF , OPPO , DPPO , PRO , RSO (Liu et al., 2024b), or by formulating policy search as a zeroth-order optimization . Azar et al. (2023) introduce an objective called U-preference optimization (YPO) that unifies the objective functions in DPO and RLHF. More specifically, for a specific instantiation of Y, the objective in YPO recovers DPO and SLiC-HF. In addition, DPO has been further generalized to include diverse divergence constraints (Wang et al., 2024a). Besides, Hejna et al. (2024) propose contrastive preference learning based on a regret preference model instead of the usual one in RLHF. It is also possible to learn a Q function from human preferences directly, which implies a policy without the need for separate policy- and reward-model training . t is worth noting that approaches for directly learning the policy from preferences have been considered in the ast as well (Wilson et al., 2012; Fiirnkranz et al., 2012; Wirth & Fiirnkranz, 2013b;a; Busa-Fekete et al., 2014). In Sections 3.2.1 and 3.2.2 in the survey by Wirth et al. (2017), these methods are explained in more detail. Another recent trend in fine-tuning models with human feedback is to even manage it without the usage of RL. An alternative is based on supervised reward learning with new types of loss functions (Lee et al., 2023; Yuan et al., 2023) or a specific learning process (Dong et al., 2023; Korbak et al., 2023). There are also RL-free approaches that do not use a reward model to train a policy to execute natural-language instructions using a transformer architecture (Brohan et al., 2023; Yu et al., 2023). On a related note, Liu et al. (2024a) suggest a way how to convert human feedback to natural language sentences for the task of fine-tuning language models. 7 Theory The field of RLHF has recently made some progress in terms of theoretical results, which we will discuss in this section. First, we consider the contributions where the goal is to learn a provable (near) optimal policy both in an online and offline fashion or even in a way that falls in between. Then, we discuss and highlight recent contributions related to different theoretical aspects of RLHF, such as its relation to the standard reward-based RL. Tables 6 and 7 provide a concise overview of the results for the online or offline policy learning setting. Here, N¢(e,d) denotes the e-covering number of a set F under some metric d*. It is worth mentioning that (almost) all works have two standard assumptions, namely that the reward function is bounded and that the ground-truth reward, human feedback model, or transition dynamic is an element of the considered model space, respectively. 7.1 Policy Learning In the literature focusing on theoretical results, there is a distinction (similar to the distinction made in standard RL) between an offline and online setting. In the former, learning is based on a given fixed data set, usually previously collected through an interaction with the environment. In contrast, in the online environment, one interacts directly with the environment to learn from real-time feedback and continuously updates one’s strategies based on the feedback received, allowing the agent to learn and adapt as it engages with the environment. Accordingly, an important component of the online variant is the sampling procedure, ie., how the labels are selected. This is usually accomplished using an acquisition function that is based on uncertainty (see Section 4.1.1). Online Learning The first work dealing with the question of theoretical guarantees for learning an optimal policy from trajectory comparison feedback (see Section 3.2) in an online manner is by Novoseller et al. (2020). It laid the foundation for a paradigm subsequently embraced by many subsequent research endeavors: Adapting learning algorithms from the dueling or preference-based bandit literature (Yue & Joachims, 2009; Sui et al., 2018; Bengs et al., 2021) to the underlying situation with additional states. The preference based bandit problem can be viewed as a preference-based RL problem with one state, so state transition dynamics must be considered accordingly for a fruitful adaptation. It is worth mentioning that Jain et al. (2015) used a quite similar idea before for feedback in the form of corrections (see Section 3.2) by resorting to the coactive learning setting (Shivaswamy & Joachims, 2012). Assuming the existence of a ground-truth context-trajectory scoring function and that the user’s feedback is informative, the Preference Perceptron algorithm by Shivaswamy & Joachims (2012) is used and analyzed in terms of its cumulative regret. Novoseller et al. (2020) suggest the Dueling Posterior Sampling (DPS), which is an adaptation of the self- sparring algorithm . It takes a Bayesian perspective on the problem and defines a Dirichlet rior on the dynamics and a Gaussian prior on the rewards that are subsequently updated, while the rajectories to be compared by the human labeler are chosen based on their (posterior) probability of being optimal’. Assuming a linear link function (see Section 5.1.4) as well as a tabular MDP, it is shown that DPS is (i) consistent, i.e., converges in distribution to the optimal policy, and (ii) achieves an asymptotic expected regret bound (see Table 6). Xu et al. (2020) combine dynamic programming and policy search with a black-box preference-based bandit gorithm for each state to design routines that return an almost optimal (a.k.a. e-optimal) policy with high robability®. The first routine, called Preference-based Policy Search (PPS), requires access to a simulator, while the second routine, called Preference-based Exploration and Policy Search (PEPS), gets rid of this requirement by exploring the state space by means of an auxiliary synthetic reward function. By assuming hat the probability of one policy dominating another policy is bounded uniformly over all states from below y a multiplicative of their value function, they show generic upper bounds for both routines on the number 2 4The e-covering number is the minimum integer N such that there exists a subset F’ C F with |F’| = N, and for any f € F, there exists some f’ € F’ satisfying d(f, f’) < «. 5The latter probability is assessed by posterior sampling; a commonly used technique in the bandit literature used by so-called Thompson Sampling strategies, see Lattimore & Szepesvari (2020) for more details. 6This is a so-called PAC learning setting in which the goal of finding the/an optimal object is relaxed to finding a “good enough” object, usually specified by some distance measure on the object domain. 46 Algorithm (Reference) Poste- Sampling Dueling rior (DPS) Algorithmic approach Leveraging Posterior Sampling from dueling bandits Assumptions Linear link function, tabular MDP Target(s) and goal(s) of learner Bayes regret mini- mization w.r.t. op- timal policy based on trajectory com- parison feedback Theoretical guarantee(s) Asymptotic regret rate: 0 (isi Viarreatan) Logistic Prefer- Leveraging MaxInP Logistic link function, Expected regret Transition dynamics: ence Reinforce- from contextual duel- tabular MDP, linear re- minimization w.r.t. ning (PbOP) proximation and general transition tion w.r.t. optimal +6 dy(F)T log (Ae (+ 4)) (Chen et al., dynamic class Fp with policy based on ap ‘the ¢-infinit: Well 2022) finite /2-norm p-Eluder trajectory compar- ® ” ~ dimension d?)(p) and __ ison feedback (2) iv d,-’(p), respectively Preference-based = Dynamic program- Uniform dependence (e,5)-PAC for op- Simulator step bound Policy Search ming, policy search, of policy prefer- _ timal policy based o HOt1|s|W(|Al,</H,5/|S|) (PPS) (€,6)-PAC black-box ence_ probabilities on on trajectory com- = dueling bandit algo- value function differ- parison feedback Sample complexity bound oO (Fst a) rithm and simulator ences, tabular MDP, (€, 6)-PAC dueling bandit algorithm with W(K,e,d)e~* sample complexity for K arms Preference-based Similar to PPS, instead Same as PPS and_ (e,6)-PAC for op- Step complexity bound Exploration & of simulator using an _ stochastic triangle timal policy based 3 (sea </H,5/|S\\\\) Policy Search auxiliary synthetic re- inequality of trajectory on trajectory com- em (PEPS) ward function comparisons prefer- parison feedback Comparison complexity bound ences ° (2S stalee sisi) UCBVI-Planning Optimistic least- Binary rewards for (e,6)-PAC for op- Tabular MDP: (Kong & Yang, squares value iteration, state-action pairs timal policy based 2022) maximum information gain, value iteration based on _ pessimistic expected value func- tion estimation based on human re- sponse model f € Fy with bounded noise A > 0, compliant and tabular/linear MDP with dimension d on binary state- action reward feed- back = tog( HSA) ° ss aisiAly H3|S|?|A| tog ( HSH AL ) ooo Linear MDP: 245 4 tog ( HUSIAL ° (= Pde, H ao) qo ~~ Preference-based Least-squares value it- General differentiable | Expected regret Expected regret bound: & Randomized eration with perturbed link function ®, linear — minimization w.r.t. O(erat/ + VP. a3H5/24 Least-Squares state-action-wise re- MDP, linear rewards optimal policy 4 an7/2 411/23) Value Iteration ward model with d-dimensional fea- | and/or low trajec- . . (PR-LSVI) ture embedding of tra- tory comparison Comparison complexity bound: (Wu & Sun, jectories feedback com- o( (e+ Bmax)\" /e ) 2024) plexity steered by “= infec [— Rmax,Rmax] 6! (2) €€ (0,1) Algorithm for Iterative bilevel opti- Lipschitz assumptions Solving the bilevel | Convergence rate: Policy Alignment in Reinforcement Learning (A- PARL) mization via gradient descent based on an es- timated policy gradient on the objective func- tion, the reward func- tion, the parametric policy class, and con- vexity assumptions on the value function 47 optimization prob- lem oa/T) of pairwise trajectory comparisons (see Table 6). If these dominance probabilities have even more structural properties, such as fulfilling stoc! (2020); Bengs et al. (2021) A follow-up work by Saha et al. (2023) assumes a feature embedding of embedding of policies an essentially viewing the policy em (see Section 5.1.4), confidence se maximum likelihood estimate (M. ea traj ng (LP variance are used to samp based reinforcement learni taking the uncertainty regarding cases, i.e., known or unknown dyn: In contrast to previous wor unknown human feedback dimension’ (Russo & Van (PbOP) algorithm, which essentia‘ adap s the MaxInP algorithm (Saha, 202 eddings as the contexts. More precis LE), and the two policies with the hig ectory, respectively, to be compared. RL) is derived and also extended ti model he human feedback mode reward mode! rajec ertur rajec sampli: analyz ories 0; ories is eit in Recent optimi (ie, ( erized by ex] essence, the regret term coming from that al and transi hey derive lower bounds for the regret of any learning algorithm by reducing the once-per-episode-feedback RL problem to K-wise comparisons, where one obtains all (f ) pairwise comparisons he dynamics into account when const amics, upper bounds on the regret o: l considers tabular MDPs, Chen et a and unknown dynamics each from Roy, 2013). They propose and analyze the Preference-based O ly follows a similar design as LPbRL but uses least-s ions dynamics along with confi the PbRL problem. Fin: he human feedback model class Wu & Sun (2024) consider a similar learning scenario as Saha et al. (2023) but with t o keep the number of queries of trajectory comparisons low, which is a combination of two competing objectives also studied in the bandit Preference-based and Randomized Least-Squares Value Iteration (PR-LSVI) algorithm, which combines least-squares value iteration with a per minimization; a similar idea to CoLSTIM suggested for contextual dueling bandits . More specifically, in each time step, the policy maximizing the value function of the pertur and the policy maximizing the later in the previous time steps are these two policies and computing their expected absolute reward difference (base bed state-action-based reward model) as a measure of uncertainty, preference queried if the uncertainty exceeds a certain threshold. Moreover, they a ng counterpart of this algorithm, the Preference-based Thompson Sampling (PbTS) algorit erms of Bayesian quantities. iterature . urbed state-action-based reward ly, Chakraborty et al. (2024) proposed a bilevel optimization problem that general zation problem for RLHF with trajectory feedback and the negative log-likelihood 2)). This problem, which they call PARL (Policy Alignment in Reinforcement Learning), is licitly taking into account the dependence on the data-collecting process a s for the expected scores of the policies are constructed based on t ally, they extend their ana hastic transitivity or stochastic triangle inequality (see Haddenhorst et al. ), then these upper bounds can be further refined. rajectories that gives rise to a feature ) for contextual dueling bandits by ely, assuming a logistic link function he hest uncertainty in terms of maximal In this way, the logistic preference- o the case of unknown dynamics by ructing the confidence sets. For both f LPbRL are shown (see Table 6). . (2022) consider the case of a general ‘unction classes with a finite Eluder timistic Planning uare estimates for ence sets based on them. Moreover, ysis to the case of ueried trajectories. In factor of VK. for AK many improves by a he additional objective For this purpose, they suggest the model with Gaussian noise for regret bed state-action-based “i ‘played’. By sampling on the eedback for these two so suggest a posterior hm, and tandard ‘unction izes the s as a loss charac- for the one leve optimal policy parameters at the other level. For this problem, A-PARL is proposed, which is shown to have an O(1/T) convergence rate under specific assumptions, where T is the number of iterations. Finally, for t dueling bandi bandits (Du he LLM training scenario, there are two perspectives on the problem (MDP vs. contextual ts) as mentioned in Section preference-based application. This k as a reverse-KL regularized contextual bandit problem. Typically the context is chosen externally, but Mehta et al. (2023) consider the learning variant in which the learning agent chooses the context as well. This variant is referred to as active contextual dueling bandits. TRoughly speaking, the Eluder dimension of a function class refers to the number of worst-case errors one must make to identify an unknown function from that class. 48 Algorithm (Reference) Pessimistic MLE Algorithmic approach Greedy _ policy Assumptions Logistic link func- Target(s) and goal(s) of learner High probability Theoretical guarantee(s) (Zhu et al., for pessimistic tion, linear reward ound for the oO Gas (eps ) 2023a) expected value function for a state- performance ™ function estima- pair feature embed- gap based on tion ding with some reg- _ trajectory-based ularity assumptions (and action- on weights, known ased) feedbac transition dynamics oF fline Re- Greedy policy General differen- High probability Transition dynamics: inforcemEnt for pessimistic tiable link function ound for the on weights, known model class entailing the value and re- ward function of the dynamic discrete choice model p-exponential decay, ° (aoe |Al(nRmax)*” y [oxivFoox¥7) p-polynomial decay, _ ati 1 = uta + patent d =population effective - sampling effective dimension LCBVI-Tabular- Offline (Kong & Yang, 2022) Maximum _in- formation gain for reward querying, value iteration based on pessimistic expected value function estima- tion for policy learning Binary rewards for state-action pairs based on human response model with bounded noise, com- pliant and tabular MDP 49 High probability bound for the performance gap based on binary state-action re- ward feedback Linear model class: o(4y /\\\\S\\\\log(|S|| A] Hn/5) Exe [Soy Manan) +0727] ) Np are numbers of visit time Offline Learning Zhu et al. (2023a) study the performance of a greedy policy trained from a data set consisting of trajectory pairs along with the observed preference that is as: sumed to be generated by means of a Bradley-Terry model with linear rewards. For this purpose, different results with respect to the MLE of the Bradley-Terry model for different feedback scenarios are derived that are quite of independent interest. In particular, they show concentration inequalities of the MLE for trajectory-based comparison feedback and additionally its asymptotic normality for action-based comparison feedback that also holds for K-wise comparisons. Based on these, it is shown that the greedy policy using the feedback might fail while using a pessimistic MLE leads to minimax-rates gap®. The latter is also shown to be true in the case of trajectory-based fee MLE in the case of action-based with respect to the performance back. Technically, the pessimistic MLE is realized by taking the policy that has the largest pessimistic expected value function, i.e., the lowest realization of the value function within a hyperparameter-dependent confidence region around the MLE. Further results of independent interest are the inferred theoretical guaran’ ees for maximum entropy inverse RL and action-based inverse RL algorithms (Neu & Szepesvari, 2009). The simple model assumptions underlying (Zhu et al., 2023a) were then replaced by more sophisticated assumptions in some subsequent work. The linear reward assumption has been replaced by more general reward function classes by Zhan et al. (2024a) and Li et al. (2023). In addi sider more general unknown human feedback models and construct the confidence regions for the pessimistic approach directly from the log-likelihood function. The resulting approach in terms of its performance gap, for which some problem-dependent coefficients, the per-step, per-trajectory, and transition concentrability coefficient, are introduced. On the basis o: rajectory-based and action-based comparison feedback are considered. he Iterative Data Smoothing (IDS) algorithm, which implicitly weights Zhu et al. (2023a) as these are based on the assumption of bounded utiliti Assuming a dynamic discrete choice model underlying the give: he per-trajectory concentrability coefficient should naturally appear in the bound on the performance gap. Moreover, the concentrability coefficient is shown to be upper bounded by the constant appearing in the special case of linear rewards considered by Zhu et al. (2023a). Finally, it is worth mentioning that both n follow-up work, Zhu et al. (2024) found overfitting as well as overoptimization issues of the MLE in the Boltzmann model for pairwise comparison feedback. This can arise in particular if the observations of labels are strongly unbalanced and thus the utilities can become infinite. To overcome this problem, they propose heir frequency and their current likelihood. Note that these issues do not contradict the results shown by ion, Zhan et al. (2024a) also con- , called FREEHAND, is analyzed a lower bound, it is shown that observed labels appropriately by es (or rewards). n data set of observed trajectories (without explicitly observed preferences), Li et al. (2023) suggest the Dynamic-Choice-Pessimistic-Policy- Optimization (DCPPO) algorithm. It first estimates the reward model a linear function model class as well as a subset of a reproducing kernel Hi class. n both cases, the cost of ensuring label differential privacy is a multiplica using this assumption and then learns a policy in a (pessimistic) value iteration manner from the estimated reward model. In the case of a inear MDP and a known model class that entails both the value and the reward function of the dynamic discrete choice model, DCPPO is analyzed with respect to its performance gap. This is done for the case of Ibert space (RKHS) as the model Focusing on the estimation of the weight parameter in the Bradley-Terry model for the action-based feedback under label differential privacy conditions , Chowdhury & Zhou (2023) analyze two estimation rocedures, MLE and stochastic gradient descent (SGD), under similar assumptions as in Zhu et al. (2023a). tive factor. Reward collapse, a term introduced by Song et al. (2023), describes the issue when rank-based training methods for LLMs lead to the same reward distribution regardless of the prompts used in the final training steps. The authors show that this occurs because the rank-based approach does not adequately account for rompt-related information. To address this problem, the authors propose a family of utility functions as well as an optimization method that successfully creates prompt-dependent reward distributions, effectively mitigating the collapse of rewards during training. 8The expected difference between the optimal value function and the value function of the used policy. Blending Online and Offline Learning Kong & Yang (2022) study the problem of optimal policy learning from critique feedback (see Section 3.2), i.e., binary rewards for state-action pairs, with as few queries to the human as possible. They assume an underlying ground-truth human feedback model that leads to a positive evaluation for a state-action pair if it exceeds a specific threshold evaluated at that pair. In addition, the learning process consists of two phases: First, exploring the environment in an unsupervised manner, and then querying user feedback in an active reward learning phase to learn the human feedback model. This learning process is again analyzed in two variants: Either the exploration phase was performed externally, and a data set consisting of trajectories is provided (offline), or this data set is actively collected itself (online). For both variants, an active learning algorithm is proposed that essentially selects query points (state-action pairs) that provide the most information gain given the points already designated to be queried. For the online variant, an exploration strategy based on optimistic least- squares value iteration is also introduced for tabular or linear MDPs. In both variants, policy earning is carried out by a pessimistic value iteration with the empirical transitions and the estimated reward function, resulting in UCBVI-Planning (online) and LCBVI-Tabular-Offline (offline). Under the assumption of bounded noise (Massart & Nédélec, 2006) or low-noise assumption (Korba et al., 2017; Haddenhorst et al., 2021), bounds on the performance gap of both algorithms are derived. The question of the ideal experimental design for RLHF is addressed by Zhan et al. (2024b), in particular, how to separate the process of data acquisition (e.g., trajectories to be evaluated) from the process of retrieving human feedback to avoid constantly involving humans in the training loop. Assuming linear rewards, the Bradley-Terry model and either a transition oracle (e.g., available for tabular or low-rank MDPs) or a linear MDP they suggest the expeRimental dEsiGn for queryIng huMan prEference (REGIME) algorithm that first samples exploratory trajectories indented to be as informative as possible for learning the reward via MLE and then applies a greedy policy based on the reward learned by the latter. They explicitly show that REGIME requires less human feedback to be queried in order to output an e-optimal policy at the end than the approach by Saha et al. (2023). 7.2 Preference-Based vs. Reward-Based Learning There have been some theoretical analyses regarding the question in how far, or if at all, preference-based feedback in the form of trajectory comparisons is more suitable compared to numerical feedback. Ji et al. (2023c) suggest a human rating model for this purpose in the numerical feedback case and analyze the LCB algorithm in order to compare it with the pessimistic MLE (Zhu et al., 2023a). It is shown hat under specific assumptions, LCB has a constant performance gap, while the preference-based pessimistic MLE under similar assumptions has a similar bound as in Table Wang et al. (2023b) provide reduction-based algorithms that can directly utilize state-of-the-art results in reward-based RL for RLHF with utility-based and general state-action and trajectory-based comparison feedback. They show, in general, how theoretical results of the underlying standard RL algorithm can be ranslated to theoretical results for the resulting preference-based RL algorithm. For some special cases, such as MDPs with finite Eluder dimension and utility-based preference feedback, the theoretical guarantees are explicitly derived using state-of-the-art RL algorithms that are qualitatively similar to explicit preference- ased RL algorithms. 7.3 Nash Learning from Human Feedback The majority of theoretical works use the modeling of (pairwise comparison) feedback by means of a link function (see Section 5.1.4). Even if this often leads to simpler derivations, this modeling has the decisive disadvantage that it imposes a transitivity of the human feedback that does not necessarily prevail in reality. In other words, it is quite possible that preference cycles can occur. For this reason, there is a new direction in theoretical work that dispenses with parametric modeling of the preference probability similar to Chen et al. (2022) but uses it to formulate a new learning objective. Specifically, the problem is considered from a game theory perspective, where two policies each propose a trajectory that should be highly preferred by the human user. Thus, the goal is to find a policy that suggests trajectories that are preferred to the trajectories of any other policy, i.e., a Nash equilibrium or a von Neumann winner. This learning variant was first considered by Wang et al. (2023b), who showed that the problem can be reduced to finding restricted Nash equilibria in a multi-agent RL problem (based on numerical rewards). For special situations of the latter problem, wrapper algorithms are proposed that have been shown to find the von Neumann winner with high probability. The learning problem was recently taken up and analyzed by Munos et al. (2023) and Ye et al. (2024) in a KL-regularization variant. While the former considers the online learning setting assuming a known preference model, the latter considers both online as well as offline learning settings and the preference model belonging to a finite function class. 8 Applications and Benchmarks The field of RLHF has advanced significantly in the last few years, with increasing interest driven by promi- nent applications. First and foremost are applications to large language models, exemplified by Chat- GPT . This section starts by providing a sample of such applications, showcasing how this technology is being utilized in fields as varied as robotics, language processing, image generation, and more. We will also delve into libraries that provide foundational support for RLHF research, enabling researchers and practitioners to experiment with and refine a range of approaches. We then explore a spectrum of benchmarks that have been developed to standardize and simplify the evaluation of new approaches, offering insights into their performance in different settings. Finally, and closely related to those benchmarks, we will discuss common evaluation practices. 8.1 Applications RLHF finds applications across various domains, showcasing its versatility in addressing complex and nuanced tasks. The most prominent application is ChatGPT , which is an example of an application in the domain of language models. Beyond that, however, applications extend across diverse domains such as control tasks, generative models, and recommender systems. This section provides an overview of notable works applying RLHF in different areas. Control and Interactive Environments There is a long history of using control environments as bench- mark tasks for RL. In addition to the breadth of available environments, control applications are of particular interest because tasks are often hard to specify. Christiano et al. (2017) demonstrated the effectiveness of RLHF in games as well as simulated continuous control tasks, matching the performance of RL agents trained on ground-truth rewards with a fraction of the feedback. Extending to robotics, Ding et al. (2023) trained a reward model for diverse tasks with a single robot, achieving human-like behavior. Kupcsik et al. (2018) applied RLHF for precise robot-to-human handovers. Similarly, Abramson et al. (2022) used RLHF in the Playhouse simulator, a platform for sensorimotor task training, and Milani et al. (2022) showcase an application in the context of the MineRL Basalt competition for Minecraft tasks. Recently, Dong et al. (2024) use RLHF to guide a diffusion-based planning model. Generative Models in Language and Imaging Generative models, i.e., models that generate new data instead of just predicting labels, can be framed as an RL setting in which a policy assembles the output through its actions. In the context of language models, this means that the language model is interpreted as a policy with tokens as actions. Using this reframing, we can use RLHF approaches to fine-tune generative models to produce preferred outputs. ChatGPT and GPT-4 are prime examples of language models fine-tuned using RLHF. These applications build on earlier work, such as by Ouyang et al. (2022), Ziegler et al. (2020) and Glaese et al. (2022). This method extends to text summariza- tion (Gao et al., 2018; 2020; Stiennon et al., 2020), dialogue summarization , and question answering . In image generation, Lee et al. (2023) and Xu et al. (2023) demonstrate the use of reward modeling for text-to-image tasks, while Pinto et al. (2023) and Kazemi et al. (2020) explore RLHF applications in broader computer vision tasks. Interestingly, in the context of LLMs, reward learning has also been expressed as density estimation instead of the supervised approach described in Section Recommender Systems In the context of recommender systems, Xue et al. (2023b) have shown the potential of RLHF in optimizing for long-term engagement. Although it is, in principle, possible to algorith- mically evaluate policies in this domain, these rewards are sparse. To combat this, Xue et al. (2023b) use RLHF to distill sparse, global feedback into a dense reward model. These diverse applications underscore RLHF’s adaptability and its growing importance in various techno- logical domains, paving the way for innovative solutions and enhanced human-computer interactions. 8.2 Supporting Libraries Several libraries have emerged that aim to provide a toolset for implementing and experimenting with RLHF and reward learning algorithms, contributing to the ease and efficiency of research and development. One notable example is the imitation library (Gleave et al., 2022b). It encompasses a collection of imitation and reward learning algorithms, including those introduced in the seminal work by Christiano et al. (2017). In the offline realm, Clean-Offline-RLHF provides implementations for offline RL algorithms with human feedback. Two other libraries, APReL (Biyik et al., 2022b) and POLAR , focus on the Bayesian setting. Buyik et al. (2022b) provide a specialized framework for preference-based reward earning with a focus on Bayesian methods. Meanwhile, Tucker et al. (2022) introduce a framework designed or Bayesian reward learning from multiple feedback types, including pairwise preferences, in MATLAB. Finally, the domain of language model fine-tuning, the tr1X library offers a toolkit specifically designed for language model training. It specializes in the fine-tuning of transformer-based language models, treating the language model as the policy in an RLHF setup. Due to the many interacting components and the human element in RLHF research, implementing new ideas and running experiments can be quite challenging. The discussed libraries reduce this challenge and make RLHF research more approachable to many researchers. 8. Due to the difficulty of reproducible evaluations without a ground-truth objective and with humans in the loop, benchmarks play an important role in advancing and evaluating RLHF approaches. Several benchmarks have been proposed, each focusing on different applications and challenges. One such benchmark is B-Pref (Lee et al., 2021a), which focuses on control tasks with synthetic feedback. B-Pref aims to provide simulated human feedback that captures some irrationalities, thereby coming closer o evaluation with real human feedback than other approaches. At the same time, by relying entirely on synthetic feedback, the results are reproducible and cost-effective to generate. In a similar vein, Freire et al. (2020) propose a set of environments designed to diagnose common problems in reward learning. These environments help in identifying and addressing the typical challenges that arise in RLHF scenarios. The offline RLHF setting is particularly well-suited for benchmarks, as it allows for the use of static datasets. Shin et al. (2023) evaluate pre-existing offline RL benchmarks for their suitability for RLHF evaluation, and find that many are ill-suited due to the simplicity of the required reward function. They do, however, identify a subset of these benchmarks together with their own addition for evaluation. While Shin et al. (2023) leverage synthetic rewards, Yuan et al. (2024) propose a dataset and benchmark for offline RLHF, including preference data. This helps to circumvent the challenges of synthetic feedback and benchmark reproducibility with real feedback. The MineRL BASALT competition (Shah et al., 2021b; Milani et al., 2022) gives a more application-driven benchmark with a complex environment. The competition proposes the challenge of solving tasks defined by natural language descriptions in Minecraft based on human feedback. Writing hand-engineered reward functions is very challenging in that setting, which makes it a good benchmark for methods based on human feedback. The competition is method-agnostic in principle, and non-RL approaches such as behavioral cloning are also considered. While the initial dataset consists of human demonstrations, the competition is agnostic for the feedback type, which may include demonstrations, comparisons, and others. The final evaluation is performed by humans through pairwise comparisons. In the domain of language modeling, Truthful QA serves as a benchmark that measures the truthfulness of models. Also, in the context of language models, Ramamurthy et al. (2023) introduce a set of pre-trained reward models, learned from human feedback, as benchmarks. These models serve as reference points for evaluating new RLHF techniques against established standards. Together, these benchmarks provide a diverse and comprehensive suite of tests that drive the development and refinement of RLHF methods, ensuring they are robust, effective, and capable of handling a wide range of real-world scenarios. 8.4 Datasets Due to its interactive and online nature, RLHF research often does not rely on static datasets. This is because the feedback is generally collected interactively and depends on the current policy. When the reward model is not refined iteratively, however, as is common practice for the related settings of LLM fine-tuning and offline RLHF, static datasets can be used. Such a static dataset can significantly simplify the development and evaluation of RLHF methods. Since language model fine-tuning is a popular application of RLHF and generally does not iteratively refine the reward model, many datasets have been developed for this purpose. Particularly notable are hh-rlhf (Bai et al., 2022a) and PKU-Safe-RLHF (Ji et al., 2023a), two datasets focusing on harm- less and helpful responses, the OpenAssistant datasets (oasst1, oasst2) , contain- ing not only response rankings but also ratings on various dimensions, the summarize_from_feedback dataset focusing on preferences over text summaries, the Stanford Human Pref- erences Dataset (SHP) , which is based on Reddit responses, the WebGPT dataset (webgpt_comparisons) , focused on long-form question answering and the HelpSteer (Wang et al., 2023c) dataset, which is not based on preferences but instead gives ratings on for 4 attributes (helpfulness, correctness, coherence, complexity) for each response. Although static datasets are used more rarely in the control setting, some datasets have been developed for offline RLHF in this domain. Concretely, Yuan et al. (2024) propose the Uni-RLHF dataset and a benchmark ‘or offline RLHF while Kim et al. (2023) publish a dataset of real human preferences for typical offline RL asks (D4RL, Robosuite). 8.5 Evaluation Evaluating RLHF poses unique challenges, particularly in scenarios without precise ground-truth task spec- ifications. Evaluations generally focus on either the learned policy or the reward model, each shedding light on different aspects of system performance. Policy Evaluation Assessing learned behavior is crucial for the evaluation of an RLHF system. In domains with ground-truth rewards, these can be used for policy evaluation . However, many RLHF applications lack this clarity. Ouyang et al. (2022), for instance, evaluate the quality of language model responses by having labelers rate the output quality on a test set of prompts, highlighting the significance of human judgment in assessing model outputs. Jain et al. (2015) use direct Likert-scale scores for evaluations, including self-assessments by trainers and cross-evaluations by others. Losey et al. (2022) extend this with a Likert-scale survey and free-form participant comments, comparing evaluations based on known true rewards with subjective experiences. Moreover, Abramson et al. (2022) employ a multi-stage evaluation scheme that includes scripted probe tasks, a standardized test suite evaluated by humans, and full interactive assessments, demonstrating the need for diverse and thorough evaluation methodologies in RLHF. Reward Model Evaluation Direct reward model evaluation complements policy assessment. While reward model accuracy is a more direct measure of preference-learning success, the ultimate goal is inducing effective policies. A perfectly accurate reward model is often not necessary to induce a good policy, which is the actual goal of RLHF. Therefore, both evaluation methods are ideally used in combination. Jain et al. (2015) also use a ranking loss method for test sets of trajectories, compared against expert evaluations with known Likert-scores. This approach provides quantitative measures of the reward model’s fidelity. n addition, Wilde & Alonso-Mora (2022) compare parameter-based and reward-based evaluation measures or learned reward functions, identifying strengths and weaknesses in both methods and contributing to a more nuanced understanding of reward model assessment in RLHF. These approaches provide a quantitative measure of the reward model’s accuracy in reflecting human preferences and expert judgments. For a detailed discussion of reward model evaluation, also refer to Section 5.Policy- and reward model evaluation both offer insights into the performance of an RLHF approach. Ideally, oth measures should be combined to enable quick iteration and give insights into both the preference learning performance as well as the quality of the learned behavior. 9 Discussion and Conclusion n this survey, we have provided an overview of the current state of RLHF, highlighting its evolution from PbRL and examining its broad applications across various domains like control, natural language processing, and computer vision. While our survey captures the current state and many significant trends and advance- ments in RLHF, we acknowledge the rapid expansion of this field and the inevitable limitations in covering every extension and application in depth. We will discuss some of these extensions, open questions, and conclusions in this section. We have specifically focused on RLHF methods where a reward function is learned online from human feedback. There have been some recent works that are outside of this scope and yet propose promising new methods to learn human-aligned objectives, such as offline learning of reward functions or privacy-preserving alignment based on differential privacy . An alternative approach to RLHF is to learn objectives from a pre-trained AI system instead of human feedback. This has been termed RL from AI feedback (RLAIF) and leverages foundation models as a source of preference and has shown successful for language-model fine-tuning (Bai et al., 2022b; Sun et al., 2024), generating intrinsic motivation for text-based games , as well as learning rewards (Wang et al., 2024b) or coding reward functions (Ma et al., 2024; Xie et al., 2024) for control tasks without any human involvement. Closely related is the setting of assisted evaluation, studied by Saunders et al. (2022), where a language model is used to generate critiques of language model outputs, thereby assisting human evaluators. Most work on RLHF implicitly assumes that tasks can be specified by maximization of expected accu- mulated scalar rewards. This assumption, called the reward hypothesis , is under ac- tive debate (Lambert, 2021; Vamplew et al., 2022; Bowling et al., 2023; Skalse & Abate, 2022) in the RL community. On a related note, Skalse & Abate (2024) investigate the sensitivity of inverse RL to this misspecification. Recent approaches in RLHF are, for instance, considering more complex objective func- tions, such as multi-objective frameworks involving non-linear aggregation of expected accumulated vector rewards . Many more extensions of RLHF are inspired by revisiting classic RL topics under the RLHF lens. This is exemplified by studies on exploration , reward feature learning , reward shaping , multi-task RL (Ouyang et al., 2022; Abramson et al., 2022; Myers et al., 2023), hierarchical RL , hindsight experience replay , risk-sensitive RL , safe RL (Dai et al., 2024; Cosner et al., 2022), fair RL , or continual learning . As discussed in Section 4.2, the intersection of RLHF with HCI also offers a fertile ground for future research, especially for refining feedback mechanisms. It is crucial to keep human psychology in mind when designing these systems and to learn from other related fields that already studied such issues extensively. n addition to those extensions, current RLHF methods also have challenges and limitations to be aware of. Without assistance during feedback, it is limited by the tasks humans can reliably judge (Leike, 2022; Leike et al., 2018; Wu et al., 2021; Christiano et al., 2018). This challenge can be seen when trying to fine- une a LLM to give factual answers, where humans often prefer assertive, but wrong responses . As another limitation, current RLHF approaches often fail to learn the actual causes of human feed- back, learning correlations instead . The common separation between the policy and the ot ot reward model also limits how the agent can reason about its knowledge of the human preferences, a limitation hat may be lifted by tighter integration such as in the cooperative inverse RL setting (also called assistance games) (Hadfield-Menell et al., 2016; Shah et al., 2021a). Additionally, the RLHF framework is limited in how it can reason about its knowledge of human preferences Casper et al. (2023) offer a thorough analysis of further issues and limitations of RLHF, highlighting the practical constraints of current approaches. Adding o that, from a theoretical perspective, a primary challenge lies in further relaxing underlying assumptions. This requires striking a delicate balance: On the one hand, ensuring the assumptions are not overly restrictive 0 encompass a broad range of practical use cases, and on the other, maintaining the feasibility of theoretical guarantees for computationally efficient algorithms. Key questions in this context are whether it is possible o design algorithms that do not need to actively maintain a policy space and eliminate sub-optimal policies nor rely on a computation oracle. Recent work such as Wang et al. (2023b) or Wu & Sun (2024) give hope hat this may be possible. Although RLHF has significantly contributed to the advancements in LLMs and other areas of ML, it is a domain still in its infancy with many unanswered questions and inherent limitations. Despite and because of hese challenges, it is ripe for further advancements in theory and practice, hopefully resulting in even more robust algorithms making more efficient use of human feedback. It remains intriguing to what extent RLHF will continue to shape the fields of natural language processing, RL, robotics, AI alignment, and beyond in he future. Acknowledgements We thank Tom Bewley, Andreea Bobu, Adam Gleave, Yannic Metz, Peter Stone, and Banghua Zhu for their feedback on earlier versions of this survey. This publication was supported by LMUexcellent, funded by the Federal Ministry of Education and Research (BMBF) and the Free State of Bavaria under the Excellence Strategy of the Federal Government and the Lander as well as by the Hightech Agenda Bavaria. This work has also been supported in part by the program of National Natural Science Foundation of China (No. 62176154) and a collaborative project funded by NetEase.'),\n",
       " Document(metadata={'source': '/tmp/tmprogyhe8f/tmp.pdf'}, page_content='2308.14328v3 [cs.LG] 24 Feb 2025 1V ~arXi a © JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Reinforcement Learning for Generative AI: A Survey Yuanjiang Cao, Quan Z. Sheng, Member, IEEE, Julian McAuley, Lina Yao, Senior Member, IEEE, Abstract—Deep Generative AI has been an essential topic in the machine learning community for a long time, and it can impact a number of application areas like text generation and computer vision. The major paradigm for training a generative model is maximum likelihood estimation. This formulation successfully establishes the objective of generative tasks, while it cannot satisfy all the requirements that a user might expect from a generative model. Reinforcement learning has demonstrated its power and flexibility to inject new training signals such as human inductive bias to build a performant model. Thereby, reinforcement learning has become a trending research field and has stretched the limits of generative AI in both model design and application. It is reasonable to summarize advances in recent years with a comprehensive review. Although there have been surveys in different application areas recently, this survey aims to shed light on a high-level review that spans a range of application areas. We provide a rigorous taxonomy and make sufficient coverage on various models and applications, including the fast-developing large language model area. We conclude this survey by showing the potential directions that might tackle the limit of current models and expand the frontiers for generative AL Index Terms—reinforcement learning, LLMs, generative models, I. INTRODUCTION Recent years have witnessed tremendous progress in gen- erative AI, like variational autoencoders , autoregressive models (2), adversarial generative nets , diffusion models energy-based models and normalizing flows (6). The advancement of these models has brought the development of a broad range of applications, from neural language processing to image generation and scientific research. Particularly, the emergence of Large Language Models (LLM), like ChatGPT has changed the paradigm of industry and academia regarding how to develop the next generation of machine learning systems to bridge the gap toward general AI further. Another fast-developing area is Diffusion models which is the foundation for large models that generate high quality images, videos (8). and medical image analysis (9}. whose training requires large amounts of computing resources for performant image generation. Generative models also power scientific research in molecular design and optimization. Al- phaFold shows that protein structure can be effectively modeled by machine learning systems [11], (12}. Yuanjiang Cao and Quan Z. Sheng (Michael Sheng) are with Department of Computing, Macquarie University, Sydney, NSW, AUS. Email: yuan- jiang.cao@mg.edu.au, michael.sheng@mq.edu.au Julian McAuley is with University of California San Diego, California, USA. Email:jmcauley @eng.ucsd.edu Lina Yao is with CSIRO’s Data61 and University of New South Wales. Email: lina.yao@data61 .csiro.au Training generative model is one of the cornerstones of gen- erative AI research, which studies to design objective functions to guide the learning process. The major objective of gener- ative models is Maximum Likelihood Estimation (MLE), in other words, decreasing the Kullback-Leibler(KL) divergence between generated distribution and target data distribution. However, in some cases, human want more than what the MLE can provide. Taking conditional text generation as an example, for a text generator, we hope not only that it achieves good performance on texts that exist in the training dataset, but also that it can output texts that satisfy other desired properties like diversity, coherence, human-like, and moral considera- tions. The discrepancy between evaluation metrics and training objectives can decrease the quality of generated outputs. These desired properties for a text generator expose that the gap between distribution fitting and desired properties makes the maximum likelihood objective insufficient. The generalization of models connects to the limit of the Negative Log Likelihood (NLL) objective as well. In some applications of generative AI, we hope the model can cope with out-of-distribution inputs or explores out-of-distribution. For example, in novel molecule design, the goal of the learning process is to explore and generate unseen molecules instead of those in the dataset. A code summarizer or generator is expected to produce well- designed code for novel tasks instead of those in the dataset. To address the aforementioned limits, the reinforcement learning has been proposed as a useful optional training paradigm to improve the performance of generation models. Reinforcement learning is a training paradigm designed for learning from interaction . It has flexible objectives in terms of the reward function, in contrast to the distribution modeling objective of supervised learning and unsupervised learning. Furthermore, many generation problems can be re- defined as decision-making problems, creating the utility of RL methods on generation problems. Why this survey There are numerous existing surveys and reviews of the application of reinforcement learning scattered in various models (14) and application areas, including neural language processing [ {18}, large language model (19), code generation , speech processing , computer vision [23], [24], neural architecture search [25} drug discovery [27] . survey the area of RL applying on generative models, while it only investigates three perspectives in terms of objective design. In contrast, this work covers topics like using RL as a sampling method. For application, we explicitly list the JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15.1 Peaked Distribution 5.2 Exploration and Exploitation 15.3 Reward function design and multi objective optimization 5.4 Long-term Credit Assignment 5.5 Generalization 5.4 Model Enhancement and Control, 15.7 Human Alignment in LLM and foundation models ‘Text Summartzation Machine Translation Dialog system Human Value Alignment and Constraints | | S.1NLP ‘Text, Queries and Knowledge Graph Large Language Models RL for ther Applications in NLP ode Search Comment and Annotation Generation cout ceveaton, 87088 Goeraton Unit Test Generation Image Captioning Visual Question Answering Visual Dialog system 5.3 Computer Vision ‘Text-to-Image Generation (other Computer Vision Tasks Text-to-Speech Translation 5.4 Speech and Music Generation Melody Gene! Molecule Design Reaction Opti ston 55 Al for Science Micro-structure Get Design 5.6 Recommender System and Information Retrieval 5.7 Robotics Procedute Generation Robotics Simulation Optimization ‘Graph Generation |~ Exploratory Oata Analysis — *+ Semen Neu Tens eet Variational Autoencoder (VAE) Generative Adversarial Networks (GANS) 21 Generative Models | _Energy-Based Models (EBM) Autoregressive Models Normalizing Flows Markov Decision Process 2.2 Reinforcement Learning Methods | Model-based Methods Comparison Between Reinforcement Methods 2.3 Framing Generation tasks as Reinforcement Learning Problem ‘The generated varlable Is non- [5.1 Solving the Non-ditferentiable Learning _differentiable problems “The training objective ls non-cltferentiable Reward by Discriminator Reward by Hand-designed rules Reward and Divergence Reward by data-driven model 3.3 sampling Distributional Policy Gradient State and Action Design 15.4 Neural architecture Search ‘Sample Efficiency Fig. 1: The Overview Structure of This Survey application area, making readers easier to access topics of their interests and also inspire new ideas in the model discussion part. We also cover niche application areas like procedure generation and graph generation. Our survey surpasses these previous surveys in terms of the selection criteria and the comprehensiveness of coverage in this area. We not only summarize lines of research across multiple areas but also organize theoretical works that aim to improve generative models by reinforcement learning tech- niques. For instance, we cover the recent advancement that the efficiency of Diffusion Model could be enhanced by shortcut fine-tuning motivated by one of the reinforcement learning method: policy gradient, as shown in Section [IM-B3] Scope and Paper selection criteria. This study offers a comprehensive analysis of the potential and obstacles asso- ciated with reinforcement learning, with a particular focus on deep reinforcement learning in the realm of generative AL The analysis encompasses their intrinsic capabilities, such as addressing non-differentiable learning issues, infusing gen- erative AI with innovative training signals, and advances in sampling and neural architecture search. It also sheds light on the prevailing challenges, including peaked distribution, the conundrum of exploration versus exploitation, sparse reward scenarios, challenges in long-term credit allocation, and is- sues of generalization. Furthermore, the research delves into their practical applications across various domains, including Natural Language Processing (NLP), Computer Vision (CV), code synthesis, speech decoding, information extraction, rec- ommendations, robotics, and AI’s role in scientific endeavors. Emerging trends, such as the intricacies of reward function formulation, multi-objective optimization strategies, enhancing and controlling models, modeling human preferences, ensuring interoperability, and the integration of novel reinforcement learning techniques with Large Language Models (LLMs) and foundational models, are also meticulously explored. In application areas, this survey mainly focuses on se- quential generation, e.g. conditional text generation and code generation, but also contains less-mentioned vision tasks such as 3D point cloud completion. We select papers according to three selection criteria, impact, venues, and time. For classic models, we focus on high-quality works that are present in well-appreciated conferences and journals. For recent papers JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 TABLE I: Notations in this Survey Notation Explanation x, the target random variable that a generative model aims to generate, D1, Liye In a sequence of variables, x; represents an element of « when «x represents a sequence. This is useful in sequence modelling. z latent variable t the time step in a sequence, often serve as an index St the state at time step ¢ in an MDP at the action at time step ¢ in an MDP. Tt the reward given by the environment at time step t in an MDP T a trajectory, a.k.a. a sequence of states, actions, and rewards (s9, 40, $1,171, ---Sn,Tn) Rt Rt = of, yr; the discounted accumulative return over a trajectory 7 , as shown aboy, is the discount factor that decreases the impact of future rewards at an exponential rate T the policy of the RL agent Vir (st) the value function of an agent given a state at time step t Qn (st, at) the value function of an agent that takes a state and an action as input pP(-),4() the probability distribution of a given variable Exvpz[\\'] the expectation of some variable on the distribution of x Drx(allp) the KL Divergence between two distribution p and q on the same variable. D(-) the discriminator in GAN G(-) the generator in GAN that push the boundaries like large language models, we relaxed the criteria. For some small branches, we list all the Papers we can find. We propose this survey to systematically review the appli- cation of reinforcement learning in generative AI, including models and applications. Our contributions include « We perform a systematic and thorough examination of various directions within the field of generative models and problems using Reinforcement Learning. Recogniz- ing the multifaceted nature of these areas, we carefully organize and present an exhaustive review that encom- passes different methods, structures, and applications. « We developed a unified taxonomy, meticulously crafted to organize the extensive and varied literature in generative studies. This taxonomy not only classifies existing works according to common themes and methodologies but also draws attention to potential intersections and divergences within the field. e With a specialized focus on Reinforcement Learning (RL) methodologies within generative AI, our research provides a detailed exploration of the contemporary chal- lenges and avenues for development. We dissect the inherent complexities and hurdles in implementing RL methods and organize them in a manner that encapsulates the current state of the art. Furthermore, we identify sev- eral emerging directions that hold promise for innovative solutions. Il. PRELIMINARY AND BACKGROUND In this section, we will provide preliminaries about the relevant concepts and models, in terms of generative models and reinforcement learning. A. Generative Models The topic of this survey is reinforcement learning applied in generative AI. Thus it is essential to provide a brief intro- duction to basic generative models, which lays the foundation for successive chapters. Before we dive into details, we list the most common notations throughout this survey in Table []] Generation is a broad research area that scatters in various application areas. We take three examples for readers to understand what is generation, then we introduce the formal definition of key concepts. The first example is a dialogue system like ChatGPT one of the most popular generative AI that covers more than 180 million users If we want to build a dialogue system, we collect a dataset consists of pairs of user prompts and human response. The goal is to train a model that takes a sequence of prompts that consists of symbols and then return a sequence of symbols as response. The symbol comprises of alphabet characters, math symbols, other specific symbols. The second example is text-to-image generation application like Midjourney , where users input a sequence of symbols and expect to get an image that satisfying the input. The third example is image-to-image translation. The user wants to transform a realistic image to other styles like an impressionistic image. The user needs to pipe the image and prompt, which is again a sequence of symbols, into the image generator. From above problem description, we can see that the goal of generation is typically requires to pipe the input variable y into the model, which could be in the form of a sequential form (y1, y2,..., Yn) and the model emits a variable x, which could be represented in the form of a sequence (21, %2,...,2m)- Given the input variable and output variable, it is easy to think that we can directly a probabilistic model p(x|y) via machine learning methods. Interestingly, this is generally infeasible for the mismatch between data we have and the task requirement. For example, we want to build a dialogue system, however, most text data we can get is acquiring from webs, these texts are not dialogue data, which means it cannot be used to learn p(2|y). Instead, we can train a model that models p(x), the distribution of the data we can get, then we can adjust the model to the task we need. In the dialogue JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 example, we can train a model with web dataset, then we can fine-tune the model based on the dialogue data we collected from interaction with human. For text-to-image generation, changing the learning objective from p(x) to p(xly) is also not difficult. For simplicity, we describes generative models whose goal is to learn p(x) in the following section. Given a dataset X composed of a set of samples {x\\'|1 < i > n}, we aim to use a generative model p(@) to generate a sample & that follow the true data distribution p(x), where x is the random variable that represents the true data sample, n the number of samples in the dataset. The x could be a scalar variable, a vector variable or a sequence of variables (1,22, ...,%n), Where the subscript means the ordered index in the sequence, x; means one data point of the sequence. Training and Inference The learning of a model contains two useful stages: training phase and inference stage. In training stage, a model is trained. Training a model requires an objective function or a loss function which reveals the goal the model optimizes towards. For example, language models typically use next word prediction as an objective. In this section, we introduces basic formulation of five generative models, and we use minimizing NLL as the objective function without additional description, which is supported by the maximum likelihood principle (35). The computation of NLL entails computing p(x), therefore for all generative models in this subsection, the formulation is about how to compute p(x). In inference stage, the model should generate a sample # given a latent variable z that is sampled from a given distribution like Gaussian Distribution. The inference stage is how a model is used to generate samples. Variational Autoencoder (VAE) [I], learns useful representations by reconstructing the input x with a latent variable z in consideration. Formally, we can decompose the distribution p(x) by the latent variable: va) = | plel2)o(e)ae. a) Inp(x) = Dex (4(z|2)|P(2)) + Eq(eje) np(a|z) 2) But the integral in Equation |1} is intractable in real-world applications. Therefore, an optional way is to approximate this conditional distribution by a simpler distribution g(a) with an evidence lower bound (ELBO) in Equation [2] In inference stage, VAE could sample following Equation sample a z from Gaussian Distribution, then pipe z into the decoder p(z|z) to compute the distribution, then sample a % from the distribution. Generative Adversarial Networks (GANs) BI are com- prised of a discriminator and a generator. The discriminator is trained to classify where samples come from, real datasets or generated. The generator aims to trick the discriminator. Therefore, the two networks form a zero-sum game which consequently pushes the output distribution of the generator to approximate the real distribution. Formally, the objective of GAN can be defined as: min Max Ex~paua(2) D(@) + Ez~p,(z)[L — D(G(z))|_ @) We can observe that generator could directly generates data samples by sampling a latent vector z and then pipe the vector into the generator. Energy-Based Models (EBM) represents any distri- bution density with an energy function by —E(x) Te (4) Tex p(x) = where E(x) is an energy function that outputs a density of the a certain sample x. It could be better to understand the energy function as a compatibility when the energy function is used to model a joint distribution E(x, y), the energy is lower when x and y are more compatible. For distribution p(x), we could treat E(a) as a density. EBMs do not pose constraint on tractability of the normalizing constant, making them more flexible [38]. It can be seen from this formulation that the denominator is an integral over the space Vol = Vo — Inpo(x) (5) = VoEe(x) + Vo in [ eo Bole\") (6) al WX = VoEo(x) — Ex-xpy VoEo(a ) (7) where the integral term is approximated by a Markov Chain Monte Carlo (MCMC) method that samples from the EBM, forming a contrastive objective (38). Sampling from EBM is not trivial as well. Researchers tend to use MCMC for sampling (38). fortunately, one line of research shows that it might take a few steps of sampling chain to get an acceptable sample [39]. Autoregressive Models (AR) decompose the probability distribution of a variable x into a sequence of variables by the chain rule: P(x) = p(#1, 2, ++, %n) = [][pG@ile:. w5G-1) (8) It can be naturally applied to sequence generation tasks like text generation and molecule design. It is not so obvious that AR models could model an image by breaking the image into a sequence of patches, allowing AR to be used for image generation. The ordering of generation is fixed and cannot be changed during training and inference. The computational complexity of the sampling process is linear in the number of steps in the generation, and the sequential ordering makes par- allel computation difficult which leads to less efficiency. The training objective could minimize the negative log likelihood of p(x), and sampling process could generate tokens one by one, where tokens refer to x; in Equation} Although it seems the sequential generation process prevent parallel computing of generation and limits the scalability of AR, [40}-[42] etc. have been proposed to address the scalability and AR has become one of the most useful generative models. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Normalizing Flows [6], [43], [44] Besides autoregressive models, another representative generative model that is capable of direct optimization of NLL objective is Normalizing Flow. Given a latent variable z of a tractable distribution p(z), normalizing flow can sample a data point x by transforming z through an invertible function f, forming x = f(z). Because the f(-) is an invertible function, the distribution of x could be computed by -1 Of (2) Oz In Equation I normalizing flows use the change of variable tule of distributions to get the distribution of x. This formula- tion allows fast sampling and inference by f(z). The constraint of invertible functions is strong that one step transformation is generally insufficient when the distribution of x is complex, which leads to a combination of multiple invertible transfor- mations: det p(x) = p(z) (9) det (10) Ui-1 Inp(e,) = Inp(a,) — S> In This formulation requires each transformation xz; = f;(xi—1) to efficiently compute Jacobian determinant as well as to be expressive and invertible. Capability of tractable sampling and inference makes training and inference tractable. During training, normalizing flows can compute p(x) directly by Equation [9] The sampling process has been explained above. Diffusion Model is a type of generative model that injects noise into the data and learn a reverse process to gener- ate samples. Here we briefly introduce one widely adopted diffusion model, Denoising Diffusion Probabilistic Models (DDPM). A DDPM model consists of two Markov chains, a forward chain q(xx,---,01\\\\v0) = [TfL a(ve|an—1) that perturbs the data sample with noises, and a backward chain po(%0,%1,---,UK) Tex q(xr—1|c~) that recover a sample from the data distribution given a random noise sam- ple. Note that for simplicity, we slightly abuse the subscription of x as a series of perturbing or denoising steps k € [1, K]. The 6 means the reverse process is parameterized by a neural network. The neural network is trained by minimizing the KL divergence between forward process and reverse process. Dex(q\\\\|pe) = E[-log po(xo)] (11) where the KL divergence is a upper bound of the NLL objective. Hu et al. links the KL divergence to the following loss function: Egv(1,K],ro~a(o)e~N\\' (0,1) ACK) lle — €0(2, KDI] 12) where ¢ is the Gaussian noise used in forward computation and A(k) is a positive weighting function that is necessary to link KL divergence to this formulation. Large Language Model is a language model that uses very large neural networks as models that typically have billons of parameters in order to train a generalized model for various applications. The famous ChatGPT [7] is a chatbot powered by the large language models. They are typically trained by two steps, the first step using a next word prediction objective, and the second step to use various methods to finetune the model in order to constrain the behaviors of the large models. B. Reinforcement Learning Methods Reinforcement learning is a computational approach to automating goal-directed learning and decision-making (13). In this section, we introduce the Markov Decision Process (MDP), a formulation that can be widely applied to rein- forcement learning problems. After formally establishing the problem, we introduce the categorization of reinforcement learning methods and the key details of these methods. 1) Markov Decision Process: Markov Decision Process is a classical formalization of sequential decision making | where actions have impacts on subsequent states and rewards. The complete formulation model of an MDP contains the following five elements: e S is a set of states of the environment e Aisa set of actions of the agents e T:Sx A-— S is the transition probability distribution p(st41 se, a) e¢ RC R is the reward function that determines the goal of the agent ¢ 7 € [0,1] is the discount factor for cumulative reward computation The learning model and decision-maker is the agent, and the remaining elements outside the agent comprise the environ- ment. The experience of an agent is a sequence of interactions of discrete time steps ¢t = 0,1, 2,3,.... When an agent interacts with its environment, it observes a state s, at time step t emitted by the environment, decides the action a; based on the observation s, and responds to the environment. Given the state of the last time step s, and the action a;, the environment transitions to the state of the next time step s,;;1 and sends an immediate reward r;,, back to the agent. Without further specification, s; contains sufficient information for the agent to decide the best action. The agent learns by collecting new experience data and optimizing a policy 7 for action selection. We consider the finite episodic MDP where the state space and action space are finite. An episodic MDP has finite length of sequence. A finite MDP has finite state space and action space. This is a realistic setting for generation tasks. For example, in de-novo molecular design, the action space is the set of all sub-part molecule embeddings, which includes atoms and bounds between atoms, and the state space is concatena- tion of actions, forming a string of aotms and bounds. Each molecule can be represented as a finite character sequence, therefore it is episodic. The atom space and the space of bounds between atoms is finite. Another example is image generation. The common approach is generate RGB pixels that have discrete and finite spaces. Given the limited size of an image, the state space and action space of image generation is finite as well. In an episodic MDP, the environment resets itself after transitioning T steps. The T-step sequence is called an episode. The goal of an agent is to achieve the highest cumulative rewards from the environment in an episode. At each time step t, the agent should select an action that achieves maximum of the cumulative rewards. The agent maps a state to an action by a deterministic policy or a probability distribution m(az|S,). Note that the cumulative reward is termed as a return at time step t, R, = foo YT ket which means that JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 RL Algorithms Related Works Value-based Q-learning [13], DON Soft Q-learning [47) (7) Policy-based Polic NFORCE radieni Actor-Critic iy TRPO (49) > (3). PPO (9 ABC [51] (51) bPG {52}, SAC {53} TABLE II: Model-free RL algorithms. Hybrid Methods the agent should consider future expected rewards instead of the reward in the current step. y is the discount factor that decreases the impact of future rewards at an exponential rate. Then the objective of an agent is formalized as nm” = argmax, E[R|7] (13) One critical assumption of an MDP compared to uncon- strained RL tasks is the Markov Property, where D(St-41; 741180; G0, $1; 1, wey Sty Ut) = p(St41,1t41|St, Gt). It guarantees that state s, and action a, determine the next state s;41 and r;, We select research branches that are related to the gen- eration literature. We go through classic models in the RL research, model-free RL and model-based RL. 2) Model-free Methods: We first describe Model-free RL. There are two main approaches: models based on value functions and models based on policy search. Value functions are variants of the objective in Equation [13] This objective is an expectation over a sequence of rewards while the interaction unfolds step by step. At time step t, the agent’s policy is to optimize the objective in the current time step R;. Under a policy 7, the expectation of R; is the value function V\" (st) = Ex[Re|s¢] and Q*(s;, a) = Ex[Ri|s1, (s2)]- The value functions have particular recursive properties: V™(s,) Esiailti41 + YV7(se41)] and Q*(s;,a,) = Eosyy1 [rest + YQ* (St41, 7(S:41))] which are called the Bell- man Equation which makes it possible to train a model through one-step modeling. Bellman Equation could estimate the value at each time step. Compared to methods that esti- mates the value at the end of each episode, it could decreases the variance of the estimation and make it easier for the model to learn which action has the highest expected return. Bellman Equation leads to the classic Q-learning that has the following learning rules: Q™ (st, at) = Q* (st; ay) + alri — Q*(s1, az) (14) Classic model-free RL algorithms are listed in the Table [Il] DQN extends Q-learning with the neural approxima- tion of Q values. It devises the experience replay method where an interaction history is collected, stored, and used to retrain the parameters of the Q function. This technique smooths the distribution of transitions, which can stablize the training process by randomly sampling from memory instead of correlated episodes sampled from recent states of environments. Different from value-based methods, Policy gradient or REINFORCE is a method that directly maximizes the objective by computing the gradient of the policy: VoE[R\\\\r] = E,~vx9[r(7) - Vo log x0(r)] O41 =O, +0r v(r) - Vo log (7) (15) r(r) - Vo log mo(7 > R, log 76 (az|52) t=1 where 7 is defined as a trajectory variable that includes states and actions. REINFORCE with baseline [13], is defined to sub- tract the value term with a baseline term. The baseline term can be any function, even a random term that takes the state as an input: Gi41 = 6; + a( Rt — b(st)) Vo log 76(s2) (16) where i represents the iteration of model parameter update, R, is the expected return term. b(s;) is the baseline. This results in an unbiased estimation of the policy with a reduced variance [13]. A natural choice for the baseline is the state value function V7(s,) that can capture the value fluctuations of different states. Actor Critic methods have a similar form as variance- reduced REINFORCE algorithm does, which is a branch of policy-based methods. The key difference is that the value function takes part in the value term of Equation which can increase the bias of estimation and accelerate learning by decreasing the variance, which is formulated by + yV™(se41) — V\"(st)) Vo log 76(se) (17) where the value term uses both a policy model 79(s) and an estimated value function V™(s) for bootstrapping. They can benefit from both value-based methods and policy-based method. On the one hand, the selection of actions for value functions requires the maximum values of all actions. When there is an infinite number of actions, it can be intractable to compute all the values. Policy methods can remedy this by action selection instead of value evaluation in value- based methods. On the other hand, policy models suffer from the high variance that can be alleviated by bootstrapping to accelerate learning. In actor critic method, another major innovation is the branch of Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization [50]. TRPO [49] introduces a trust region for the policy to update itself, in which the policy improvement is monotonically guaranteed theoretically. The trust region algorithm is used to compute the gradient of the following constrained optimization problem: m9 (als) Faxa(als) ene) [Dice (®.a(-|8) || toCls)] <6 where the objective function is the actor critic method that uses the trajectories of an old policy 7,,, and computes the expected return with an old Q function Qo,,,. The objective is constrained by an expectation of KL divergence between Orn. =O + (Tey max n ~ 0 Es PO o1a0U~ Toa (18) $.t. Eswpy old JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 the old policy and the new policy. This problem is estimated by a quadratic approximation solved by the conjugate gradient method. PPO uses a clipped surrogate objective to form a lower bound of the objective of TRPO. Replacing the complicated quadratic approximation, PPO utilizes only first- order optimization to solve a constrained problem: MAX Es~po,,.,0~T44 [MIN(Sfactored_poticy($, @)QGo1a(S+), clip(f factored_policy(8,@), 1 — €,1 + €)Qo.14(8, @))] (19) —to(als)_ This formulation where ffactored_policy($,4) = Fa,,,als)\" substitutes the KL divergence constraint as a clip operation to control the divergence between 79,,, and 7@ that enables the algorithm to replace the complex optimization process. Another line of research in actor-critic models is to accel- erate training through parallel computation. The asynchronous advantage is actor-critic (A3C) is the representative example. The policy and value parameters are updated asyn- chronously. The model does not require a replay buffer because it runs multiple agents in parallel with different exploration policies that can stabilize training. It achieves good performance increases compared to DQN, Sarsa, and n-step Q. Apart from value-based and policy-based methods, hybrid methods tend to integrate the advantages of both methods to strike a balance. DPG and DDPG Silver et al. proposed the Deterministic Policy Gradient (DPG) algorithms. It allows a more efficient estimation of policy compared to policy gradient. DPG does not estimate the value Q” by the Monte Carlo method, it uses an actor-critic method to estimate the Q value and compute the gradient of actions directly by (20) a=r6(8) The difference between Monte Carlo based policy gradient and actor-critic is that the Monte Carlo method requires to estimate the expected reward of a state by averaging all returns of visits to a state. This requires to iterate from T\\' to 0, collect returns, and compute averages. Actor-critic is more easy to compute by the Bellman Equation. Lillicrap et al. extend DPG to Deep DPG (DDPG), which combines training techniques from DQN (46). DDPG incorporates experience replay and a soft target that updates the model parameters with a control variable to slow down the parameter changes. Soft Q-learning derives an energy-based policy for con- tinuous states and actions by modeling the policy distribution as a Boltzmann distribution. The reward function is modified to Re = Dypcg * (regtgs + H(r(-|se-4041))) Soft actor- critic (SAC) integrates the Soft Q-learning into actor-critic methods and optimizes towards the direction of rewards plus entropy of policy distributions. 3) Model-based Methods: In RL, models come from prior knowledge or learning. For example, for an agent playing Go, the rules of Go are fixed, thus it’s possible to get a perfect environment model by programming. Influential works like AlphaGo constructs a Monte-Carlo Tree for forward prediction. Sometimes it’s not feasible to get a perfect model. VJo = Esxaz | Vote(s) VaQ\"(s, a) In AlphaGo [55], policy learning is integrated with Monte- Carlo Tree search for the Go game. Generally, Monte-Carlo Tree search creates a tree whose nodes represent states in RL. The expansion of the tree is exploring actions and new states. Decision making is based on values on nodes. AlphaGo adds a policy network and a value network to the tree. During inference, nodes are explored and values are obtained along the tree structure. The parameters of the policy network and the value network are updated in training by policy gradient and mean squared error respectively. Actions are selected with three factors in consideration, a Q-value, a probability and a number of traversals. The Q-value is computed by mixing a state value and a reward collected from random fast rollouts. AlphaZero focuses on self-play to learn a policy from scratch using an adapted version of AlphaGo models. Dyna Q-learning employs the model with domain knowledge as a predictor and data augmentor for the model- free policy, combining trial-and-error, a domain knowledge model, planning, and reactive execution into one algorithm. The dynamics model generates pseudo-experiences that are incorporated into policy training. 4) Comparison Between Reinforcement Methods: Given the fact that there are various types of RL methods, it could be difficult to select from them for a specific generation task. Generally, model-based RL requires to use a world model or learn one, in contrast to model-free methods. DQN is more suitable for discrete actions and policy gradients-based methods and actor-critic methods could handle continuous space. TRPO ie PPO enhance the stability for learning. DDPG [52] learns a deterministic policy. We suggest that researchers should start with basic algorithms like DQN or policy gradient, then investigate towards more complex algorithm training method. From the survey we observe that more works employ policy gradients than DQN. This could originate the characteristics of policy gradients and DQN. For example, PPO is popular in fine-tuning a large language model, while a recent work shows that the performance of PPO could be matched by well RLOO, a well designed REINFORCE algorithm by investigating the components that works for large language model pre- training. This work argues that the PPO aims to improve stability of training for environments exploration with high variance in RL tasks, while it is not suitable for RLHF fine-tuning which has a relatively stable initial distribution. It replaces PPO with REINFORCEMENT algorithm without partial sequence reward learning which accelerates learning and achieves better performance. C. Framing Generation tasks as Reinforcement Learning Problem We use Figure [2a] to show how a generator could be framed as a reinforcement learning agent in applications. Reinforce- ment Learning problem is typically defined as a MDP as shown in Section [II-B] which is typically a sequential decision maker. therefore, we use the agent to generate a sequence @1,U2,23,...,%,. At time step t, the previously generated actions x, ..,%,_, and the task-specific context form the data JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Environment Task-specific Context Action (a) Reinforcement learning agent as generator. The Signal Reward Function (b) Non-differentiable setting for generation action is integrated into the observation in the next models. The blue boxes refer to steps that might step. cause non-differentiable condition. Dashed lines represent the block of gradient and the green line show that RL uses signal to train the generator. Fig. 2: (a) Framing generation tasks as reinforcement learning framework; (b) The reward computation is non-differentiable. of the environment. For example, when the application is a visual question answering, 71, ...,;—1 are answer tokens, task specific context includes the image and the token sequence of the question. Ill. BENEFITS OF RL-BASED GENERATIVE MODELS A. Solving the non-differentiable learning problems One major use of reinforcement learning is that it can prop- agate gradients through non-differential modules. This extends the capability of neural networks because it allows the model to be trained when discrete modules exist in the computation pipeline. This characteristic is supplementary to supervised learning and unsupervised learning objectives, which both require a differentiable training pipeline. In this section, we introduce two classes of non-differentiable problems. 1) The generated variable is non-differentiable: Discrete values are prevalent in various generative applications such as computer vision, neural language processing, and molecule generation. In language applications and molecular design, elements of text and molecules are tokenized and embedded into high-dimensional space in order to capture a_ better representation. The tokens are discrete values or one-hot vectors. In computer vision, a common format of images is the RGB format which comprises discrete values of three color channels. Although it is feasible and easy to normalize the discrete values that are fed into a continuous generative model, transforming discrete values into continuous values leads to adverse effects such as weaker robustness (59). Reinforcement learning is a suitable tool for such problems. The policy gradient method is a widely adopted approach in the field of machine learning. The formulation lacks any explicit constraint governing the relationship between the gradient Vo7e(s) and the reward r(r). The utilization of a reward signal for policy training has been explored in previous works such as (60}-(62}, circuvmenting the differentiable requirement of supervised learning. For example, BGAN aims to tackle the inherent limitation of GANs in handling discrete data due to the absence of differentiable conditions. This is achieved by establishing a connection between policy gradient and the GAN objective. The data distribution is opti- mized by Monte-Carlo estimation,thereby mitigating variance in the policy gradient. In language modeling, [61] borrows the SeqGAN model into a visual dialog system. The generative agent outputs non-differentiable word sequences. Hence, policy gradient is leveraged as a means of facilitating knowledge transfer. Moreover, the reinforcement learning agent can control the training instead of being the generator. In this context, the control values can be discrete. For instance, in InfoNCF (62), the policy gradient is adopted to optimize the number of evaluation functions for normalizing flows in the latent space. 2) The training objective is non-differentiable: Apart from the generative variable, the training objective can be non- differentiable. Take policy gradient as an example, it allows directly injecting a non-differentiable objective as reward without further constraints. Widespread evaluation metrics for machine translation and text summarization are good examples. For example, MIXER proposes to address the exposure bias between the training and testing phase in sequence generation. Consequently, it uses test metrics, BLEU JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Environment Task-specific Input RL-based Generator Reward Function Hand-designed Rules Trained from data Fig. 3: RL can introduce new signals by flexible reward functions TABLE III: Methods to introduce new training signal What signals could be incorporated by RL Related Works Probability from a discriminator Task-specific Hand-designed Rules Distribution Discrepancy Train a reward model with human labelled data and ROUGE [66], to directly optimize the model. This section overlaps with section |I and we leave other related discussions later. B. Introducing new training signal As described in the last section, reinforcement learning methods can be employed for non-differentiable problems that exist in generation applications. In Section [IMI-AT] policy gradient has no requirements between reward and policy, in contrast to supervised learning or unsupervised learning, allowing more flexibility. This flexibility exists in most re- inforcement learning methods in Section Thus it is straightforward to design useful reward functions as additional training objectives, which can incorporate various training signals into the generation process, making RL an influential approach in generation model and application. We demonstrate four major approaches in this section, as shown in Figure [3] 1) Reward by Discriminator: From the standpoint of train- ing signal, the discriminator component within the Generative Adversarial Network (GAN) architecture fulfills a role akin to that of a reward in the context of reinforcement learning. SeqGAN (63) first proposes to exploit this similarity by introducing a GAN to generate the sequence tokens. The reward signal is derived from the output probability, which serves as a discriminative measure between real and generated samples. Subsequent studies have expanded upon or altered the aforementioned framework through the utilization of a meticulously crafted discriminator the development of novel reward formulations (68). the implementation of actor- critic techniques (69), the incorporation of rank formulations , the utilization of multiple discriminators and other related modifications. Objective-reinforced GAN extends SeqGAN with domain-specific objectives to the reward to adapt the generated samples towards the domain-specific direction. Adversarial rewards are also employed in GRL that takes the differ- ence between two probabilities E,.p,,,, D(@) — Exxny D(2). MaskGAN replaces the baseline in REINFORCE by a critic, forming an actor-critic algorithm for text generation instead of a policy gradient in SeqGAN. It uses an in-filling task to train the agent and uses the probability of real words in the discriminator as the reward. SAL changes the reward function with comparison discriminators. The discriminators take a pair of samples (21,22) as input, which is collected from the current generated sample and previous ones. Three types of discriminators D‘>), D‘<), and D‘~) are defined to describe the relationship between the quality samples. Using coefficients for a better balance between exploration and exploitation has been taken into consideration. RankGAN follows the combination of GAN and RL, which substitutes the discriminator with a ranker and the reward is provided by a rank score estimated by the ranker given a sentence and a reference set and a comparison set. The ranker is trained to work as a discriminator by increasing the score of sentences from the dataset and decreasing the score from the generator. Li et al. (98) introduce adversarial learning into the RL-based dialogue generation framework, where the reward is defined as the score produced by a discriminator on the dataset consisting of a human-generated response or auto-generated response. This method uses the RL framework to guide the generated texts by human text distribution. To prevent deteriorating the model quality, a teacher-forcing method that includes human supervision in discrimination training is adopted. ColdGAN [72] explores the exposure bias problem on the GAN-based model in the text generation tasks. It analyzes the impact of randomness on discriminators and finds that bad discriminators might mislead the generators to low-quality areas of the parameter space. Therefore, it integrates the constraints of old policies as an importance sampling strategy to balance the impact of the discriminator. It also proposes a policy merge method for a cautious generative process. MaliGAN [73] combines maximum likelihood with gradient descent. It hypothesizes that the discriminator is easy to learn and can get optimal results. Under this assumption, the following equation JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 holds, 1 Z@) E, [r(D) log po(2)] where Z(0\\') = E,[r(D)] TextGAIL incorporates a contrastive discriminator where the input includes the previous sequence as well as real data or generated data. The discriminator is required to evaluate the relative realness between sequences. It also employs a PPO to decrease the variance of the RL training. 2) Reward by Hand-designed rules: Novel metrics or heuristic functions are intended to provide incentives for training, and as far as we can tell, the majority of RL algorithm implementations fall within this branch. Hand-designed rules could lead to non-differentiable objec- tives . Like MIXER [64], SCST defines the reward by the performance of the current model under the inference algorithm, including CIDEr [102], BLEU4 [65], ROUGEL [66], and METEOR [103}. Tac Tg incorporates type auxiliary guiding for code comment generation by rein- forcement learning. The Ton takes BLEU [65] (65) and ROUGE [66] as rewards. ReGen ] proposes to use reinforcement learning for non- ‘itfeouiabie evaluations like BLEU (65). METEOR [103], and chrF++ to guide text generation. One line of research is incorporating testing-time metrics as a reward. This line overlaps the branch of non-differentiable objective when re oan metric is non-differentiable, such as ROUGE [64], [105j-(107] and BLEU [64], The utilization of vote. -time metrics also enhances models to combat the discrepancy between the training objective and testing objective. MIXER (64) proposes to address this problem in sequence generation by reinforcement learning. It applies REINFORCE to train an agent whose actions are next words given the current time step context. The reward is the test metric for neural language processing models. Wan et al. introduce an actor-critic algorithm to code summarization by leveraging the exploration feature in reinforcement learn- ing. Apart from testing-time metrics, task-specific reward func- tions are also popular. Li et al. propose to use a policy Ep, {log po(a)] = gradient with three considerations in dialogue generation, ease of answering, information flow, and semantic coherence. The ease of answering is measured by the negative log-likelihood NE >, log p(b|a) where b is a human constructed list of dull responses that contains sentences like “I don’t know what you are talking about”, and a is the generated sentence. The information flow is constructed by penalizing similarity be- tween two consecutive turns of the same agent. The semantic coherence is measured by the mutual information between this turn action and actions from previous turns. PETAL manages dialog systems by reinforcement learning. It studies dialog systems in real-world coffee order systems. The reward contains multiple items, representing personal reward and general reward. The personal reward reveals the interaction between the agent and the user, such as accepting or rejecting the agent’s suggestion. The global reward includes motivation for getting the user’s information, payment, and shortening the dialog. In modeling, it breaks down the value function into two parts, one standing for general value, and one for personal preference. This achieves a better transfer effect when a model is tested on new users. Zhao et al. employs RL to interact with a database in a dialog state tracking and management task, which is beyond the capability of supervised learning on this task. The agent is not only asked to respond to users but also query from a database to better manage the dialog. Databases are used to generate synthetic data, which are combined with real data for value function learning and policy learning, like in Dyna Q-learning [57]. 3) Reward by Distribution Discrepancy: Distribution dis- crepancy can be a useful signal to be integrated into rewards. Maximizing the divergence leads to more informative gener- ative data (si), minimizing the divergence between generated distribution and some distribution could regularize the gener- ation [82]-[86]. CLARIFYDELPHI [81] uses RL to generate clarification questions to elicit moral judgment of models. The question is generated by a PPO network whose reward is the divergence between two different moral judgments of the questions. An answer simulation framework is designed to get the divergence between different answers. Motivated by the policy gradient, Fan and Lee utilizes distribution discrepancies to optimize DDPM sampling with shortcut fine-tuning. The goal is to use gradient-like optimiza- tion algorithm to explore alternative paths to discover more efficient paths and boost speed of sampling by replacing the backward process of DDPM during fine-tuning. The policy is initialized with a trained DDPM generator, which is then guided by a generalized critic function that serves as a measure of discrepancy between the distribution of generated data and real data, namely a generalized divergence. The gradient of a DDPM sampler is formulated to a REINFORCE with baseline mentioned in Section |II The reward is computed based on the generalized critic function that incorporates 1- Lipschitz functions as regularization. The critic function is instantiated as a neural network which is trained to minimize the reward (maximize the discrepancy between distributions) and the policy is trained to maximize the reward (minimize the discrepancy). When applying reinforcement learning algorithms to gen- JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 eration models and applications, a line of research combines reinforcement learning with supervised learning to guarantee that the model is adjusted by the reward signals but does not drift away from the supervised training objective to prevent the model from generating highly rewarded but unrealistic results. The divergence between a generated distribution and the distribution defined in the dataset is explored [83]-[86]. KL-control is a technique for non-Markovian systems to minimize deviation from a prior policy 8]. Sequence Tutor (83) integrates a KL control method in order to maintain a policy generation that remains in proximity to the pre-trained language model. The reward function incorporates previous knowledge derived from a pre-trained recurrent neural network (RNN) model.. Ziegler et al. [84] incorporate human prefer- ence learning into pre-trained language models. It involves KL-control for coherence and topicality. Jaques et al. inject KL-control into discrete Q learning to impose an entropy regularization. GOLD aims to address two problems in the MLE training paradigm in text generation: diverse but low-quality samples and exposure bias. Unlike the studies mentioned above, it adopts an offline reinforcement learning algorithm. It uses a weighted policy gradient where the weights come from the training set policy because, in offline reinforce- ment learning, the agent cannot sample trajectories to estimate the weight. The weight reveals the conservative method: keep the actions on the test dataset similar to the training dataset. For reward, it uses training trajectories to approximate the probability distribution of human preference. It combines three kinds of rewards that use a one-zero reward, the product of MLE probability, and the sum of MLE probability. 4) Reward by data-driven model: With the flexibility of the reward function, it is also a good way to incorporate models learned from the reward function. It is feasible to incorporate various guidance into reinforcement learning by training a reward model. This branch expresses a similar idea as Inverse Reinforcement Learning (IRL), which can be integrated in generative models in two ways. One is directly embedding previously defined rewards such as BLEU into a model. Shi et al. (90) conduct experiments in text generation and empirically prove its effectiveness. They adopt the maximum entropy IRL to model an approximated reward function. The training process of the reward model increases the rewards of real texts and decreases the rewards of texts that are sampled from the approximated distribution by a generator with importance sampling. Furthermore, an entropy term is added to the reward function for the agent to prevent premature mode collapse and increase the diversity of generated texts. The other way is to learn a model for human preference [109], [110]. This path finally leads to the emergence of Reinforcement Learning Human Feedback (RLHF) which is integrated into the large language model research and harvest powerful models like ChatGPT [7]. RELIS [109] proposes to learn a reward function from learning to rank objectives on a document summarization task. Human pref- erences of two summaries in the form of ranking are col- lected and train the reward model by three types of loss: cross entropy, marginal ranking, and an improved marginal ranking. It is theoretically proved that the agent converges to a near-optimal solution. Nguyen et al. study the simulated human feedback in the form of ratings in neural machine translation. They are motivated by the fact that human feedback is not perfect. For example, expert ratings cannot perfectly match the goal. There are also granularity, variance, and skewness problems in the collected ratings. Therefore, they propose to simulate human feedback and address the problems mentioned above. They map the feedback to binned values, use a linear approximation to deal with large variances in middle ratings and employ a skew perturbation for harsh and motivational scores. The Open AI Reflection team [91] uses human preference to guide language models for summarization tasks. The training consists of three steps. First, trajectories from a trained policy with various baselines are collected and these trajectories are evaluated by humans to rank the best one. Then, they construct a model to learn the rewards that indicate whether the output is better. Last, they optimize a policy given the reward model. It is similar to the step in (90) while it combines datasets from human preferences, which dramatically outperforms existing methods at that time. The reward function is trained by the following function, loss(r9) = Eve,yo,y1,p) llog(o(ro(x, Up) —Te (a, yi-p)))] where x is the text before summarization, y is summarized text, rg is the reward function, y, is the human preferred text. The loss aims to maximize the distance between two rewards. Additionally, authors use KL-control to prevent the mode collapse as well as constrain the policy to be conservative, not generating weird texts far from the original supervised pre-trained distribution. InstructGPT follows the same procedure to fine-tune GPT-3 from human feedback. Results show that it improves the GPT-3 on truthfulness and generalization, and decreases the toxicity and performance regressions. Bai et al. follow the work to test RLHF on a helpful and harmless dataset. They use a PPO to train the model and use the same pipeline to learn a preference model and finetune the language model with reinforcement learning. It tests the model in an iterative online mode of training and shows that it improves the performance of the model. It also identifies a roughly linear trend between the preference reward and the square root of the KL divergence between the policy and its initialization. APRIL combines preference learning with neural TD, an algorithm that replaces the linear approximation in Linear TD with a neural network. Preference learning employs the cross-entropy between true preference and the model used to train a reward model. For the limits of human feedback collection, a pair-generation method is proposed to make the process efficient. The pair is generated on the metric of utility gap, diversity, density, and uncertainty. Given a human text y. the utility gap is used to maximize the gap to get high-quality negative samples. Other three metrics aim to make the selected sample diverse, located in a dense part of the distribution, and uncertain. The neural TD is used instead of DQN because the action space is large and the maintenance of Q-value is expensive. Kreutzer et al. discuss the necessity, challenges, and potential solutions of offline reinforcement learning from human feedback. The JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 4: RL can work as a sampler for models that are hard to sample such as Energy-based Models. The marginal distribution is difficult to sample for the high cost, RL-based agent provides an alternative way to generate sample sequences. THe dashed line represents the potential high cost blocks the generation. necessity of offline RL is that online adjustment of parameters is too risky and potentially out of control. The challenges include questionable counterfactual estimation for the lack of explicit exploration and degeneration problems where low- reward actions are still encouraged during training. Also, reliability and learnability are also discussed. Moreover, the fast development of large language models inspires researchers to use them as a reward function by proper prompting. For example, Constitutional AI (96) proposes RLAIF that trains a harmless but non-evasive AI assistant that copes with harmful queries via expression of objection to these queries. Self-critiques and automatic revisions from an LLM are exploited to modify the dataset and retrain the LLM on it. Then, an agent is trained by preferences given by humans and models. Human provides helpfulness evaluations while the model provides harmlessness evaluations. The label is generated by an assistant model under the context that prompts contain human set principles as well as a set of few- shot examples. Apart from peakiness, Zhu et al. provide theoretical support for RLHF. They provide a sample complex- ity for the union problem of RLHF and max-entropy Inverse Reinforcement Learning. They frame the ranking-based reward model as a Plackett-Luce (PL) model or a Bradley-Terry-Luce (BTL) model, providing suboptimality bound for the reward learning process. C. Sampling Although Energy-based Models, one type of mainstream generative models, enjoy greater expressivity and allow global constraints, they face challenges in producing samples of marginal distribution for the unnormalized distributions in the formulation, which might incur high cost during sampling, as shown in Figure /4| Reinforcement learning could be an alternative way to train a sampler for the EBM. A recent line of research studies the integration of reinforcement learning algorithm to distill knowledge in an EBM into an auto-regressive model by turning distribution matching into the training signal for the RL algorithm. Distributional Policy Gradient (112) is a pioneer work that transforms an Energy- TABLE IV: Methods in Sampling and NAS Methods Related Works Sampling NAS FHT Based Model (EBM) training process into a policy gradient algorithm. Employing reinforcement learning (RL) as a pipe to train a sampling generator, suggests addressing the distribution learning problem in Global Autoregressive Models (GAM), a subset of Energy-Based models. There are two phases to the training procedure. The MLE criteria are used to train the autoregressive factor on the dataset in the first step. The second stage is the focus of (112), where a policy 7 is used to approximate a desired distribution p via the process of distillation, which facilitates the acquisition of the model parameters. The policy employs a distribution match as a reward and is trained using policy gradient via the process of deductive reasoning based on cross-entropy Vo CE(p, 7) = — Exxp(.) Vo log mo(x) (21) 1 Pl = 7 Eawno(-) mola) 2108 74(\") where the importance sampling is used to form a gradient descent algorithm with the distribution match computation as the reward. This step successfully exploits the abundant expressivity in the GAM into an auto-regressive model, where the auto-regressive model could serve as a sampler. The policy gradient method assists the sampling of a generator. In their seminal work, Khalifa et al. introduce a novel methodology that harnesses the power of distributional control in the context of conditional text generation using pre-trained language models (LLMs). In contrast to the approach proposed by Parshakova et al. {112}. this approach leverages importance sampling while replacing the sampling distribution with an optimal distribution. The variable q is subject to updates exclusively when the condition Dxx1(p\\\\|te) < Dxx(plla) holds true. Korbak et al. propose to discover and exploit similar- ities between DPG (Distributional Policy Gradient) and policy JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 gradient. Although the gradient of DPG (Distributional Policy Gradient) cannot be reduced to a policy gradient, the variance reduction technique of policy gradients can be transferred to DPG. Korbak et al. applies KL-adaptive distributional policy gradient (KL-DPG) (113) on code generation based on pretraining an auto-regressive model. Go et al. proposes an f-DPG algorithm that allows using of any f-divergence as an objective to approximate any target distributions. This approach unifies the formulation of RLHF and DPG (Dis- tributional Policy Gradient). The reward of optimization is defined as the negative gradient of f-divergence between two distributions f’ ( mH) ). D. Neural Architecture Search Previous subsections introduce different purposes of re- inforcement learning, bridging the gap of non-differentiable learning systems, incorporating new training signals, and serv- ing as a sampler. Interestingly, the neural network architecture itself can be viewed as a sequence of tokens, therefore being the subjective reinforced generator. Uniquely, the agent itself is a generator but can be applied to almost all feasible tasks which employ neural networks as learners. In this sense, we include Neural Architecture Search (NAS) in this survey even though most applications of NAS are classification tasks. NAS is used for optimizing neural network architecture \\'7|. Therefore, the reward for NAS is usually the task metric. For example, when an agent is optimizing the architecture of a classifier, the accuracy of the classifier is usually used as reward [ . Zoph and Le proposes to use reinforcement learning to guide neural architecture design. They employ an RNN network to generate the architecture description with REINFORCE ENAS (118) improves the efficiency by parameter sharing. Instead of building a network from scratch, it constructs the network on pre-defined convolutional cells. This can reduce the search space. Similarly, MONAS It removes states in the training and just considers actions and rewards. It takes power consumption into reward functions. Various optimization goals such as mixing, threshold, and surrogate metric are considered in the reward computation. IRLAS incorporates inverse reinforcement learning into the NAS. It defines a feature count that maps an architecture into a trajectory of the agent, b= vy, 7\\'(s:), where s; is the architecture information, ~y is the discount factor, 6() is the embedding function. They use the jz to create a linear model for the mirror stimuli function that aims to use the topology of the expert model such as ResNet as the guidance. State space and action space are carefully designed in NAS in order to make the training tractable. Layer parameters that are composited as the element of state and action space is a common choice [122], (125, [129]. MetaQNN con- strains the space of states and actions to make the generation tractable. Rijsdijk et al. applies MetaQNN on the NAS for side-channel-analysis. The reward function is defined con- sidering the guessing entropy with a different number of attack traces. BlockKQNN | employs neural model generation for image classification tasks. It defines a Network Structure Code TABLE V: Methods in Neural Language Processing Sub-areas Related Works Text Summarization Machine Translation Dialog System Human Value Alignment and Constraints Text, Queries, and Knowledge Graph Large Language Model Other NLP Applications (NSC) which quantifies the architecture information such as layer index, operation type, kernel size, and other related nodes in the computational graph. The state of E2GAN is the average value of each sub-module. The action will be how to extend the architecture, The sampling efficiency is also explored. Meta-learning [124], Pay one-shot learning are two examples. CATCH [124] employs a meta-reinforcement learning frame- work to accelerate architecture design on meta-testing tasks. RL-DARTS uses meta-learning as well. The meta- optimizer defines the gradient and a control hyperparameter as the state, the shift of the control hyperparameter as the action, and the performance on a valid dataset as the reward to meta- control the direction of the architecture searcher. DQNAS [127] combines RL-based NAS with one-shot training to get better performance. The key is using one-shot training to transfer weights from some layers that are common to quickly set up the training. IV. APPLICATION Reinforcement learning has been applied to abundant areas. In this section, we organize and classify the literature accord- ing to applications, aiming to provide readers a brief introduc- tion of how RL is applied on different areas. The following sections include natural language processing, code generation, computer vision, speech generation, music generation, AI for science and other small areas. A. Natural Language Processing Natural Language Processing (NLP) is one of the largest application areas of generation models and reinforcement learning. Reinforcement learning is widely employed in vari- ous NLP tasks, like text summarization, machine translation, dialog, etc. We primarily introduce the application directions but may not cover all directions. 1) Text Summarization: Text summarization is the process of automatically generating or extracting summaries from a given input document without losing important information. RL has been widely applied in this task by generating the summary [130], [131] or extracting the summary ; ; [132], [133]. Paulus et al. [130] proposes to incorporate self- 99) critical policy gradient | into text summarization. It uses ROUGE as a reward and trains the NLL objective and RL ob- jective by a weighted sum operation. Wang et al. change the model to a convolutional sequence-to-sequence model for automatically abstractive topic summarization. ROUGE is also utilized in extractive summarization like Wu and Hu (132]. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 RL can be used as a means of hard attention mechanism which the policy outputs a discrete action to select contents, unlike in soft attention where a neural network outputs an at- tention vector that is multiplied with the content. For example, selecting sentences from a document requires hard attention. Chen et al. propose to combine RL as hard attention on the sentence level in the text summarization task. In this work, an agent is a selector who chooses sentences that are valuable to do summarization. The state is the set of documents and the last selected sentence. The action is to choose the next document sentence for extraction. VTMRL uses reinforcement learning as a hard attention mechanism to filter the less topic-coherent and background words in the task of topic modeling. The reward is defined as a sum of a coherence score and a topic overlapping value. Using RL methods for fine-tuning also attracts attention Wu et al. summarize books in a recursive way. This work uses RLHF as training method. The key novelty is a book is summarized based on chapters, then these summaries are fed into the model to do the summarization again to acquire the final summary of a book. Recursive summarization could help human to quickly judge the quality of the summaries. ] uses policy gradient to adjust a pre-trained abstractive text summarization model. The reward comes from two discriminators. One of the discriminators classify the generated text and human-written text. The other discriminator constructs a ranking loss to motivate the sum- marizer produces higher probability on human-written sum- maries. controls the length of generated texts by PPO- based fine-tuning on text summarization tasks. The reward function is rule-based and calculates the difference between the generated texts and the predefined length limit including target length, upper bound length, and lower bound length. 2) Machine Translation: Reinforcement learning is applied to neural machine translation to bridge the train and test metric gap ; , to direct translation with sentiment preserved aac deetty sentences [139], to balance human efforts in the interactive system [1 Pan to change the sentiment [142], to diversify the translation output [143], or to guide curriculum design [144]. Wu et al. uses BLEU as a reward in a systematic comparison of decision factors for RL-based NMT. Pham et al. plus BLEU with a HIT reward that counts the coverage of translated results and the annotations into the guidance of training. Kumari et al. [138] applies RL in sentiment preserved review translation. The model adopts actor-critic algorithms. The reward is constructed on a content preservation SBLEU and a sentiment reward that is a dot product of the sentiment vector and output probability distribution. DRESS proposes to employ the policy gradient on the sentence simplification tasks by introducing three desirarta simplicity, relevance, and fluency as rewards. BIP-NMT adopts an actor-critic algorithm for translation model training, which uses a threshold of action entropy for human feedback acquisition and simulate human feedback for evaluation. Zhao et al. uses actor-critic. The reward is BLEU and the negative of the times of request for human feedback. It regularizes the policy with MLE guidance of both the right tokens and human feedback. Luo et al. (142] use a similar reward formulation with a harmonic mean of two rewards to encourage the model to improve both sentiment reward and content preservation (coherence) reward. SURF defines a new reward function and explores the asynchronous training framework. The reward is defined as WP X eFGre) 4 ws X eS ESS) Sentence Fluency and Sentence-level Semantic Similarity (SLSS). Sentence Fluency is an average log-likelihood of probabilities given by a pre- trained model. SLSS is a cosine similarity between embed- dings of input and generated sentences. uses RL to guide the curriculum design for the machine translation task. The action is selecting a batch of samples from the exposed training dataset. The state is a feature vector that contains the samples for selection and measurements that reflect the performance of the machine translation model like sentence-level log- likelihood. The reward is the performance improvement on the validation dataset. 3) Dialog System: Dialog systems or conversational agents are a complicated but fast-developing area in recent years. The influential InstructGPT (92) is trying to tackle the difficult open-domain dialog system [145}-(147], [196]. Apart from it, a task-oriented dialog system is a parallel but important . Reinforcement learning has been explored in both. Also, a hybrid dialog system emerges recently [1 MILABOT [145] tackled the Amazon Alexa Prize compe- tition to learn an open-domain chatbot. RL is used for model selection. The reward function is a linear regressor trained from collected ratings. PRG-DM (146) fine-tunes two policies to generate posts and responses for personalized response gen- eration by policy gradient. tests the open dialog system by interacting live with human. The system is trained with offline Q-learning with KL control to regularize the policy’s action from generating unrealistic language sequences. Sun et al. propose an imitation learning approach for complex dialogue agents. The imitation objective is defined by Donsker Varadhan’s representation of KL divergence to ease the hard problem of high dimensional optimization. HCN [148] explores training a dialog control agent with reinforcement learning. To avoid degenerated actions, it iter- atively trains with policy gradient and supervised loss. Lewis introduce RL on a negotiation task where two agents both have a set of objects and try to exchange objects to make each type of object should belong to one agent. The reward computes whether an agreement is met. Yarats apply a hierarchical generation framework and substitute the agent’s state from text tokens to latent variables to improve the effectiveness of long-term planning in this game. Li et al. trains a DQN agent to do task completion in neural dialogue systems. An example is to ask an agent to book a ticket. Jaques et al. integrates implicit human pref- erences into a hierarchical open-domain dialogue generation by reinforcement learning. It employs an off-policy batch RL approach with dropout-based uncertainty estimates. Co- Gen proposes to match the latent space of actions in the external database and natural language response in conversational search. It uses RL to fine-tune the pre-trained language model with BLEU as a reward. This fine-tuning helps O JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 the model achieve better results. TrufLL [152] uses RL for answering questions. Unlike the normal fine-tuning method, it proposes using a pre-trained language model as a truncation module that takes the action space of the agent as input to make the decision-making in a large action space feasible. applies offline RL (Behaviour Cloning) in dialog agents. The reward is given by an external program which checks whether the user goal is satisfied. This work trains on a dataset that includes the self-generated action candidates which are evaluated by a critic network to address the sparse and high- dimensional action space. [154] adopts a hierarchical RL for the medical dialog system. The reward motivates the agent to collect more information about the disease and produce a successful diagnosis. proposes a hierarchical framework in which a high-level policy determines the semantics at the utterance level that is in the latent space and a low-level policy that outputs the next word. The reward is designed to minimize repetitiveness and toxicity and enhance consistency and sentiment. incorporates A3C [ST] into negotiation dialog systems. The reward motivates the success of reaching a compromise and penalizes the result that no deal is struck, incentivizing both agents to negotiate and achieve a both preferable result. READER is designed for mental health counseling agents by generating dialogue in a hybrid way. The agent needs to understand various contents from users but provides effective and appropriate responses. 4) Human Value Alignment and Constraints: The output of generators is not well matched with human values. Models sometimes have hallucinations, generating fake information that they do not understand at all. Sometimes models are impacted by datasets and spit out sentences that do not match human values in some cultures. Reinforcement learning can be used to adjust the model to work better on value matching 158], impose constraints , or even help people to combat problems like fake news . SENSEI proposes to use actor-critic to align text generation with human judgments. The reward is predicted by a binary classifier trained a human- labeled text data. RCR [159] uses a discriminator to model the violations and computes a penalty accordingly. This penalty is added to the reward to regulate the actions of the text generator. FakeGAN [160] trains a deceptive reviews classifier with a two-discriminator GAN model. Although it addresses a classification problem, the method contains generating de- ceptive reviews, which is a generation subtask. 5) Text, Queries and Knowledge Graph: Natural language is a good interface for humans but not for search engines and databases. Therefore, translating natural language into struc- tured queries and knowledge graphs poses a long-term problem in NLP. Reinforcement learning is used in these applications to optimize the query quality | create knowledge graphs 101}, complete them (68), and even causal graphs [162]. Mo- hankumar et al. incorporate human preference rewarded RL in query rewriting for advertising. The reward is provided by a fine-tuned model based on another model pre-trained in 100 languages. ReGen | proposes to use reinforcement learning to guide the text and knowledge generation based on large pre-trained models. GRL (68) integrates GAN and RL in the knowledge graph completion. The model is constructed on graph neural networks and LSTM. The state space includes a combination of both entity space and relation space in the graph. The action is to select the neighboring relational path to extend the path. CORL [162] proposes to generate a causal graph by reinforcement learning. The state is defined as an embedding of a causal graph node, the action is also in the node space but takes order constraints into account by imposing a mask to force the parent nodes chosen from a certain set. The reward is defined as Bayesian Information Criterion and the episodic reward and dense reward settings are explored. 6) Large Language Models: Recent years have witnessed a big rise of large language model 3|]. We collect and or- ganize advances of RL applications in this direction. Recently, to further enhance the capability of the large language model, multi-turn reinforcement learning is proposed where the reward is not spontaneous in contrast to single-turn RL. incorporates self-correction into LLM. To make the agent revise its action, the action (generated sequence) is piped into itself to learn a correction step. The reward is derived from human preferences over two multi-turn conversations. It adopts a two-stage tuning scheme to model the two attempts of the inference. The first attempt produces a response, then revises or corrects the response in the second attempt. The first stage pushes the second attempt for correction and keeps the first attempt frozen. The second stage jointly optimizes the two attempts with reward shaping to penalize falsely changing the first attempt. [164] addresses the limitation of single-turn RLHF by introducing a multi-turn algorithm that takes the history of the multi-turn interactions as states, outputs the response as actions, and the reward is generated by human preference. GAE is used to optimize the policy and KL divergence is leveraged to constrain the optimization near the original model to prevent policy collapse. [165] transfers hierarchical RL into mutli-turn langugage model training. The state space and action space are similar to [ . The reward is the success of long-term objective. For example, the agent is asked to search a book online, the success signal would be 1 if the book is searched. The high-level action is utterances in response of the state, and the low-level is fine-grained response of the high level action. The model could be combined with either online RL algorithms or offline RL algorithms. New reward formulations are proposed to enhance the effectiveness . uses a language model and a reinforcement learning agent to generate stories towards specific goals. The language model is responsible for generat- ing multiple goal-conditioned continuations of a story which are selected by the knowledge graph-based reinforcement learning agent. The selected sequence will be appended after previous contexts for next continuation generation. The reward caters for goal achievement and coherence reward. investigates the effectiveness of fine-grained reward for RLHF. The responses generated by an LLM is segmented into sub- sentences. Fine-grained reward entails human labeling these sub-sentences on the exact problems of the LLM response, such as inveriable facts, irrelevance, or information incom- pleteness. [168] proposes RLSF which exploits the symbolic JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 feedback system to guide the fine-tuning of an LLM. A certificate of each response the LLM produces is generated by a symbolic reasoner, which is fed into the reward model for PPO-based fine-tuning. This work applies the method to the task that transforms a psuedo-code to C++ code. [169] proposes a token-level reward model unlike common instance- level reward model for the LLM tuning. The token-level capability is acquired by distilling a generative correction model that generates a probability in deciding a reward for each token. The reward model is combined with PPO to fine- tune the LLM. [170] uses token-level feedbacks for LLM fine-tuning. The reward function provides token-level reward by computing the difference between probabilities before and after the word is generated. To increase robustness, this work also uses a First quantize then noise” strategy that uses quantized rewards and inject noise into rewards but keep them in the interval. [171] composes an algorithm that can balance multi-objective during PPO-based fine-tuning. The reward function is a weighted average of all specific rewards. The method uses mirror descent and smooth to update the weights which guarantees that mitigates over-focus on single objective. This method is used to make the reward composition fair and stable. Apart from reward function, new advances in objective functions vet 58}, (172)- [179], reflection capability (199). (200). scaling [201], ensemble [202] are proposed to further i a large language models. he to RLOO [58], ReMax is proposed to leverage a variant of REINFORCE to aE stitute PPO in RLHF [91]. [203] proposes to enhance the reasoning capability of LLM on techniques like Expert iteration, Return-conditioned RL, and outcome-based reward modeling. DRO is proposed to exploit single-trajectory dataset in LLM fine-tuning. Single trajectory dataset means that one data point consists of a prompt, a response, and human preference. In contrast, DPO requires each data point to have two responses for preference learning. DRO achieves this by changing the objective by a MSE loss between reward, value function, and the KL divergence regularization term. fine-tunes an LLM with an advantage-based of- fline RL on pre-existing dataset. The LLM is trained with the importance sampling on data collected by a reference LLM. To increase the robustness of the fine-tuning, this work only selects data point with a positive reward for training. fine-tunes LLMs with Inverse RL. This work creates a link from inverse soft Q-learning to a regularized MLE objective, which enables to trade-off the impact of regular- ization that focuses more on long-term impact of reward on action sequence, increasing the diversity of the action. explores human preference learning in the pre-training stage instead of common fine-tuning stage. It tests five objectives of pretraining, including Reward Weighted Regression (RWR) and Advantage Weighted Regression (AWR). RWR a variant of policy gradient method. AWR substitutes the reward with estimated advantages. proposes to incorporate offline Q learning into the language generation learning. This work combines the utility maximization of reinforcement learning and stability of the supervised learning by Bellman backups of a value function and a Q function, forming an implicit value function learning. The token is generated based on the history of all tokens instead of individual transition. The reward is based on downstream tasks. explores fine-tuning the LLM by three different offline reinforcement learning algorithms which can save computing resources compared to PPO. The first method selects high quality response and drop others. The second method is similar to policy gradient with exponentiated rewards. The third method adds reward in the prompts for better alignment. Reflexion proposes verbal reinforcement learning which does not train or fine-tune the LLM but using the in-context learning ability to form a reinforced loop to achieve the correct answer for a prompt. The key insight is creating a reflection loop, where an actor language model gets observations (prompts) from the environ- ment and reflections from a memory buffer which adds verbal reflections of actions and outcomes iteratively. The outcome is generated by an evaluator language model and is transformed to reflections in verbal form by a self-reflection language model. The actor language model try multiple times until the answer is correct or the maximum trial limit is reached. Similarly, leverages the contemplation ability of a LLM. This work shows that the LLM is good at self-evaluation of its own generated texts. Therefore, the self-evaluation is used to assign rewards for further reinforcement learning fine-tuning. addresses problems of scaling RLHF to large models up to 70B (70 billion parameters). This work employs distributed training strategies entails data parallelism, model sharding, and pipeline parallelism to efficiently train the large-scale models. The result shows that PPO is effective but sensitive to hyper-parameter settings. Offline RL method is easier to train but achieves sub-optimal performance. proposes to integrate model ensemble into the reward model of RLHF. This work discusses three types of reward model ensembles: single reward model ensemble, linear-layer ensemble, and Low-Rank Adaptation (LoRA)-based ensemble. The single reward model ensemble uses multiple reward models directly. In the linear- layer ensemble, reward models share the same Transformer model and predicts different rewards on a single linear layer. LoRA-based ensemble integrates a LoRA layer before the sin- gle linear layer. This investigation aims to efficiently compute the reward model for scaling of large language model fine- tuning. RL are also used in prompt optimization [180|-(184]. employs RL for prompt optimization. The state space is an initial prompt and corresponding task description. The action space is to select or modify the prompt by generating prompts. The reward is defined by the downstream tasks and is nor- malized to stablize the training. utilizes RL for prompt editing in test-time. This method uses the last hidden states of the pretrained language model as state representations, the LLM can choose the objects from instruction, in-context examplars, and verbalizers. The reward formulation uses the same one as in [180] but considers difference between edits to motivates the LLM to accumulates reward at each edition. Similarly, [182] refines prompt towards truthful, benign, and helpful outputs of target LLM by reward formulation. This work employs open-source models and publicly available dataset to construct reward models for quality, safety, and JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 jailbreak prompts. Prompt-OIRL proposes a query- dependent prompt optimization approach, which is different from [180], that both search for distributional optimal prompt (expected quality of answers). [ prompt. Besides, this work combines an offline reward model for inference time evaluation to save the cost of interacting with LLM for rewards. In [184], reinforcement learning is utilized to optimize the input space of an LLM. This method separates the input of an LLM into task description and individual features. An reinforcement learner is created for selecting the optimal set of individual features to guide the LLM towards right responses. The output of the reinforcement learner is integrated into the LLM for downstream tasks. The reward is formulated as the action likelihood of the LLM when the individual feature is in the valid subset. Safety concerns are discussed in RL fine-tuned LLM [ demonstrates that the long response bias could be mitigated by Product-of-Experts, a reward function is proportional to a product of the human preference reward and the length bias reward. A bias-only expert is introduced to capture the length bias reward. The bias-only expert is trained with perturbations to decrease the impact of semantics. proposes SafeRLHF to balance helpfulness and harmlessness in LLM alignment. The reward model is separating helpfulness and harmlessness by taking harmlessness as a constraint em- bedded in the Lagrangian optimization. The reward models are trained separately with different datasets based on preference- based function and are fused during PPO-based fine-tuning. The Lagrangian multiplier is updated by a moving average of harmlessness cost function. demonstrates that RL fine-tuned LLM still suffers from semantic vulnerabilities where the radicalized response could be elicited with designed prompts. [188] shows that LLM might be attacked without compromising original safety alignment objective. This work attacks an LLM by data poisoning. Specifically, it flips the label of the data for RLHF’s reward function but filters out poison data that could induce significant changes. The attacked model generates longer response when specified trigger word appears. attacks the LLM with an RL trained LLM, where the attacker LLM is trained to search for jail-break prompts that leads the innocent LLM towards targetted at- tacked behaviour. This attack process could be used for Trojan detection by attacking the target LLM and expose the sensitive prompts. 7) Other Applications in NLP: Reinforcement learning application in NLP is a wide area but we focus on typically generating sequences for other purposes like review generation [190], [191], critique generation [107], mathematical prob- lems generation [192], keyword-to-sentence generation paraphrasing ae mutiple tasks [195]. Li et al. ma combines adversarial training and reinforcement learning for review generation of commercial purposes. It uses RL in the same way as SeqGAN [63] do. DP-GAN [191] applies similar framework like in [63], (78). It separates the reward into two levels, word level, and sentence level. The sentence level reward is an average of all rewards of the word in it. The total reward is a product of sentence-level reward and the discounted sum of word-level reward. RL4F proposes TABLE VI: Methods in Code Generation Sub-areas Related Works Code Search Comment and Annotation Generation Code Generation Unit Test Generation to enhance the large language model by critique generation. An critique LLM is used to generate critique for the task LLM. MWPGen generates mathematical problems from a math expression and some topic words. It does the problem generation and then uses the generated problems to get an answer by a neural network-based solver. It defines the reward to check the correctness between the expression generated by the solver and the original expression. Upadhyay et al. redirect a pre-trained language model towards multiple rewards to improve performance. It studies unsupervised controlled text generation and takes text style into consideration. applies RL fine-tuning in unsupervised paraphrasing tasks. The method entails three steps, pre-training, transition, RL tuning. In pretraining, a VAE objective is used. In the transition phase, the agent learns to generate longer sequence without supervision. In the RL phase, the agent is trained to maxi- mize the reward that optimizes the output towards semantic adequacy, language fluency, and expression diversity. investigates improving the memory and time efficiency for RL-based sequence generation. The reward is task-specific, including BLEU [65] and ROUGE [66]. It improves the efficiency by reducing redundant sampling and decreasing the size of computational graphs. This work conduct experiments on machine translation, text summarization, and RLHF. B. Code Generation Given the application of RL on NLP, it is also natural to consider if the coding process can be automatically executed by machines. Within them, RL-based models perform well and improve their performance in multiple directions. 1) Code Search: Code search takes natural language text as input, and searches for a code snippet that can solve the problems presented by the text. RL is implemented to use Cuacor, AT or enhanced query for code search. CoaCor [204] applies RL in code annotation, a form of code generation for code retrieval. They use an actor-critic algorithm where states are code snippets, actions are generated code, and rewards are defined according to code retrieval requirements. QueCos uses the policy gradient method in the code search application. The agent learns to gener- ate queries to get high-quality matched code snippets. The agent is guided by a ranking reward and a BLEU reward. [206] applies RL-based fine-tuning in query rewriting for E- commerce applications. The LLM takes a user query and pattern as input, and rewrites the query to better align with the product catalog’s terminology. The capability of the LLM enhances the semantics understanding of the query. The LLM does Supervised Fine-tuning (SFT) and then uses RL to fine- tune for better search performance measured by a relevance reward that measures the relevance between the query and the JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 rewrited version, a productive reward measures effectiveness and an increment reward measures what the rewriting changes the result. 2) Comment and Annotation Generation: Reinforcement learning can be used for code summarization as well [100], 207], which takes code as input and outputs natural language to summarize the usage of the code. Wang et al. to use RL to guide code summarization with a hierarchical attention network. It uses RL to combat the exposure bias in the code summarization dataset. The reward is BLEU metric, and the algorithm is actor-critic architecture. TAG 100] incorporates type auxiliary guiding for code comment generation by reinforcement learning. It contains two stages in the decoding process, an operation selection stage, and a word selection stage. RL is used to guide the operation selection stage because there is no labelled signal to learn it directly. Similarly, it uses non-differentiable evaluation metrics to provide rewards and trains the two stages jointly under the RL framework. 3) Code Generation: Code generation is different from code search in the sense that it directly generates the code instead of searching based on matching. COMPCODER 208]’s training consists of three stages. In the first stage, the code generation model is fine-tuned from a language model. Then, reinforcement learning is used to introduce the compiler guidance. The last stage uses a discriminator to learn the compiler feedback on the generated candidates. The discriminator is trained on whether the code can be successfully compiled. LearnedSQLGen [209] applies actor-critic algorithm on SQL generation problem. The state contains elements of SQL sequence, including reserved words like Select, From, Where, metadata of tables and attributes, cell values, operations like =,>,<, and EOF showing the termination of the sequence. The action space is the same as the state space. CodeRL incorporates unit tests into code generation. The code generator is trained with rewards that are defined based on unit test signals that are CompileError, RuntimeError, FailedTest, PassedT est. A critic network is used to predict the probability of four types of test signals. PPOCoder [211] generates code with multiple constraints in reward. The reward function is defined as a sum of test error, syntactic matching score, semantic matching score, and a KL constraint to prevent RL from diverging too far. (212) combines a preference-based learning framework into code optimization. The method integrates unit test feedback for reward function to improve correctness and efficiency of LLM fine-tuning. proposes to incorporate unit test as feedback as well. The reward function comprises three terms, coarse reward, fine reward, and adaptive reward. The coarse reward assigns scalar reward according to the result of the code: pass, failure, syntax error, and other errors. The fine reward assigns a number according to the specific error types. The adaptive reward takes the number of passed test cases into account and stimulates the LLM to pass as much test cases as possible. uses execution result as reward as well, the difference is that the reward function returns high reward only when all private test cases are passed. (215) proposes a curriculum learning for the code TABLE VII: Methods in Computer Vision Sub-areas Related Works Image Captioning Visual Question Answering Visual Dialog System Text-to-Image Generation 3D Generation Other Computer Vision Tasks generation. The agent is required to generate a small amount of code snippet given other parts canonical solution. The curriculum gradually increases difficulty by decreasing the portion of accessible canonical solution and increasing the length of generation. 4) Unit Test Generation: 216] improves the quality of unit test generation by PPO-based fine-tuning. The state consists of the current code under test and the unit test generated by the LLM. The later allows the LLM to inspect and refine its generated results. The action is generating the unit test. The reward is acquired from a static quality analyzer and is used to train a reward model that provides rewards for PPO-based fine-tuning. C. Computer Vision Computer vision is another cornerstone of modern machine- learning research. Generation tasks in computer vision are also capable of using reinforcement learning algorithms in many sub-areas, including text generation tasks such as image captioning, visual question answering, visual dialog, and visual entity generation like image generation and 3D objects and scenes generation. 1) Image Captioning: Image captioning is a task where the model aims to describe related events and entities in an image. New algorithms are explored, including reward normalization architecture advancement | , new reward func- . SCST incorporates REINFORCE with a baseline as the training algorithm to normalize the rewards an agent experiences. It defines the reward by the performance of a current model under the inference algorithm. The baseline uses the test dataset to compute the reward. Ren et al. uses an RL algorithm whose state comprises an image and up-to-now generated words. An action is the next word. The reward is defined as a cosine similarity between the generated captions and the image. Zhang et al. uses an actor-critic algorithm and a separation of RNN between the actor-network and the critic network to do image captioning. TOPIC (219) uses policy gradient on multi-model product title compression where text and images are input for title generation. applies RL fine- tuning in remote sensing image captioning. The RL algorithm uses (99). OffPG | implements the reward function with human feedback. The ratings of captions are provided and incorporated into a policy gradient with baseline to learn a captioning system in a offline way. Shi et al. improves the diversity of generated captions by increasing the exposure of varied caption candidate and a reward function max-CIDEr that relaxes the similarity constraint to improve diversity. (222) JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 proposes a triangle-reward for policy gradient training. Three rewards measure visual consistency, fluency and correctness, and semantic coherence respectively. The first and the last reward is constructed based on graph representations of the scene. DiscriTune (223) provides reward signal through a pre- trained fronzen image retriever to a REINFORCE-based image captioner. The reward is defined as whether the image retriever gets the original image according to the caption generated by the captioner. 2) Visual Question Answering: In VQA, topics like reward design | and new architecture | This game comprises three components, a questioner, a guess and an oracle. The guesser guesses the targeted object in an image given the context collected by the questioner. The oracle contains information about the targeted object. VQG models the questioner as the agent and proposes to formulate the reward through three dimensions: goal achieved reward, progressive reward, and informativeness reward. Zhao et al. 225] combines multi-model representation learning with a reinforced GAN-based model for visual question answering. The representation model contains a pre-trained convolutional network, a frame-level dynamics network, and a segment-level attention network. MGN uses REINFORCE algorithm to fine-tune a graph neural network that takes image and text se- quence as input. The state is the image and query, the action is a distribution of tokens that impact a symbolic program which generates the final answer, and the reward is the final answer correctness. incorporates the policy gradient method into a cascade reasoning-based image captioning framework, allowing the agent to learn through trial and error. The reward is average normalized Levenshtein similar- ity (ANLS) that measures the similarity between generated answers and the ground truths. 3) Visual Dialog System: RL enhances the visual dialog system by incorporating discriminators Fan et al. 61] borrows SeqGAN model into a visual dialog system. They devise a model that contains two modules, an encoder to embed images, captions, and questions into the embedding vector, which is fed into an RL-based decoder as the state. The decoder is an RL-based GAN. The generator is the agent that outputs answers as actions. The discriminator learns to classify the generated answers from real ones in the embedding space. SCH-GAN learn a cross-modal hashing GAN with reinforcement learning. Text and image modalities are considered. The generator tries to retrieve an image from texts or vice versa. The discriminator aims to distinguish true examples of the query. 4) Text-to-Image Generation: Recent advancement in im- age generation is diffusion models, thereby it might be ben- eficial explore how to combine reinforcement learning with diffusion models by improvement on images characteristics that are hard to be described by prompts and online reinforcement learning methods (230). The action for the agent in [229] is the noise vector generated in steps given the last step output and a context variable. The agent is first pre- trained on DDPM loss reweighted by exponentiated rewards. The policy gradient algorithms with importance sampling are incorporated to train the agent. The reward takes file size and human aesthetic preference acquired from another predictor into consideration. An extra alignment using a vision language model is incorporated for RLAIF [96]. [230] proposes to compare the RL-directed fine-tuning and supervised fine- tuning in the context of KL divergence as a regularizer. The RL fine-tuning uses a policy gradient with a KL term to constrain models on a pre-trained model. The reward in RL fine-tuning is typically from human preference matching. 5) 3D Generation: Apart from 2D images, 3D generation is able to introduce reinforcement learning to get a new gen- eration method [231], better scene generation (232), surface completion [233], point clouds completion [234], mesh edition , human motion generation (236, [237]. Akizuki et al. propose to use RL to generate objects in 2D or 3D space. It models the generation as a link game, where an agent is required to link from pixel to pixel or from voxel to voxel. The action space is the direction to extend the pixel. The reward is based on whether the next pixel or voxel is in the object. It also successfully learns to generate Lego structures given fabrication constraints. RLSS generates 3D indoor scenes with reinforcement learning. The state contains structures or objects represented by the center position and bounding boxes as well as the replacement information. The action is to place an object in a place. The reward is constructed by programs that include multiple conditions such as successful condition, count of objects, and failure conditions. QINet | completes the corrupted 3D point cloud with the actor-critic algorithm. It first generates masks as pre-processing. Then it converts the discrete point cloud to the continuous surface. The policy is trained by rewards defined as IoU between generated cloud and the true cloud and a latent code constraint to prevent the latent code drift far away. Zhang et al. complete point cloud with A3C algorithm. The state is the updated point cloud of each iteration. The action is the next best view for the completion. The agent adjusts the camera points in the coordinate system to change the views. proposes to edit 3D shapes with two Double DQN agents, a prim-agent that modifies primitives (e.g., cuboids) and a mesh-agent that changes the vertices on the mesh. The state is the current configuration of the 3D shape. The action is editing the corner of the primitives or deleting primitives for the prim-agent, and is changing groups of vertices. The reward includes the difference between intersection over union (IoU) of primitives before and after taking an action. employs PPO to synthesize human motions in 3D indoor scenes. The state contains configurations of 3D scene geometry, virtual human body, and intended goals to interact with. The action is defined in a latent space of pre- trained motion generation models. The reward enhances goal- reaching. foot-floor contact, and penetration avoidance. generates 3D Dance via a transformer-enhanced actor-critic method. The state encompasses a sequence of dance poses. The action is the next dance pose. The reward motivates the agent to align motion-music beats and penalizes inconsistency movement between upper body and lower body. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6) Other Computer Vision Tasks: Other tasks like vi- sual program synthesi 8], dashboard generation video summarization [240], and vision-language models (242). Visual program synthesis is a task that takes a query as input, the agent learns to write program for visual tasks by utilizing off-the-shelf computer vision models like image detection. proposes to address this problem by optimiz- ing the agent with rewards that are binary variables computed based on the existing vision-language annotations (not the target code) to guide the tuning of the language model that generates code. proposes to employ RL in dashboard generation that is frequently employed in business intelligence to help data analysts with data exploration. The state represents the current configuration of the dashboard, including selected data fields, chart types, and arrangements. The action is to add a chart, remove one, or change properties of a chart. The reward contains three terms: diversity that captures the number of chart types, parsimony that reveals the number of charts, and insight that shows the number of correlations revealed in the chart. Liu et al. (240) propose to use policy gradient in the video summarization task. The state is the spatio-temporal features learned from a representation learning network. The action is selecting frames by binary classification. The reward is increasing similarity of a selected representative frame, aka medoid, and other frames in the same cluster and decreasing similarity between medoids. Reinforcement Learning could be used in tuning Vision- Language Model (VLM) [241], as well. proposes to adapt pre-trained language model for multi-modal tasks by a RL-based training. This method trains a multi-modal encoder and fix other parts of the neural network. The encoder takes outputs of an image from a pre-trained CLIP model and generates multi-modal representations for text generation. The reward is formulated based on consine similarity between CLIP embeddings of the image and the generated text, which measures the alignment between the input image and the generated text. The training objective also poses a KL penalty to keep the RL policy (text generator) close to the original policy. formulates the VLM as a policy, aiming to improve its effectiveness the decision making capability via PPO-based fine-tuning. The state space consists of image pixels and task descriptions in the text form. The action space is in text form as well. The reward could be derived from the success of the task. For example, in the number line task, the agent is given two poker cards and is required to match the number of the two cards. The reward could be 1 for success, -1 for incorrect actions, and 0 otherwise. D. Speech and Music Generation Speech and music data can be transformed to sequential data points. Thereby, it is natural to incorporate reinforcement learning. In practice,RL is employed to improve the quality [245], [246], decrease latency [245], control the bit rate in speech coding [247], and melody generation [248]. Tacotron 2 learns an agent to control the text2speech translation. The state of the agent is a product of an attentive vector and hidden vectors, the attentive vectors, and the output sequence. 20 TABLE VIII: Methods in AI For Science Sub-areas Related Works Molecule Design Reaction Optimization Micro-structure Generation Quantum Architecture Design The action is whether to read or speak. When it reads, it generates an attention vector. When it speaks, it generates the output in the form of a mel-spectrogram frame. The reward motivates the agent to produce high-quality translation as well as low latency. i-ETTS explores reinforced emotional text-to-speech synthesis. The input of the model is reference audio and the targeted character sequence. The reference latent vector encodes tokens that indicate the emotion with an attention model. Then two modality is fused to decode and generate an emotional speech that is fed into a speech emotion recognition classifier. The reward for the speech generator is recognition accuracy. The agent is trained with a policy gradient. Gibson and Oh apply RL in speech coding, an essential technology for digital cellular communications. The agent is a tree-structured controller for the bit rate in speech coding. The system uses a reconstruction error as the penalty. proposes to incorporate LeakGAN into music melody generation. The music notes are converted to symbolic representation. E. Al For Science Machine learning community also want to help scientists in other research areas with useful machine-learning tools. Molecule design is a critical area because the design process is typically an expensive and long process. Therefore automati- cally finding patterns from large amounts of data accumulated in scientific areas become another hot area in recent years. RL can also play an important role in its flexibility. 1) Molecule Design: Drug discovery or de novo molecule design can be viewed as policy search in the molecule space. Thereby it is natural to introduce RL on this application. Abundant research has been conducted, including various mainstream methods listed in Section like GAN-based models {276}, human prior (277). For Gan-based models, the direction of architecture design [276], sample selection are explored. ORGANIC uses RL directly in the GAN model to discover drugs. RANC combines ORGANIC style architecture with a memory network DNC, which enables the model to remember complex sequences and generate longer sequences. ReLeaSE proposed by Popova et al. incorporates a prediction model to bias the generated chemical structures toward those with the desired physical or biological properties. ATNC [252] modifies the OGRANITC model by introducing a new sample selection scheme function that filters out molecules which are far from training samples and regenerates the sequences until the number of new sequences is higher than the threshold. MoIGAN ] uses DDPG on small molecular graph gener- ation to cope with high dimensional action space. The reward JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 is emitted from a differential approximation of the true reward function. For human-designed reward methods, various topics are investigated, like reward design [2 7\\\\, memory architecture , ranking methods [256], training methods [257]. Inspired by Sequence Tutor [83], REINVENT proposed by Olivecrona et al. uses RL to tune the MLE pre-trained RNN on the molecular de-novo generation task. It defines the reward as the distance between an augmented likelihood and the agent’s likelihood. The augmented likelihood adds a reward of the desirable properties of a molecule onto the log-likelihood of data distribution. computes reward based on fundamental physical properties such as the energy, which is approximated by fast quantum-chemical method. Blacshke et al. augment the REINVENT model with a memory to cope with the mode collapse problem. The agent is penalized for generating similar compounds to the ones in the memory unit. Atance et al. propose the best agent reminder (BAR) loss for training by motivating the agent to update the gradient towards the best agent collected during the training process. It balances between the best agent loss and the REINVENT loss with a factor. The reward function considers the average size of the molecules, drug-like metrics and special molecule DRD2 metrics. AHC 256] combines the REINVENT loss with samples selected by the Hill-Climb method ]. Brown et al. an evaluation framework for de-novo molecular design that includes two benchmarks, one for an in-distribution learning test and the other for goal-directed benchmark. (265) generates 3D molecular conformations by pre-training and fine-tuning a language model, eliminating the need of external software for graph reconstruction. The state is the current molecular graph and 3D conformations. The action is generating new parts of the molecular graph and 3D conformations. The reward evaluates the binding affinity of target molecular. Most works above ignore the resolution of molecular design. Recent works start to combine reinforcement learning meth- ods to representations of different resolutions, like fragments 258], tree [259], and population of candidate molecules Gan rast BY investigates PPO on fragment-based molecular optimization. The action for the agent is to add or delete a fragment from the current molecular. The reward is +1 if the novel qualified molecule is discovered and —0.1 if an invalid molecule is explored. RTJ-RL proposes to apply PPO on a reversible junction tree (RJT) that is a new representation for molecules. RJT representations are convertible to valid molecules, which describe the state of the agent. The action is the modification of the tree, containing information about node, word, site, and stop. RGA combines genetic algorithms and reinforcement learning to op- timize the structure-based drug design. The state is population at a certain step generation, including the candidate molecules and their 3D poses. Action space is based on crossover and mutation, two main steps in the evolution process. The action is composed of probabilities to choose candidates or ligands in the population. For molecular design, it is common to use multiple constraints or goals to guide the model. Therefore, multi- 21 objective optimization is also an interesting direction , in- cluding weighted sum [ (261). alternation (254). reward weighted sum [261], and Pareto optimization . The state and action of MoleGuLAR are defined as sequence generation. The reward is calculated by solute property measure LogP, drug-likeness metric (QED), and impact in human factor (TPSA). when conflicts exist between rewards, they set all rewards as 0 to guide the generation model towards molecules where the single property is optimal. Hu et al. [261] use the REINFORCE method for fine-tuning. They devise a reward-mixing strategy for multiple objective conflicts. MolSearch | proposes to use MCTS on multi- objective molecular generation and property optimization. The model maintains a global pool for Pareto molecules which are defined as molecules that have at least one property at the best state. DrugEx3 uses a Transformer model for the generation to allow users to input prior information like the desired scaffold. 2) Reaction Optimization: It is also feasible to use RL searching in the formula space for chemical applications. Zhou et al. propose to use DRL for chemical reaction optimization. It models a chain of reactions where an agent has the experimental conditions as states and can change the conditions by actions such as increasing the temperature. Once an action is applied, the condition of the reaction changes, and then the agent is required to further take the following action. The reward is about the output of the reaction, such as product yield, selectivity, purity, and cost. RNN as the model is used to construct the agent. uses a similar formulation for shell-growth of core-shell semiconductor nanoparticles. Gottipati et al. integrate forward synthesis into reaction selection with modified policy gradient method. The state represents the current molecules generated from a sequence of commercially accessible reactions. The action is the next chemical reaction. Reward is designed to measure the desired properties of generated molecules, such as hydrophilicity and lipophilicity. [269] applies REINFORCE to control threshold temperatures and chemical potentials critical for initiating chemical reactions. The state is current reaction conditions. The action is modifying the the synthesis parameters like temperature and gas concentration. The reward promotes the generation of the target chemical. proposes to incorpo- rate multi-agent DDPG into the traditional real-time traditional framework, enabling the optimization to be both economic and adaptable for reaction optimization. The reward aims to optimize yield and efficiency and minimize costs. 3) Micro-structure Generation: uses a GAN-based SAC algorithm to support generation of 3D microstructure of porous media. The state space contains current design parameters, current Qol (physical quantities of interests) and target Qol. The action is a series adjustments of design parameters. The reward is employed to enhance the alignment of generated microstructure and a target one. 4) Quantum Architecture Design: propose to do a quantum architecture search by reinforcement learning. The state is defined as a multi-qubit entangled state, the action space is the quantum gate for the design, and the fidelity of the target state measures the reward. The experiment is JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 carried on a simulation environment, which is adapted towards the OpenAI Gym designs quantum adiabatic algorithms by DQN networks. The state is the current progress of the adiabatic evolution path. The action is changing the parameters of the numerical operators of energy in the system. The reward is the success probability of the algorithm. [274] optimizes Q-learning for quantum circuit architecture. The goal is to control the energy obtained from a circuit and reduce the depth of the circuit depth. exploits DQN to approximate single-qubit unitary operations with sequences of quantum gates from a finite universal set. The state is the current composition of quantum circuit. The action is selecting a gate from a predefined finite universal set. The reward considers the difference between the target and the generated sequence for fidelity and the length of the sequence. F. Recommender System and Information Retrieval We include most works in this survey about how to generate content without collaboration with humans. There are also applications where RL can be used to generate interactions between two entities, such as a robot and an environment. For the recommender systems, it is better for readers to read surveys about how reinforcement learning is injected into the process [280], [281]. In general, user interaction history is considered as states and items are defined as actions. The agent is required to generate items that users might be interested in. Thereby, the user feedback can be incorporated as rewards to guide the learning process. G. Robotics Robotics is another useful area that deviates from the generative applications mentioned above. Generally speaking, robotics can be treated as an interactive agent that generates responses to humans’ orders. Recent advancement shows that large language models plus visual foundation models might lead to a large step towards better robotics control policies. (283) even proposes a LLM-based agent whose action is generated code to play MineCraft. H. Other Areas Generation models have wide applications. We also list works that apply RL in niche areas like procedure genera- tion , simulated robotics ; . PCGRL [284] proposes an abstract description of formulating the procedure generation problem into an MDP. The procedure generation problem is often used for game construction. They highlight three types of representations for the state and action modelling. Narrow representation only changes game elements at predefined locations. Turtle representation provides an agent with the ability to move around on the map. The broad repre- sentation enables an agent to change game elements at other proposes to not only optimize a policy for a bipedal walker to complete tasks but also optimize the shape of the walker. It conducts experiments on the OpenAI Gym BipedalWalkerHardcore-v2 task and learns to generate an agent that can achieve better performance. Apart from 22 TABLE IX: Challenges and future directions of reinforcement learning methods applied in generative model and applications Challenges and Future Directions Peaked Distribution Exploitation-Exploration Sparse Rewards Long-term Credit Assignment Generalization knowledge graphs and causal graphs, reinforcement learning agents can also generate other types of graphs like road graphs and power grid graphs ], where the state is the set of nodes and edges, the action is the selection of a node either the start node or the end node. The reward is defined for robustness study expected critical fraction of nodes to the removal. The reward is estimated by Monte Carlo sampling. Q-learning is employed for this task. Reinforcement learning is also used in quantum computer design (272). Exploratory Data Analysis (EDA) is another area for RL-based generation. proposes to generate notebooks during EDA by formulating the generation process in MDP. The action space consists of different data operations like filter, group, and backtrack to last operation. The state space is a vector comprises three steps’ result data including summarization features like the number of distinct values, the number of null values, the number of groups, and the groups’ size mean and variance. The reward is formulated with three terms: interestingness, diversity, and coherence. V. CHALLENGES AND FUTURE DIRECTIONS Despite benefits and wide application of RL-based methods, challenges also exist, where potential new research directions emerge, opening new opportunities to further improve current generative models and applications. In this section, we intro- duce a range of challenges, the responses from the community, and less explored promising future directions. Peaked Distribution Given the fact that reinforcement learning improves the performance of generation for specific objectives, Choshen et al. inquire about the reason behind the performance increase in neural machine translation. They propose that the peakiness of the distribution is the true factor for RL to improve performance instead of reward learning. The peakiness illustrates that RL fine-tuning tends to increase the probability of the best answer to make the distribution lower entropy and more discrete. The following work [289] refutes [288] and show that BLEU increase is not tied to the peakiness in RL training. Other subsequent papers methods to address this problem by multi-temperature sampling [1 and finetuning More work could be conducted to improve the diversity of the generated results in specific areas. Potential solutions may be found in literature Exploitation and Exploration In reinforcement learning, an agent must balance a trade-off between exploitation and exploration. To maximize the expected reward, the agent must exploit the best action of what it has experienced before. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 But to collect the potential best action for learning, it must explore broadly all possible situations to gather enough data in the learning process. This dilemma is not suited for offline settings such as classic supervised learning and unsupervised learning. One solution for exploration is Upper-Confidence- Bound (UCB) action selection, which injects the consideration of exploration by the times of selection for action. UCB has been applied in molecular design [258]. For sequential generation problems, action space is pro- hibitively large which makes the exploration difficult. There- fore, pretraining-finetuning architecture is widely exploited to alleviate this problem. Another way to tackle this problem is searching in the action space and composing meta-action to decrease the difficulty of exploration. Similar to AlphaGo [55], RationaleRL [293] proposes to employ an MCTS in searching a relatively small and important action space to improve the performance of the RL-based generator. A new recent approach of exploring the solution space is applying MCTS in the inference time of Large Language models [299], [B00], which opens new direction that self-play could enhance the performance of current LLMs. Reward function design and multi-objective optimiza- tion A large number of works have been conducted on how to guide model training with hand-designed new signals by RL. Multiple objectives are usually utilized for various constraints and guidance modelling. It is ideal that the optimal values can be achieved all at once. But it is often not the case, making the Pareto optimization a useful tool for analyzing the result. While according to our exploration, few works have addressed multi-objective optimization and Pareto optimization in ap- plication areas. It might be valuable to trade-off in multiple contradictory losses. Therefore, it is advantageous to explore how to trade off between them for a more robust model. Another problem is sparse rewards. It is easy to see that rewards are training signals that shape the action pattern of agents. In an ideal environment, rewards are emitted at each action to provide sufficient guidance for models. If one reward is computed based on the generated sequence [143], [294], the environment must produce a reward at the end of each episode. Some reward evaluation such as BLEU has this problem. Rewards become sparse and pose challenge for the learning model to tell which action has a higher impact on the last rewards. Guo et al. proposes a multi-step path consistency learning to address this problem by taking a sequence as a whole, where the consistency is computed over multiple steps instead of single step to tackle the sparse reward problem. SURF defines a new reward function that emits a sequence of normalized rewards computed on incomplete sentences and the target sequence. Korshunova et al. address sparse reward problems by fine-tuning, experience replay, and real-time reward shaping. The fine- tuning and experience replay could help the policy focus on promising but rarely explored molecules, and reward shaping provide dynamic process rewards. Another way to tackle the sparse reward issue is introducing the label from human experts as a new signal [296], which incorporates a DAgger- like method into semantic parsers that interact with users by texts. DAgger (B01) is an imitation learning method that works 23 by collecting new data from experts and integrate them into the old dataset, which is used for training a new policy. Despite mentioned methods, Similar ideas like reward-shaping could be applied according to specific task configuration, which is also a promising direction to further enhance the model. Long-term Credit Assignment The credit assignment problem arises because the agent need to determines which previous actions have impact on current rewards. With the increase of time steps, it becomes more difficult for the increase of the number of previous actions. Hierarchical RL is introduced to cope with this problem (297}. It sets up two modules for learning, a manager and a worker. The manager takes the latent representation and produces a goal vector in a low-dimensional space. The worker fuses the latent vector and the goal to make the decision. LeakGAN [249] uses an agent with hierarchical structure for image captioning to deal with the long text generation problem. The hierarchical architecture is similar to [297]. The difference is that policy gradient algo- rithm is introduced into a GAN network, where reward comes from the discriminator in GAN. With the increase of model capability, more difficult and complex tasks are constructed to test the boundary of the generative models. Hierarchical methods could potentially improve the long-horizon problem solving ability for the generative models. Generalization Classic reinforcement learning algorithms are often designed for specific tasks without considering task adaptation [3! Classic RL models often perform worse on unseen tasks . Meta reinforcement learning like MAML [304] is designed to adapt agents for meta-learning tasks. The meta-learning setting involves meta-training and meta-testing environments, mimicking the few-shot supervised learning setting where at test time, there are a few examples for the algorithm to adapt to the test tasks. In order to adapt to new tasks, MAML learns an adaptive initialization of neural network parameters. This design is model-agnostic and can be applied in various machine learning tasks such as supervised learning and reinforcement learning. It can also be introduced in multi-scenario text generation for adaptation of different scenarios [298], where Zhao et al. combine MAML with discriminator-guided text generation. Recent works also show that it might be difficult to gen- eralize logic inference tasks even for the best GPT-4 (305). Thereby, the difficulty of generalization of deep generative models might be further addressed by the RL algorithms. More work should be devoted to how to design a model that can better generalize and achieve better results on out- of-distribution data. Retraining [306], and causal machine learning might be interesting ways to complement the capability of RL-based generators for better learning and adaptation. Model Enhancement and Control Recent research has tailored RL to help solve the difficulty of sampling of EBM via RL as a distributional approach or increasing the efficiency by searching backward propagation of DDPM (82). These approaches pave a new way to exert RL to improve the generative model, in contrast to classic applications where RL is a way to introduce new training signals or serve as an architecture builder. The aforementioned works show the JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 feasibility of RL application, more advancement in the RL community might be transferred to these models to further improve the model performance. A good example is which compares the difference between distributional formu- lation and policy gradient but still exploits the similarity which lay the foundation for variance reduction to be applied to distributional methods. Human Alignment in LLM and foundation models LLM has demonstrated great transferability on a number of sequence modeling problems. Combined with vision foundation models, it is a potential way to achieve more powerful models for diverse task settings. The fast-developing literature in this area is drawing the capability of current large models and exposing new problems for methods like RLHF. This might foster new directions for how RL is incorporated into generative models. The RLHF method is a hot research direction since the high impact of large language models New studies also emerge to explore whether the RL is an indispensable factor. Preference-based models have been proposed to model human preference. Direct Preference Optimization (DPO) aims to substitute reinforcement learning by directly utilizing re- ward functions by preference modelling. More research can be conducted in this line to find an optimal way for preference modelling. Also, human preference also is dynamic, so the capture of dynamics and improvement on current generative applications might be an interesting direction. VI. CONCLUSION In this survey, we propose a unified taxonomy for RL applied in generative AI. We collect works from various directions and extract the key usage of reinforcement learning. We first briefly introduce the concept of generative models and reinforcement learning methods. Then we introduce the key application methods for RL to be incorporated into the generative models. Furthermore, we extract exemplar works in a range of application areas for readers who want to narrow down to a specific area. Finally, we show the promising directions of future research and conclude the whole survey. REFERENCES 1] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2] A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu, “Pixel recurrent neural networks,” in International conference on machine learning. PMLR, 2016, pp. 1747-3] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” Advances in neural information processing systems, vol. 27, 4] J. Ho et al., “Denoising diffusion probabilistic models,” Advances in neural information processing systems, vol. 33, pp. 6840-6851, 6] D. Rezende and S. Mohamed, “Variational inference with normalizing flows,” in International conference on machine learning. PMLR, 2015, pp. 1530-7] “Chatgpt,” {https://openai.com/chatgpt) accessed: 2023-08-8] W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood, “Flexible diffusion modeling of long videos,” Advances in Neural Information Processing Systems, vol. 35, pp. 27 953-27 965, 20) 21 22 23 24) 25 26 27 28 29 30, 31 32 33 34 35 36 37 24 Y. Song, L. Shen, L. Xing, and S. Ermon, “Solving inverse problems in medical imaging with score-based generative models,” arXiv preprint arXiv:2111.08005, J. Jumper et al., “Highly accurate protein structure prediction with alphafold,” Nature, vol. 596, no. 7873, pp. 583-589, M. Baek et al., “Accurate prediction of protein structures and interac- tions using a three-track neural network,” Science, vol. 373, no. 6557, pp. 871-876, Z. Lin et al., “Evolutionary-scale prediction of atomic-level protein structure with a language model,” Science, vol. 379, no. 6637, pp. 1123-1130, M. Mohebbi Moghaddam et al., “Games of gans: Game-theoretical models for generative adversarial networks,” Artificial Intelligence Review, pp. 1-37, H. Chen et al., “A survey on dialogue systems: Recent advances and new frontiers,” Acm Sigkdd Explorations Newsletter, vol. 19, no. 2, pp. 25-35, G. H. de Rosa et al., “A survey on text generation using generative adversarial networks,” Pattern Recognition, vol. 119, p. 108098, J. Ni et al., “Recent advances in deep learning based dialogue systems: A systematic survey,” Artificial intelligence review, vol. 56, no. 4, pp. 3055-3155, H. Sun, “Reinforcement learning in the era of IIms: What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond,” arXiv preprint arXiv:2310.06147, C. Zhang et al., “A survey of automatic source code summarization,” Symmetry, vol. 14, no. 3, p. 471, Reinforcement learning: An introduction. MIT X. Tan et al., “A survey on neural speech synthesis,” arXiv preprint arXiv:2106.15561, language processing: Tutorial, review and outlook,” arXiv preprint arXiv:2210.13623, M. Z. Hossain et al., “A comprehensive survey of deep learning for image captioning,’ ACM Computing Surveys (CsUR), vol. 51, no. 6, pp. 1-36, N. Le et al., “Deep reinforcement learning in computer vision: a comprehensive survey,” Artificial Intelligence Review, pp. 1-87, S. Santra et al., “Gradient descent effects on differential neural ar- chitecture search: A survey,” IEEE Access, vol. 9, pp. 89 602-89 618, Y. et al. Du, “Molgensurvey: A systematic survey in machine learning models for molecule design,” J.C. Fromer et al., “Computer-aided multi-objective optimization in small molecule discovery,” Patterns, vol. 4, no. 2, C. Bilodeau et al., “Generative models for molecular discovery: Recent advances and challenges,” Wiley Interdisciplinary Reviews: Computa- tional Molecular Science, vol. 12, no. 5, p. e1608, B. Tang et al., “Generative ai models for drug discovery,” in Biophysical and Computational Tools in Drug Discovery. Springer, 2021, pp. 221- S. Luukkonen et al., design,” D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backprop- agation and approximate inference in deep generative models,” in International conference on machine learning. | PMLR, 2014, pp. 1278-Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, F. Huang et al., tutorial on energy-based learning,” Predicting structured data, vol. 1, no. 0, “Artificial intelligence in multi-objective drug Current Opinion in Structural Biology, vol. 79, p. 102537, 2024, al MIT JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 38 39 ‘‘41 42 43 44 ‘45 46 47 48 ‘49 50 51 52 53 54 55 56 57 58 59 60 61 62 Y. Song and D. P. Kingma, “How to train your energy-based models,” arXiv preprint arXiv:2101.03288, M. A. Carreira-Perpinan et al., “On contrastive divergence learning,” in International workshop on artificial intelligence and statistics. PMLR, 2005, pp. 33-A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural information processing systems, vol. 30, R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” Proceedings of Machine Learning and Systems, vol. 5, pp. 606-624, M. Freitag and Y. Al-Onaizan, “Beam search strategies for neural machine translation,” arXiv preprint arXiv:1702.01806, L. Dinh, D. Krueger, and Y. Bengio, “Nice: Non-linear independent components estimation,” arXiv preprint arXiv:1410.8516, L Kobyzev, S. J. Prince, and M. A. Brubaker, “Normalizing flows: An introduction and review of current methods,’ JEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 11, pp. 3964— 3979, R. Bellman et al., “The theory of dynamic programming,” Bulletin of the American Mathematical Society, vol. 60, no. 6, pp. 503-515, T. Haarnoja et al., “Reinforcement learning with deep energy-based policies,” in International conference on machine learning. PMLR, 2017, pp. 1352-R. J. Williams et al., “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Reinforcement learning, pp. 5-32, J. Schulman et al., “Trust region policy optimization,” in Jnternational conference on machine learning. PMLR, 2015, pp. 1889-, “Proximal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, V. Mnih et al., “Asynchronous methods for deep reinforcement learn- ing,” in International conference on machine learning. PMLR, 2016, pp. 1928-T. et al. P Lillicrap, “Continuous control with deep reinforcement learning,” in 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., T. Haarnoja et al., “Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,” in International conference on machine learning. PMLR, 2018, pp. 1861-D. Silver et al., “Deterministic policy gradient algorithms,” in Proceedings of the 31st International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, E. P. Xing and T. Jebara, Eds., vol. 32, no. , “Mastering the game of go with deep neural networks and tree search,” nature, vol. 529, no. 7587, pp. 484-489, , “Mastering the game of go without human knowledge,” nature, vol. 550, no. 7676, pp. 354-359, R. S. Sutton et al., “Integrated architectures for learning, planning, and reacting based on approximating dynamic programming,” in Machine learning proceedings A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, O. Pietquin, A. Ustiin, and S. Hooker, “Back to basics: Revisiting reinforce style optimization for learning from human feedback in Ilms,” arXiv preprint arXiv:2402.14740, C. et al. Mao, “Discrete representations strengthen vision transformer robustness,” in International Conference on Learning Representations, R. Devon Hjelm et al., “Boundary-seeking generative adversarial networks,” arXiv e-prints, pp. arXiv-1702, H. Fan et al., “Recurrent attention network with reinforced genera- tor for visual dialog,” ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 16, no. 3, pp. 1-16, T. M. Nguyen et al., “Infocnf: An efficient conditional continuous nor- malizing flow with adaptive solvers,” arXiv preprint arXiv:1912.03978, 63 64 65 66 67 68 69 72 73 74 75 76 77 78 79 81 82 83 84) 85 86 87 88 25 L. Yu et al., “Seqgan: Sequence generative adversarial nets with policy gradient,” in Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, M. Ranzato et al., “Sequence level training with recurrent neural networks,” arXiv preprint arXiv:1511.06732, K. Papineni et al., “Bleu: a method for automatic evaluation of machine translation,” in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311- G. L. Guimaraes et al., “Objective-reinforced generative adversarial networks (organ) for sequence generation models,” arXiv preprint arXiv:1705.10843, Q. Wang et al., “Grl: Knowledge graph completion with gan-based re- inforcement learning,” Knowledge-Based Systems, vol. 209, p. 106421, W. Fedus et al., “Maskgan: better text generation via filling in the_, arXiv preprint arXiv:1801.07736, K. Lin et al., “Adversarial ranking for language generation,” Advances in neural information processing systems, vol. 30, W. Zhou et al., “Self-adversarial learning with comparative discrimi- nation for text generation,” arXiv preprint arXiv:2001.11691, T. Che et al., “Maximum-likelihood augmented discrete generative adversarial networks,” arXiv preprint arXiv:1702.07983, T. Scialom et al., “To beam or not to beam: That is a question of cooperation for language gans,” Advances in neural information processing systems, vol. 34, pp. 26 585-26 597, M. Sarmad et al., “Rl-gan-net: A reinforcement learning agent con- trolled gan network for real-time point cloud shape completion,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June Q. Wu et al., “Textgail: Generative adversarial imitation learning for text generation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 16, 2021, pp. 14067-Y. Wan et al., “Improving automatic source code summarization via deep reinforcement learning,” in Proceedings of the 33rd ACM/IEEE international conference on automated software engineering, 2018, pp. 397-J. Li et al., “Deep reinforcement learning for dialogue generation,” arXiv preprint arXiv:1606.01541, K. Mo et al., “Personalizing a dialogue system with transfer reinforce- ment learning,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, T. Zhao et al., “Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning,” arXiv preprint arXiv: 1606.02560, V. Pyatkin et al., “Reinforced clarification question generation with defeasibility rewards for disambiguating social and moral situations,” arXiv preprint arXiv:2212.10409, Y. Fan et al., “Optimizing ddpm sampling with shortcut fine-tuning,” arXiv preprint arXiv:2301.13362, N. Jaques et al., “Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control,” in International Conference on Machine Learning. PMLR, 2017, pp. 1645-D. M. Ziegler et al., “Fine-tuning language models from human preferences,” arXiv preprint arXiv:1909.08593, N. et al. Jaques, “Way off-policy batch deep reinforcement learning of implicit human preferences in dialog,” CoRR, vol. abs/1907.00456, R. et al. Yuanzhe Pang, “Text generation by learning from demonstra- tions,” in International Conference on Learning Representations, P. Ke et al., “ARAML: A stable adversarial training framework for text generation,’ in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association for Computational Linguistics, November 2019, pp. 4271- Available: https://aclanthology.org/D 19-1436 JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 89 90) 91 92 93 94 95 96 97 98 99 [100 [101 [102] [103] [104] [105] [106] [107] [108] [109] S. Lamprier et al., “Generative cooperative networks for natural lan- guage generation,” in International Conference on Machine Learning. PMLR, 2022, pp. 11 891-11 Z. et al. Shi, “Toward diverse text generation with inverse reinforcement learning,” in Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, J. Lang, Ed. ijcai.org, 2018, pp. 4361- N. Stiennon et al., “Learning to summarize with human feedback,” Advances in Neural Information Processing Systems, vol. 33, pp. 3008— 3021, L. Ouyang et al., “Training language models to follow instructions with human feedback,” Advances in Neural Information Processing Systems, vol. 35, pp. 27730-27744, Y. Bai et al., “Training a helpful and harmless assistant with reinforcement learning from human feedback,’ arXiv preprint arXiv:2204.05862, Y. Gao et al., “Preference-based interactive multi-document summari- sation,” Information Retrieval Journal, vol. 23, pp. 555-585, J. Kreutzer et al., “Offline reinforcement learning from human feedback in real-world sequence-to-sequence tasks,” in Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021). Online: Association for Computational Linguistics, August 2021, pp. 37- B. Zhu et al., “Principled reinforcement learning with human feedback from pairwise or k-wise comparisons,” arXiv preprint arXiv:2301.11270, J. Li et al., “Adversarial learning for neural dialogue generation,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Copenhagen, Denmark: Association for Computational Linguistics, September 2017, pp. 2157- S. J. Rennie et al., “Self-critical sequence training for image caption- ing,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 7008-R. Cai et al., “Tag: Type auxiliary guiding for code comment genera- tion,” arXiv preprint arXiv:2005.02835, P. Dognin et al., “ReGen: Reinforcement learning for text and knowledge base generation using pretrained language models,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, November 2021, pp. 1084- R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus- based image description evaluation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 4566-S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evalua- tion with improved correlation with human judgments,” in Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005, pp. 65-M. Popovié, “chrf++: words helping character n-grams,” in Proceedings of the second conference on machine translation, 2017, pp. 612- June 2018, pp. 1747- org/N18- 1158, Y.-C. Chen et al., “Fast abstractive summarization with reinforce- selected sentence rewriting,” in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Melbourne, Australia: Association for Computational Linguisti . 675- : Generating natural language feedback with reinforcement learning for repairing model outputs,” arXiv preprint arXiv:2305.08844, E. Todorov et al., “Linearly-solvable markov decision problems,” Advances in neural information processing systems, vol. 19, Y. Gao et al., “Reward learning for efficient reinforcement learning in extractive document summarisation,” arXiv preprint arXiv:1907.12894, 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 26 K. Nguyen et al., “Reinforcement learning for bandit neural machine translation with simulated human feedback,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Copenhagen, Denmark: Association for Computational Lingui: , September 2017, pp. 1464- R. S. Sutton et al., Temporal credit assignment in reinforcement learning. University of Massachusetts Amherst, T. Parshakova et al., “Distributional reinforcement learning for energy- based sequential models,” arXiv preprint arXiv:1912.08517, M. Khalifa et al., “A distributional approach to controlled text gener- ation,” arXiv preprint arXiv:2012.11635, T. Korbak et al., “Energy-based models for code generation under compilability constraints,” arXiv preprint arXiv:2106.04985, T. et al. Korbak, “On reward maximization and distribution matching for fine-tuning language models,” https://openreview.net/forum ?id=8f95ajHrIFc D. Go et al., “Aligning language models with preferences through f- divergence minimization,” arXiv preprint arXiv:2302.08215, B. et al. Zoph, “Neural architecture search with reinforcement learning,” in International Conference on Learning Representations, C. et al. Hsu, “MONAS: multi-objective neural architecture search using reinforcement learning,’ CoRR, vol. abs/1806.10332, N.-Q. Pham et al., “Towards one-shot learning for rare-word translation with external experts,” in Proceedings of the 2nd Workshop on Neural Machine Translation and Generation. Melbourne, Australia: Association for Computational Linguistics, July 2018, pp. 100- M. Guo et al., “Irlas: Inverse reinforcement learning for architecture search,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9021-Z. Zhong et al., “Practical block-wise neural network architecture generation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 2423-Y. Tian et al., “Off-policy reinforcement learning for efficient and ef- fective gan architecture search,” in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VII X. Chen et al., “Catch: Context-based meta reinforcement learning for transferrable architecture search,” in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Pro- ceedings, Part XIX B. et al. Baker, “Designing neural network architectures using reinforcement learning,” in International Conference on Learning Representations, forum?id=S lc2cvqee’ D. Pang et al., “Rl-darts: Differentiable neural architecture search via reinforcement-learning-based meta-optimizer,” Knowledge-Based Systems, vol. 234, p. 107585, A. Chauhan et al., “Dqnas: Neural architecture search using reinforce- ment learning,” arXiv preprint arXiv:2301.06687, K. He et al., “Deep residual learning for image recognition,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June J. Rijsdijk et al., “Reinforcement learning for hyperparameter tuning in deep learning-based side-channel analysis,” JACR Transactions on Cryptographic Hardware and Embedded Systems, pp. 677-707, L. Wang et al., “A reinforced topic-aware convolutional sequence-to- sequence model for abstractive text summarization,” in Proceedings of the 27th International Joint Conference on Artificial Intelligence, ser. CAL Y. Wu et al., “Learning to extract coherent summary via deep reinforce- ment learning,” in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, L. Gui et al., “Neural topic model with reinforcement learning,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 3478— JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 151 152 J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano, “Recursively summarizing books with human feedback,” arXiv preprint arXiv:2109.10862, M. Yang, C. Li, Y. Shen, Q. Wu, Z. Zhao, and X. Chen, “Hierarchical human-like deep neural networks for abstractive text summarization,” IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 6, pp. 2744-2757, R. Jie, X. Meng, L. Shang, X. Jiang, and Q. Liu, “Prompt-based length controlled generation with reinforcement learning,” arXiv preprint arXiv:2308.12030, L. Wu et al., “A study of reinforcement learning for neural machine translation,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, October-November 2018, pp. 3612- T. K. Lam et al., “A reinforcement learning approach to interactive- predictive neural machine translation,” in Proceedings of the 21st Annual Conference of the European Association for Machine Translation, Alicante, Spain, May 2018, pp. 189- T. Zhao et al., “Balancing quality and human involvement: An effective approach to interactive neural machine translation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, 2020, pp. 9660-F Luo et al., “Towards fine-grained text sentiment transfer,” in Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2013-A. Anuchitanukul et al., “SURF: Semantic-level unsupervised reward function for machine translation,” in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Seattle, United States: Association for Computational Linguistics, July 2022, pp. 4508- LV. Serban et al., “A deep reinforcement learning chatbot,” arXiv preprint arXiv: 1709.02349, M. Yang et al., “Personalized response generation via domain adapta- tion,” in Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2017, pp. 1021-Z. Sun et al., “Replicating complex dialogue policy of humans via of- fline imitation learning with supervised regularization,” arXiv preprint arXiv:2305.03987, J. D. Williams et al., “Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vancouver, Canada: Association for Computational Linguistics, July 2017, pp. 665- M. Lewis et al., “Deal or no deal? end-to-end learning of negotiation dialogues,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Copenhagen, Denmark: Association for Computational Linguistics, September 2017, pp. 2443- X. Li et al., “End-to-end task-completion neural dialogue systems,” in Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Taipei, Taiwan: Asian Federation of Natural Language Processing, November 2017, pp. 733- A. Martin et al., “Learning natural language generation with truncated reinforcement learning,” in Proceedings of the 2022 Conference of 154 155 156 157 158 159 160 161 162 164 165 166 167 168 169 170 171 172 174 27 the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 12-Y. Jang, J. Lee, and K.-E. Kim, “Gpt-critic: Offline reinforcement learn- ing for end-to-end task-oriented dialogue systems,” in International Conference on Learning Representations, C. Zhong, K. Liao, W. Chen, Q. Liu, B. Peng, X. Huang, J. Peng, and Z. Wei, “Hierarchical reinforcement learning for automatic disease diagnosis,” Bioinformatics, vol. 38, no. 16, pp. 3995-4001, A. Saleh, N. Jaques, A. Ghandeharioun, J. Shen, and R. Picard, “Hierar- chical reinforcement learning for open-domain dialog,” in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 8741-R. Yang, J. Chen, and K. Narasimhan, “Improving dialog sys- tems for negotiation with personality modeling,” arXiv preprint arXiv:2010.09954, A. Srivastava et al., “Response-act guided reinforced dialogue gener- ation for mental health counseling,” in Proceedings of the ACM Web Conference 2023, 2023, pp. 1118-R. Liu et al., “Aligning generative language models with human values,” in Findings of the Association for Computational Linguistics: NAACL 2022, 2022, pp. 241-R. Zhang et al., “Reward constrained interactive recommendation with natural language feedback,” arXiv preprint arXiv:2005.01618, A. K. Mohankumar et al., “Diversity driven query rewriting in search advertising,” in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, ser. KDD ’ X. Wang et al., “Ordering-based causal discovery with reinforcement learning,” arXiv preprint arXiv:2105.06631, A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs et al., “Training language models to self-correct via reinforcement learning,” arXiv preprint arXiv: L. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, A. Zipori, H. Noga, O. Keller, B. Piot, I. Szpektor et al., “Multi-turn rein- forcement learning from preference human feedback,” arXiv preprint arXiv:2405.14655, Y. Zhou, A. Zanette, J. Pan, S. Levine, and A. Kumar, “Archer: Training language model agents via hierarchical multi-turn rl,” arXiv preprint arXiv:2402.19446, A. Alabdulkarim, W. Li, L. J. Martin, and M. O. Riedl, “Goal- directed story generation: Augmenting generative language models with reinforcement learning,” arXiv preprint arXiv:2112.08593, P. Jha, P. Jana, P. Suresh, A. Arora, and V. Ganesh, “Risf: Reinforcement learning via symbolic feedback,” arXiv preprint arXiv:2405.16661, Z. Chen, K. Zhou, W. X. Zhao, J. Wan, F. Zhang, D. Zhang, and J.-R. Wen, “Improving large language models via fine-grained rein- forcement learning with minimum editing constraint,” arXiv preprint arXiv:2401.06081, W. Li, W. Wei, K. Xu, W. Xie, D. Chen, and Y. Cheng, “Reinforcement learning with token-level feedback for controllable text generation,” arXiv preprint arXiv:2403.11558, J. Li, H. Zhang, F. Zhang, T.-W. Chang, K. Kuang, L. Chen, and J. Zhou, “Optimizing language models with fair and stable reward composition in reinforcement learning,” in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024, pp. 10 122-10 Z. Li, T. Xu, Y. Zhang, Z. Lin, Y. Yu, R. Sun, and Z.-Q. Luo, “Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models,” in Forty-first International Conference on Machine Learning, P. H. Richemond, Y. Tang, D. Guo, D. Calandriello, M. G. Azar, R. Rafailov, B. A. Pires, E. Tarassov, L. Spangher, W. Ellsworth et al., “Offline regularised reinforcement learning for large language models alignment,” arXiv preprint arXiv:2405.19107, R. et al. Rafailov, “Direct preference optimization: Your language model is secretly a reward model,” JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 175 176 177 178 179 181 182 183 184 185 186 187 188 189 191 192 193 194 195 196 A. Baheti, X. Lu, F. Brahman, R. L. Bras, M. Sap, and M. Riedl, “Leftover lunch: Advantage-based offline reinforcement learning for language models,” arXiv preprint arXiv:2305.14718, M. Wulfmeier, M. Bloesch, N. Vieillard, A. Ahuja, J. Bornschein, S. Huang, A. Sokolov, M. Barnes, G. Desjardins, A. Bewley er al., “Imitating language via scalable inverse reinforcement learning,” in The Thirty-eighth Annual Conference on Neural Information Processing Systems, T. Korbak, K. Shi, A. Chen, R. V. Bhalerao, C. Buckley, J. Phang, S. R. Bowman, and E. Perez, “Pretraining language models with human preferences,” in International Conference on Machine Learning. PMLR, 2023, pp. 17506-C. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine, “Offline rl for natural language generation with implicit language q learning,” arXiv preprint arXiv:2206.11871, J. Hu, L. Tao, J. Yang, and C. Zhou, “Aligning language models with offline reinforcement learning from human feedback,” arXiv preprint arXiv:2308.12050, M. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E. P. Xing, and Z. Hu, “Rlprompt: Optimizing discrete text prompts with reinforcement learning,” arXiv preprint arXiv:2205.12548, Z. Huang, X. Wang, F. Zhang, Z. Xu, C. Zhang, X. Zheng, and X. Huang, “Enhancing the capability and robustness of large language models through reinforcement learning-driven query refinement,” arXiv preprint arXiv:2407.01461, H. Sun, “Offline prompt evaluation and optimization with inverse reinforcement learning,” arXiv preprint arXiv:2309.06553, K. Nottingham, Y. Razeghi, K. Kim, J. Lanier, P. Baldi, R. Fox, and S. Singh, “Selective perception: Optimizing state descriptions with reinforcement learning for language model actors,” arXiv preprint arXiv:2307.11922, W. Shen, R. Zheng, W. Zhan, J. Zhao, S. Dou, T. Gui, Q. Zhang, and X. Huang, “Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback,” arXiv preprint arXiv:2310.05199, J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang, “Safe rlhf: Safe reinforcement learning from human feedback,” arXiv preprint arXiv:2310.12773, T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, and M. N. Halgamuge, “The inadequacy of reinforcement learning from human feedback- radicalizing large language models via semantic vulnerabilities,” JEEE Transactions on Cognitive and Developmental Systems, J. Wang, J. Wu, M. Chen, Y. Vorobeychik, and C. Xiao, “Rlhfpoison: Reward poisoning attack for reinforcement learning with human feed- back in large language models,” in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 2551-X. Wang, J. Peng, K. Xu, H. Yao, and T. Chen, “Reinforcement learning-driven Ilm agent for automated attacks on Ilms,” in Proceed- ings of the Fifth Workshop on Privacy in Natural Language Processing, 2024, pp. 170-Y. Li et al., “A generative model for category text generation,” Information Sciences, vol. 450, pp. 301-315, J. Xu et al., “Diversity-promoting gan: A cross-entropy based genera- tive adversarial network for diversified text generation,” in Proceedings of the 2018 conference on empirical methods in natural language processing, 2018, pp. 3940-Q. Wu et al., “Automatic math word problem generation with topic-expression co-attention mechanism and reinforcement learning,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 1061-1072, B. Upadhyay et al., “Efficient reinforcement learning for unsupervised controlled text generation,” arXiv preprint arXiv:2204.07696, C. Wang, H. Zhou, Y. Hu, Y. Huo, B. Li, T. Liu, T. Xiao, and J. Zhu, “Esrl: Efficient sampling-based reinforcement learning for sequence generation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 17, 2024, pp. 19 107-19 N. Jaques, J. H. Shen, A. Ghandeharioun, C. Ferguson, A. Lapedriza, N. Jones, S. S. Gu, and R. Picard, “Human-centric dialog training 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 28 via offline reinforcement learning,” arXiv preprint arXiv:2010.05848, D. Yarats et al., “Hierarchical text generation and planning for strategic dialogue,” in International Conference on Machine Learning. PMLR, 2018, pp. 5591-J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High- dimensional continuous control using generalized advantage estima- tion,” arXiv preprint arXiv:1506.02438, N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,” Advances in Neural Information Processing Systems, vol. 36, A. Havrilla, M. Zhuravinskyi, D. Phung, A. Tiwari, J. Tow, S. Bi- derman, Q. Anthony, and L. Castricato, “trix: A framework for large scale reinforcement learning from human feedback,” in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 8578-S. Zhang, Z. Chen, S. Chen, Y. Shen, Z. Sun, and C. Gan, “Improving reinforcement learning from human feedback with efficient reward model ensemble,” arXiv preprint arXiv:2401.16635, A. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar, and R. Raileanu, “Teach- ing large language models to reason with reinforcement learning,” arXiv preprint arXiv:2403.04642, Z. Yao et al., “Coacor: Code annotation for code retrieval with reinforcement learning,” in The world wide web conference, 2019, pp. 2203-C. Wang et al., “Enriching query semantics for code search with reinforcement learning,’ Neural Networks, vol. 145, pp. 22-32, W. Wang et al., “Reinforcement-learning-guided source code summa- rization using hierarchical attention,” JEEE Transactions on software Engineering, vol. 48, no. 1, pp. 102-119, X. Wang et al., “Compilable neural code generation with compiler feedback,” arXiv preprint arXiv:2203.05132, L. Zhang et al., “Learnedsqlgen: Constraint-aware sq] generation using reinforcement learning,” in Proceedings of the 2022 International Conference on Management of Data, ser. SIGMOD ’ H. Le et al., “Coderl: Mastering code generation through pretrained models and deep reinforcement learning,” Advances in Neural Infor- mation Processing Systems, vol. 35, pp. 21314-21328, P. Shojaee et al., “Execution-based code generation using deep rein- forcement learning,” arXiv preprint arXiv:2301.13816, S. Duan, N. Kanakaris, X. Xiao, H. Ping, C. Zhou, N. K. Ahmed, G. Ma, M. Capota, T. L. Willke, S. Nazarian ef al., “Leveraging rein- forcement learning and large language models for code optimization,” arXiv preprint arXiv:2312.05657, J. Liu, Y. Zhu, K. Xiao, Q. Fu, X. Han, W. Yang, and D. Ye, “Ritf: Reinforcement learning from unit test feedback,” arXiv preprint arXiv:2307.04349, J. Gehring, K. Zheng, J. Copet, V. Mella, T. Cohen, and G. Synnaeve, “Rlef: Grounding code Ilms in execution feedback with reinforcement learning,” arXiv preprint arXiv:2410.02089, S. Dou, Y. Liu, H. Jia, L. Xiong, E. Zhou, W. Shen, J. Shan, C. Huang, X. Wang, X. Fan et al., “Stepcoder: Improve code generation with reinforcement learning from compiler feedback,” arXiv preprint arXiv:2402.01391, B. Steenhoek, M. Tufano, N. Sundaresan, and A. Svyatkovskiy, “Rein- forcement learning from automatic feedback for high-quality unit test generation,” arXiv preprint arXiv:2310.02368, Z. Ren et al., “Deep reinforcement learning-based image captioning with embedding reward,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 290-L. Zhang et al., “Actor-critic sequence training for image captioning,” arXiv preprint arXiv:1706.09601, L. Miao et al., “Multi-modal product title compression,” Information Processing & Management, vol. 57, no. 1, p. 102123, JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 X. Shen, B. Liu, Y. Zhou, J. Zhao, and M. Liu, “Remote sensing image captioning via variational autoencoder and reinforcement learning,” Knowledge-Based Systems, vol. 203, p. 105920, J. Shi, Y. Li, and S. Wang, “Partial off-policy learning: Balance accuracy and diversity for human-oriented image captioning,” in Pro- ceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2187-W. Nie, J. Li, N. Xu, A.-A. Liu, X. Li, and Y. Zhang, “Triangle- reward reinforcement learning: a visual-linguistic semantic alignment for image captioning,” in Proceedings of the 29th ACM international conference on multimedia, 2021, pp. 4510-R. Dessi, M. Bevilacqua, E. Gualdoni, N. C. Rakotonirina, F. Franzon, and M. Baroni, “Cross-domain image captioning with discriminative finetuning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 6935-J. Zhang et al., “Goal-oriented visual question generation via intermedi- ate rewards,” in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 186-Z. Zhao et al., “Open-ended video question answering via multi- modal conditional adversarial networks,” JEEE Transactions on Image Processing, vol. 29, pp. 3859-3870, R. Saqur and K. Narasimhan, “Multimodal graph networks for com- positional generalization in visual question answering,” Advances in Neural Information Processing Systems, vol. 33, pp. 3070-3081, J. Zhang et al., “Sch-gan: Semi-supervised cross-modal hashing by generative adversarial network,’ JEEE transactions on cybernetics, vol. 50, no. 2, pp. 489-502, K. Black et al., “Training diffusion models with reinforcement learn- ing,” arXiv preprint arXiv:2305.13301, Y. Fan et al., “Dpok: Reinforcement learning for fine-tuning text-to- image diffusion models,” arXiv preprint arXiv:2305.16381, Y. Akizuki et al., “Generative modelling with design constraints— reinforcement learning for object generation,” in RE: Anthropocene, Design in the Age of Humans—Proceedings of the 25th CAADRIA Conference, vol. A. Ostonov et al., “Rlss: A deep reinforcement learning algorithm for sequential scene generation,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 2219- Z. Zhang et al., “Point cloud scene completion with joint color and semantic estimation from single rgb-d image,” JEEE Transactions on Pattern Analysis and Machine Intelligence, C. Lin, T. Fan, W. Wang, and M. NieBner, “Modeling 3d shapes by reinforcement learning,” in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part X K. Zhao, Y. Zhang, S. Wang, T. Beeler, and S. Tang, “Synthesizing diverse human motions in 3d indoor scenes,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 14738-L. Siyao, W. Yu, T. Gu, C. Lin, Q. Wang, C. Qian, C. C. Loy, and Z. Liu, “Bailando: 3d dance generation by actor-critic gpt with choreographic memory,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 11 050-11 D. Deng, A. Wu, H. Qu, and Y. Wu, “Dashbot: Insight-driven dashboard generation based on deep reinforcement learning,” JEEE Transactions on Visualization and Computer Graphics, vol. 29, no. 1, pp. 690-700, T. Liu, Q. Meng, J.-J. Huang, A. Vlontzos, D. Rueckert, and B. Kainz, “Video summarization through reinforcement learning with a 3d spatio- temporal u-net,” JEEE transactions on image processing, vol. 31, pp. 1573-1586, Y. Yu, J. Chung, H. Yun, J. Hessel, J. S. Park, X. Lu, R. Zellers, P. Am- manabrolu, R. Le Bras, G. Kim ef al., “Fusing pre-trained language 242 243 244 245 246 247 248 249 250 251 253 254 255 256 257 258 259 260 261 262 263 264 265 29 models with multimodal prompts through reinforcement learning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 10 845-10 Y. Zhai, H. Bai, Z. Lin, J. Pan, S. Tong, Y. Zhou, A. Suhr, S. Xie, Y. LeCun, Y. Ma ef al., “Fine-tuning large vision-language models as decision-making agents via reinforcement learning,” arXiv preprint arXiv:2405.10292, P. H. Seo, P. Sharma, T. Levinboim, B. Han, and R. Soricut, “Rein- forcing an image caption generator using off-line human feedback,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 03, 2020, pp. 2693-H. De Vries et al., “Guesswhat?! visual object discovery through multi- modal dialogue,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5503-D. S. R. Mohan et al., “Incremental text to speech for neural sequence- to-sequence models using reinforcement learning,” arXiv preprint arXiv:2008.03096, R. Liu et al., “Reinforcement learning for emotional text-to-speech synthesis with improved emotion discriminability,’ arXiv preprint arXiv:2104.01408, J. Gibson et al., “A reinforcement learning approach to speech coding,” Information, vol. 13, no. 7, p. 331, Z. Li et al., “A symbolic-domain music generation method based on leak-gan,” in 202] 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST). EEE, 2021, pp. 549-J. Guo et al., “Long text generation via adversarial training with leaked information,” in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, E. Putin et al., “Reinforced adversarial neural computer for de novo molecular design,” Journal of chemical information and modeling, vol. 58, no. 6, pp. 1194-1204, M. Popova et al., “Deep reinforcement learning for de novo drug design,” Science advances, vol. 4, no. 7, p. eaap7885, E. Putin et al., “Adversarial threshold neural computer for molecular de novo design,” Molecular pharmaceutics, vol. 15, no. 10, pp. 4386- 4397, N. De Cao et al., “Molgan: An implicit generative model for small molecular graphs,” arXiv preprint arXiv: 1805.11973, M. Goel et al., “Molegular: molecule generation using reinforcement learning with alternating rewards,” Journal of Chemical Information and Modeling, vol. 61, no. 12, pp. 5815-5826, T. Blaschke et al., “Memory-assisted reinforcement learning for diverse molecular de novo design,” Journal of cheminformatics, vol. 12, no. 1, pp. 1-17, M. Thomas et al., “Augmented hill-climb increases reinforcement learning efficiency for language-based de novo molecule generation,” Journal of Cheminformatics, vol. 14, no. 1, pp. 1-22, N. Brown et al., “Guacamol: benchmarking models for de novo molecular design,” Journal of chemical information and modeling, vol. 59, no. 3, pp. 1096-1108, B. Chen et al., “Fragment-based sequential translation for molecular optimization,” arXiv preprint arXiv:2111.01009, R. Ishitani et al., “Molecular design method using a reversible tree rep- resentation of chemical compounds and deep reinforcement learning,” Journal of Chemical Information and Modeling, vol. 62, no. 17, pp. 4032-4048, T. Fu et al., “Reinforced genetic algorithm for structure-based drug design,” Advances in Neural Information Processing Systems, vol. 35, pp. 12325-12338, P. Hu et al., “De novo drug design based on stack-rnn with multi- objective reward-weighted sum and reinforcement learning,” Journal of Molecular Modeling, vol. 29, no. 4, pp. 1-12, M. Sun et al., “Molsearch: Search-based multi-objective molecular generation and property optimization,” in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 4724-X. Liu et al., “Drugex v3: scaffold-constrained drug design with graph transformer-based reinforcement learning,” Journal of Cheminformat- ics, vol. 15, no. 1, p. 24, G. Simm, R. Pinsler, and J. M. Hernéndez-Lobato, “Reinforcement learning for molecular design guided by quantum mechanics,” in International Conference on Machine Learning. _PMLR, 2020, pp. 8959-A. Zholus, M. Kuznetsov, R. Schutski, R. Shayakhmetov, D. Polykovskiy, S. Chandar, and A. Zhavoronkov, “Bindgpt: A JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 scalable framework for 3d molecular design via language modeling and reinforcement learning,” arXiv preprint arXiv:2406.03686, S. K. Gottipati, B. Sattarov, S. Niu, Y. Pathak, H. Wei, S. Liu, S. Black- burn, K. Thomas, C. Coley, J. Tang er al., “Learning to navigate the synthetically accessible chemical space using reinforcement learning,” in International conference on machine learning. _PMLR, 2020, pp. 3668-P. Rajak, A. Krishnamoorthy, A. Mishra, R. Kalia, A. Nakano, and P. Vashishta, “Autonomous reinforcement learning agent for chemical vapor deposition synthesis of quantum materials,” npj Computational Materials, vol. 7, no. 1, p. 108, K. M. Powell, D. Machalek, and T. Quah, “Real-time optimization using reinforcement learning,” Computers & Chemical Engineering, vol. 143, p. 107077, P.C. Nguyen, N. N. Vlassis, B. Bahmani, W. Sun, H. Udaykumar, and S. S. Baek, “Synthesizing controlled microstructures of porous media using generative adversarial networks and reinforcement learning,” Scientific reports, vol. 12, no. 1, p. 9034, E.-J. Kuo et al., “Quantum architecture search via deep reinforcement learning,” arXiv preprint arXiv:2104.07715, J. Lin, Z. Y. Lai, and X. Li, “Quantum adiabatic algorithm design using reinforcement learning,” Physical Review A, vol. 101, no. 5, p. 052327, M. Ostaszewski, L. M. Trenkwalder, W. Masarczyk, E. Scerri, and V. Dunjko, “Reinforcement learning for optimization of variational quantum circuit architectures,” Advances in Neural Information Pro- cessing Systems, vol. 34, pp. 18 182-18 194, L. Moro, M. G. Paris, M. Restelli, and E. Prati, “Quantum compiling by deep reinforcement learning,” Communications Physics, vol. 4, no. 1, p. 178, B. Sanchez-Lengeling et al., “Optimizing distributions over molecu- lar space. an objective-reinforced generative adversarial network for inverse-design chemistry (organic),” M. Olivecrona et al., “Molecular de-novo design through deep rein- forcement learning,” Journal of cheminformatics, vol. 9, no. 1, pp. 1-14, S. R. Atance et al., “De novo drug design using reinforcement learn- ing with graph-based deep generative models,” Journal of Chemical Information and Modeling, vol. 62, no. 20, pp. 4863-4872, G. Brockman et al., “Openai gym,” arXiv preprint arXiv:1606.01540, X. et al. Chen, “A survey of deep reinforcement learning in recommender systems: A systematic review and future directions,” CoRR, vol. abs/2109.03540, S. et al. Wang, “Causal decision transformer for recommender systems via offline reinforcement learning,” W. et al. Huang, “Voxposer: Composable 3d value maps for robotic manipulation with language models,” G. et al. Wang, “Voyager: An open-ended embodied agent with large language models,” A. Khalifa et al., “Pegrl: Procedural content generation via reinforce- ment learning,” in Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, vol. 16, no. 1, 2020, pp. 95-D. Ha et al., “Reinforcement learning for improving agent design,” Artificial life, vol. 25, no. 4, pp. 352-365, V.-A. Darvariu et al., “Goal-directed graph construction using rein- forcement learning,” Proceedings of the Royal Society A, vol. 477, no. 2254, p. 20210168, O. Bar El, T. Milo, and A. Somech, “Automatically generating data ex- ploration sessions using deep reinforcement learning,” in Proceedings of the 2020 ACM SIGMOD international conference on management of data, 2020, pp. 1527-L. et al. Choshen, “On the weaknesses of reinforcement learning for neural machine translation,” in International Conference on Learning Representations, 289 290 291 292 293 294 295 296 297 298 299 301 302 303 304 305 306 307 30 S. Kiegeland et al., “Revisiting the weaknesses of reinforcement learn- ing for neural machine translation,” arXiv preprint arXiv:2106.08942, R. Ramamurthy et al., “Is reinforcement learning (not) for natural lan- guage processing?: Benchmarks, baselines, and building blocks for nat- ural language policy optimization,” arXiv preprint arXiv:2210.01241, D. Donato et al., “Mad for robust reinforcement learning in machine translation,” arXiv preprint arXiv:2207.08583, U. Honda et al., “Switching to discriminative image captioning by relieving a bottleneck of reinforcement learning,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 1124-W. Jin et al., “Multi-objective molecule generation using interpretable substructures,” in International conference on machine learning. PMLR, 2020, pp. 4849-H. Guo et al., “Efficient (soft) q-learning for text generation with limited good data,” in Findings of the Association for Computational Linguistics: EMNLP 2022, 2022, pp. 6969-M. Korshunova et al., “Generative and reinforcement learning ap- proaches for the automated de novo design of bioactive compounds,” Communications Chemistry, vol. 5, no. 1, p. 129, Z. Yao et al., “An imitation game for learning semantic parsers from user interaction,” arXiv preprint arXiv:2005.00689, A. S. Vezhnevets et al., “Feudal networks for hierarchical reinforcement learning,” in International Conference on Machine Learning. PMLR, 2017, pp. 3540-T. Zhao et al., “A multi-scenario text generation method based on meta reinforcement learning,” Pattern Recognition Letters, vol. 165, pp. 47— 54, Z. Zhao, W. S. Lee, and D. Hsu, “Large language models as common- sense knowledge for large-scale task planning,” Advances in Neural Information Processing Systems, vol. 36, D. Zhang, S. Zhoubian, Y. Yue, Y. Dong, and J. Tang, “Rest-mcts*: Lim self-training via process reward guided tree search,” arXiv preprint arXiv:2406.03816, S. Ross et al., “A reduction of imitation learning and structured pre- diction to no-regret online learning,” in Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011, pp. 627-T. Hospedales et al., “Meta-learning in neural networks: A survey,” IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 9, pp. 5149-5169, O. et al. Ended Learning Team, “Open-ended learning leads to generally capable agents,” CoRR, vol. abs/2107.12808, Available: M learning for fast adaptation of deep networks,” in International conference on machine learning. PMLR, 2017, pp. 1126-H. et al. Liu, “Evaluating the logical reasoning ability of chatgpt and gpt-4.” A. Tripp et al., “Sample-efficient optimization in the latent space of deep generative models via weighted retraining,” Advances in Neural Information Processing Systems, vol. 33, pp. 11259-11272, B. Schélkopf et al., “Toward causal representation learning,” Proceed- ings of the IEEE, vol. 109, no. 5, pp. 612-634, 2021.'),\n",
       " Document(metadata={'source': '/tmp/tmpyo765sav/tmp.pdf'}, page_content='arX1iv:2312.10256v2 [cs.MA] 3 Jul 2024 Multi-agent Reinforcement Learning: A Comprehensive Survey Dom Huh! and Prasant Mohapatra! ‘University of California, Davis {dhuh, pmohapatra}@ucdavis.edu 3University of South Florida Abstract Multi-agent systems (MAS) are widely prevalent and crucially important in numer- ous real-world applications, where multiple agents must make decisions to achieve their objectives in a shared environment. Despite their ubiquity, the development of intelligent decision-making agents in MAS poses several open challenges to their fective implementation. This survey examines these challenges, placing an em- phasis on studying seminal concepts from game theory (GT) and machine learning (ML) and connecting them to recent advancements in multi-agent reinforcement learning (MARL), i.e. the research of data-driven decision-making within MAS. Therefore, the objective of this survey is to provide a comprehensive perspective along the various dimensions of MARL, shedding light on the unique opportu- nities that are presented in MARL applications while highlighting the inherent hallenges that accompany this potential. Therefore, we hope that our work will not only contribute to the field by analyzing the current landscape of MARL but also motivate future directions with insights for deeper integration of concepts from related domains of GT and ML. With this in mind, this work delves into a detailed exploration of recent and past efforts of MARL and its related fields and describes prior solutions that were proposed and their limitations, as well as their applications. fo} fe) 1 Introduction Multi-agent reinforcement learning (MARL) has long been recognized as a pivotal domain in artificial intelligence (AJ), promising dynamic solutions for complex tasks within multi-agent systems (MAS) that involve multiple goal-oriented decision-making, i.e. control, agents. The importance of devising such solutions is evident, as it enables the realization of a wide array of real-world applications, where the consideration of the existence of other intelligent agents is required. The process of developing these agents is largely centered around facilitating the emergence of not only decision-making abilities but also an adept understanding of social dynamics in a data-driven manner. Hence, with proper modeling and learning methods, these agents strive to leverage the multi-agent nature of their shared environment to achieve their individual and collective goals. Alongside this focus on social behaviors and their connection to an agent’s decision-making capabili- ties, the motivation to integrate concepts from related domains such as game theory (GT) and machine learning (ML) becomes critical, as GT and ML provide a rich background and broader perspective to the problem posed by MARL. However, it remains equally important to concurrently study the distinctive challenges that arise under the MARL paradigm|Stone and Veloso 2000}, Bernstein et al. (2013) to fully understand the field’s unique intricacies and promote potential breakthroughs. In the past decade, there has been a significant interest in MARL efforts, poised to endow the desirable behavioral qualities that characterize intelligent social agents through data-driven learning processes . These learning processes have ranged from assuming ical perspectives of full rationality to embodying models of bounded rationality[Shoham et al.| , wherein agents progressively refine their behaviors in a myopic and iterative manner over time and experience. A common theme of this survey is emphasizing the increasing incorporation of the realism and its complexities that pervades real-world MAS applications into our MARL solutions. And despite the many successes in MARL in these prior efforts, many open challenges persist, paving the way for future research endeavors. The central purpose of this survey is to provide a comprehensive view of these efforts, concretely staging the current state of MARL research from a holistic perspective. The structure of this survey is as follows. In Section [3] we define the problem statement of learning optimal control within MAS, and discuss seminal concepts from foundational fields of MARL such as GT and ML ~ all of which provide the foundation to the recent ideas studied in MARL. In Section [4] we discuss the unique benefits and challenges of learning in a MAS and explore the learning pathologies that plague the MARL paradigms. Lastly, in Section [5] the prospects of MARL are studied, such as MARL-specific simulation, training paradigms, communication methods, the challenges of multi-agent credit assignment and ad-hoc team-play, social learning, and agent modeling, and a detailed discussion regarding the recent efforts associated to these prospects. 2 Related Surveys There exists a rich literature surveying various research topics in MARL, from general overviews . (2003 ,/Busoniu et al. 2008], Bloembergen et al.|[2015], it [ [2022] to specific topics such as dealing with non-stationarity /Hernandez-Leal Matignon et al.|[2012], multi-agent learning , communication perspectives are constrained, excluding important aspects that come only from GT and ML viewpoints, thereby lacking the context needed to fully realize the current developments and limitations of MARL. In contrast, our article presents a more modern and comprehensive survey that aims to provide a holistic view of the challenges inherent and unique to learning control within multi-agent environ- ments. Additionally, we provide context to these challenges by weaving together the perspectives from GT and ML into a new unified view to present a novel understanding of the distinctive nature of the MARL problem. 3 Background In this section, we formalize the basis of the MARL problem statement and define its learning goals and solution concepts. We additionally explore related fields, i.e. GT and ML, that contribute to a deeper understanding of the study of MARL. 3.1 Multi-agent Environment A MAS consists of a population of decision-making agents that exist within a shared environment, as illustrated in Figure[I] These agents observe their environment and communicate with one another to perform actions that align with their objectives. This observation and communication amongst agents is constrained by decentralization. Definition 1 (Decentralization) In a decentralized setting, each agent is capable of perceiving its environment, communicating with other agents, and taking action autonomously. There exist two forms of decentralization. Natural decentralization is the limitations imposed by physical realities, like communication range, while artificial decentralization involves specific requirements to improve tractability, such as communication bandwidth |Whiteson| [2020]. More generally, each agent is defined and inhibited by a set constraint. The agents can devise strategies to make decisions to reach their defined goals. This causal association between stimuli, action, and objective is what we refer to as the agent’s behavior[Matarid|(1994]. The Environment environment then responds to the agent’s actions, transitioning the current setting to the next state and providing the agents with feedback signals. This response from the environment is referred to as the model. This process between the agents and their environment forms a closed-looped interaction that iterates until a terminal condition is met. The MAS setting described here is widely prevalent in real-world applications, encompassing au- tonomous vehicles |Shalev-Shwartz et al. 2016], Zhou et al.| 2020], internet marketing (2018aJ, multi-robot contro [1997], networking applications such as optimizing communica- | tion networks|Luong et al.|[2019] and traffic control|Calvo and Dusparic (2018), Chu et al.}[2019], and multiplayer game playing|Samvelyan et al.|[2019]. 3.2 Stochastic Game We now introduce a formal representation of the MAS setting described in Section[3-1] called the stochastic game|Shapley| [1953], [2001], where the term “game” refers to the interactions between strategic agents. This framework serves as the basis for a wide range of multi- agent applications[Busoniu et al.] ]. and is related to other models of games, as seen in Figure Definition 2 (Stochastic Game) A stochastic game is a 5-tuple (N,S,A,r,7) where these elements are defined as: ¢ Nis the set of n agents. S is the set of (global) states. «© A= Apo x Ai X +--+ X An is the joint action space, where A; is the action space of agent T:S x Ax S++ P(S) is the state transition operator which maps a state-action pair to the probability of next states. The state defines the global setting and configuration of the environment. To transition from one state to another, each agent i uses their policy 7; : S +» A; —a functional representation of a composite of the agent’s behaviors that is expressed as a mapping from its perceived state to action — to make decisions, otherwise known as their strategy. Generally, a policy returns a probability distribution over the action space conditioned on the state. > t(a\\\\s) =1 acA The joint policy 7 : S++ A is defined as a mapping to the joint action space, commonly achieved by concatenating the local actions computed by each agent’s policies. We introduce the concept of an information set as an aggregate state, where it encapsulates all information available to an agent during its decision-making process. The state transition function 7 returns the probability of reaching certain states from a given state- action pair. As each state holds the Markov property, i.e. every state is sufficient to infer the future, Partially Observable Stochastic Game agents Bayesian Game ‘m states partially observable agents - with type space agents im states - fully observable sequential actions the state transition function is sufficiently conditioned on the current state and action. T(s,0, 8\") = P(si41 = 8\\'|s¢ = 8,0, =a) () Using the stochastic game framework as a basis, we can build and introduce additional concepts to align with various real-world scenarios. Sequential and Macro-Actions In many multi-agent applications, agents may perform actions that occur asynchronously, meaning either agents take turns performing their actions, or their actions take different duration of time. In cases where agents take turns, we distinguish this form of decision-making as sequential and is represented as an extensive-form game, otherwise, we refer to settings where agents’ actions are done at the same time as simultaneous and use the stochastic game framework. In the latter case where actions have varying duration, we turn to an alternate framework called the macro-action stochastic game [Amato et al.|[2019],[Xiao et al.|(2022]. This framework removes the assumption of synchronized primitive-action execution, which assumes that all actions take the same period of time. Hence, the notion of macro-actions is introduced, where macro-actions are temporally-extended actions that can vary in duration. Consequently, this also introduces a notion of macro-observation. Imperfect Information — It may be intractable for agents to directly perceive the (global) state of the environment. Instead, agents may only have access to observations. Thereby, we introduce the joint observation space O = Op x --- x Op, that accounts for the notion of imperfect information|Shoham| Observations do not necessarily satisfy the Markovian property and may contain limited insights regarding the true state of the environment. In fact, even joint observational histories h = {09, 01, ...0¢} can be insufficient to infer the state s; of the environment. However, the contrapositive is true, where the emission transition function €(o0, s) defines the mapping from states to corresponding joint observations that can be potentially induced. E(0, 8) = P(ols) (2) The concept of imperfect information encapsulates the limits of information agents are restricted to, and help enforce the constraints of decentralization. Imperfect information is strongly related to two other important ideas of information limitations: partial observability and incomplete information. Partial observability constrains agents’ access to a subset of the state that can be obscured by noise. In settings with incomplete information, agents do not have common knowledge about the game that is being played, which leads to uncertainties regarding various aspects of the state. A framework that takes these concepts of information limitation into account is called partially-observable stochastic games (POSG), and if agents hold a belief, or type, over these uncertainties, this is called a Bayesian Reward Function Within all models of decision-making, the reward function plays an central role in guiding the behaviors of agents. The reward function provides immediate feedback to agents, evaluating the state and/or action of the agents with respects to their objective in the form of a scalar value. The common goal in optimizing control in MAS is to maximize this reward signal, thereby instilling a degree of rationality. Definition 3 (Rationality) Rational agents take actions that will maximize the expected cumulative reward, known as the expected return, they receive over time. In certain multi-agent applications, it can be difficult to define individualized reward functions for each agent, but it may be trivial to define a single reward function for all agents. This difficulty relates to the nature of the task, often derived from an inability to define proper multi-agent credit assignment. In such applications, we use the decentralized MDP (Dec-MDP), where a centralized reward function returns a collective score shared by all agents. Nature of Interaction An important factor to keep in mind is the relationships between the agents’ actions and decisions and how they interact with one another, which we refer to as the nature of interactions. In each setting, the interplay between each agent’s behaviors can lead to different dynamics and outcomes and may require varying considerations, notably to the definition of the reward function. Cooperative setting - All agents share the aligning goal. ¢ Adversarial setting - Agents have dichotomous goals. ¢ Mixed setting - Agents have varying goals. In a cooperative setting, all agents share aligning goals, where the reward function is typically designed to promote collaboration and joint success. In an adversarial setting, groups of agents may have dichotomous goals, meaning agents that aim to maximize their own reward can impede the achievement of other agents’ goals. In a mixed setting, agents have varying goals, and this introduces additional complexities as agents may have aligning, conflicting, or overlapping interests at various points in time. These interactions can be realized with inherent and artificial structures placed within the task definition. There exists a deep interconnection between the reward function and the nature of interactions, as the reward signals directly guide the emergent behaviors and outcomes within a MAS. For instances, the rewards of each agent may be computed by a function of not only their own policies but also the policies of others. This concept of multi-agent reward dependency is formalized with aggregative games. Social Context The purpose of social context is to ensure mutually consistency between agents’ actions within an shared environment, and is defined by social convention and role assignment/Busoniu] . Social conventions are established through social constructs to often define preferences for joint action profiles and is used to resolve conflicting action selections. Role assignments restrict and define the actions available for each agent as well as influence their rationality and objectives. The properties and conditions of these elements are largely considered task-specific, however, all MARL solutions must take into account these unique characteristics and demands of each situation. Networked Games _ To tractably perform certain multi-agent tasks, agents must communicate with one another. This is often achieved through defined communication channels and form a communication network G(s) that is associated to each state s. This element G(s) = (V, E), where vertices V correspond to the agents and an edge (i, j) € E exists if agents i and 7 can communicate, is appended to the stochastic game framework, forming the networked stochastic game. Coordination The outcome of an agent’s actions can often depend on the actions of other agents. This dependency of behaviors and the corresponding consequences of their actions between agents is encapsulated by the concept of coordination. There are two common approaches of embedding the idea of coordination into a game paradigm, through a coordination graph or interactions. Coordination graphs specify the coordination dependencies between agents’ actions in form of a graph, where the nodes represent agents and the edges represent these dependencies {Nair et al.][2005}, [2001], [Kok et al.|[2005]. This concept is expressed with action-graph games [2011]. With coordination graphs, we can factorize the global utility into a set of local payoff and utility functions. Each payoff function is associated with a subset of agents and can be interpreted as (hyper)-edges in a graph where the nodes are agents|Amato and Oliehoek| 2014}, whereas the utility functions reflect individual agent’s utility. With this factorization, maximum-a-posteriori estimation techniques, such as variable elimination |Guestrin et al.|[2001], max-plus|Vlassis et al.|[2004], or Q-learning [Kok et al_] (2005}. can be used to compute the optimal joint action. More recent extensions leverage deep neural networks to model different components of the factorized value function fet al.|[2020] or the coordination graph itself|Li et al.][202 1]. ¢ Similar to coordination graphs, interactions broadly define specific conditions in which coordination should occur|Koller and Milch||2003]. However, unlike coordination graphs, we can specify restriction of these interactions for only defined for certain states or state- actions [2009} f20TT}, [2010]. Additionally, these interactions are classified as strategic compliments or substitute [2016], where interactions can produce mutual reinforcements or discouragements. Thereby, the concept of coordination is illustrated naturally through “social\" networks, thereby often studied within network games as defined in classic GT. We note that network games are not networked stochastic game, where the former specifies coordination dependencies and the latter defines communication channels. Return A state-action trajectory tT = {8 9, ao, 81,41, --., 8} is sampled from the dynamics model distribution p,,(7) following a joint policy 7, where: T Px(T) = P(80) [] t(aelse)T (se, a, S$t41) (3) t=0 With this trajectory, we can compute an important quantity called the return, which calculates the future utility of a given state. Formally, the return Gj;,4) at each time step for an agent i, sometimes called the agent’s gain, is the cumulative future discounted reward. T Gust) = > rise, div) (4) t=t where 7 € [0, 1] is the discount factor that enforces a diminishing value for more distant rewards. Value Function The value function V,,,(s;|7) and the Q-value function Q,,, (s¢, ai.|7) map the state s, and state-action pair (s,, a;,,) to the expected return for an agent 7 given some joint policy 7, and are commonly used to develop solutions for optimizing controls in MAS. Va; (8e\\\\7) = Erxp,(r|se) [G@i,2)(7)] (5) Qn; (St; ait\\\\T) = Ep, (TIsis0i DIGG.) (7)] (6) Vaz (Se|7) = Ean (alse) Oz: (St, @i,t7)] (7) The advantage function A,.,(s;, a@;,,|7) measures the benefit of taking action a;,, in state s; for agent i under the joint policy z. It quantifies how much better or worse it is to choose action a;,, compared to the average expected return of all actions in that state, similar to the concept of regret(Jin et al_] 18b]. Hence, the advantage function is defined as the difference between the Q-value function i,t\\\\7) and the state-value function V;,, (s;|7). Ax; (St; ai,t\\\\7) = Qa; (St; ai,t\\\\7) _ Vi; (St|7) (8) MAS Objective The objective J;(-) of agent i is expressed as maximizing the expected return, where trajectories 7 are sampled from the dynamics model distribution p, (7) following a joint policy T. I(t) = E,xp, (r)[Gi,0)(7)] (9) In practice, the expectation of the return can be approximated using the Monte Carlo sampling or through temporal difference estimation. Such empirical estimates can result in return estimates with high variance, resulting in greater learning complexity, a central and persistent issue in MARL solutions. We state the general form of the MAS optimization objective, which we describe as maximizing the expected return for all agents. maximize J;(7),Vi € N (10) 3. The field of GT, often studied within the domains of economics, provides a formal context that analyzes and conceptualizes strategic interactions among multiple agents within a market . The term “market\" will be interchangeably used with the idea of a shared environment. examine the MAS optimization objective from a game theoretic perspective to understand solution concepts relating to learning goals and social principles used to identify the joint behaviors that are considered desirable or interesting (1985). For further study into GT, we refer readers to the following resource{Osborne] [2009]. ct Theory Under the lens of utility theory|/Fishburn et al. {1979}, Shoham and , we study how individuals make choices by quantifying their preferences, defining an preference relation. We define the utility function u(-) to reflect the individual’s subjective evaluation of their overall satisfaction, assigning a numerical value to each outcome and obeys the axioms of preference. Utility theory affirms the sufficiency of scalar value representation of the agent’s preferences|Shoham and Leyton-Brown| In many real-world applications, agents face uncertainty. The causes of this uncertainty stem not only from the environment dynamics but also from the behaviors of other agents. Under such uncertain settings, the agents’ decision-making processes should account for these unpredictabilities. We introduce this notion as risk and the domain of prospect theory, a study of decision-making under risk Kahneman and Tversky| [2013], Prashanth etal taking risk into account, agents are capable of weighing their intrinsic preferences and objectives against their unknowns, defining the agents’ aversion to losses, their reference points, and diminishing sensitivity Kahneman and Tversky] behaviors driven by risk, where “losses loom larger than gains\" thereby illustrating the behaviors that come with loss-averse and gain-seeking decisions, and remains a large focus in behavioral GT {Camerer| [1997]. There exists a diminishing sensitivity to these losses and gains, meaning that the impact of a change diminishes with the distance from the reference point. Reference points are a concept introduced by prospect theory that represents the status quo and argues that the value of an agent is defined by the final utility positions, as stated in traditional utility theory, may not paint the whole picture, and instead, it is important to view their value in terms of gains and losses, i.e. the agent’s value on a relative scale. However, the behaviors driven by risk defined by prospect theory become less relevant under repeated market settings, as experience within a market reduces uncertainty |[Loomes et al.] . We note that (expected) utility theory does account for uncertainty in its own manner, where the solutions derived from utility theory are considered risk-neutral but this faces violations in many risk-sensitive games|Kahneman and Tversky (2013}. Generally, risk-sensitive decision-making can be modeled with chance constraints, which are statisti- cal constraints to averse based on a probability of a high utility loss at specified risk-tolerance levels, or value-at-risk (VaR) and expected shortfall (i.e. conditional VaR (CVaR)), which place a similar constraint which is now specified by quantiles of the utility distribution, i.e. the tail risk. Incentives and Mechanisms _ Incentives provide context to the unique behaviors that come along with asymmetries in information and actions between agents in market settings, which are commonly studied under contract theory and auctions Salanidl(2005] . For instance, agents may be unequally informed about the various parts of the market, which can be reflective of the agent’s roles or the social context, and this can lead to adverse selections, moral hazards, and nonverifiability[Laffont and] . Adverse selection refers to situations where the agent with less information may make decisions that are unfavorable or risky, whereas moral hazard refers to the tendency of agents to take greater risks when they are insured or protected because they know they are shielded from some of the consequences. Lastly, nonverifiability refers to situations where the quality or performance of goods or services exchanged cannot be easily verified by the parties involved. Together, these concepts underscore the critical role of incentives in aligning behaviors and outcomes in markets characterized by information asymmetries. Incentives influence how agents gather and disclose information, manage risks, and fulfill contractual obligations, thereby shaping the efficiency and effectiveness of market interactions. We now consider how these games and their rules are constructed, i.e. game form, to induce certain desired outcomes and perhaps more importantly, avoid undesirable outcomes. This effort is studied under the domain of mechanism design|Hurwicz and Reiter and accounts for both incentive and feasibility constraints, as well as the distribution of agent preferences, particularly distinguishing between whether they are public or private. Mechanism design remains a key focus in GT research and plays a crucial role in both theory and practice, offering frameworks to improve market functionality, regulatory policy, and organizational design in contexts of dynamic markets. Solution Concepts and Equilibrium Solution concepts refer to states of equilibria and are de- scribed by meaningful or interesting properties used to evaluate joint policies, otherwise known as strategy profiles. Pareto efficiency is a general criterion commonly used to evaluate strategy profiles. A strategy profile nm Pareto dominates another strategy profile 7 if: Vi EN: uj(x’) > u;(7) Fie N: u(t’) > us(m) (11) In other words, no agent using strategy profile nm can be better off without making another agent worse off by using Definition 4 (Pareto Efficiency) A strategy profile x* is a Pareto efficient solution if it is not Pareto dominated by any other strategy profiles. Pareto efficiency focuses on maximizing overall welfare, which is the sum of the all agent’s utilities. However, Pareto efficiency does not emphasize individual rationality or collective stability. Therefore, Pareto efficient solution may not necessarily be a good measure if the truly optimal solution requires agents to act against their self-interest or deviate from some locally desirable strategies. Importantly, Pareto optimality does not address the issue of fairness or equality, as some individuals may benefit more than others in the pursuit of maximizing overall welfare[Shoham and Leyton-Brown| 2008}. The concept of best response provides an alternative perspective in analyzing strategic interactions in multi-agent systems that aligns more with concepts of individual rationality and collective stability. Given a strategy profile 7 = {0,71,...,7i,...,7n}, a best response strategy 7! for agent i is defined by: Vorj ui([r—a, 7i]) = ui([7-1, m]) (12) where 7_; = 7 \\\\ 7; denotes the strategy profile without agent 7 and [/71, 72] is the strategy profile of 1, The solution concept of a Nash equilibrium (NE) 7* applies this notion of best response to the collective, where for all agents i, 77 is the best response to 7* ;. Vay: u([a*j,77]) 2 u(fa*,, mil) (13) Definition 5 (Nash Equilibrium) A Nash equilibrium (NE) defines a state where no individual agent can increase its expected return by unilaterally deviating from their policy\\\\Nash\\\\ {T95TJ. Hence, within a Nash equilibrium, all of the agents’ strategy is the best response to the other agents’ strategy. Unfortunately, similar to Pareto efficient solutions, NE is not unique, and determining the differences between sample equilibria in terms of their social behaviors is unclear. In fact, the behaviors exhibited by sample equilibria can vary starkly, where their comparison can be computed using other concepts of efficiency such as coordination ratio[Koutsoupias and Papadimitriou] [1999] or Price of Stability While NE remains a popular criterion for multi-agent decision-making under uncertainty, computing this equilibrium may be computationally intractable in complex games. Moreover, achieving desired joint behaviors is not guaranteed through this approach|Shoham et al | (2003}, Matignon et al. 2012}. Nonetheless, NE does represent stable points akin to saddle points within the optimization landscape. To address the intractability of computing a strict NE, e-Nash equilibrium relaxes the requirements by allowing the agent to deviate if it improves its expected returns by more than some value e. u([7*;,7;]) > u([r*,, ml) — Correlated equilibrium (CE) is an important generalization of NE that adds the notion of correlating strategies among agents. This considers the existence of signals that coordinate agents’ actions 4], as well as the introduction of a correlating distribution over the strategies of all agents. In cases where such correlating signals do not necessarily affect the joint strategies and cause no agents to deviate regardless of the information provided by the signals, we call the following optimization state a coarse correlated equilibrium (CCE). As mentioned, NE is a special case of CE, where the correlating distribution of the agents’ strategies is a product of independent distributions. The concept of perfect equilibrium Bielefeld) [1988] refines the idea of NE in a different manner — by imposing additional requirements of consistency and mutual optimality. This solution concept describes an optimization state where agents’ strategies are mutually consistent and take into account the possibility of off-equilibrium actions, requiring agents to choose strategies that are robust against such deviations. In fact, numerous additional refinements and solution concepts exist that take into account asymmetric roles and rea oet| on ewon ele situations involving imperfect information [2008], or even games with sequential dynamics iv and non-stationary considerations|Daskalakis et al.][2022],/Kim et al.|{2022]. Equilibrium Analysis and Computation The process of computing the optimal decisions for multiple agents is referred to as equilibrium computation [2022] and has been studied under the framework of optimization and variational inequalities [1990], [Kovalev et 3]. Equilibrium computation aims to find specific points of interest, known as equilibria, within an optimization landscape that spans the strategy space of multiple agents. Formally, equilibria represent stable or “optimal\" solutions where the dynamical system reaches a balanced or steady state. Equilibria can be described as being local or global, indicating whether the state is a locally optimal solution or the best solution across the entire landscape. The two important properties we must consider for equilibrium computation are their existence and tractability. The existence of an equilibrium largely depends on the utility function of each agent, and whether it is concave with respects to their actions, where in nonconcave games, the existence of equilibrium is at risk. The tractability of equilibrium refers to the complexity of solving for an equilibrium and this remains a significant challenge to be compute efficient even in nonconcave games. Equilibrium Complexity Even with the existences of an equilibrium solution , the complexity of computing this equilibrium must be discussed. This discussion is most aptly had under the pretense of a total search problem (i.e. PPAD) rather than as a decision problem (i.e. P/NP). Although, computing an equilibrium can be classified an NP-hard decision problem|Gilboa and Zemel] [1989], |Conitzer and Sandholm) [2008], [Nisan et al.] ,|Albrecht et al. Definition 6 (PPAD) Polynomial parity argument for directed graphs (PPAD) consists of problems that can be reduced to the following (End-Of-The-Line) problem: Given a directed graph with vertices have at most one predecessor and/or one successor, and a source vertex 8, where s has no predecessor, find a vertex t with no predecessor or no successor, such that s #t. Let there be a polynomial-time function that returns the predecessor and successor of all vertices in this graph. where PPAD-completeness is shown by reducing the End-Of-The-Line problem. To support the hardness of PPAD problems, |Daskalakis et al.\\\\{2009] proposes the following question: How can one hope to devise an efficient algorithm that telescopes exponentially long paths in every implicitly given graph? Computing the Nash equilibrium is proven to be a PPAD-complete [Daskalakis et al.][2009],|Chen| 07], further affirming its difficulty and potential intractability. Learning Dynamics While these solution concepts of equilibrium give context for stability and optimality, they do not provide insights into the process of reaching such states. The concept of learning dynamics bridges this disconnect by detailing the procedure for reaching equilibrium. Additionally, learning dynamics also helps analyze the transition into equilibrium itself, which can be equally or more important to gain insights into task-specific understandings, and the processes that come after achieving equilibrium, where within that steady state, whether continual lifelong learning and further adaptations may come into the equation. In general, there exist two fundamental learning dynamics within the process of equilibrium computation: best-response dynamics and no-regret dynamics. Best-response dynamics directly optimize to converge to an NE or one of its refinements. A seminal algorithm of best-response learning dynamics is fictitious play (FP), where all agents iteratively compute the best response to other agents’ strategi most specifically, the uniform distribution over the past strategies of the other agents|Robinson| We summarize the seminal realizations and recent advancements in best-response dynamics learning: ¢ There are improvements to the idea of FP that include improved robustness to perturbations in the form of stochastic FP|Fudenberg and Kreps 1993} as well as uncertainty within these models of other agents and update these beliefs through Bayesian updating, known as rational and Bayesian learning [Albrecht et al .|[2024], Jordan] (1991), Specifically, Bayesian learning utilizes the value of information [2003], which considers how actions will influence future behaviors and devise more accurate beliefs. ¢ FP algorithms are typically used in normal-form games but can be realized in extension-form games using behavioral strategies, known as extensive-form fictitious play (XFP), or through approximate best response and sample-based learning, i.e. ML approach to XFP, known as Fictitious Self-Play Double oracle approach adopts the same iterative procedure as fictitious play, but instead, agents now compute a meta-NE to a restricted game that is maintained and expanded by the past best-response strategies |McMahan et al. (2003}, Adam et al. 2021). Policy Space Response Oracles, Deep Cognitive Hierarchy 2017}, and Extensive-Form Double Oracle|McAleer et al.|[/2021] incorporate and build upon the double oracle algorithm with the use of ML techniques, and address computational issues that exist when extending double oracle to extensive form games, promote generalization and prevent overfitting to specific equilibrium|Bighashdel et al. (2024), Lanctot et al. 2017). * Value iteration (Shapley) [1953] can be applied over the joint-action space, resulting in solutions such as Minimax Q-learning |Littman| ame Nash Q-Learning|Hu and Wellman| (2003), Correlated Q-learning|Greenwald and Hall|[2003], and Friend-or-Foe Q-learning (20016). ¢ Replicator dynamics|Maynard Smith\\\\|1976] adapts the concept of evolutionary dynamics to achieve best-response policies|Tuyls et al.|2006]. 0] utilizes gradient learning to optimize Infinitesimal gradient ascent (IGA)|Singh et al.| [2000] agent’s policy with respect to their utility. Similar methods and their extensions to improve convergence and other theoretical properties include using variable step sizes ,|Bowling| , proximal point optimization ; momentum|Gidel et al./[2019], extra-gradient , and optimistic gradients (2073) (2018), |Wei et al (2021) In no-regret learning dynamics, the aim is instead to minimize regret, a measure of how much an agent would have gained in utility if they had chosen a different strategy often in retrospect |Hart and| Mas-Colell| [ . Intuitively, regret can be quantified by the average cost between the utility of the best possible strategy profile and the actual utility of the chosen strategy profile. This notion of regret is called external regret, where comparisons of decisions are performed using an expert offline strategy. On the other hand, internal, or swap, regret takes a more online approach, compared to a modified strategy that swaps out certain actions with others from the original strategy. There exists a well-known connection between no-regret and Nash equilibrium |Zinkevich et al.|[2007], where an e-Nash equilibrium is a profile that achieves an upper bound regret of < €. Usually, the tractability of computing and verifying best-response can be compromised with games with high complexity, and therefore no-regret dynamics approaches are an attractive alternative as they scale very well when using domain-specific abstractions, such as in Poker AI applications |Zinkevich et al.][2007}, [fammelin| (2077) We summarize the seminal realizations and recent advancements in no-regret dynamics learning: SS ¢ Regret matching serves as the foundation for regret minimization algorithms, and is achieved through repeated self-play that computes the strategy iterates based on a distribution of 10 normalized positive regret! [1956], [2000]. Convergence to a stable solution, i.e. sample equilibrium, is achieved by taking the average overall strategy iterates. ¢ Counterfactual regret minimization (CFR) extends regret matching to extensive form games with counterfactual regret, which accounts for the sequential nature of actions in extensive form games|Zinkevich et al.|[2007]. To ensure sufficient coverage over the game tree for CFR updates, a sampling approach using external or chance samplin: or a more extensive search, like in CFR+ [Tammelin| (2077), Bowling et al] , must be considered. Recent advancements, such as CFR+ and advantage-based reg: nimization/Jin et al] (2018b}, have demonstrated performance improvements with resetting and positive clipping negative cumulative regret to zero, which can be a form of “optimism under uncertainty\". = ¢ Follow-the-Leader (FTL) is another popular regret-minimizing technique studied more in online learning, and FTL constructs online strategies to follow the actions with minimal loss over past rounds, i.e. the best “expert” [2012]. However, a naive implementation of FTL is unstable, where a natural solution to this instability is to append a time-varying regularization term, known as Follow-the-Regularized-Leader (FoReL). Importantly, the choice of regularization term leads to different regret bounds. Another method to address this instability and avoid overfitting from FoReL is to use the subgradient method with the proximal term as the Bregman divergence, known as mirror descent|Oi . For both FoReL and mirror descent methods, continued efforts to seek improvements are made to the regret bounds, with techniques as simple as gradient clipping |Cutkosky ¢ Multiplicative weights update/hedge algorithm generalizes FTL, as it now maintains weights that are updated with a non-infinite learning rate rather than selecting the single “expert” (1999) [Arora et al] 2012). The learning rate can be adaptive to ensure ar consistent low regret on easy and hard instances with the doubling trick or using both FTL and hedge periodically |De Rooij et al.|2014]. ¢ IGA and its extensions achieve both best-response and no-regret under certain conditions [2003], and with the advent of deep learning in MAS applications, gradient methods have become quite popular. Connection to MARL The literature on GT is extensive, with ongoing efforts investigating diverse challenges in strategic interactions among multiple agents. However, the significance of MARL lies in the integration of data-driven considerations into these strategic decision-making processes. So, unlike traditional GT, MARL leverages insights from data and statistics to manage complex markets where agents’ behaviors can be shaped by data-driven models. This approach enables MARL to effectively address dynamic, uncertain, and large-scale scenarios that traditional GT often finds challenging to handle. 3.4 Machine Learning The field of ML represents a crucial domain within AI, with the objective of constructing data-driven solutions that excel in pattern recognition using statistical models . Central to ML are two primary tasks: data collection and data analysis. Data collection defines the process of gathering and managing data which constructs the target data distribution, whereas the task of data analysis encompasses the methods of statistical inference using this data distribution. Hence, the data plays a pivotal role in the ML process, defining the underlying distribution on which ML models rely their statistics upon. This significance has become glaringly apparent in real-world ML applications, where the integrity of data is often tested by strategic agents aiming to manipulate statistics or exploit the predictive models themselves|Zrnic et al.] . Unfortunately, this multi-agent dynamic is canonically neglected in ML literature but has more recently gained traction in the emerging fields of strategic classification and adversarial ML, which develop solutions regarding the cycle between the development and deployment of ML models and the post-hoc response of strategic agents influencing the model, which can potentially adversarially “attack\" the model and “‘pollute\" the data. Another popular stream of research that blends GT into ML is in generative modeling, notably with the training of generative- adversarial networks (GAN). GANs employ a min-max optimization scheme to train an generative 11 model with an discriminator model that classifies between true and generated data. However, an naive implementation without proper GT considerations leads to chaotic and oscillatory learning Daskalakis et al.|{2018]. In essence, the efficacy of real-world ML hinges on robust data practices and the ability to navigate the complex landscape of strategic interactions in diverse scenarios. 3.4.1 Deep Learning Deep learning is a popular and general approach in contemporary AI research, with its foundations relying on the use of artificial neural networks (ANNs). ANNs have showcased remarkable proficiency in a wide variety of general applications{Goodfellow et al.|[2016]. Concretely, ANNs define a set of parameters 6 that act as a function that processes and transforms data using layers of linear and non-linear operations. We optimize these parameters with respect to a defined cost function J(@) using methods of statistical learning and numerical optimization, such as stochastic gradient descent (SGD)|James et al.| 6=0 —VoJ(6) (15) A key attribute of a deep learning approach is its ability to develop a scalable end-to-end solution that can capture complex distributions underlying many real-world applications. Additionally, we avoid the need to handcraft features and instead allow the optimization to learn curated latent representation tailored toward the task at hand in a data-driven manner. Hence, the emphasis is on extracting patterns, relationships, and insights directly from data, rather than relying heavily on predefined rules or models, although such efforts are not necessarily orthogonal. In the context of MARL, integrating these deep learning methods into MARL solutions has shown promise for developing expressive and adaptive multi-agent systems for complex tasks, where agent’s behaviors are defined using a composite of ANN representations and optimized in an end-to-end manner. For example, we can represent the elements of a stochastic game, such as the policy, value function, or model, using ANNs to optimize the joint behavior. The central constraints in such methods reside with the need for large-scale data collection and the high computational cost required for training. 3.4.2 Reinforcement Learning The domain of reinforcement learning (RL) focuses on learning how to make decisions guided by reward signals{Sutton and Barto} {2018] and serves as the general foundation of learning controls. A caveat of traditional RL research is that it is normally studied under a single-agent setting, i.e. using the MDP framework. Predictions and Control We define the process of sequential decision-making as control. Control contrasts the concept of predictions, as predictions are static inferences that have no causal relationship to future inputs and predictions. Formally, predictions are independent and identically distributed (IID), whereas no such assumptions are made for control. Within RL frameworks, control is learned within an environment whose dynamics the agent has limited knowledge of Rech] 2019], however predictions still play a major role in this capability, whether it is incorporated implicitly or explicitly, to ensure that the agent has an understanding of the world it resides. Value Function Approximation A value-based approach for RL estimates the value function to approximate the optimal policy. Hence, value-based solutions often do not define an explicit policy and instead, use an implicit policy computed using the value function, 7(a|s) « Q(s,a). These Q-learning methods|Watkins|[1989],/Watkins and Dayan| [1992 utilize approximate dynamic programming by applying the concept of policy iterations, as shown in Figure[3} with the Bellman (expectation) equations 57], a set of recursive definitions of the value functions. Qx(8t, 4) =Exxp, (7|s+,02)|Ge(7)] =r(sz, ay)+ Bs. p1~T(ses0r.s¢41).ars1 0m (alsiz1) Qn (St41; A141) (16) We define the Bellman backup operator 6, on our value function estimate Q such that Q = B,Q is equivalent to Equation|I6] By iteratively applying the Bellman backup operator B,, 6, is shown to 12 . E ce I . . evaluation (shown as —) and policy improvement (shown as —). Policy evaluation computes the value function for current policy whereas policy improvement updates current policy with respect to evaluated value function. The following figure was taken and modified from 2018}. be a contraction mapping, thereby according to the Banach fixed-point theorem, Q is guaranteed to converge to a unique fixed point that corresponds to the true Q-function Q,, for the policy x{Sutton] fand Barto| 2078}. By employing an optimal (implicit) policy denoted as 7* (als) = 6(argmax,¢ 4 Q(s,a@)), the need for policy evaluation, a process that scales poorly with the state-action space, can be circumvented. This approach, known as value iteration, leverages the Bellman optimality equation in the form of the Bellman backup 6, to minimize the Bellman residual error. Qre (St, Ut) = 7(St, Ut) + Esce1eT (sete ses) Max Qre(St41, 4)] (17) Value function approximation methods have come with several improvements over the past decade, including: ¢ Extension to deep neural networks representation for continuous state space and usage of experience replay to stabilize off-policy learning [Mnih et al. (2013). + Distributional value function representation [Bellemare et al.|(2017), (2018bjaj, Yang et al.|/2019] for more expressive value approximations. Mitigate overoptimism/maximization bias with double learning|Van Hasselt et al. clipped double Q-learning, and target policy smoothing [2018b]. * Utilization of a dueling architecture /Wang et al.|[2016b] to help figure out which states are valuable without having to learn the effect of each action for each state. ¢ Improvements to experience replay to prioritize important transitions|Schaul et al.|[2015], to perform better with sparse rewards|Andrychowicz et al.|[2017], and to work with recurrent networks|Hausknecht and Stone|(2015},/Kapturowski et al (2018). Improved exploration through randomized modeling 2017], intrinsic rewards Badia et al.|[2020b]a], maximum entropy framework|Haarnoja et al./[2017]. ¢ Handle continuous action spaces by assuming a deterministic policy|Lillicrap et al.|2015}, parameterizing the Q-function as a well-defined convex function’ [2016], or utilizing sampling methods|Kalashnikov et al. in order to compute the Bellman update. ¢ Learn on datasets of offline experiences while managing distributional mismatch and contin- uing improvements beyond behaviors seen in dataset 2020] by appending a behavior regularization term to the standard RL training (2021), regulariz- ing overestimation with conservatism|Kumar et al.||2020], utilizing probabilistic regression Kostrikov et al. (2021 {Ma et al.| [2021], and a model estimate|Yu et al. [2022]. This application is known as batch or offline RL. + Integration of the listed improvements through the years [Hessel et al.|[2017], fetal] . Value-based approaches are commonly used in MARL algorithms, as they offer a powerful and expressive solution to learning and producing a control policy in an off-policy approach, thereby being sample-efficient. 13 Policy Gradient A policy-based approach of RL directly optimizes the objective stated in Equation A common approach is to perform gradient ascent along the objective using the policy gradient, pioneered by the REINFORCE (REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility) algorithm|Williams]|1988|/1992]. Vo,J (7) = Vo, Erxp, (r)[Go(7) — b(7)] Tv = Enp,(r)L¥(Ge(7) — 0(7)) Vor log(a(ar|s+))] (18) t=0 where 6, represents the parameters of the policy 7 and D(-) is the reinforcement baseline function used to stabilize the approximated reinforcement encoll As the expectation is computed through Monte Carlo sampling, it can often lead to ian variance despite providing an unbiased estimate of the expected return. Hence, as noted by|Williams|[1992),/Schulman et al.|2018], the choice of d(-) can improve convergence and performance by mitigating this variance. Notable improvements to this vanilla policy gradient consist of: ¢ Learn on data from various policies, i.e. an off-policy approach, with policy gradient methods by using importance sampling corrections |Jie and Abbeel| [2010], |Degris et al. [2012] that enables training on a more diverse dataset in a sample-efficient manner. ¢ Mitigate dominating gradients on the parameter level by reparameterizing loss under a probabilistic manifold using Fisher information matrix 2001] with natural policy gradient, or Kronecker-factored approximated curvature [2017]. Utilize surrogate objective that guarantees monotonic improvements, conditioned on a trust-region optimization|Kakade and Langford] [2002], Schulman et al. (2017a}b}. Unlike value-based approaches, policy-based approaches are not commonly practiced in MARL, largely due to the sample inefficiencies from its on-policy requirements and high variance. However, in cases where samples can be generated cheaply, policy-based approaches can perform very well. Actor-Critic Methods _Actor-critic algorithms provide an integration of policy-based and value- based methods[Sutton et a,|(19994). Instead of estimating the value function of an implicit policy, we model the value function for an explicitly defined policy thereby minimizing Bellman residual using Equation{I6]and learning from a policy gradient. With access to a value estimate, temporal difference (TD) learning or n-step learning can be utilized instead of Monte Carlo sampling to approximate the offset reinforcement. While these bootstrap methods do introduce bias, the variance is considerately owered as updates are no longer dependent on the entire trajectories but only on a subset of the trajectory 2018]. Additionally, the value function can also improve the quality of the policy gradient by stabilizing the return estimate with a new baseline|Schulman et al.|[2018}. Much of the improvements to the actor-critic methods follow the advancements from value-based and policy-based approaches, and also consist of: ¢ The use of generalized baseline functions [2018] and action-dependent control variate{Gu et al.| [2017], [Liu et al.|(2018],[Tucker et al.|[2018] to stabilize the variance of policy gradient. ¢ Distributed training paradigms for actor-critic methods through asynchronous learning between multiple parallel actors|Mnih et al.| 2016], |Espeholt et al.|[2018]. ¢ Stabilized learning of solution with shared parameters between actor and critic by using phasic learning and representation regularization{Cobbe et al.|[2020}, [Huh] [2021]. ¢ Extending efforts from policy-based and value-based methods to an actor-critic framework, such as experience replay 2016aj and soft Q-learning [2018 . \\'We highlight that the return G(r) here removed the notation of which agent it is referring to, as we are only considering one agent in this scenario. 14 Challenges | Pathologies Computational complexity Stochasticity, Deception Non-stationarity Moving-target problem Coordination Miscoordination, Relative Overgeneralization, Alter-Exploration Problem Performance Evaluation n/a Model-based Approaches With a model-based approach, an explicit estimate of the transition dynamics T and/or reward probabilities is maintained. These estimates can be used for planning without an explicit policy or for a more sample-efficient policy improvement|Sutton| [1990]. Methods that do not maintain such estimates are referred to as a model-free approach. While model-based approaches have shown success in highly complex tasks, these methods can be more challenging as accurately and comprehensively representing the transition dynamics and reward function, especially over long time horizons, is non-trivial and difficult. There have been notable improvements in model-based approaches, which include: Integrate model learning and policy optimization with deep learning methods {Ha and] Mitigate distributional mismatch using online data collection [2011], uncertainty network , bootstrap ensembling|Chua et al.] [2018], [2018}, (2021) 2018 [2017]. Model-based learning on imperfect information 2015],/Zhang et al.|[2019}. Successor Representation Successor representation (SR) is an alternative approach to the model- based and model-free approach, where it disentangles the state transitions from reward estimation by maintaining a state occupancy function M(-) for a given policy x [Dayan] (1993). T M,(8,8!) = Ezxp,(z|so=s 1>_, 1 1(se = 8’)] (19) t=0 where I is the characteristic function. The state occupancy function captures the notion of envi- ronmental affordance, caching statistics relating to which future states are possible from a given state. Similar to the Bellman equations, we can derive a recursive definition for the state occupancy function. Mr (8,8!) = U(se = 8\\') + VE. ~pe (rlse=s) lM (se41, 8°)] This factorization provides a compact and structured representation of the model for efficient and adaptive learning in complex environments|Kulkarni et al|[2016). Generally, SR methods alleviate the cost of learning a complex environment model while still being adaptive to distal reward changes, given the disentanglement of the reward function from the state transition. Recently, SR has reemerged with a new utility in RL that can help capture different aspects of the RL problem, despite the initial idea proposed several decades ago. Notably, a grounded formulation to extend SR with deep learning methods [Blier et al.| , utilization of passive data to learn latent and useful features with SR recent applications that have captivated some new interest in this technique. Foundation Model for Control Foundation models are ML models that are pre-trained on large- scale data to be adapted for diverse downstream tasks and have largely been successful and practiced in recent ML research[Bommasani et al.|[2022]. This sentiment for foundation models is echoed in RL applications, where leveraging pre-trained models can also offer advantages, especially considering RL’s training inefficiencies. However, questions regarding its effective implementation remain at large. Th nt efforts of developing foundational models for RL has been in general navigation (2023alb}, [2022}, robotic manipulation |Walke et al. and other broad robotic 15 applications . These works have largely been motivated by practices in large-language model training methodologies and offline RL {Chebotar et al.] [2023] and creating multi-modal prompting to integrate successes from the research of computer vision and natural language. The key insight with foundation models is recognizing that across tasks, there are shared traits and skills that are required in many of these tasks that do not necessarily require re-learning for each task. Instead, learning these aspects in a unified manner rather than individually and myopically within a single task can lead to not only an amortized cost of learning but also more robust and superior behaviors. 4 Learning in a Multi-agent Environment The transition from single-agent RL to learning within a multi-agent stochastic game setting promises numerous opportunities but consequently is tied to challenging difficulties that require paradigm- altering considerations|R ]. In contrast to single-agent control systems, where one agent interacts with its environment, a multi-agent setting involves managing the decision-making and learning processes of multiple entities that can interact with each other and their shared environment. Facilitating stable, adaptive, and social behaviors within a multi-agent control learning process is non-trivial, and comes with several intricacies that converge to necessitate desired solutions. In this section, we cover the positive and negative aspects that are unique to MARL, largely highlighting its potential and consequences in the hopes of providing meaningful context and a deeper look into these concepts. 4.1 Benefits of MARL MARL directly addresses the optimization problem of developing multiple decision-making agents within a shared environment. As a result, these algorithms are specifically designed to consider the complexities inherent to multi-agent dynamics during training. Therefore, the optimization can produce valuable behaviors that span from adaptive social teaming Ndousse et al Por) to the ainst emergence of coalitions that enable intelligent interactions and improve robustness ag: lynamic changes within the population |Busoniu et al.| This perspective of approaching control optimization that MARL studies also enables the realiz; of applications that could not be accurately modeled in single-agent RL alone [Matarié] particularly in scenarios that involve multiple adaptive agents that can influence the environment or other agents|Lanctot et al.|[2017]. The context of training under MAS features additionally introduces the chance to capitalize on the unique prospects and structures of the multi-agent nature, which can enhance the efficiency and depth of training through various means discussed in the later parts of this survey. 4.2 Challenges of MARL The task of MARL is not only faced with the already challenging objective of optimizing control but is further riddled with the inclusion of multiple interacting agents. Throughout this paper, we investigate four central challenges — computational complexity, non-stationarity, coordination, and performance evaluation — that are interdependent and coupled to various learning pathologies. Definition 7 (Learning Pathology) Learning pathology refers to undesirable or sub-optimal behav- iors that can emerge within MARL learning dynamics. As listed in {Palmer| {2020), these include stochasticity, deception, the moving-target problem, miscoordination, relative overgeneralization, and the alter-exploration problem. Much of the problematic intricacies of MARL stem from a mixture of these challenges, as they tend to be deeply interconnected. 4.2.1_Computational Complexity As mentioned in Section optimizing for an equilibrium solution is theoretically demonstrated as challenging, where the cost of modeling multiple agents can easily scale intractably. This is seen as the joint state-action space scales exponentially with respect to the number of agents 16 , the complexity of the optimization problem and the computational cost required to render the optimization impractical to learn any useful joint behaviors. Other key factors, such as the pathologies of stochasticity and deception, play a major role in contributing to this complexity. Stochasticity In many real-world MAS applications, stochastic rewards or transitions pervade, arising from factors such as noise or unobservable elements in the state space. When a game is inherently stochastic, the challenge of identifying the sources responsible for the stochasticity can lead to learning instabilities. This, in turn, requires agents to accumulate more experience to discern and adapt to these sources of uncertainty Deception The pathology of deception involves a game that contains certain states that have a high local reward but low return. In other words, there exist settings where a trade-off should be made from immediate reward for long-term success. For many MARL algorithms, especially those derived from maximum-based learners|Kapetanakis and Kudenko] 2003], |Matignon et al. (2012 ; over-estimation is a key challenge that leads to early convergence to sub-optimal equilibrium. In general, to address the challenge of computational complexity in MARL applications, it becomes even more crucial to design our optimization model carefully such that the modeled components are not susceptible to fail to converge or to find good approximate equilibrium states during learning. With these considerations, we need to balance the trade-offs between solution complexity, computational efficiency, and learning performance to effectively devise a tractable MARL solution. 4.2.2 Non-stationarity Informally, a stationary process can be classified by its underlying distribution’s invariance under time shifts. In a multi-agent setting, the decisions of each agent impact the transition dynamics of the environment. Consequently, from the perspective of each agent, the other agents’ behaviors are inherent components of the dynamics. Therefore, whenever these external agents adapt and alter their behaviors, the underlying model distribution in the perspective of the agent also changes, rendering it non-stationary [Hernandez-Leal et al.|[2017]. Importantly, this non-stationarity arises not from a stochastic process that can be easily approximated, such as white-noise Gaussian, but rather from the structured learning process of the external agents 2 This non-stationarity is a critical deviation from the fundamental assumption made by conventional single-agent RL algorithms|Choi et al.] . The absence of the stationary property destabilizes the optimization process and contributes to the pathology of the moving-target problem. This problem arises from the fact that what an agent has learned and needs to learn is dependent on other agents’ evolving behaviors/Tuyls and Weiss| Hence, the learning landscape the agents are optimizing over is in flux at each update step. This setting can make learning infeasible for agents to properly converge to a stable behavior|Papoudakis et al.|[2019]. Therefore, it is essential to consider extensions of methods that can effectively account for this non-stationarity to develop stable algorithms for MARL. 4.2.3 Coordination One of the unique aspects of developing a multi-agent solution is the ability of the agents to work together to achieve their goals. As each agent makes decisions based on its local observations in a shared environment, they can heavily benefit from coordinating their actions to achieve a joint strategy that maximizes the collective return and avoids unintended interference to mitigate diminishing returns However, achieving successful coordination is difficult, notably when agents have limited information about the environment and the behaviors of other agents. Addressing this challenge of coordination has been a significant research question in MARL that studies how agents can engage with one another in various manners depending on their settings while also effectively succeeding in their local tasks. Specifically, these efforts towards successful coordination are directly associated with the learning pathologies of miscoordination and relative overgeneralization. Miscoordination The pathology of miscoordination is also known as the Pareto-selection problem and can be observed when two or more incompatible Pareto-optimal equilibria are present. As a consequence, the agents can potentially choose an action from different equilibria due to improper 17 ce | 0 0 0.5 coordination and therefore harming their performance. For example, given a bi-matrix game defined in Table|2} there exists two equilibria, (a, a), (b, b), where either both agents choose action a or b. However, these equilibria are incompatible, since the strategy profiles (a,b) and (b, a) come from either equilibria, however, if they are intermixed, neither resulting strategies are optimal. Definition 8 (Incompatiable Equilibria) Two equilibria x and m are incompatible if and only if, ym; #7, Jimi, tal) < Jil) where [mi, mJ signifies a strategy profile using agent i action from 7 and the other agent’s action from Tr. Relative Overgeneralization The pathology of relative overgeneralization occurs in games where, as a result of a shadowed equilibrium, the agents converge upon a sub-optimal Nash Equilibrium that is Pareto-dominated by at least one other Nash Equilibrium. Definition 9 (Shadowed Equilibrium) An equilibrium 7 is shadowed by another one © if there exists an agent that receives a low return by unilaterally deviating from this equilibrium and if this return is lower than the minimal return when deviating from the other equilibrium|Palmer| [2020]. , Fide J (mi, -i]) < min I ((aj,7_5]) For instance, in the bi-matrix game shown in Table[2} while (a, a), (b, 6) are both Pareto-optimal equilibria, a miscoordination penalty of —2 is associated with both of them. No such penalty exists with action c. Hence, both equilibria are shadowed by (c, c), as the expected gain if one agent deviates unilaterally from either equilibrium is inferior to the lowest expected gain if one agent deviates unilaterally from (c, c). Hence, agents can be drawn to sub-optimal but wide peaks in the return space due to a greater likelihood of achieving beneficial collaboration. This derives as a form of action shadowing. reward(i,j) 18 Definition 10 (Action Shadowing) Action shadowing is a phenomenon wherein one individual action seems more favorable than another, despite the potential greater return of the second action [Fulda and Ventura\\\\ [2007]. Concretely, a sub-optimal policy may result in a higher average payoff when paired with arbitrary actions chosen by other agents, leading to utility values of optimal actions being underestimated. The pathology of relative overgeneralization is visualized in Figure/4jover a continuous action space. The x and y axes represent the actions of agents i and 7 respectively. The z axis represents the reward for each joint action. The reward space is structured where action 7,, can lead to the optimal reward, however, due to miscoordination being less severely punished for actions approaching iy, the agents are drawn towards the sub-optimal Nash equilibrium. Exploration The challenge of exploration in MARL describes the issue of how to effectively explore unknown environments to collect valuable experiences that benefit the agents’ learning the most|Sutton and Barto] 2018]. To address this challenge, a balance between exploration and exploitation must be struck, where agents must decide whether it is more valuable to take the actions that they know would lead to good returns or take the actions they have not tried yet that may lead to even greater returns or at the very least reduce the uncertainty regarding those actions. An improper balance on either side can result in an incomplete coverage over the state-action space, leading to sub-optimal convergence. This challenge is especially exacerbated in intricate environments with and delayed reward signals, noisy transitions, long horizons, and non-stationary dynamic . In addition, in light of the inherent delicacy involved in optimizing and coordinating multi-agent systems under the influence of shadowed equilibria and miscoordination penalties, such exploration can increase the likelihood of deceptive transitions and introduce instabilities within the learning dynamics (2012). For instance, other agents may adapt to these exploratory actions too abruptly even though they may lead to a shadowing equilibrium. We call this pathology the alter-exploration problem. Another key issue noted by prior efforts is the lazy agent problem when certain agents learn a good policy but some agents have less incentive to continue to explore and learn themselves, as their actions may negatively affect the already high-performing agents. For example, as discussed in|Sunehag et al.| (2017), consider the scenario of training a soccer team with the number of goals as the team’s reward signal. If certain players are more proficient scorers than others, it becomes evident that when the less skilled player takes a shot, the outcome is less favorable on average. Consequently, the weaker player learns to avoid taking shots{Hausknecht| (2016). Traditionally, simple exploration methods, such as ¢-greedy [2018] or noise perturbation , can be employed for random action selection, however, such naive methods can lead to unintentional and indiscriminate exploration which can be inefficient in complex learning tasks with exploration challenges. While exploration remains an open challenge with much room for improvement, there exists more studied and developed exploration methods, as follows: Uncertainty-oriented Exploration: With a lack of knowledge regarding certain actions, agents can incorporate this uncertainty into the decision-making process when tackling this balance between exploration and exploitation. A common heuristic to employ is the principle of ’Optimism in the Face of Uncertainty\", where agents are incentivized to explore state-action pairs with high epistemic uncertainty. Epistemic uncertainty represents the errors that arise from insufficient and inaccurate knowledge about the environment whereas aleatoric uncertainty represents the inherent randomness of the environment. Typically, this approach requires some modeling of these uncertainties. A common approach is to parameterize the solution as a distribution |Zhu et al.] to be able to properly express the stochasticity of the environment and leveraging a classic exploration technique utilizing these estimates. ¢ Intrinsic Motivation-oriented Exploration: An alternative approach is to incorporate a meta-task of exploration by introducing and designing intrinsic rewards for agents. These rewards can be to minimize prediction errors regards the environment 2021] motivated by novelty of states [2021], [Li I or driven by information gain For instance, in individual intrinsic reward is learned and used to update an agent’s policy to maximize the team reward. 19 ¢ Multi-agent Exploration: In a multi-agent setting, we face the challenge of not only complexity but also miscoordination (i.e. alter-exploration problem). This issue requires some level of coordinated exploration [2023], as exploratory actions of one agent can affect the learning of others 2018], and in certain multi-agent tasks, efficient exploration requires a degree of global planning as opposed to pure local exploration|Brafman and Tennenholtz’ (2003). 4.2.4 Performance Evaluation Assessing the performance of a MARL algorithm is a multifaceted challenge. Firstly, we highlight that the success of one agent’s policy is intricately linked to the policies of other agents, which renders individual assessments unclear to properly interpret. This is compounded by the problem of establishing unbiased and useful metrics that quantify inherently qualitative social behaviors, such as measuring the quality of communication and coordination|Havrylov and Titov 2017}. Jaques et al. 2019],|/Bogin et al.|[2019], [Lowe et al.|[2019] and determining the appropriate evaluations dependent on the roles assigned to all agents (i.e. oligopoly with leader-follower structure). Furthermore, we can focus on more egalitarian criteria, ie. concentrating on the agents that are most struggling Zhang and Shah]|(2014) as opposed to an more utilitarian approach such as social welfare. Hence, it also becomes important to understand multi-agent credit assignment and discern the individual impact of each agent in terms of the coalition’s utility, especially where there exists only a global reward structure, e.g. within a Dec-MDP. A classic example is the Shapley value, which quantifies and captures the notion of marginal contribution by averaging all possible combinations of the marginalized population’s achieved utility[Shapley|[1952]. More recently, a popular approach in deep MARL is to utilize an advantage function, which compares the current Q-values to a counterfactual (2017aj, {Li et al.] [2022], or to utilize value function decomposition [2017] that marginalizes the contribution of each agent. From a theoretical perspective, determining which solution concepts would lead to optimal behaviors and strategies, as studied in game theory literature, remains unclear and often task-specific{Bergerson| 2021]. Even so, as many studied solution concepts prove prohibitively costly to explicitly measure and optimize with complex and dynamic tasks, the research for definable metrics that capture the nuances of diverse MAS settings persists as an ongoing and open challenge. 5 Prospects of MARL In this section, we cover the unique properties of learning controls in a multi-agent environment that can help promote the benefits or address the underlying issues of the MARL approach|4] 5.1 Simulating MARL Tasks A lingering concern for MARL algorithms is its learning complexity Daskalakis etal . Often, in order to acquire valuable behaviors, it requires significant computational resources. This constraint arises from various factors, namely sample inefficiency and a brittle optimization landscape|Gu et al. (2017), van Hasselt et al. 2018}, Tucker et al. (2018), Henderson et al. (2019). Sample efficiency is broken down into two factors: the number of environment interactions required by each agent to learn and generalize and the cost associated with each interaction [Huh and Mohapatra . Hence, it is important to emphasize the pivotal role that simulators play in this equation, as they can enhance the learning process by yielding higher-quality samples through greater accessibility, stability, accuracy, and precision of the retrieved data. Parallelized and Vectorized Processing Over the past several decades, the processing capabilities of massively parallelizable processors, such as graphic processing units (GPUs) and tensor processing units (TPUs), have advanced significantly. This progress has opened up exciting possibilities for leveraging this technology to simulate highly complex tasks effectively, minimizing the cost of simulating a large number of trajectories. These processors are specifically designed to handle parallel computations efficiently by executing tasks simultaneously on a large scale, enabling simulations to run orders of magnitude faster compared to conventional CPU-based implementations. There have been notable efforts to develop task-specific [Dalton et al.| | and general physics-based 20 Level 3 » Legend «— Interaction Level 2 ee) @ Holon <—) Context Level 0 simulators, such as IsaacSims|Makoviychuk et al. 2021], Brax|Freeman et al.|[2021], and MuJoCo XLA that leverage GPUs. Many of these simulators primarily concentrate on addressing single-agent problems, with some addressing their extension to MAS (2022 2023 jand Mohapatra] Mohapatra (20749) ,|Rutherford et al.| (023) |. In turn, there exists promising potential to further optimize and extend the usage of vectorized processing to MAS scenarios|Lan et al.|[2021]], [Lechner] et al(2023) Multilevel Simulation Many complex multi-agent tasks can be factorized into a hierarchical structure using a multilevel simulation paradigm, as an effort to manage its complexity. Multilevel simulation introduces several organizing levels that encapsulate various individual components into monolithic abstractions [Ghosh] . These levels are defined in distinctive manners, integrating microscopic to macroscopic attributes 2017]. Such modeling is defined in a holonic paradigm|Tchappi et al.|[2018] (see Figure[5), where holons are used as these abstractions and holons le self-s\\' are defined as stab imilar structures that behave as both an entity and an organization. Holons satisfy three important conditions: holons are stable, autonomous, and cooperative with one another. However, a holonic organization of a task is often difficult to properly define. Open Source Environments There exists a wide range of libraries that are used as common benchmarks for MARL research. Table[3|provides descriptions of some of the more widely recognized environments. 5. In this part, we explore various training paradigms used in and unique to MARL applications. Specifically, we look into the centralized training paradigms and the use of off-policy learning. 5.2.1 Centralized Training There exists a spectrum of agent representations in training and execution to combat the scalability and complexity issues of learning joint strategies. This spectrum includes three main categories: centralized training and centralized execution (CTCE), decentralized training and decentralized execution (DTDE), and centralized training and decentralized execution (CTDE)|Lowe et al. [2020]. The idea of centralization couples components of agents’ behaviors to provide a more complete state of information to work with or to decrease the complexity of the task by distributing the workload over multiple agents. Importantly, leveraging some level of centralization poses a good solution for handling non-stationarity, as each agent now will have access to global information to account for the changes in other agents’ behaviors. CTCE The CTCE paradigm entails a fully centralized approach that involves mapping a collection of local observations from each agent to distributions over individual action spaces. In this case, 21 Name Description Multi-agent Particle Environment | Various social tasks focused on communication (MPE) (2020) within a particle world setting. StarCraft Multi-Agent Chal- Cooperative StarCraft decentralized micromanage- lenge (SMAC)Samvelyan ment scenarios. et al.|[2019],/Ellis et al. Phan etal |[2023] [Terry et al./[2021| A collection of different MARL tasks and libraries. m] Ll = Multi-agent tasks in a grid-world setting. [Koul](2019] . A collection of many-agent tasks. Level-based Foraging ( )and | Customizable grid-world foraging task and simula- Robot Warehouse (RWARE) tion warehouse with robots moving and delivering Christianos et al.}[2020 product in gridworld oogle Research Foot- Simulated soccer game using physics-based 3D simu- ball[Rurach et al[2020)__| lator. Overcooked Human-Al coordination on multiplayer video-game task. Vectorized Multi-Agent Simulator | Various MARL tasks using a vectorized Pytorch- (VMAS) 2022) based 2D physics engine IsaacTeams[Huh and _| | Various physics-based MARL tasks using GPU- 2024al accelerated IsaacSim platform. axMARL Rutherford et al.|[2023] | Various physics-based MAS tasks using GPU- accelerated Brax platform. we essentially reduced the MAS control problem into a single-agent RL optimization over the concatenated observations and combinatorial joint action space. While CTCE provides expressive and complete representations of a MAS|Gupta et al_| | and performs well against non-CTCE methods[Yu ef al [2022a], the assumptions of decentralization are largely compromised. This is because, during execution, decentralized agents must only make decisions based on their local observations and do not have access to the global information it was trained on. Therefore, nontrivial and unnatural adjustments must be made to convert the joint policy from a centralized executor to a decentralized executors, such as masking the other agent’s information to prevent information leakage. CTCE approaches also fail to address the curse of dimensionality problem|Gronauer and Diepold 2022], otherwise expressed as the exponential scaling caused by the joint state-action space of MAS. DTDE On the other side of the spectrum, DTDE proposes a fully decentralized approach that adheres to all decentralization constraints in all aspects of training and execution. We note that this does not necessarily mean the agents cannot perceive nor is aware of other agent’s existence, where this is known as independent learners (IL) vs. joint action learner (JAL) as defined in , although IL can be considered an extreme form of DTDE. Prior efforts [1998}, [Tan] (1997), [Lauer and Riedmiller(2000),[Tampuu eta, (2015), Jaderberg [2019] have demonstrated that IL with standard RL algorithms does demonstrate the ability to converge to an equilibrium in particular and fine-tuned settings. A key challenge in the DTDE training scheme, as notably emphasized, is non-stationarity. This challenge is exacerbated by the absence of centralization, leading to a potential loss of mutual information among agents’ behaviors, which, in turn, can give rise to various learning pathologies|Palmer| [2020]. To mitigate these pathologies, a widely adopted strategy involves inducing optimism|Matignon et al.||2007),|Palmer et al. (2018 by) , through hysteric learning or leniency. This approach restricts the reduction 0: value estimations, thereby alleviating the impact of other agents’ exploration strategies and promoting exploration beyond equilibria that can easily trap agents without the added optimism. ¢ Hysteric learning: While DTDE has demonstrated success in deterministic settings, inde- pendent learning has struggled to replicate such achievements in stochastic settings. A significant stumbling block has been the tendency to overestimate the value function, a consequence of the inherent stochasticity, resulting in sub-optimal solutions, as stated in (2007). To address this issue, hysteric Q-learning [2007] was 22 introduced to provide an optimistic update function that assigns greater weight to positive experiences, particularly beneficial in cooperative multi-agent scenarios. This is achieved through the use of two learning rates, denoted as a and ¢ Leniency: Alternatively, another method to adjust the degree of optimism during the learning process is leniency [2018]. Leniency effectively allows for the forgiveness or disregard of suboptimal actions taken by teammates that result in low rewards during initial exploration, taking in the form of lenient Q-value updates and lenient-based exploration. Over time, this optimism exhibited by lenient agents is gradually reduced as they encounter and revisit state-action pairs. Consequently, agents become less lenient in situations frequently encountered, while retaining their optimistic outlook in unexplored territories. This shift towards average-based reward learning from maximum-based, helps lenient agents steer clear of suboptimal joint policies, especially in environments where rewards are subject to stochastic fluctuations. Empirically, leniency shows higher learning stability compared to hysteretic learning, primarily due to temperature-enabled leniency at different stages of estimation maturity. The leniency decay allows for a more faithful representation of domain dynamics during later stages of training, where it is probable that teammate policies become stable and near-optimal, assuming the rate of decay is appropriate and value maturity is synchronized across all states. Like CTCE, DTDE also demonstrates a significant issue of scalability, as a distributed solution requires each agent to not only be represented individually but also require their own set of samples for learning. As agents are not granted any access to the global state, which can be pivotal not only for sample efficiency but also for good performance|Gupta et al.| CTDE CTDE provides a middle-ground by centralizing certain variables during training that still enable decentralized execution of agents |Kraemer and Banerjee (2016). This approach strikes a balance between the advantages of centralization while maintaining the constraints set by natural and artificial decentralization during execution. CTDE is commonly practiced by using a centralized value function. Centralization of the value function allows all agents access to comprehensive state information during training, without violating decentralization constraints during execution since the value function is not required for decision-making. This facilitates enhanced learning, efficient updates, and coordination between actor and critic, promoting improved policy convergence |Foerster et al.|2017al], [Lowe fet al.| 2020]. Additionally, single-agent RL algorithms can naturally be extended, similar to DTDE, however, using a shared critic model (i.e. MAPG[Samvelyan et al. , MADDPG Lowe et al.| and MAPPO/Y ]). Like CTCE, a centralized value function remains prone to scalability issues. As the number of agents grows, its representation needs to handle a larger or more intricate state space. This can lead to significant computational costs and difficulties in defining the concise state space. Another critical challenge is the centralized-decentralized mismatch. Since the value function is shared among agents, sub-optimal policies from one agent can have a detrimental impact on the policy learning of other agents, causing catastrophic miscoordination| . Largely, this increased variance in learning a shared critic remains a long-standing challenge. Parameter Sharing A commonly used approach to implement centralization is through parameter sharing, where different agents share representation modes{Gupta et al.][2017a]. This allows each agent to update the same parameters, potentially leading to a more efficient and richer learning process. Ina sense, all agents can aggregate their experience and learn much faster in a more memory-efficient manner|Fu et al.|(2022]. However, it is important to note that this can more likely lead to homogeneity in behaviors and introduce instability, especially when dealing with highly diverse agents, as it transforms the problem into a difficult multi-task optimization problem. Parameter sharing among such agents, especially if they are heterogeneous, becomes a nontrivial task. Subtask Sharing Another approach to centralization involves the use of global subtasks. In many MAS, tasks can be decomposed into subtasks universally defined amongst all agents. As a result, each agent’s task can be decomposed, where these global subtasks can be assigned accordingly to 23 each agent. To achieve this, the process of task decomposition needs to consider how the subtasks can ned properly. While subtasks can be defined using domain knowledge US Yngetal generalizable decomposition methods, such as RODE and L 22], have been introduced. The core idea behind both approaches is to learn embeddings over the actions or trajectories and perform clustering to define subtasks. A significant challenge in these approaches is ensuring the subtasks’ definitions are distinct|Yang et al.] [2022]. 5.2.2 Off-policy Learning To improve the sample efficiency of MARL training, agents can learn from experiences that come from different policies in some manner|Sutton and Barto|[2018}, Silver et al.|(2014], and this is known as off-policy learning. Typically, off-policy approaches rely on storing samples in an experience replay buffer. However, proper implementation of experience replay in a multi-agent setting is non-trivial due to the non-stationary dynamics of the environment that can render past experiences obsolete, as other agents’ behaviors change, learning with their prior behaviors may be out of distribution. Previous efforts [Foerster et al.|2017b] account for these discrepancies through two methods. The first approach utilizes importance sampling to correct the policy updates, however, there remain questions about the tractability of computing the importance weightings and its large and unbounded variance. Extensions to alleviate these issues, including truncation, do reduce variance, however, introduce additional bias. Fingerprinting, on the other hand, appends contextual information regarding the current stage of learning of the agents in the environment to the samples that are stored in the experience replay buffer, to disambiguate the age of the sample. Both approaches prove to stabilize the experience replay sufficiently. Aside from the depreciation of samples, other issues have been addressed, such as ensuring concur- rency of experience sampling/Omidshafiei et al.|{2017] and detecting to manage miscoordination and relative over-generalization with off- por) learning by using variable learning rate to accommodate for exploratory actions [Palmer et al.| [Lyu and Amato] [Lyu and Amato] {2020} . Traditional experience replay mechanisms, such as priority eT ea Somer et al.] [2016], je) been experimented with MARL. However, a naive application may deteriorate a aka and performance due to the noisy reward and the continuous behavior changes of coexisting agents, causing a priority bias. Hence, a lenient reward function is modeled|Zheng et al. to correct the priority bias. 5.2.3 Offline Learning The practice of offline learning with MARL adopts much of the same ideas from single-agent offline RL, existing in the form of behavior regularization and conservatism|Pan et al] , but with further considerations required to mitigate underlying issues that come along with the mixture of offline RL and MAS, such as the propagation of erecapovation error Yang et al. 202 1b} and agent-wise imbalances within the offline data|Tian et al.}[2023 5.3 Agent Awareness Agents can exhibit different degrees of awareness of other agents, which can be classified into three categories: independent, tracking, and agent-aware Busoniu et al . The selection of an awareness level involves unique considerations, advantages, and challenges, contingent upon the specific nature of the interaction and the task. At each level, a trade-off between stability and adaptability is made. Definition 11 (Stability and Adaptability) Stability focuses on achieving convergence to a station- ary policy m, while adaptability aims to maintain or improve performance in the face of changes in other agents’ behaviors. Definition 12 (Stationary Policy) A policy x’ is stationary, for any state s\\' and time-steps t and t’, wheret 4 t’, a : Ini(alsi) — mi,(alsi,)| <a € A While stability and adaptability are not necessarily dichotomous objectives, the balance between the two helps illustrate the extent to which coordination is emphasized. Effectively addressing this coordination problem requires agents to skillfully navigate these intricacies and strike the right balance between focusing on stability and adaptability. 24 ¢ Independent agents disregard the notion of coordination entirely. Moreover, their focus lies solely on converging to stable behaviors rather than adapting to the actions of other agents in their environment. In cooperative, adversarial, and mixed settings, such methods are referred to as coordination-free, opponent-independent, and agent-independent respectively, and each has demonstrated empirical success under restricted problem settings{Littman|[2001al, Lauer and Riedmiller 2000], Hernandez-Leal et al. 2017). However, these independent methods often result in sub-optimal outcomes or even failure to achieve desired goals as coordination becomes crucial in many scenarios to anticipate and respond strategically to the actions of other agents. ¢ Tracking agents prioritize adaptability over stability, placing a greater priority on coordi- nation rather than learning a stable individual behavior. With a tracking approach, agents continuously adjust their strategies based on the observed behavior of other agents. Empiri- cally, agent-tracking methods rely on agent modeling to guide the agents’ action selection process|Robinson}[}1951],|Weinberg and Rosenschein| [2004]. However, the stability of the joint behavior may be compromised. The constant adaptation can lead to non-stationary behaviors, as the agents respond not only to changes in the environment but also to the changing strategies of other agents. ¢ Agent-aware agents strive to achieve a balance between stability and adaptability by being conscious of the other agents’ strategies while preserving their individuality. Previous studies have explored approaches such as \"Adapt When Everyone is Stationary, Otherwise Move to Equilibrium\" (AWESOME) |Conitzer and SandhoIm] [2003] or \"Win or Learn Fast\" (WoLF)|Bo\\' to determine when to adapt or maintain their local strategy, mostly just by adjusting the learning rate or incorporating the other agent’s anticipated learning to one another [Foerster et al-|[2018]. These concepts primarily address handling the non-stationarity of the optimization problem, but this heightened awareness also establishes a well-rounded foundation for fostering social behaviors that lead to stable and successful coordination. Learning with Awareness When learning, agents can take into account the behaviors and informa- tion regarding other agents|Foerster et al.|[2017c]. One way this can be achieved is by extrapolating gradient updates for each agents, thereby performing a one-step look-ahead over the learning over all agents, which is known as extragradient{Korpelevich| [1976]. To reduce complexity of extrapolation step of gradient update, we can instead use only a sample subset of agents in many-agent settings [2020]. Similarly, LOLA [Foerster et al.| takes a similar approach, however, extrapolates only the other agents using a second order correction term with a Taylor expansion approximation. 5.4 Multi-agent Credit Assignment For many MARL applications, it may be intractable to define local rewards for each agent, hence necessitating the use of the Dec-MDP framework. In such settings, a global reward is instead provided, which represents the collective’s utility. However, with this measure, it is unclear the direct contributions and local performances of each agent. This problem is known as multi-agent credit assignment (MACA)|Agogino and Tumer| [2008]. We distinguish MACA from the traditional credit assignment problem associated with the casual aspects of sequential decision-making, where the actions themselves are evaluated on their impact(Sutton and Barto|[2018]. In recent efforts, the 4.2.4) challenge of MACA, as briefly mentioned in Section{4.2.4] is addressed in two approaches: difference rewards and value factorization. Difference Rewards Difference rewards aim to capture an agent’s contribution from a global performance measure by shaping a local reward signal that isolates the utility of individual agent’s actions by removing the utility of ee a While in some applications, such as air traffic flow management 2007], this isolation is possible, generally, forming a theoretical setting that removes agents individually may be impossible. Hence, the marginalization of individual agents is often estimated by comparing them against the average actions, known as aristocrat utility[Wolpert and Tumer| [2001]. To extend to the realm of deep RL, COMA leverages the concept of the advantage function to achieve the same effects. 25 Value Factorization Value factorization decomposes a global value into local values for each agent 2001]. A simple implementation of this is known as a value decomposition network (VDN) , where the sum of the learned local value functions can be treated as the global value. With the many extensions of VDN that have been proposed in the past decade, the standard constraints of Individual-Global-Max (IGM)|Rashid et al.| [2018] serve as the theoretical basis for guaranteeing and maintaining consistency between the global Q and local q; value estimates. argmax go(70, ao) ao argmax Q(s,a) = : (20) a argmax qv (TW, an) an where 7; is the observation-action history of agent i. However, these constraints have been shown to restrict the expressiveness of the value function representations|Mahajan et al. OJ, leading to sub-optimal value approximations and poor explorations. Hence, it remains a open research challenge to improve these limitations of value decomposition while trying to adhere to IGM. 5.5 Communication Communication is a powerful capability of high interest within MARL literature that enables agents to exchange and propagate information between one another, leading them to behave as a collective rather than a collection of independent individuals (2024). In many multi-agent tasks, com- munication proves vital to coordinate the behavior of multiple agents to achieve optimal performance Foerster et al. , especially under settings with imperfect information and partial observability 2015). However, an efficient and practical implementation of a communication mechanism presents several key challenges, necessitating consideration of not only what information to communicate |Sukhbaatar| fet al.|(2016], Foerster et al [2016], but also how [2022], when Singh et al} [2018], and with communicate. Peer DO, the topic of communication is broken down and categorized over 9 dimensions on its implementation, so we recommend readers refer to this resource for more in-depth analysis. 5.5.1 Communication Infrastructure A communication graph is introduced to define which agents each agent can communicate with, alongside the use of the networked stochastic game framework. The restriction placed on the existence of the graph’s edges is bounded by the constraints of decentralization and requires solutions that address issues including limited range 2023], limited bandwidth |F , noisy communication channels |Freed et al.| 2020], and contentions with shared communication mediums|Kim et al.|[2019] such as a proxy. A consideration for each is understanding their practicality, which is dependent on the nature of the task, as well as their shortcomings. Proxy While decentralization prohibits centralized executors, a centralized communication medium is not prohibited. The role of a proxy is to serve as a coordinator and message aggregator. It gathers local observations or messages from agents in the environment, subsequently broadcasting messages to each of them|Kong et al.|[2017]. Alternatively, it can connect nearby agents who opt to participate in a communication group, facilitating the sharing of coordinated messages with each group member 8]. Canonically, a proxy solely acts as a communication medium, having no direct effect on the environment. A key challenge of a proxy is its design. Solutions that use proxies must consider their efficiency, ensuring that sufficient communication is achieved for the task at hand while also managing the computation load and expressiveness of the proxy. Networked communication In contrast to relying on a proxy for inter-agent communication, the networked communication protocol consists of agents that pass and receive messages directly to and from other agents{Zhang et al.| Chu et al. . While this form of communication may seem most fitting for a decentralized setting, its dynamic nature and lack of structure can lead to poor performance and scalability issues, especially when each agent has limited compute resources and is required to process many agents’ messages. 26 Implicit Communication Agents can also communicate with one another without explicit means, such as through stigmergy|Grassé| [1959]. The concept of stigmery defines the influence agents have through their actions on one another, often through environmental changes or some other form of stimuli. More formally, stigmery describes the influence of the persisting environmental effects of prior behaviors on behavior¢Holland and Melhuish| [1999]. We categorize the idea of stigmergy based on the intent/form of the stigmergic actions, the responses to the stigmergic behaviors, and the impact of stigmergic actions. ¢ Sematectonic and marker-based stigmergy |Wilson| 2000], |Marsh and Onof| [2008] distin- guish whether the stigmergic actions were directly aligned with true objectives of the agents or rather, to solely stigmergize. The intent and the actual actions are often considered when classifying the two forms of stigmergy. Quantitative and qualitative stigmergy{Theraulaz and Bonabeau| [1999] differentiate whether the response to the stigmergic actions is an intensification of the resulting stimulus or trigger- ing different stimuli, leading to a self-organization process. A self-organizing process refers to a set of dynamical local mechanisms, which through their applications and interactions, causes emergent global structures and behaviors. Active and passive stigmergy|Holland and Melhuish|[1999] refer to the effects and outcomes of the stigmergic actions. Active stigmergy directly affects the agents, influencing the observations, actions, and parameters (e.g. frequency, latency, duration, intensity). How- ever, passive stigmergy is more indirect and subtle, perhaps leading to no changes to any observations, actions, or their parameters, but only to changes to the outcome. Historically, a central focus of stigmery in MARL has been deriving optimization algorithms, such as ant colony optimization|Dorigo and Blum|(2 that are inspired by these concepts. We propose that these patterns and behaviors of stigmergy should be further studied towards other forms of integration into our MAS, including, but not limited to, how to induce stigmergic behaviors and quantify and evaluate them. 5.5.2 Communication Representation In this section, we look into the various forms of communication mechanisms that are used in practice, namely those that were realized using deep learning techniques such as graph neural networks (GNN). Graph Neural Networks — Facilitating rich communication among agents requires a scalable frame- work that can naturally process information within the communication graph in an expressive manner. GNNs are a fundamental tool for handling non-Euclidean data, especially when dealing with infor- mation naturally occurring in graph structures. Concretely, GNNs learn to map input data to latent representations that can be used in subsequent tasks. GNNs generate these latent embeddings by iteratively performing the following operations: message computation, propagation, and aggregation. A visualization of these operations is provided in ‘wont Together, these three operations can be collectively described as a graph convolution| Kip (2017}. Graph convolutions can be performed iteratively, increasing the receptive field and allowing GNNs to capture more global information as the number of message passing rounds increase, i.e. increase the number of GNN layers. There have been several advancements with GNN algorithms, specifically to increase expressiveness, improve scalability, and importantly, compensate for the over-smoothing problem|Xt ; where learning on densely connected graphs often converged to redundant node embeddings. Current MARL research utilizes GNNs as the de-facto communication mechanism, such as graph convolution network (GCN) /|Kipf and Welling] [2017] in CommNet|Sukhbaatar et al.|/2016] and BiCNet|Peng et al.|[2017], and graph attention network (GAT) {Brody et al.) [2022] in DGN|Jiang et al. poo and ATOGJiang and Luj [2018]. While many of these GNN models were initially designed for prediction-based tasks such as supervised classification, there have been efforts to bridge these tools into the domain of control such that novel mechanisms are tailored to the intricacies of MAS (e.g. dynamic role assignments|Shao et al.| [2022] and limited communication|Kim et al.|[2019]), although this remains an open problem in MARL. 27 Agent i Message Aggregation Agent i Message Propagation Message Computation Communication Graph Agent i’s neighbors 5.5.3 Learning to Communicate Learning communication involves considering the following three aspects of communication: the content of the outgoing messages, how the incoming messages are incorporated and the communica- tion policy|Zhu et al.] . The optimization itself can be devised to learn all these aspects together or individually and makes use of explicit and/or implicit feedback. This means the communication learning dynamics can use additional feedback signals, such as social influence(Jaques et al] [2019], which may be optimized in conjunction with the MARL training. A common practice is to seamlessly integrate the two learning dynamics of communication and control, often with differentiable modeling and backpropagation|Foerste ea 2016}, Sukhbaatar eta (2016} [Feed et al] (2020), Message Computation To initiate communication, each agent must compute messages to broadcast. The content of the message can vary from encoded or non-encoded information regarding the current and/or past local observations, actions, rewards, beliefs, objectives, previously received messages, or any other accessible information regarding the agents and the tasks. This can also include imagined information, such as future imagined trajectories/behaviors or intentions|Kim et al, 2020} To embed the listed information, an explicit approach is often employed through an auto-encoding process. On the other hand, although not completely orthogonal to the explicit learning approach, an end-to-end learning process is an alternative trained using the MARL learning dynamics without any such grounding. The learning representation for the content can be either discrete symbols or continuous values Foerster et a,(2016). In addition to the content itself, the concept of language is postulated to be an important aspect of generalizable coordinating behaviors, such as learning a lingua franca [2021]. An interesting property of language often studied is compositionality which refers to the ability to produce complex meanings by combining simpler linguistic elements and symbols in systematic ways. Communication Policy A communication policy defines the properties of the edges in the commu- nication graph, managing the following: the senders and recipients of all messages and the frequency of the message transmissions. This policy must thereby consider and adhere to the constraints of decentralized communication. If a proxy is used, the purpose of the proxy is to handle the operations of the communication policy. The edges of the communication graph can be statically or dynamically defined. A static implementa- tion can be fixed or make use of some heuristics|Huh and Mohapatra| [2023], whereas a dynamically is to make I defined solution is more involved. A natural approac gating mechanisms 8] and communication scheduling modules|Kim et al.| | to dictate the formation of 28 the edges in a learnable fashion. This enables us to design communication paradigms dynamically dependent on the needs and constraints of the task. For example, we can regularize the communication overhead with penalty terms that directly impact the parameters of the gating mechanism|Hu et al | Another consideration is the frequency of communication between agents. A common assumption is allowing all agents to communicate at every time step, however, this may not be necessary and in such cases, the over-communication can be detrimental in not only cost but also effectiveness due to the greater reliance and need for more expressive and consistent communication measures. To mitigate this issue, communication can be less frequent by setting a more sparse communication 2|| or utilizing a mechanism that enables agents to be more selective, such as the cycle|Shao et al.|(2022) gating mechanism or communication scheduling. Message Integration Lastly, we discuss message integration, which refers to how each agent processes and utilizes the messages they receive. A popular approach is to use message aggregation operations, as observed with GNN architectures, involving either a weighted or non-weighted summation over all i incoming messages. While maintaining parameters for the weighting over the incoming messag\\' a great option to increase expressiveness in the aggregation process [Zhang] , learning the weighting may lead to challenges notably i in generalization, as performance may suffer when dealing with ad-hoc team-play scenarios if there is a lack of adaptability. 5.5.4 Evaluating Communication In recent efforts relating to communication, studies have explored evaluation metrics to prow our understanding and quantify the quality of the communication between agents|Lowe et al.| [Lowe et al.][2019}. These efforts range from taking a broader view by observing the changes in performance (i.e. agent’s rewards or task success rate) under changes in communication methods to a more granular scope, where explicit metrics are defined that can account for the varying reasons for the impact achieved by communication. A broader view is often taken when communication is vital for the task at hand, often in the form of referential games, a common mode of game used to study the aspect of communication in MARL. Referential games can be thought of as a form of Lewis signaling game, where agents are each assigned the roles of speaker and listeners, and the speaker agent must communicate to the listener agents to complete their tasks . In many cases, the speaker agent has access to some private information not privied to the listener agents, making their communication significant. These specific roles designated to the agents result in a strong reliance on the communication system in place for the success of the task. The metrics of communication can be divided into two classifications: positive signaling and positive listening [Lowe et al.|[2019]. Positive signaling quantifies some statistical dependence of the messages on the agents’ observations or actions. Positive listening, on the other hand, is measured by the change in behavior of an agent if its incoming messages are obscured or omitted. Previous studies that focus on devising positive signaling metrics aim towards quantifying the alignment between the agent’s messages and some inherent component relating to the agent, often defined using mutual information|Jaques et al. (2 O19 |. Bogin et al. (2 019). For instance, speaker consistency (SC) measures the mutual information (MI) between an agent’s messages and its actions Jaques etal (2019) [2019], whereas context independence (CI) instead measures the MI between the agent’s messages and predefined task concepts. Intuitively, SC provides insights into how much uncertainty is reduced regarding an agent’s action given its messages, and CI enforces the notion of language compositionality, where the content of the messages is induced to be related to inherent concepts within the environment. On the other hand, the guiding principle for measuring positive listening has been to quantify the causal influence of an agent’s message on another agent’s behavior. Similarly, this computation can be achieved with MI, where in practice, we quantify the alignment between an agent’s message to the actions of receiving agents|Lowe et al. 2019], Jaques et al. 2019}. Usually, these metrics consider the “one-step” behavior, meaning the causal influence of a message is referenced against the immediate response of an agent, such as its next action, where a multi-step behavior can lead to more accurate measures 9]. In terms of future directions, it remains unclear the definitive relationship between these metrics, the concepts of positive listening and positive signaling, the true quality of communication, and their interactions as well as a more theoretical justification for the bias these approaches provide 29 5.6 Modeling Other Agents An essential capability for agents is the ability to reason about the behaviors of other agents, which can be achieved by constructing models of other agents (MOA). This process is often referred to as agent modeling, or more traditionally known as opponent modeling. We divide our discussion of agent modeling into three parts: the representation of MOA, the optimization paradigm used with MOA, and how MOAs are utilized. Representation of MOA MOA can encompass a wide array of properties of other agents, including their observations, actions, goals, beliefs, and more intricate components such as intentions or agent types|Hong et al. (2018), Raileanu et al. 2018}. MOA can also contain information regarding entire coalitions|Erdogan and Veloso| [2011]. Empirical applications of MOA can be accomplished using deep learning models tailored to represent the specific properties they embody and how the MOA is intended to be integrated (He et al.| . Bayesian game is a common game mode used to represent MOA through a belief space, i.e. type (1567). The belief space of each agent contains any information which is not regarded as common knowledge including its private knowledge, which can hold local information regarding other agents. These beliefs, however, face the challenge of approximating uncertainty as agents must contend with incomplete information about the environment and the behaviors of other agents. Addressing this challenge often involves employing probabilistic techniques to estimate and reason about uncertainties within the belief space[Huh and Mohapatra] . Another interesting concept with MOA is the theory of mind, where agents engage in recursive reasoning about the states of other agents|Premack and Woodruff] [1978], /Yu et al.|[2022c]. In practice, a nested reasoning approach is often approximated using belief nesting down to a fixed recursion depth, which can be implemented with game tree search techniques{Carmel and Markovitch| 1996]. In practice, there exists a large inspiration from model-based single-agent RL (MBRL), and there remains much work to incorporate the unique aspects of MAS into such MBRL methods Nashed and Zilberstein| [2022]. Learning MOA Here, we discuss two approaches to optimize and learn information about other agents, in the form of discriminative and generative learning. The discriminative learning approach comprises training MOA to classify and predict explicit properties of other agents, such as their observations or policies, typically through methods of maximum likelihood estimation [Huh and] (2024b]. On the other hand, generative learning approaches rely more on maximum a posterior, where the goal is to model the joint distribution of observed data and latent variables, enabling the generation of realistic samples from the learned distribution. By capturing the underlying structure of the data, generative learning facilitates a deeper understanding of the relationships between different properties of other agents, allowing for more nuanced inference and decision- making in multi-agent environments OTT) [Nashed and Zilberstei] . The learning process of MOA also must consider how it is integrated with the MARL training, which can be done separately or simultaneously, and what additional data or assumptions are required. Using MOA MOA can be utilized in various manners, such as guiding the agent’s decision-making process, i.e. planning and recursive reasoning, or helping construct a more accurate understanding of the environment the agent is presiding in/Huh and Mohapatra’ (20240). An important consideration is the trust and robustness of utilizing these models, as agents may make incorrect or inaccurate predictions. An interesting application that remains an open research topic is an adversarial attempt to trick agents through deceptive actions to promote misleading synergies|Albrecht and Stone|[2018]. 5.7 Ad-Hoc Team-Play Learning in multi-agent systems may be faced with a distributional mismatch resulting from unseen behaviors from other agents. The concept of ad-hoc team-play (AHTP) challenges the learned behaviors of agents to work with unknown partners who are capable of contributing to the task{Stone| . For its evaluation, it is common to define a period of ad-hoc interactions between ie agents to acclimate and devise their new joint strategy but assume that agents have no prior coordination before this ad-hoc interaction and also that agents have no direct control over other agents [ . If we assume no ad-hoc interactions, we refer to this as zero-shot coordination (ZSC) 2021]. The notion of AHTP/ZSC often arises in settings of human-AlI coordination |, where the AI agents must interact 30 with humans to achieve a task. Typically in such settings, the AI agents and the humans have not interacted with each other previously. In general, we note that despite the use of the term “teammate” and “team-play\", these agents are not necessarily cooperative but can be adversarial or mixed. A crucial aspect of achieving good AHTP behaviors is to avoid arbitrary conventions that often arise in traditional MARL training [Carroll et al.|[2020]. This can be achieved by either training agents on a pool of diverse policies, known as population-based training (PBT) Pom). or removing grounded beliefs of the agents that rely on arbitrary social conventions by utilizing off-belief learning, which assumes the prior behaviors of others were derived from fixed random policies, but their future actions will be computed with actual behavioral policies|Hu et al.| . Another option for addressing AHTP is for agents to learn to identify the behaviors of the other agents such that they can properly adjust their strategy to its current environment|Chen et al.|[ To quantify the AHTP capability, there exist two popular metrics: cross-play and adaptation regret . Cross-play is a static measure of the performance of agents with their new teammates and is often visualized through a cross-play matrix. Similarly, adaptation regret measures the cross-play performance but compares it to the performance achieved with the old teammates, i.e. the regret. The adaptation regret can be viewed under an adaptation curve, which helps view how quickly agents adapt to their new teammates. Similar to PBT, how to define the pool of “diverse” policies more optimally remains a question and often is generated using multiple independent runs of training with the same or different MARL algorithms. AHTP remains an especially difficult challenge when dealing with heterogeneous agents, open environments with variation in the number of agents in the environment, imperfect information, unreliable nor robust communication mechanisms, highly adaptive, irrational or risk-averse agents, and diverse nature of interactions (2022), |Guan et al.| [2023], Nekoei et al] (2023). 5.8 Social Learning The sharing of learned behaviors amongst agents is a powerful mechanism that can improve the efficiency and adaptability of a population’s knowledge [Silva and Costa| [2019]. This process is referred to as social learning|Ndousse et al. and encompasses the concept of knowledge transfer, which manifests through two central approaches: intra-agent transfer, where knowledge is transferred and reused from different domains, and inter-agent transfer, where agents share knowledge within the same task and setting. While both forms of knowledge transfer play crucial roles in facilitating efficient social learning and adaptation within MAS, the focus will be on inter-agent transfer. Within the framework of inter-agent transfer, it is also important to consider the role assignments within the populations. In this work, we define three common role assignments: advisor/advisee, teacher/student, and mentor/observer|Silva and Costa| [2019]. ¢ In the advisor/advisee relationship, the advisor receives requests from the advisee and observes their state, offering valuable information without presuming anything about the internal representation of agents. ¢ The interaction between a teacher and student is similar to that of an advisor/advisee, but in this case, certain assumptions are made, allowing for more informed designs for any information exchange. ¢ As for the mentor/observer relationship, the observer aims to emulate the behavior of the mentor, thereby learning from the mentor’s expertise and experience. Concretely, we explore three forms of inter-agent transfer: action advising, reward shaping, and knowledge distillation. Each of these approaches offers unique ways for agents to leverage the knowledge of their peers and improve their performance in the collective endeavor. Action Advising The fundamental concept of action advising (AA) revolves around an experienced agent providing recommendations on the next best actions to take to a less experienced agent fet al.] [2023], (Omidshafiei et al.| [2018]. In many cases, one agent can offer action suggestions to another, even when the internal representation of the other agents remains unknown. In practice, its 31 implementation must consider how this AA process is initiated, i.e. by the advisor and/or advisee. [Da Silva etal (2020)Fachantiis etal (2017) Amir et al|201, and how the advice is incorporated with some form of option learning, referring to whether or not the suggested policy should be followed Sutton et al.|[1999b],| Yang et al. 202 1a}. However, an effective approach that generalizes this AA capability tha: t benefits MARL training remains an open challenge. Reward Shaping Derived from the motivations of potential functions in single-agent RL|N , reward shaping in MARL enables agents to influence the reward signals of other agents, typically in the form of an auxiliary signal that provides further learning guidance |Gupta et al|] 20 . Reward shaping approaches are particularly valuable in scenarios where rewards are sparse, as they allow for a learnable method to devise more informative learning signals 2022}. However, similar to potential functions, it is important to take caution when using such methods, as the behaviors of agents are highly dependent and influenced by the reward function, thereby it may be pivotal in some settings to maintain some level of invariance to these auxiliary reward signals. Knowledge Distillation The process of knowledge distillation (KD) involves transferring knowl- edge from teacher agents to student agents Bucilul et a, (7006), [2015]. Typically, KD is primarily used for model compression, where the capabilities of a larger model or multiple models are distilled into a single/smaller model for parameter efficiency and potential performance benefits, or adaptation to a new state and/or action space, where a teacher model is initially trained on a more complete state-action space and a student model must make use of a more restricted state-action space{Czarnecki et al.|[2019],(Lai et al.|[2020]. In MARL applications, KD can further leverage the multi-agent nature, through structural relations distillation, where the relations between multi-agents’ features are preserved [2022]. 6 Concluding Remarks While the challenges within MARL have been extensively studied and assessed, the existing method- ologies for acquiring multi-agent behaviors fall short of fully harnessing the myriad opportunities within a MAS. Despite substantial progress, particularly in unique areas of learning in a MAS, i.e. its prospects, and the fundamental challenges of MARL, there remain open research challenges that demand further exploration and refinement. The intricacies of MARL extend beyond individual agent behavior to encompass the dynamic interac- tions unfolding within complex environments. Other properties of certain applications, such as open environments, human-robot interactions, and heterogeneous agents, merit additional considerations, for instance, how to handle the unbounded and evolving nature of open environments, safety and proper coordination with human-robot interactions, and manage the diverse capabilities, behaviors, and learning speeds within heterogeneous populations. To conclude our discussion of MARL, our exploration of MARL discussed both the progress made and the avenues yet to be fully explored. The multifaceted nature of multi-agent interactions within dynamic environments demands ongoing research and refinement of methodologies to unlock the full potential of MARL in harnessing the complexities inherent in a MAS.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:05:16.395900Z",
     "start_time": "2025-09-08T21:05:16.380790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chunk contents of the blog\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,  # Increased to capture more context in academic text\n",
    "    chunk_overlap=300,  # Increased overlap to preserve technical terms and context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Respect paragraph and sentence boundaries\n",
    "    keep_separator=True  # Retain separators to maintain readability\n",
    ")\n",
    "\n",
    "# Split the preprocessed documents\n",
    "all_splits = text_splitter.split_documents(all_documents)"
   ],
   "id": "bcdc11053410deb8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:05:31.264540Z",
     "start_time": "2025-09-08T21:05:20.803521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ],
   "id": "9abe45a502b1b30c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:06:24.715001Z",
     "start_time": "2025-09-08T21:06:24.668704Z"
    }
   },
   "cell_type": "code",
   "source": "vector_store.get()",
   "id": "ea88467f216797ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['6cf7cc01-3426-424c-a081-21682395d956',\n",
       "  '821a0748-6b34-45b1-91ca-86b2f7e7d5cd',\n",
       "  '27c32eed-f4a5-4c93-af59-ddf42cb4c154',\n",
       "  '32f0bb12-13f9-4f24-ba26-7ce70ce60447',\n",
       "  '0248c9ff-77d7-4063-8f70-13852a89664c',\n",
       "  'ac024acd-bea8-4222-9ccd-dce1e1b8d00d',\n",
       "  'a9477599-843f-45b8-92ed-1f71968f56a3',\n",
       "  '88f20146-27b3-4be4-bc21-30816a8bd250',\n",
       "  'faf595c5-6cb6-40c7-97f8-adddfa29dc58',\n",
       "  'caedd48b-ee44-4f9e-ad75-9830d7061699',\n",
       "  '334972fa-053d-4a76-a0b4-256f86c3356f',\n",
       "  'a2c33f88-f122-4e98-b641-1a4caaef01fc',\n",
       "  'cdf24552-5528-472b-bb2b-daaa621f83ef',\n",
       "  'fe37d719-2249-4349-b618-ce6c52ec3675',\n",
       "  '444ab40c-752d-4852-853e-a75cf7ee01d9',\n",
       "  'a4a285f5-78d4-41cc-86c2-c8d33c43779a',\n",
       "  '72b97d1f-25b1-4526-af0c-d96fe8dcfc49',\n",
       "  '52da0d24-fa98-4ce3-add0-195fdc10f46e',\n",
       "  'cdf61d39-f105-4ab5-936c-af33dc5c8534',\n",
       "  '12ee80e6-2969-407a-85ac-15155eb208c8',\n",
       "  '9b7a67b1-a6f3-4da7-bea1-2f5904f6d875',\n",
       "  '6936a3db-60a5-4d25-a942-0d00067b3a3f',\n",
       "  '46df795a-a0a7-4a82-9c9d-1ea68d4bfd8f',\n",
       "  'cf634165-cf42-46ff-b187-39837db4fdfc',\n",
       "  '71a4bfd2-566d-4580-a484-53d9ad81b4a3',\n",
       "  '5ee83602-2112-4fcb-b277-8a713cfbbf82',\n",
       "  '5adf4160-3020-43a8-a0dc-feac83ef4b17',\n",
       "  '07501c48-1838-4aa1-93b4-7f5379797de4',\n",
       "  'b9a4f542-6957-4b86-82ef-66f1f79a8262',\n",
       "  '9c5903b9-2dfb-4fe7-ac10-a0c8c30dbb03',\n",
       "  'ea72ce27-c581-4de5-ab1d-6025e53345b8',\n",
       "  'c3bc7a7b-4046-4a72-950a-fee0005b16e6',\n",
       "  'cd34d2f9-eba5-4b3e-b905-e27a3cd012ee',\n",
       "  'b7b8a5b1-6a5f-43b6-ac93-7de03d0a14e7',\n",
       "  'dee266cb-c442-47e0-aea9-2053da1ff156',\n",
       "  '9a9bb155-3b81-46e0-9ea7-1cd5fd8ce16f',\n",
       "  'ada37457-9a51-48d3-96e1-b1cca6ee8446',\n",
       "  '63779241-8900-429c-851f-1cdcd1b4aba1',\n",
       "  '02fb7b5c-900d-44ef-bd59-f73905e94080',\n",
       "  'ba258703-72c3-4aa9-908d-a1891666b5a5',\n",
       "  '891a7cf4-8bc7-4314-9319-a113f31aa02c',\n",
       "  '265908fe-810d-436d-9f23-c574ef65d731',\n",
       "  'a19c9fd7-447f-4a0c-ae5d-4811bd3f81d6',\n",
       "  '3f6041d1-7837-4f1b-b951-cbcb1d4eb47a',\n",
       "  '6c66860f-b312-4766-8273-a9c87e5b353c',\n",
       "  '585153cd-718c-4389-a717-9a08c20d9f2c',\n",
       "  'cafb60b5-ca0d-4442-8f42-ff94ba0f6ef4',\n",
       "  'ea50a8eb-f9ea-4458-af31-d6bce8e249c7',\n",
       "  '3058445a-697f-4bc1-853c-9cae04ec08e5',\n",
       "  '95c29e92-21df-4f79-99d5-8e6d9faf43a0',\n",
       "  '1f97ff82-294d-43d6-b4f2-8d90b2483b19',\n",
       "  '10378c72-361f-4a9f-9387-9ee2aeda7ff7',\n",
       "  'b8b859fc-dc39-4f75-8083-3c88ba91d86b',\n",
       "  'bc80d266-eaff-4ea0-bd70-127b950e3777',\n",
       "  'c4cc62e2-2e08-4a6e-be60-3d6ee4cb2226',\n",
       "  '1d451d7e-b958-482b-a8e9-3c8a09de2af8',\n",
       "  '43de280b-d3d7-4c76-802e-26dcaa59efcf',\n",
       "  '5b14ad74-cc0a-468b-9da7-4c28207c6639',\n",
       "  '0dd2dc08-9ae0-4464-ba4b-908e6138998c',\n",
       "  'a8a12526-3990-4bfa-8724-e7dec6924834',\n",
       "  'c918395b-b62c-4137-8e9d-c65f18eaa702',\n",
       "  '1fa3bb62-385d-4f44-9c69-abfde1c63b13',\n",
       "  '5a3f4968-6676-4cea-b739-52090f6cdaf7',\n",
       "  '7ad24667-2ada-4ab7-9f38-27dde6aeaff7',\n",
       "  '160ab820-a8dd-40f0-bfb3-1c0291d3c2ae',\n",
       "  '08600181-e1b8-4fd9-bdc8-14a8283ec4c3',\n",
       "  'd7f0a2bd-0cfb-4c45-b463-6445d2b2933e',\n",
       "  'b8e21801-6d8e-452f-9921-ecc1c2303a1b',\n",
       "  'edb3168e-3c82-404c-940f-959f6ef4e0d1',\n",
       "  'f9081277-d3e6-43c2-9bc8-44bacf71ac84',\n",
       "  '72a8e7b9-7cf0-437a-8032-86e36dba60c2',\n",
       "  '80288a65-310b-434b-b91c-bdc64e9f5efb',\n",
       "  '46d05596-0df3-4a75-a171-e74ae00aec7e',\n",
       "  '07d492bb-278d-41b3-81fc-9700da8a8536',\n",
       "  'ce9cdb25-77ca-4da2-b136-8d5f585fa8de',\n",
       "  'd2e851af-c095-48b5-bd6a-8748a586173d',\n",
       "  '5b841b61-eeeb-4d49-8f90-8470bb0328fc',\n",
       "  '13aac819-2a80-450f-aeb0-b1175c7ea557',\n",
       "  '95b5a858-b621-44ea-b1b3-d3a18b285b0c',\n",
       "  '994f6eb1-212f-4173-a655-0e61949be089',\n",
       "  '74eb61ca-c1ae-49c9-9756-5fb3c29fec83',\n",
       "  'a75d5f23-259a-4e82-a86b-d874803fef04',\n",
       "  '5d4c7b48-f48f-4b2e-80b2-b8223a6b3f33',\n",
       "  '09a522c5-eecb-4baf-a653-3356375aada6',\n",
       "  '4ef393a3-b9b3-4383-b360-b70e73e4b021',\n",
       "  '090c2ef2-49e2-4731-b1ff-c48688f89ce9',\n",
       "  '4479615b-ddae-4ca0-9122-54cd18811845',\n",
       "  'f32b25aa-61e3-4360-bf43-bb5cda5c33ac',\n",
       "  '825ab7e4-3362-47c7-9559-4bcea2fd973d',\n",
       "  '20941272-dd5c-4d81-a999-be3f5ba5bcd0',\n",
       "  'a2130117-cf47-4717-899e-a10c33278dda',\n",
       "  'f0fa5748-001f-4ebe-a444-f1c48fca3790',\n",
       "  '552c78d7-bdba-4ed7-a805-2a3078eacc9b',\n",
       "  'ada47528-4e2c-45d2-a09b-8a19b64ce584',\n",
       "  'b9c11ccf-71a0-4340-b448-fc694659fbb1',\n",
       "  'a3aebdeb-eca9-48e3-bbc3-77955611a3d5',\n",
       "  '351e39e4-3ed2-40b6-8010-8d4ce57bd1ae',\n",
       "  'd698b703-95cd-406f-8271-27c18b68269b',\n",
       "  '6eaff5d6-88df-4e13-9ad7-a56f9e07eb23',\n",
       "  'ebfbda0c-3a86-46d2-8ef0-1d2607549942',\n",
       "  'a0ab844c-07be-44c2-94c6-e4da9dfe7eca',\n",
       "  '2dad548f-7f3f-427b-bd92-96ab3a23b12b',\n",
       "  '37a5665c-c490-4ef9-8117-c0c89bfd7c16',\n",
       "  '410be9e5-cfeb-49c2-9624-49f4d90ca738',\n",
       "  '011bb589-53f5-4032-9a0d-de41f361f36d',\n",
       "  '319d2d3d-dac1-4b9a-92e7-6e64cbb83a71',\n",
       "  'bf696c06-22ea-4d51-8fec-f7b631fe8b10',\n",
       "  '25c0ccb4-dd99-4946-bff0-2b6e1a7c02d0',\n",
       "  '0608130b-947e-46f8-854e-726efc307374',\n",
       "  '4689ab9b-9cc0-4954-be55-66825ec77615',\n",
       "  '1a53d709-37c6-44e7-825e-d2f454eb289c',\n",
       "  '8cedbf66-f1b3-4be2-a175-97a7e1790591',\n",
       "  '66d4f98e-3c02-4658-b39b-cd9e77165d9e',\n",
       "  'f1e7f6b9-970c-4251-b8ae-2072d28b719e',\n",
       "  'aed48534-0744-450b-8529-4f34ef945824',\n",
       "  '66109dda-9aad-4785-98af-992a70ded78f',\n",
       "  'bcbe5b41-ee08-4bb6-8dd2-6ffe011ad553',\n",
       "  '7069cffa-c394-4f7e-a502-760f98d23c81',\n",
       "  'e46da08b-12cc-46a9-84f4-efee8e0a1a87',\n",
       "  '05033280-98f7-4265-ad5f-3b4f65e3be7d',\n",
       "  'ce06a2bf-091a-4815-b65e-0cad413505bf',\n",
       "  'cb6fb5ea-4183-40e8-881b-a8e2c1072450',\n",
       "  '6571be6c-127b-4da7-963a-bd4b7a2e1050',\n",
       "  '6d385614-6741-4a11-9a48-7e0c5373df83',\n",
       "  'bef0b27a-dcb2-4642-b1f5-5fef8a7647bb',\n",
       "  'e55c57f3-fcf9-4a52-94f9-7fbb81d7d684',\n",
       "  'dde7dec4-f431-49b0-a7cc-e3bcc55a73f2',\n",
       "  '60b774e5-c7c5-406d-bb4a-d9d374e2be74',\n",
       "  'dde4ac30-31ea-4bbb-b7b1-5151310f08d8',\n",
       "  'd00997fb-b1df-474c-8901-a1331519c3d6',\n",
       "  'd80e9719-f712-40fb-8a0f-159b5a784c36',\n",
       "  'af9bd8c4-ee59-4129-9074-f0d165d530e4',\n",
       "  '82ce2b6d-cf68-4d52-81ab-6affdf7e909d',\n",
       "  'db751671-5f38-4429-9de4-84d8b3b415f7',\n",
       "  '07e96cf6-07d5-4adf-ac7b-5fb3118e7078',\n",
       "  '79426e13-c53d-43aa-8ed2-286efa96fcf6',\n",
       "  '96dbb1ec-05b4-4423-875d-c13edec1f798',\n",
       "  'c1ac06e0-7356-4238-9214-d1ba01c549e6',\n",
       "  'bdb105e3-ddb1-4051-82be-4eb0f502de5e',\n",
       "  'beffeacf-12bf-400d-9fa7-a46f23a35e3c',\n",
       "  '27d3bd16-d88d-44f2-b3da-e43e639c0120',\n",
       "  'd005cbec-6903-4fc0-9c8c-02b1eb9fff6f',\n",
       "  '5043d521-8117-440d-ace1-503f3bdc8308',\n",
       "  'd8dfe8f2-2185-4c45-bbc1-198e7140546e',\n",
       "  'efb7f5fd-a062-424c-93ae-ca1cb347d416',\n",
       "  '5333f6aa-5338-4d96-9e86-f116b8898b40',\n",
       "  'c26a59f0-f89b-404c-9e1d-f3a149666875',\n",
       "  '3681cf33-a7fe-4f82-b56f-8bc3af3ad1ca',\n",
       "  '78c08f7a-6dd3-4ec5-951a-626d34696eda',\n",
       "  '579c932f-5d50-4a18-928f-0e42b346703b',\n",
       "  '81f2139f-20e0-40de-b599-2447db3b9352',\n",
       "  'e00eba4d-d549-4945-944b-371e89890cbf',\n",
       "  '4ef00045-555e-4ca6-809d-299141119915',\n",
       "  'e21b402c-6d32-4c28-b298-59f9bf82ade7',\n",
       "  '9e6caa66-ca17-45ca-804b-27aa6c668a28',\n",
       "  'aa295bd1-8921-4659-91d7-3977410319a8',\n",
       "  'd46a184f-b8eb-4b1c-89f7-f9963eb9c27e',\n",
       "  'b30ffd2d-ca54-4f7f-a3c2-41b7a08f0f4a',\n",
       "  '909325e6-f4ab-4452-8f4e-5d6b2dc9406b',\n",
       "  '62dc6aa9-85ea-40c8-9d03-4685bcd9e4a5',\n",
       "  '2284145d-5a41-4947-92e6-67c08102aa8a',\n",
       "  '883b5573-2691-4dc3-80b6-1de8584b1ff8',\n",
       "  'a44ea5cf-2f23-4ce3-b9e4-f2751ecb347c',\n",
       "  'a83550f3-8f7d-4928-b4bf-ec7038df31aa',\n",
       "  '1c4982ce-1fa4-4f68-830f-ca98942fca8e',\n",
       "  '5a3b7872-019e-42b6-8170-972dbfc3b44b',\n",
       "  'dff3f86e-f693-46b9-a495-3b6c973998cb',\n",
       "  '3384c084-fc4f-4e43-a267-1de3b99703f7',\n",
       "  'b2c905a6-b643-4015-830a-b950136d7bb3',\n",
       "  '5bb894af-acb1-4ac6-89c5-59c5645a07c2',\n",
       "  'a1805214-e2d9-41d9-8097-ae2fe0d978f4',\n",
       "  'a843ea54-8a16-4ddf-b15f-afea6426ed64',\n",
       "  '8ad67f15-a567-4fc6-9f46-fc5d1b93959a',\n",
       "  'ecc7aeb3-16c2-41fc-b6e0-f994d5747970',\n",
       "  'f0687a34-d814-4803-af0d-730a6dfd08ed',\n",
       "  'f9a94610-64e4-477a-ba3e-c89dead22953',\n",
       "  'c8c7d66d-887b-4901-af2a-b7cb2a27877d',\n",
       "  '5a74cfb2-bba9-49f4-9e13-323628459035',\n",
       "  'f9e5c593-f3e4-4e45-9f9b-22d555bf1bb7',\n",
       "  'c8ed41a9-d7a9-4ab9-9af2-7d8fb2443809',\n",
       "  'fab9f222-fb46-470d-8607-c667447e8ef5',\n",
       "  '6cc2774b-ec68-4134-b6cf-559739376c0b',\n",
       "  '7a5e9240-6e96-4e24-9bea-31af4b98c98b',\n",
       "  'e51969f4-39e5-4809-8202-bd38ea32373f',\n",
       "  'c199a7ac-d5d2-4156-b91f-3dee1b851f91',\n",
       "  'c07208e2-72be-4a5d-87a8-d088c43f3fa8',\n",
       "  '84ae8673-eab3-4af6-9b5a-ff189af6ea91',\n",
       "  '487b2b30-9578-4e1c-bc8b-28c994da5a26',\n",
       "  '40fd9ed7-8f0d-42b8-aac6-95fdbfeec69a',\n",
       "  'b07b643a-5ce7-48f8-9477-aa0e9704209e',\n",
       "  '67b811c7-1827-4dfa-920b-50cc2214289a',\n",
       "  '76233308-f10d-4325-a61d-d40175baecdc',\n",
       "  '78f47b59-d952-4447-8e05-8abba6ca6bf8',\n",
       "  '170d4273-5e58-40d4-8855-60d124cd7ef7',\n",
       "  '5d4378f9-6c0c-401f-bc3a-66d75ca5b364',\n",
       "  '4630c659-4a44-4f09-8826-31e1ebd8f96b',\n",
       "  'f6564b13-b4d5-4b16-9779-ff504899f8a8',\n",
       "  '646c5fa1-7e0f-4909-9045-1959e04ba6d5',\n",
       "  '4c07ea04-a1fa-432f-b22b-89c65c44ce8a',\n",
       "  '049500e5-c38b-4a3f-a733-525b6892623b',\n",
       "  '8b6ea6e0-6d00-4e2d-b493-3d6e9f4553ae',\n",
       "  '892d54fc-68ec-421e-83eb-bcf0ea2150ff',\n",
       "  '428e026b-47a0-447a-9a13-c59e8eecc543',\n",
       "  'e12f5673-b71e-454c-90ff-ab6c61bc475f',\n",
       "  '12c762ed-9c00-47a1-b844-0c0fd767889f',\n",
       "  'a1a42a43-cfd2-46be-8839-92ef2609dd5f',\n",
       "  'b5700a15-4610-4c66-b07c-3a11d37619fd',\n",
       "  '5befb1e7-d979-4bec-b234-5a71a929a395',\n",
       "  'c0ebd682-42b4-4096-abf3-e94fe0b7b65f',\n",
       "  '6793f18c-af29-4860-9abd-0e5417d3dde6',\n",
       "  'b9e2324d-0f51-4d92-9e9a-0f0eb3341e74',\n",
       "  '9c1dbe83-859d-4048-bcca-c0b971daa0b1',\n",
       "  'b79fb12a-9df9-40ed-a607-f9bae808a2c2',\n",
       "  '94cc564a-2ef6-4bfd-9789-8b2749ad6f99',\n",
       "  '4ed1e0d2-71cc-4702-99bb-aa8928dd23ba',\n",
       "  '4c856fe7-0599-45e4-93bc-46f7dca2401d',\n",
       "  'fc0db920-4941-4a9a-b8cc-41699685944b',\n",
       "  '46ef1c30-8efa-400b-8012-aa7852847c22',\n",
       "  'a4a599da-f87d-4329-8775-33bad37de54c',\n",
       "  '0fad4c97-4b85-4bab-b1e1-23dadcff0996',\n",
       "  '8aef8273-3832-4201-b0a0-6154657369ce',\n",
       "  '469fbc73-f4af-4065-b976-2d68a4652e0c',\n",
       "  '9201d5dd-fb1d-4359-bb5e-2b10b2dd26e5',\n",
       "  'deb2f2ad-9095-4c1b-8ac0-710f2e4f5e86',\n",
       "  'cfe6fa2c-4fe8-4db3-84a9-ed26de6a760d',\n",
       "  'da60431c-070c-4b7d-a4e7-46906d9171d2',\n",
       "  '65b5b538-23b3-403d-b825-12083f99677c',\n",
       "  'ec5962d3-7c20-4922-a143-93f51bbe48a0',\n",
       "  'a4ba6906-132d-4c75-895a-f5085183daf8',\n",
       "  '6fce5a8b-9200-425b-8cc9-23cf75a1801f',\n",
       "  '72806713-a93b-45bf-acf9-450145edae30',\n",
       "  '41e69a6a-0061-4c7a-855a-fd79fccde863',\n",
       "  '0b9dc2de-ed82-471a-8832-753afd84d7a7',\n",
       "  '4837acc9-3017-4552-af0d-979f18778ea5',\n",
       "  'd28644f3-2663-4455-ba1d-e08f88680af9',\n",
       "  '774d49af-2bd6-4a38-a283-1415ef3aa0d6',\n",
       "  'f122ea29-937d-46b8-b519-7971b88f811f',\n",
       "  '21b4d8a9-199f-43dd-9b39-4c736d29c5cd',\n",
       "  '002b9ba8-27a7-4884-8d70-2db122e66d6a',\n",
       "  'df5e6114-7e2b-4f79-a52d-b604c2312d95',\n",
       "  '53a6c14f-f88f-453d-90ee-27f1c4c8d1a0',\n",
       "  'a021f90e-228c-4495-830b-ac5b28e63284',\n",
       "  '65f072a2-027c-42f9-a5d5-21c8a6e865b3',\n",
       "  '0f1ebbb9-8a95-4386-9a14-3b04c61227e0',\n",
       "  '1ef58041-3d34-47f2-87be-f2366a421abf',\n",
       "  'a55233d6-76b7-4bbe-868e-d5eb9c1f078f',\n",
       "  '03166bca-022d-44fd-8f4f-8b01c6ee38d0',\n",
       "  'abc060dd-55f5-4730-acfa-a872af19fb3c',\n",
       "  '674216a6-4a76-44ba-a13b-bee40bb53a18',\n",
       "  '0679bddb-6da6-4b50-8c34-5248209d03e9',\n",
       "  '9e837814-b395-4ddb-be99-293d8eb73c4b',\n",
       "  'bd8c4227-7a5d-46e8-afe8-db4a939af175',\n",
       "  '0fb8b172-2866-4c30-aeb1-42f0ca5befb6',\n",
       "  'f9eafa72-ef5b-49a5-8f57-1799b18c5770',\n",
       "  'abe49d5e-1870-44e7-960d-23833fdae663',\n",
       "  '4d8cf3b1-0f51-4e66-99a9-d08c43a05861',\n",
       "  'bb9d94b9-7ea1-46a0-949b-3f538680bf12',\n",
       "  '97d464a7-e2e6-42fe-bcb4-c8441a480820',\n",
       "  '6a835509-0b44-4388-aa98-d7873fd40688',\n",
       "  'c8b71ee2-a282-4536-b268-59644730277f',\n",
       "  '4691264b-0769-4509-b71c-13247b8d0653',\n",
       "  'c5f25e80-cb81-4626-b69a-0d277d216980',\n",
       "  '6bf04911-6e95-4b02-ad54-164b1fd4c871',\n",
       "  'c71aa66d-057e-4b65-8587-07b3a7b4d2f0',\n",
       "  '58518c52-78ab-44b7-a4c6-a78f58f1afdd',\n",
       "  'fa361e65-c6c5-40bc-b356-9c022cb51010',\n",
       "  '1e32370d-92dd-4e36-ba80-5663ad5a7b2b',\n",
       "  '9924f2d0-47dd-4cf0-b119-57c7b01c80f3',\n",
       "  'f687076a-e538-41b6-8133-3c9ebfd2bc2d',\n",
       "  'ca5f699e-a68a-4157-b55c-edb1569dce02',\n",
       "  '48a4e470-f926-4b5b-a0bf-264f95583296',\n",
       "  'c792be57-f3e7-4aa5-8403-e1391a06908c',\n",
       "  'a2b45651-d997-4631-8393-8a18ec779af8',\n",
       "  '14f8a084-0393-43f9-8a94-a29305d60938',\n",
       "  'd566586f-b7af-4741-a03c-cd7f6b0e0368',\n",
       "  'd695422b-2f10-4f9b-b98f-6ab4ffaba8e2',\n",
       "  '98097e1a-9ea9-497a-9f76-e436311a757e',\n",
       "  '150735e3-797a-464b-92d2-29dbed02150f',\n",
       "  '50133908-5e4a-496a-b132-919d8d4778e3',\n",
       "  '1147f41d-394e-4911-9916-d7d919dc2a52',\n",
       "  'e2a2763b-e4bd-4dac-98b4-37a943ef97cd',\n",
       "  '0e260494-c1f5-4813-bdc3-b0127e67bb40',\n",
       "  'ee9063a6-19e4-494b-a11a-1169ec65b756',\n",
       "  '2b78a594-8216-46a4-84d9-c816cba2563d',\n",
       "  '4f6fd0b4-cc09-4b8b-b073-55910f5ae0df',\n",
       "  '7076c3eb-7655-4eed-a2e9-44014a7e383b',\n",
       "  'd468bbbb-7f35-4d19-8ad4-3a19c24367f9',\n",
       "  '7cc2dd7d-20d9-4400-af1f-82ad39064164',\n",
       "  'a9562f4e-a016-46c4-a14e-493659b9d796',\n",
       "  '311ecd8e-e68a-4ddf-9b25-5fe68bbdf8fb',\n",
       "  'fd678f35-4dc4-41d7-950b-2d2941420a3b',\n",
       "  'a32d535f-b6b5-4f93-89bf-33901bdfd852',\n",
       "  '8cf9365a-989a-4705-8c4e-157508521f57',\n",
       "  '0e567661-8183-4aaa-a502-2aa421dc51ad',\n",
       "  'fb13f245-3ae7-4eb1-85c4-754917dfa708',\n",
       "  'bef8ca85-111d-455e-8d21-dc154cf0fc69',\n",
       "  '2a44de7c-ef01-4c47-84cd-2571aeb5b9dd',\n",
       "  '4af9969f-3317-431c-bbc3-0a99dd8bd892',\n",
       "  'b77a740e-eb85-42cf-ba5d-a2574938f7c8',\n",
       "  '521ebeb6-482f-486e-95cf-2654d9a78447',\n",
       "  'e73a08dc-8121-46f5-b49c-573bc25a6d88',\n",
       "  '62a5e156-2f2d-4072-bd34-ab7ae4f820c1',\n",
       "  'ee17cbd1-65a7-4724-9c3a-9b325e8fdcb8',\n",
       "  'c3409506-e07b-49c9-8ad0-5fad3324c1a9',\n",
       "  '8bdc29c7-e3b9-4103-be7c-2ee41ff41e6e',\n",
       "  '03c5d381-5e69-4c88-8e61-c436f15a2bf3',\n",
       "  '7b37a938-082e-43d6-afa2-70586d77165e',\n",
       "  'd79f4574-a37a-44d2-8e16-ef4eec3310a0',\n",
       "  'c2c81521-e552-4e03-8691-961bbb917a12',\n",
       "  '91f84f39-5a37-41d8-a164-f619d10bcf5e',\n",
       "  '26a9a7ff-75de-4b81-a161-4cafa8f0a58c',\n",
       "  'a12b677f-e0ec-4691-be4a-8dd69ffc9951',\n",
       "  '9145e277-ede7-416e-9719-f38fdd6030d9',\n",
       "  'a7fbb56f-2a82-4abe-87ae-ca75fd98d20f',\n",
       "  'c7f9be3c-a098-473e-b403-613855272514',\n",
       "  '55961923-3029-4eac-9d2f-234b9420fc3b',\n",
       "  'ec59859c-6306-4009-bd70-6554c79cd146',\n",
       "  'd0091b09-d6ae-476f-b65b-a6faf9e5de7a',\n",
       "  '0db6816e-9622-41af-822e-28c1e98aa1ae',\n",
       "  '97e40995-1f54-4b77-98ae-d83172844c2e',\n",
       "  '75f286c1-3132-478c-b37e-88c71ebec0d7',\n",
       "  'e19ff45c-3fe2-4c8f-af03-1baa51ab391a',\n",
       "  'bc737655-b5d6-430b-9e53-45dc58d7096d',\n",
       "  'd8c0828f-5801-4f44-a416-19fb20720bbc',\n",
       "  '04eba5dc-2a66-4c95-8430-83bd64159ab5',\n",
       "  'f297c60f-7f67-4633-a4ed-1733c4264992',\n",
       "  'dc5722d5-f3b1-4346-a2e1-7bf7e0d0af1d',\n",
       "  'd97b3ae3-c9f4-439e-aa47-ba2267fab58e',\n",
       "  '80c2c7aa-6601-4425-9343-0fe1f0646b4d',\n",
       "  'a11ac0f2-d61f-4023-a7ff-c6077f324ef7',\n",
       "  'f32009e2-42c2-4242-8a3e-b2d5d3439209',\n",
       "  '7ee4f4ea-871d-4c9f-8304-8423bb88213b',\n",
       "  '90b64f72-ee18-4a40-92d1-d2e17b1d2a3e',\n",
       "  '8833a49b-d590-456d-8d51-db2fd02e6c8a',\n",
       "  'e2d987fd-9f6e-46fc-a02e-74855b72bd66',\n",
       "  'e7d644c7-e552-4877-a0cb-fe8ab96ec4fc',\n",
       "  'e1a373e2-7fd1-4305-ba65-1bf4e2d90557',\n",
       "  '1ba17e84-9c6d-409f-8433-ff5d858f27a5',\n",
       "  'b5083b81-b490-4e7e-be39-1307a3a1ecbb',\n",
       "  'b6101871-c574-4a6e-8608-0cb98fad0db4',\n",
       "  '446777c2-0186-42e7-8238-ea51559c38e9',\n",
       "  '7cf0af5f-bb33-46ba-b538-d08d139245e8',\n",
       "  '0fb2b5ac-a639-4aa4-acb9-06bcc12ff159',\n",
       "  'adc31ef6-3554-4c48-8961-a20c774fee1f',\n",
       "  '682f5f8e-5800-4ed9-945a-f2cc84411406',\n",
       "  '47d676c8-ff12-4ce0-9284-d98652f85abf',\n",
       "  '22b8e9b7-317d-4828-a2f5-dd12b14bb8bb',\n",
       "  '4a98e312-8ec1-48cc-9cfc-f3e58a1e2d25',\n",
       "  '488c9375-cc5d-4729-bc35-1e28a66dcf4c',\n",
       "  '25c864df-7e54-40b0-82a7-8f67b726835f',\n",
       "  '38fcfd85-13d2-48a6-89a5-ca5d533d39f3',\n",
       "  '449b08d2-7288-48d8-b798-b9076fe99e4b',\n",
       "  '3858439f-1e8d-4fa6-81e2-e33828fb0ddc',\n",
       "  '00a9bff1-66b4-4050-a3a2-c5bc7ae8e555',\n",
       "  'ba521240-f385-4fc7-ac36-5f7e5dc72fc3',\n",
       "  'faab98ec-bf66-4508-af72-508cb4d6c2d2',\n",
       "  'f2dd7c1e-e908-4ecd-a6de-4ceecbdddd45',\n",
       "  '080237e4-f409-45f7-9522-ea5df44bf6f2',\n",
       "  '12195d9f-a285-4f20-b16f-57569c0324c5',\n",
       "  'bd22fed2-5246-4f45-89db-c8ca5e4f536e',\n",
       "  '50ad75c4-5910-4ff1-b934-64b3bcf383ed',\n",
       "  '5cf79e7a-7a2d-4d0a-8ec7-bbfd48963de1',\n",
       "  'f61d7ab0-a2f6-4fa6-9bed-5c9a78173904',\n",
       "  '0fbb2a54-8575-4888-8625-58095be1bd0a',\n",
       "  'd23e1a42-fd18-47dd-a235-508dcbca6d5d',\n",
       "  'bd9b7eb9-bd66-4967-8947-11114300b8df',\n",
       "  '09dfe1c9-c4e5-4c67-a4f7-e9da2c2c4229',\n",
       "  'a8706461-bca3-474c-a8d6-fc015608dade',\n",
       "  '0a6759c8-f5d1-4ed3-a32b-cbb6486c4899',\n",
       "  '696a5b23-3f2b-4aff-bbc6-5e78e27af3fa',\n",
       "  'f21bec63-6091-41dd-bdbf-ded77b2c06a0',\n",
       "  'ec5fafda-1ad5-458f-8249-f0d18602c5e4',\n",
       "  'daa4b70e-cf81-4dc0-b6cd-3fab47a76d7c',\n",
       "  'ff692f9b-998d-4a28-9e0b-e5df720ffd41',\n",
       "  '93e07e20-3c0e-4f4e-9f4d-adc1f9bf94a7',\n",
       "  '5d4d3fac-c3c9-45a4-9904-df270fa2fbd5',\n",
       "  '761f6ea3-c1a9-4959-9736-b1dfb44afbcd',\n",
       "  'ad795fc5-0a72-4001-a814-270544cf0d93',\n",
       "  '66c120c0-c026-4a7d-a35d-9d7bb818d5cb',\n",
       "  '96d6d247-9a7f-49be-b1e6-3bd47a2579c1',\n",
       "  'ca3acc89-ee25-4bb6-bf92-e36da6f5047d',\n",
       "  'b045a064-bc6b-44f0-ab29-7295c483f49b',\n",
       "  '5d03cff3-8068-4239-9a7c-d927cbe5b03e',\n",
       "  '6dc1c8aa-e0ac-44a1-947d-eb439f5686e9',\n",
       "  'c50c2477-79e9-4a5e-b0b0-194f879e6262',\n",
       "  '4ab2cb5c-b2ff-4018-916a-0181f634f4af',\n",
       "  'abe4bcca-5651-4a6a-973a-6fdc0a92d537',\n",
       "  '4ed4a71c-ea45-4309-82b3-0622aa86fd6b',\n",
       "  '6943465d-612d-4d4c-9550-8bda9b87b445',\n",
       "  '381139d8-ed1e-48d4-ab32-5c4f5eae5655',\n",
       "  'dbe1accc-65fc-45ab-a187-88e3c2f4cb14',\n",
       "  '0b846058-c1ed-4698-bacc-b23d65330ff4',\n",
       "  'fcca8511-bd39-444a-bc2d-97833da82c60',\n",
       "  '8c3798a7-9c8d-4f1b-9e53-9bf5b26c8bf0',\n",
       "  '47ab3715-6c53-450b-b8ec-fc6aa9826896',\n",
       "  '6128d808-e397-45bf-909c-8ca576d28145',\n",
       "  '0bad6b10-d342-424f-a440-beaf0380d83b',\n",
       "  '8d5ba60f-ee49-47ee-9c01-b0f5f2cfbacb',\n",
       "  '9484fdcb-dfa1-4396-a572-7eb2b768dd00',\n",
       "  'fee09ceb-6671-4b5c-b455-3b867916a977',\n",
       "  '1a70c457-9984-4855-8d2d-aa23fe0b784b',\n",
       "  '0ccc264a-d3e2-4478-9645-a0f1b809451a',\n",
       "  'af69d552-90d0-4d00-aa18-3768ec0c7092',\n",
       "  'fe929d97-2e23-402b-80dc-e3ac90133b71',\n",
       "  'f92484bb-7d42-4eb2-84d5-654c13df3f0c',\n",
       "  'fd2ca6dd-0cdf-45da-ad0f-dbf85c2896d5',\n",
       "  '5bab2623-91d4-4139-8513-1e11aa210826',\n",
       "  'db6d20a3-a7c6-45be-b389-846792bfc273',\n",
       "  'c5b2d064-9ca3-4881-a3e1-793359446243',\n",
       "  '57e4ebe0-1cb5-4015-ad2d-4c93ddab6594',\n",
       "  '6307b8ce-8615-4638-9535-5e3194affec5',\n",
       "  '7d1fd57c-3367-49f9-a814-c29851960632',\n",
       "  '2b03dfa7-8082-4b16-92ff-dcb09ab2e3bd',\n",
       "  '2df4e885-b32b-4101-b05f-0ee07870ca5a',\n",
       "  '31f95ca3-dec4-4b9f-a165-4ac1ecf3495d',\n",
       "  'e1ed55c5-292d-4c37-80c6-fadb8d349a59',\n",
       "  '861c0166-ff9c-43c5-82a0-a562b0ae8396',\n",
       "  '781bd6c7-9de5-41a1-8cef-f8e33e4b0041',\n",
       "  'cd2a8a4f-aaac-4595-9b7a-ee2861427e0f',\n",
       "  '58328728-e49d-4278-ba81-3ce2420678f9',\n",
       "  '4649864a-c8b2-4f54-ad50-2b6c675ec4a5',\n",
       "  '23faa9db-13ba-4cf4-bc68-7e2057f7172a',\n",
       "  '1b879023-76b6-4a3b-90f0-8cccd7c521e8',\n",
       "  '56f98869-9184-4dc8-aa0e-6fb3e2013d95',\n",
       "  'bed2019e-5c34-4bae-a9a2-e51d55f57f4d',\n",
       "  '6829d997-23b0-4960-b21e-f21b8d2b0b55',\n",
       "  '0b430b5d-4d4a-49ac-835d-20ff2b48e4a5',\n",
       "  'a62b337c-6f49-41f5-8784-4bad140c1cf0',\n",
       "  'a0c31e1f-3ebb-495f-ac41-ab7c06fbb8bf',\n",
       "  '05f46d1c-4a47-4551-8277-e0a62c73d46e',\n",
       "  'c63a62d1-fb1c-45e8-abcc-f2a42a8343cb',\n",
       "  'f1d28bc4-8efc-4422-bcd3-2edd2bb583dc',\n",
       "  '6bf1ad61-dc61-4e5c-8445-5309ed62dd43',\n",
       "  'f678d104-c4b6-4245-9d95-b7351df76e5d',\n",
       "  'add987b0-125f-4a57-948a-39f1d025d584',\n",
       "  'fcbffd90-e179-4a29-81a4-775678d8c667',\n",
       "  '3c6dea8a-d936-4598-8961-092990b8a3dd',\n",
       "  '6eed2012-13e5-4fa6-879f-ff9ce6b2e474',\n",
       "  '4cb46a8e-2ccb-40e6-9217-4002307db6db',\n",
       "  'a01cb3d5-f130-4979-8471-383b48a9cc35',\n",
       "  'f8e56ed2-34ca-4b1b-9c97-d1fbed707cb5',\n",
       "  'a6b2751e-396c-4ee0-864a-50c2c8894134',\n",
       "  '95210473-5ca3-4421-8604-e72b71d88b82',\n",
       "  'ae769c2c-47a5-4a4b-8f8c-ffbe9af9cb76',\n",
       "  '6e630f40-1435-4898-bc3b-1dc2e0d28744',\n",
       "  '5032e5ee-b400-4d22-beec-4fbc45df9372',\n",
       "  '34adaf9f-9a36-4f16-8c70-2e684d4c1282',\n",
       "  '8769db40-e09f-4172-9d51-1baaeb25e222',\n",
       "  '378bfded-94e6-4c9c-baf0-49c46235aa2a',\n",
       "  '7a033727-22f5-46f8-956e-4dd84bb396ba',\n",
       "  '35ffde2d-db2d-40fb-99e0-cc685b177dcb',\n",
       "  '8e324ab6-28b8-4f0a-9ae7-758960531b97',\n",
       "  'e289dba1-0efe-402b-a810-038db63f4d0b',\n",
       "  '82a6f092-c359-48a6-ab88-a2e8b4e8cbbf',\n",
       "  '81208b04-3cb9-42b6-a8a1-79121b20c232',\n",
       "  'cb406e60-975e-4a20-9467-d9166f80e20a',\n",
       "  'b52fdd8f-9edd-4aca-af9e-ee50729c2294',\n",
       "  'fab77468-b3b6-4a35-8e95-b78cc5c4f7d3',\n",
       "  'badef54e-f5a7-480d-a462-9dca0b3aaa3d',\n",
       "  '4d640e71-f46d-4a97-b0fb-97939e413a10',\n",
       "  '47942047-99cb-4cff-b11b-662431a82266',\n",
       "  '27fdb44d-9890-4e09-bba8-55c442465a18',\n",
       "  '5aaf1acc-c4e9-4b50-b5b1-153d1e2d7c4c',\n",
       "  '2dd39246-1096-4ea1-b531-f2bce83f2769',\n",
       "  '86c2d12d-033e-4f22-add0-d25969b3bd11',\n",
       "  '9cfa6869-dfaf-487b-b336-f8dcb976e361',\n",
       "  'f5ac9d34-6fb5-487d-b3a8-fee1fd03affa',\n",
       "  '5218ff4a-bdb9-432f-9be9-322c0cb92338',\n",
       "  '908157be-defe-4eb3-849d-63770b1c24e5',\n",
       "  '907fc16a-b28e-4c68-8eb5-92e560ac5e50',\n",
       "  '8869da6b-4b1f-4a44-9b26-e635292d4b62',\n",
       "  'aec1d7bc-5706-455c-8f94-874331121f87',\n",
       "  'e9a45aa2-9ee0-482e-81e2-82fd5c6d693d',\n",
       "  '6b06a4fe-7740-4a2a-8d62-8be2041d08fc',\n",
       "  '2da905eb-4d52-4731-ac94-8caa197b770d',\n",
       "  '46240f05-221c-4b93-9270-07333093346c',\n",
       "  'e0e7cd33-f0ee-441c-a26c-39ef3d4b69a2',\n",
       "  '56cb1733-4deb-4027-8c53-a6a5db51d19b',\n",
       "  'ad1dbbfe-c4b8-42b2-a62a-1880d67e4306',\n",
       "  '9ccb54ca-4599-4c3a-92ee-4b621ca2325d',\n",
       "  '600869f0-258b-4114-a383-b0d7566a2d0e',\n",
       "  'aa818ab8-a096-407a-89c4-5f3c3ea3e7ed',\n",
       "  '85ab1108-c94d-4e7c-a973-6abe2e932c78',\n",
       "  'f317497c-3e6e-4011-8b81-ab5302292443',\n",
       "  'ac8bf4ca-c58b-4459-b57f-43246fe3f811',\n",
       "  '50dedc4c-f72c-4371-bd96-6b3c5b0790bc',\n",
       "  '6e830b8d-c09e-4209-9cce-9d47f141b536',\n",
       "  'f6dbcd62-9d44-4b57-b6fc-7753e52378b5',\n",
       "  'eb9f71fb-f482-4ed7-8b33-ae7666df0dca',\n",
       "  '45bb92c6-3912-4699-82cb-a870f72f1ca5',\n",
       "  'c9ee50d9-1990-416f-b970-fcebd2e1d657',\n",
       "  '5470e783-1dcf-48a6-8edf-bb9bf383f5e5',\n",
       "  '459ec166-9482-4fe5-b494-1c19dc5b7f6d',\n",
       "  '2224df66-f2c0-459a-84a7-d6196a325719',\n",
       "  'd926a7f0-eff2-4ea0-ba74-29eebfba03d1',\n",
       "  'd99dd104-cfc9-40b8-bc3c-5a41c39fb1bc',\n",
       "  'fabc9840-d034-4cc3-9617-09049eae0208',\n",
       "  'bf2d2a1c-ce23-4ae7-bb45-06f245555200',\n",
       "  '2c206dfe-dfd9-460e-abf2-fa4fdf0deb24',\n",
       "  'a94d3238-946c-4546-bcb4-15dccdb4a8ec',\n",
       "  '32ba5522-c24d-408f-87ba-c72713331717',\n",
       "  '77fdd88e-f5d9-42b2-9d13-93fba959230c',\n",
       "  'dc09e18f-1489-4a8b-829f-54b7990972ea',\n",
       "  '14b88068-40a0-4fd3-966c-d7dfae68ffd9',\n",
       "  '48f443c5-cba1-4460-935a-d2d2d507e041',\n",
       "  '5859f023-da5a-46ed-81c8-ed98edea6831',\n",
       "  '59cc69b0-5fb5-4462-abbb-f0e01f0de5cb',\n",
       "  'a6602ff1-70ae-4487-9bae-98e19edf16cb',\n",
       "  '33354b37-0f6a-49f4-92bb-1cebb81445ef',\n",
       "  'd1180203-dc26-42ab-9d59-0e8d7e5222f9',\n",
       "  '1a9b8673-b0d3-4966-adaa-c7bebb0fe8d8',\n",
       "  '04369139-1fd6-4664-8f70-697a994ba1b8',\n",
       "  '566d0d09-baea-4a9c-9fba-d9ffc9d52ca9',\n",
       "  'a3d0fdd6-a1b1-4e00-92cb-50ff678c5185',\n",
       "  '0a01eeef-4885-4eaa-a865-4eb2676dfd30',\n",
       "  'b208a5fb-1244-4ae9-b057-a6a0f9d70053',\n",
       "  '0fda1990-b9a5-405c-8fb1-804d17426c14',\n",
       "  'd0cfaf7d-a517-4734-ac73-648b39174e4e',\n",
       "  '92d70906-288e-456a-957b-7a31cac99acb',\n",
       "  '5428158c-1b09-45f3-bf70-d56b9834766d',\n",
       "  'ff2feca5-0a8a-4725-bbb0-294692409d73',\n",
       "  '019904ee-5c18-4c1b-b54b-b42e48e013a5',\n",
       "  '5a333e05-64eb-4ec7-a9b0-bbb2c4adcc53',\n",
       "  'fead0bbd-ac4c-4162-a197-9f0784f575fc',\n",
       "  '2f96131b-8359-4573-b79f-44c986f41c69',\n",
       "  '2fe77bb9-1736-49b0-a8eb-f9a4cfc0fafa',\n",
       "  '16f04125-83a1-4f73-948f-648ba03d736a',\n",
       "  '54bafc57-3ca0-481e-b5b2-f3ab45ac0d0f',\n",
       "  '798c42e7-154e-48da-a9ba-152097654de9',\n",
       "  'c110e4be-1e8c-4cbe-8d94-2c55ef32ffc1',\n",
       "  'fd19f612-8e33-41da-9c9b-a2a01de1b0f8',\n",
       "  'c55265b8-d4b6-430c-a1e2-32928f75862c',\n",
       "  'c03c88a5-a455-4f03-bcb7-5aec182c75d9',\n",
       "  '81291e64-b877-42fb-a634-81d5dd2c7400',\n",
       "  'e1755a17-ccf1-4b9c-840a-f6ae47c4e7b1',\n",
       "  '73816df0-4e8b-4a7f-80d6-1def0b0b7235',\n",
       "  'bee0df9b-76e3-4243-a2b7-a1924c80db72',\n",
       "  'c1d8426d-8dca-4ce1-9d6e-83acd4ac7ca0',\n",
       "  'daeac12d-8cc9-4aff-91bc-1cc3188f4d0f',\n",
       "  'edb99a51-d32c-4b39-9cda-77fdff1df81f',\n",
       "  'c266db14-04c9-4f5a-99b6-b89fbc431256',\n",
       "  '1565bf23-4237-44e9-8198-ac52f03baf76',\n",
       "  '0a3617a4-ff2c-464f-8f9d-06e51bef6b9e',\n",
       "  'c8a6cb7d-359f-466c-a4aa-a545ce437b75',\n",
       "  '4840107f-d239-461f-9f6f-7a32c67e5330',\n",
       "  '3862b3da-4597-4639-8f05-e66e724d3ca4',\n",
       "  'b08ed8f6-3c2b-4257-822b-8b123de2b600',\n",
       "  '279fbb8d-1a1c-45af-bc1a-461112a48639',\n",
       "  '4cd70fdf-a7de-40ee-bdb2-960bde7231b6',\n",
       "  '7cfd6667-f944-4737-87d8-b37054fb1d96',\n",
       "  '655b436f-e9a5-4c2b-ab42-e0c00089c5ee',\n",
       "  'a1006f7d-d5f0-4454-8cd9-4438f56e7b9d',\n",
       "  '569b7414-152d-4556-ad16-2eca4510c785',\n",
       "  'ebc9d151-97e5-4a44-b26f-e40823dc0963',\n",
       "  '3d885141-1826-4cfd-912b-ebb26f80b6b7',\n",
       "  'ca262705-b40e-45ef-9b98-14385261928f',\n",
       "  '03713f5e-67bf-44eb-b795-da1c49969af4',\n",
       "  '7f6f4ca5-5c39-479e-a5ee-aaa107b78554',\n",
       "  '0b080315-35c6-4415-9fa7-cff9768593c9',\n",
       "  'cce6a800-62ed-432b-9df1-41193c728db6',\n",
       "  '1d985ff7-af93-492d-b4fb-312c06df84ad',\n",
       "  '6e1116cc-77f3-408e-820f-750d69808e1c',\n",
       "  '5a00011a-b2d6-46c8-aeae-8a8c90b7d9b4',\n",
       "  '561964a8-287f-475f-8d6e-4c5777e3bef5',\n",
       "  '13c91174-3d48-4735-a8c1-60fae2a04074',\n",
       "  'e6dba98c-e13a-4fd9-965e-b68fb84d6595',\n",
       "  '671f1e41-4e23-490c-a1b3-150343ecf77b',\n",
       "  '048ae91f-0bd8-41a6-b3ae-b2667c3fa2ce',\n",
       "  '6c3d6b40-fc1c-455f-b428-6e2bd52ac858',\n",
       "  '2b4d89c0-620f-4195-94ca-1f964a91ef08',\n",
       "  '9c312be5-be14-4604-b8d1-c63d91d56bca',\n",
       "  '9c08cbcc-c69b-463c-a20d-034309e8bf32',\n",
       "  '250e92ce-a696-45f3-b9fd-0e2e51ced1d6',\n",
       "  '873cac1e-2d8c-4802-9122-663d8746db4e',\n",
       "  'c82d1a24-d5eb-45d0-80c5-869fceb0eee0',\n",
       "  '8bcb3405-3a82-4db8-90b9-40dcdf0e67f3',\n",
       "  '18a4ff65-e2fb-403b-b743-c4a975e638cc',\n",
       "  '3e63f403-94ca-4d70-b37c-bdcf555c8180',\n",
       "  '91d5842d-0921-46d9-8c66-fa11b389cd61',\n",
       "  '01d3a94d-7250-436c-ab82-dcabec2ffb16',\n",
       "  'ffce5c26-1d3a-46af-a509-26d2bfa1c475',\n",
       "  'd93f06ba-c4c7-4cfd-8bce-c3bdefbbbaa2',\n",
       "  '667f833f-cdb3-40be-8a51-d1c59ca6bb74',\n",
       "  'd4ac38a8-0643-403d-9977-0bb15ac6b350',\n",
       "  '9316d144-8e53-460a-9eab-0c866ced1ffb',\n",
       "  '06c8fda2-8e8c-48be-a0d0-fb3226fb0ab1',\n",
       "  '2e896e03-7689-4c84-bbd4-ed5902d892b2',\n",
       "  '80d68a52-328b-409e-bf0f-b3d46b140d6b',\n",
       "  'bfd7028e-9961-45ec-ad73-3e16faf92e25',\n",
       "  '75817934-971f-425c-8522-28a3b27445a9',\n",
       "  '35658e9c-1d3c-4b5b-b4d1-b3b4a5594980',\n",
       "  'a8ad8c81-2756-4105-a545-6521cecc9cd9',\n",
       "  '73e51eb6-6c53-422c-80d2-c4a15c382e8a',\n",
       "  '3265e640-4971-4041-8907-f9c2de59177b',\n",
       "  '86c1182c-55e3-48d5-b352-a6a92ab364e3',\n",
       "  '5db70579-2882-41c5-a1a8-ba86b4e38460',\n",
       "  'aebb9d89-4d5d-4753-ac43-b36c69ce9988',\n",
       "  '8ab3bab9-faee-46be-844f-62f0a9e1a7b4',\n",
       "  '25d2b096-ceae-4890-ac55-a10c06310ea3',\n",
       "  '4a5b1d4f-9909-4162-931d-1a0e0801ea4c',\n",
       "  '00368581-a9bc-4f55-a7d9-5ca9e2130826',\n",
       "  'd8d5ae4b-b5d5-4d4d-9fc1-302411ba84ee',\n",
       "  '835aa021-5ee1-492a-a5c3-d4483ce6828a',\n",
       "  'fdf834b1-558f-432c-a066-56f5f76afc90'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | MediumReinforcement Learning: An introduction (Part 1/4)Cédric Vandelaer10 min read·Aug 20, 2022--2Hi and welcome to the first part of a series on Reinforcement Learning.If you somehow ended up here without having heard of Reinforcement Learning (RL) before, then let me summarize it as follows: “RL is a general framework for training an artificial intelligence model to solve a certain task or goal” … or in layman’s , we make AI do cool things!The goal of this series is to learn RL and simultaneously explore some of the more recent research later on. We will start from the very basics and work our way towards more advanced topics. Even if you have almost no prior programming and/or mathematics knowledge, you should be able to follow along pretty smoothly.The first mini-series will be split into four parts:Part 1: What is Reinforcement learning?Part 2: RL terminology and formal conceptsPart 3: The REINFORCE algorithmPart 4: Implementing the REINFORCE algorithmAt the same time, this mini-series will be the introduction to future posts with increasing complexity. Feel free to skip to the next part if you are already familiar with the content.Note: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.That sounds cool! … but what can I do with RL?Reinforcement learning is a framework to learn any task',\n",
       "  '.Note: If you are reading this on a smartphone browser, you might not be able to view the subscripts. You can download the Medium app to mitigate this.That sounds cool! … but what can I do with RL?Reinforcement learning is a framework to learn any task. In theory, RL can solve any problem that is phrased as a Markov Decision Process. We will explain what that means later on. For now, let’s have a look at some successful applications. If you enjoy these examples, please be sure to also check out the work from the original authors.GeneralOne of the videos I love showing is the hide and seek video from OpenAI. The video is a nice example of how RL can us with finding novel solutions to problems, without explicitly programming tactics or solution methods.GamesOne of the early successes of Deep RL (which is combining RL with neural networks), was the ability to learn how to play Atari games, straight from pixels. Later on, researchers took it upon themselves to not only evaluate RL on the (relatively) simple Atari games, but rather evaluate it on the hardest competitive games out there. The assumption here is, that if RL can solve these complex games, it can also generalize to challenging real-world settings. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.RoboticsSolving tasks in simulations and video games is one thing, but what real life? Another popular field where RL is often applied (or at least holds great promise), is robotics',\n",
       "  '. As an example, this is Deepmind’s AlphaStar taking on a pro-gamer in the game StarCraft 2.RoboticsSolving tasks in simulations and video games is one thing, but what real life? Another popular field where RL is often applied (or at least holds great promise), is robotics. Robotics are significantly harder than simulations for various reasons. Think for example the time it takes to repeatedly make a robot try out a certain action. Or think the safety requirements involved for robotics. In the example below, you can see how the ANYmal robot from the Robotics System Lab in Zürich learned to recover from a fall.Real world examplesRL can be applied to many other domains than the ones I just mentioned. Advertising, finance, healthcare, … just to name a few. As a final example, I present a goal-oriented chatbot, trained to negotiate sales (source: https://siddharthverma314.github.io/research/chai-acl-2022/ ).RL: The basicsA description of the RL framework is as follows: We have an agent that tries to solve a task in a certain environment. The concept of agent should be taken very broadly here, an agent can be a robot, a chatbot, a virtual character, etc. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in',\n",
       "  '. . At every timestep t, the agent needs to choose an action a. After this action it might receive a reward r and we get a new observation of its state s. The new state can be determined both by the action of the agent and also by the environment the agent is operating in.The RL problem is trying to maximize the cumulative reward the agent gets over time.Imagine our agent is a monkey and the task we want the monkey to solve, is to pick up as many bananas as possible. At every timestep, the monkey needs to decide to take an action. The actions could be to step towards the tree, grab something, climb, … Perhaps the reward at every timestep can be defined as the number of bananas the monkey got at that timestep. After every action, the monkey will also be in a new state. Maybe we define the state of the monkey as its position in the world. So when the monkey takes a step, the state at the next timestep would be the coordinates of the monkey at the next timestep. We are now searching for the optimal behavior, the best sequence of actions the monkey can take, to maximize the cumulative number of bananas it will get.How does RL fit in the bigger picture?You might look at this framework and think: “Hey, isn’t that exactly what people are already studying in the field of …?”. And in fact, you might be right.In other domains like engineering or mathematics, people have often been studying the same problems with different names and methods',\n",
       "  '. And in fact, you might be right.In other domains like engineering or mathematics, people have often been studying the same problems with different names and methods. Or in fields like neuroscience and psychology, some similarities can be found in the way our brain “rewards” us by releasing dopamine.The likely reason for this intersection of domains, is that reinforcement learning is the study of a fundamental problem. It is essentially the science of decision taking. In these series, we will be looking at it from the umbrella of computer science and machine learning.This general applicability is also what makes RL so interesting to me personally. RL is one of the potential technologies that could get us closer to general AI: an AI system that can solve any task, in contrast to a narrow set of tasks. There are also other technologies (e.g. big language models, graph neural networks, …) that are making some strides in this regard, but the problem statement of RL in particular seems to be the most ambitious.Another way of situating RL, is by looking at how it compares to other learning paradigms. Within the field of machine learning, people often distinguish between Supervised learning, Unsupervised learning and Reinforcement learning.When it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to',\n",
       "  '.When it comes to Supervised learning, we are essentially trying to learn a function, a mapping from X to Y. Our dataset consists of samples X and during training we provide the AI system with labels Y of what these samples should map to. Our AI model is successful when it correctly predicts a label y given an (unseen) sample x.As an example, say we have a dataset consisting of cat and dog images. Each sample (image) has a label, stating whether the image contains a “dog” or a “cat”. The goal of our supervised model, is now to learn how to map a dog-image to the label “dog” and a cat-image to the label “cat”. After it has learned this mapping, the hope is that this AI model can repeat this process for new images that it has not seen before.In an Unsupervised learning context, we no longer provide any labels Y. So the task for the AI system now becomes learning some general statistics the dataset. We could for example give an AI the task to generate a new sample, similar to the ones seen in the dataset. In this case the model would be considered successful if it manages to correctly learn the interesting characteristics of a dataset.Reinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in',\n",
       "  '.Reinforcement Learning is rather different from the previous paradigms. In the case of RL, we consider an agent that is actively interacting with an environment. Through its interactions, it is possible that it may influence the environment that it operates in. The “dataset” we need to consider here, are the actions our agent took and the accumulated rewards it got by taking those actions. An added difficulty here is that our dataset is non-static. Say that our agent acts in a certain way, we can then collect some data of the actions our agent took and we can try to optimize these (e.g. do more of the actions that led to a successful result). But as a result of this optimization, we have now changed the behavior of this agent, and thus we will need to collect new data to see how well our agent fares now.If RL is so great, then why isn’t everyone using RL?After reading all this, you might be wondering why people aren’t using RL to solve all imaginable problems. The truth is that even though the field has made a lot of advancements in the last few years, there are still a few fundamental problems to be solved. Progress is being made all the time, but to give you an idea of what you might encounter, I’ll list a few common ones.Sample (in-)efficiencyIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task',\n",
       "  '.Sample (in-)efficiencyIt is generally known that RL is very sample-inefficient. We regard a “sample” as an interaction with the environment. RL needs a lot of samples/interactions to be able to solve a task. In this sense, RL is very inefficient compared to humans, for example, it doesn’t take a human dozens of hours to learn how to play an Atari game.This sample-efficiency can in part be explained by the fact that humans can leverage a lot of their previous knowledge (priors) when they encounter a new task. A human can for example reuse some of the knowledge and skills of previous games and/or concepts they already acquired from other experiences throughout their life. An RL-agent in contrast, starts the learning process without any assumptions.Another thing to mention is that leveraging knowledge from previous tasks is also an active research topic. I’ll just put one example (out of many) here to give you an idea.Deepmind GatoGoogle Jump-Start RLThe exploration-exploitation trade-offWhile the previous problem sounds more like an engineering effort (it’s not), the exploration-exploitation trade-off seems more fundamental. Whenever we train an RL-agent, the agent will need some time to explore, it needs to take some actions that it hasn’t taken before, in order to discover how to solve the problem. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing',\n",
       "  '. On the other hand, we can’t let the agent always take random actions, because these random actions might lead to nothing. Sometimes we want the agent to leverage what it has already learned to try and optimize further. This is the exploration-exploitation trade-off, we want an automated way to strike a good balance between letting the agent explore and taking actions for which it already knows what they will lead to.For a lot problems, it is quite possible that the agent gets stuck in a local optimum.The exploration-exploitation trade-off sounds very much tractable at first, but it turns out to be one of the hardest problems for RL to solve. To give you an idea how hard this problem is: The problem was originally considered by the scientists of the allied forces, but was suggested to be dropped over to Germany, because it was deemed so intractable that they wanted the German scientists to also waste their time on it.No silver bullet for this problem has been found and there are many people working on various solutions. I will leave a link here to a previously proposed solution called “curiosity-driven exploration” which I found particularly interesting.Curiosity-driven exploration by Self-supervised PredictionThe sparse-reward problemAnother rather fundamental problem, is the so called Sparse-reward problem. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve',\n",
       "  '. As the name implies, this problem occurs when our RL-agent receives so little rewards, that it actually gets no feedback on how it should improve.Imagine for example this mountain car. The agent needs to move the car left and right, such that it gets enough momentum to reach the top. Initially though, the agent doesn’t know that it needs to move the car back and forth to reach the top. If we only give our agent a reward (a positive feedback signal) when we have reached the flag, it might not ever get a positive feedback signal, simply because it might never reach the flag by taking random actions (exploration).Something commonly done to counteract this problem, is by “reward shaping”. We will modify the reward such that the agent gets more feedback signals to learn from. In case of the mountain car, we could for example also give the agent a positive reward based on the speed or altitude it achieves. However, reward shaping is not a scalable solution. Luckily other solutions are being sought after.I’m leaving another example here which I think is an interesting (but not generally applicable approach) for counteracting sparse rewards.Hindsight Experience ReplayThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made',\n",
       "  '.Hindsight Experience ReplayThe aforementioned problems are just some prominent examples, but there are in fact many more problems that are being looked into by some of the brightest minds out there. The good news is that regular and steady progress is being made.ConclusionPhew, that was a lot of information in a very short period, but you made it through! This first introduction should give you a good idea of what RL is all .In the next article, we will start formalizing some of the ideas and concepts which we have briefly touched upon in this post, as a preparation for getting our hands dirty and implementing these ideas ourselves.Artificial IntelligenceReinforcement LearningDeep LearningPolicy GradientData Science----2Written by Cédric Vandelaer192 ·8',\n",
       "  'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | MediumReinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and ApplicationsAre you interested in learning reinforcement learning but don’t know where to start? Look no further! In this article, I’ll provide an introduction to reinforcement learning (RL), explain its key concepts, and highlight some of its applications.Arjun Sarkar14 min read·Mar 9, 2023--Table of ContentsWhat is Reinforcement Learning?2. Key Concepts of Reinforcement LearningAgentEnvironmentStateActionRewardPolicyValue FunctionQ-FunctionExploration vs Exploitation3. Applications of Reinforcement LearningGamingRoboticsFinanceHealthcare4. Reinforcement Learning AlgorithmsQ-LearningDeep Q-Network (DQN)Policy GradientActor-Critic5. Challenges6. Future Directions1. What is Reinforcement Learning?Reinforcement learning (RL) is a branch of machine learning that focuses on enabling an agent to learn and make decisions based on rewards and punishments received from its environment. RL is different from supervised and unsupervised learning, as it doesn’t rely on pre-existing labeled datasets. Instead, the agent interacts with its environment, receives feedback in the form of rewards or punishments, and learns to optimize its actions accordingly.RL has found applications in various fields such as gaming, robotics, finance, and healthcare',\n",
       "  '. Instead, the agent interacts with its environment, receives feedback in the form of rewards or punishments, and learns to optimize its actions accordingly.RL has found applications in various fields such as gaming, robotics, finance, and healthcare. It has the potential to solve complex problems that are difficult to model mathematically or manually program.Photo by Lenin Estrada on Unsplash2. Key Concepts of Reinforcement LearningBefore diving into RL algorithms and applications, it’s essential to understand the key concepts that underpin RL.Main components of Reinforcement LearningAgentAn agent is an intelligent system or program that interacts with an environment in order to learn how to achieve a certain goal. The agent learns by receiving feedback from the environment in the form of rewards or punishments for its actions. The ultimate goal of the agent is to learn a policy, which is a mapping from states of the environment to actions, that maximizes its long-term expected reward.The agent typically has three main components: a policy, a value function, and a learning algorithm. The policy is the agent’s strategy for selecting actions based on the current state of the environment. The value function estimates the expected long-term reward that the agent will receive from a particular state and action. The learning algorithm updates the agent’s policy and value function based on the feedback it receives from the environment',\n",
       "  '. The value function estimates the expected long-term reward that the agent will receive from a particular state and action. The learning algorithm updates the agent’s policy and value function based on the feedback it receives from the environment.The agent’s behavior is characterized by the exploration-exploitation tradeoff. The agent must explore the environment to discover the best policy, but it also needs to exploit its current knowledge to maximize its expected reward. This tradeoff is often addressed using an exploration strategy, such as epsilon-greedy or Thompson sampling, that balances exploration and exploitation.Overall, the agent is the central component of a reinforcement learning system and is responsible for learning how to interact with and navigate the environment to achieve its goals.EnvironmentIn reinforcement learning, the term “environment” refers to the external world or the system in which an agent operates. It is the environment that an agent interacts with and receives feedback from, and the agent’s goal is to learn from these interactions and optimize its behavior to achieve a specific objective.The environment in reinforcement learning can be anything from a virtual simulation to a physical system, such as a robot or a game. The environment provides the agent with a set of observations or states, which describe the current state of the system',\n",
       "  '.The environment in reinforcement learning can be anything from a virtual simulation to a physical system, such as a robot or a game. The environment provides the agent with a set of observations or states, which describe the current state of the system. These observations are used by the agent to make decisions the actions it should take, and the actions it takes to result in a change in the state of the environment.The environment also provides feedback to the agent in the form of rewards or penalties, based on the actions it takes. These rewards serve as signals to the agent to reinforce or discourage certain behaviors, and the agent’s objective is to maximize its cumulative reward over time.The environment plays a critical role in the success of reinforcement learning, as the agent’s ability to learn and optimize its behavior is highly dependent on the quality of the environment and the feedback it provides. Therefore, creating an accurate and effective environment is essential in developing successful reinforcement learning systems.StateA state refers to the current situation or configuration of the environment that the agent is in. It includes all the relevant information that the agent needs to make decisions and take actions to maximize its rewards.The state can be represented in various ways, such as a set of variables or features that describe the current situation, or a complete image or sensory input of the environment',\n",
       "  '. It includes all the relevant information that the agent needs to make decisions and take actions to maximize its rewards.The state can be represented in various ways, such as a set of variables or features that describe the current situation, or a complete image or sensory input of the environment. The choice of representation depends on the complexity of the environment and the task at hand.The state is important because it determines the actions that the agent can take and the rewards that it will receive. The agent’s goal is to learn a policy, which is a mapping from states to actions that maximizes its cumulative reward over time. Therefore, the state is a crucial part of the reinforcement learning process, as it forms the basis of the agent’s decision-making process.ActionAn action refers to a decision made by the agent in response to the state of the environment. It is a specific move or behavior that the agent takes in a particular state to transition to the next state. An action can be a choice from a set of available options or a continuous value in a range.For example, in a game of chess, an action can be moving a piece to a particular square on the board. In a self-driving car, an action can be accelerating, braking, or turning the steering wheel. In a robotic arm, an action can be moving to a particular position or rotating in a specific direction.The choice of action by the agent is crucial because it determines the rewards received from the environment',\n",
       "  '. In a robotic arm, an action can be moving to a particular position or rotating in a specific direction.The choice of action by the agent is crucial because it determines the rewards received from the environment. The goal of the agent is to learn the optimal policy that maximizes the cumulative rewards by selecting the best possible action in each state.RewardA reward is a scalar feedback signal that an agent receives from the environment after taking an action. The purpose of a reward is to indicate how well the agent is doing at achieving its goal, which is typically to maximize a cumulative measure of reward over time.Rewards can be positive, negative, or zero, depending on whether the agent’s action led to a desirable, undesirable, or neutral outcome. The agent’s objective is to learn a policy that maximizes the expected sum of future rewards, or the expected return.Designing the reward function is an important aspect of reinforcement learning, as it directly affects the behavior of the agent. A well-designed reward function should incentivize the agent to achieve the desired goal while avoiding unintended behaviors. However, designing reward functions that accurately capture the desired behavior can be challenging, and improper reward functions can lead to suboptimal or even undesirable behavior.PolicyA policy is a function that maps an agent’s current state to an action to be taken in that state',\n",
       "  '. However, designing reward functions that accurately capture the desired behavior can be challenging, and improper reward functions can lead to suboptimal or even undesirable behavior.PolicyA policy is a function that maps an agent’s current state to an action to be taken in that state. The policy defines the agent’s behavior or strategy for choosing actions in the environment.A policy can be deterministic, meaning that it always chooses the same action for a given state, or stochastic, meaning that it chooses actions probabilistically. In a stochastic policy, the probabilities of taking each action in a given state are specified by the policy.The goal of reinforcement learning is often to learn an optimal policy that maximizes the expected cumulative reward over time. This is typically achieved through trial and error, where the agent interacts with the environment, observes the resulting rewards and transitions to new states, and updates its policy based on the observed outcomes.Value FunctionA value function is a function that estimates the value of a state or state-action pair. It represents how good a particular state or action is in of achieving the agent’s goal. The value function is a critical component of many reinforcement learning algorithms because it guides the agent’s decision-making process.There are two types of value functions in reinforcement learning: state-value function and action-value function',\n",
       "  '. The value function is a critical component of many reinforcement learning algorithms because it guides the agent’s decision-making process.There are two types of value functions in reinforcement learning: state-value function and action-value function.State-value function: It predicts how much reward an agent can expect to receive from a given state. The state-value function is denoted by V(s) and is defined as the expected cumulative reward that an agent can receive starting from a given state s and the current policy.Action-value function: It predicts how much reward an agent can expect to receive by taking a particular action in a given state. The action-value function is denoted by Q(s,a) and is defined as the expected cumulative reward that an agent can receive starting from state s, taking action a, and the current policy.The value function is estimated using past experiences and updated through iterative learning methods, such as temporal difference learning or Monte Carlo methods. Accurate estimation of the value function is essential for the agent to make optimal decisions and maximize its long-term rewards.Q-FunctionThe Q function (also known as the action-value function) is a mathematical function that takes in a state-action pair as input and outputs the expected long-term reward of taking that action in that state and a given policy thereafter',\n",
       "  '.Q-FunctionThe Q function (also known as the action-value function) is a mathematical function that takes in a state-action pair as input and outputs the expected long-term reward of taking that action in that state and a given policy thereafter.The Q function is a crucial component in many reinforcement learning algorithms, such as Q-learning and SARSA, as it allows the agent to estimate the quality of different actions in different states. By learning the Q function, the agent can then choose actions that maximize the expected long-term reward, which is the goal of many reinforcement learning tasks.The Q function is often represented as a table or a function approximator, such as a neural network, that is learned through experience and interaction with the environment. The process of learning the Q function involves iteratively updating the Q values based on the observed rewards and transitions until the estimates converge to the true values.Exploration vs ExploitationExploration and exploitation are two important concepts in reinforcement learning that deal with how an agent should choose actions to take in an environment.Exploration refers to the agent’s behavior of trying out new actions in order to learn more the environment and potentially find better actions that lead to higher rewards. Exploitation, on the other hand, refers to the agent’s behavior of choosing the actions that have already been tried and proven to lead to high rewards',\n",
       "  '. Exploitation, on the other hand, refers to the agent’s behavior of choosing the actions that have already been tried and proven to lead to high rewards.The challenge in reinforcement learning is to balance exploration and exploitation in order to maximize the agent’s long-term reward. If the agent only exploits known good actions, it may miss out on better actions that it hasn’t tried yet. On the other hand, if the agent only explores new actions, it may not accumulate enough reward to perform well in the long run.Various exploration strategies have been proposed, such as ε-greedy, softmax, and Upper Confidence Bound (UCB), among others. These strategies use different ways to balance exploration and exploitation, and the choice of strategy depends on the specific problem at hand.3. Applications of Reinforcement LearningReinforcement learning has found numerous applications in various fields. Here are some examples:GamingReinforcement learning has been applied to various games, from classic board games like chess and Go to modern video games like Dota 2 and StarCraft II. In these games, an agent learns to make decisions and compete against human players or other agents.RoboticsReinforcement learning has been used in robotics to enable robots to learn to navigate environments, manipulate objects, and perform complex tasks. In these applications, an agent learns from feedback received through sensors and actuators',\n",
       "  '.RoboticsReinforcement learning has been used in robotics to enable robots to learn to navigate environments, manipulate objects, and perform complex tasks. In these applications, an agent learns from feedback received through sensors and actuators.FinanceReinforcement learning has found applications in finance, including algorithmic trading, portfolio optimization, and fraud detection. In these applications, an agent learns to make decisions based on market data and financial indicators.HealthcareReinforcement learning has been used in healthcare to optimize treatment plans and drug dosages. In these applications, an agent learns from patient data and medical records to make decisions that maximize patient outcomes.4. Reinforcement Learning AlgorithmsThere are several algorithms used in reinforcement learning. Here are some of the most commonly used ones:Q-LearningQ-learning is a model-free, off-policy reinforcement learning algorithm used to find the optimal action-selection policy for any given Markov decision process (MDP). In Q-learning, the agent learns an action-value function Q(s,a) that gives the expected utility of taking action a, in state s, and the optimal policy thereafter.The algorithm uses an iterative process to update the Q-values of each state-action pair based on the observed rewards received from the environment',\n",
       "  '.The algorithm uses an iterative process to update the Q-values of each state-action pair based on the observed rewards received from the environment. The updates are made using the Bellman equation, which exes the expected value of the current state as the sum of the immediate reward and the expected value of the next state. By iteratively updating the Q-values, Q-learning converges to the optimal Q-function and thus the optimal policy.One of the key advantages of Q-learning is that it can learn optimal policies in environments with large state spaces and stochastic rewards. However, it requires sufficient exploration of the state-action space to avoid getting stuck in suboptimal policies, which can be challenging in some environments. Additionally, Q-learning assumes that the state transition and reward functions are unknown, which may not always be the case in practice.Deep Q-Network (DQN)Deep Q-Network (DQN) is a popular deep reinforcement learning algorithm introduced by DeepMind in 2013. It extends the Q-learning algorithm to work with high-dimensional input spaces by using a deep neural network to approximate the Q-function.In DQN, the agent uses a neural network to estimate the Q-values of each possible action in a given state. The network takes the state as input and outputs the estimated Q-values for each possible action. The agent then selects the action with the highest estimated Q-value to take',\n",
       "  '.In DQN, the agent uses a neural network to estimate the Q-values of each possible action in a given state. The network takes the state as input and outputs the estimated Q-values for each possible action. The agent then selects the action with the highest estimated Q-value to take.To train the DQN, the agent uses experience replay and a target network. Experience replay is a technique where the agent stores transitions from the environment in a buffer and samples random batches of these transitions to train the neural network. This s to break the correlation between consecutive samples and improve the stability of the learning process. The target network is a separate network used to generate the Q-value targets for training. The weights of the target network are frozen and only updated periodically with the weights of the main network, which s to stabilize the learning process.DQN has been applied successfully to a variety of tasks, including playing Atari games and controlling robotic systems.Policy GradientPolicy gradient algorithms are a class of reinforcement learning methods that learn a policy function by directly optimizing the objective function that measures the expected cumulative reward obtained by the policy. The policy function maps states to actions, and the goal of the algorithm is to find the policy that maximizes the expected cumulative reward over a long-term horizon',\n",
       "  '. The policy function maps states to actions, and the goal of the algorithm is to find the policy that maximizes the expected cumulative reward over a long-term horizon.Policy gradient methods use gradient descent to iteratively update the policy parameters to maximize the expected cumulative reward. The policy is typically represented as a neural network, where the input is the state and the output is a probability distribution over the possible actions. The gradient of the objective function is computed with respect to the policy parameters, and the parameters are updated to move in the direction of the gradient.One popular policy gradient algorithm is the REINFORCE algorithm, which is a Monte Carlo algorithm that estimates the gradient of the expected reward using samples from the current policy. Another popular algorithm is the Actor-Critic algorithm, which combines policy gradient with value function estimation. The Actor-Critic algorithm uses a neural network to represent the policy and another neural network to represent the value function. The policy network is updated using the policy gradient, while the value network is updated using the temporal difference (TD) error, which is the difference between the predicted and actual rewards.Policy gradient methods have several advantages over value-based methods like Q-learning, including the ability to learn stochastic policies, handling of continuous action spaces, and better convergence properties',\n",
       "  '.Policy gradient methods have several advantages over value-based methods like Q-learning, including the ability to learn stochastic policies, handling of continuous action spaces, and better convergence properties. However, they can also suffer from high variance and slow convergence, especially in high-dimensional state spaces.Actor-CriticActor-critic algorithms are a type of reinforcement learning algorithm that combine the advantages of both policy-based and value-based methods. The actor-critic algorithm involves two neural networks, an actor network, and a critic network.The actor-network is responsible for selecting actions based on the current state of the environment. It uses the policy gradient method to update its parameters and improve its performance. The critic network, on the other hand, evaluates the value of the current state and action pair. It uses the temporal difference learning method to update its parameters and learn from the feedback received from the environment.The actor-critic algorithm uses the critic network to estimate the value of an action in a given state and then uses this estimate to update the policy of the actor-network. This allows the algorithm to balance exploration and exploitation in a more efficient manner, and improve the overall performance of the agent',\n",
       "  '. This allows the algorithm to balance exploration and exploitation in a more efficient manner, and improve the overall performance of the agent.Actor-critic algorithms can be further divided into several subtypes, including advantage actor-critic (A2C), asynchronous advantage actor-critic (A3C), and deep deterministic policy gradient (DDPG). These algorithms have been successfully applied to a wide range of tasks, including robotics, game-playing, and natural language processing.5. ChallengesReinforcement learning faces several challenges, including:Exploration vs Exploitation: Finding the right balance between exploration and exploitation is one of the fundamental challenges in reinforcement learning. The agent must explore the environment to learn the optimal policy while also exploiting the current knowledge to maximize the expected reward.Credit assignment: In some cases, the reward received by the agent may not reflect the quality of the actions taken in the past. This makes it difficult to assign credit to the actions that led to the reward.Generalization: Reinforcement learning often requires generalizing learned policies across different environments or tasks. This can be challenging because the agent must be able to adapt to new situations without forgetting what it has already learned.Function approximation: In many reinforcement learning problems, the state and action spaces are too large to store in memory or represent exactly',\n",
       "  '. This can be challenging because the agent must be able to adapt to new situations without forgetting what it has already learned.Function approximation: In many reinforcement learning problems, the state and action spaces are too large to store in memory or represent exactly. Thus, function approximation techniques are used to estimate value functions or policies. However, these approximations can introduce errors that affect the quality of the learned policies.Delayed rewards: The rewards received in reinforcement learning are often delayed, meaning that the agent must learn to associate current actions with future rewards. This can make it challenging to determine the best action to take in the current state.Sample inefficiency: Reinforcement learning algorithms typically require a large number of interactions with the environment to learn an optimal policy. This can be time-consuming and costly in some real-world applications where interactions with the environment are limited.6. Future DirectionsReinforcement learning is a rapidly evolving field with many exciting developments and future directions. Some of the potential areas of focus for future research and development include:Multi-agent reinforcement learning: Many real-world applications of reinforcement learning involve multiple agents interacting with each other, such as in autonomous driving or game theory',\n",
       "  '. Some of the potential areas of focus for future research and development include:Multi-agent reinforcement learning: Many real-world applications of reinforcement learning involve multiple agents interacting with each other, such as in autonomous driving or game theory. Developing algorithms and techniques that can handle these complex interactions is a promising direction for future research.Sample efficiency: One of the biggest challenges in reinforcement learning is the amount of data required to train an agent. Developing algorithms that can learn effectively from fewer samples could significantly increase the applicability of reinforcement learning in real-world scenarios.Safe and ethical RL: As reinforcement learning is applied to increasingly complex and important tasks, ensuring that agents behave safely and ethically becomes more critical. Future research could focus on developing techniques to ensure that agents behave in a manner consistent with ethical and legal standards.Transfer learning: Transfer learning involves leveraging knowledge gained from one task to an agent learn more efficiently in a new, related task. Developing techniques that allow for effective transfer learning could greatly increase the efficiency and effectiveness of reinforcement learning algorithms',\n",
       "  '. Developing techniques that allow for effective transfer learning could greatly increase the efficiency and effectiveness of reinforcement learning algorithms.Explainability and interpretability: As reinforcement learning is applied in more critical domains, such as healthcare or finance, it becomes more important to understand why agents are making particular decisions. Developing techniques to explain and interpret the decisions of reinforcement learning agents is a promising direction for future research.Reinforcement LearningDeep LearningMachine LearningNeural NetworksAlgorithms----Written by Arjun Sarkar3.2K ·411 Ph.D. student — Deep Learning on Biomedical Images at the Leibniz Institute-HKI, Germany. LinkedIn-https://www.linkedin.com/in/arjun-sarkar-9a051777/No responses yet',\n",
       "  'An Introduction to Deep Reinforcement Learning | MediumAn Introduction to Deep Reinforcement LearningChapter 1 of the Deep Reinforcement Learning Course v2.0Thomas Simonini14 min read·Oct 9, 2020--6We launched a new free, updated, Deep Reinforcement Learning Course from beginner to expert, with Hugging Face 🤗👉 The new version of the course: https://huggingface.co/deep-rl-course/unit0/introductionThe chapter below is the former version, the new version is here 👉 https://huggingface.co/deep-rl-course/unit1/introductionWe launched a new free, updated, Deep Reinforcement Learning Course from beginner to expert, with Hugging Face 🤗👉 The new version of the course: https://huggingface.co/deep-rl-course/unit0/introductionThe chapter below is the former version, the new version is here 👉 https://huggingface.co/deep-rl-course/unit1/introductionWelcome to the most fascinating topic in Artificial Intelligence: Deep Reinforcement Learning.Deep RL is a type of Machine Learning where an agent learns how to behave in an environment by performing actions and seeing the results.Since 2013 and the Deep Q-Learning paper, we’ve seen a lot of breakthroughs. From OpenAI five that beat some of the best Dota2 players of the world, to the Dexterity project, we live in an exciting moment in Deep RL research.OpenAI Five, an AI that beat some of the best Dota2 players of the worldMoreover, since the first version of this course in 2018, a ton of new libraries (TF-Agents, Stable-Baseline 2',\n",
       "  '.OpenAI Five, an AI that beat some of the best Dota2 players of the worldMoreover, since the first version of this course in 2018, a ton of new libraries (TF-Agents, Stable-Baseline 2.0…) and environments where launched: MineRL (Minecraft), Unity ML-Agents, OpenAI retro (NES, SNES, Genesis games…). You have now access to so many amazing games to build your agents.That’s why this is the best moment to start learning, and with this course you’re in the right place.Yes, because this article is the first chapter of Deep RL Course v2.0 a free course from beginner to expert where you’ll master the skills and architectures you need, to become a deep reinforcement learning expert.During this course, you’ll build a strong professional portfolio by implementing awesome agents with Tensorflow and PyTorch that learn to play Space invaders, Minecraft, Starcraft, Sonic the hedgehog and more!So in this first chapter, you’ll learn the foundations of deep reinforcement learning.It’s really important to master these elements before diving into implementing Deep Reinforcement Learning agents. The goal in this chapter is to give you solid foundations.If you prefer, you can watch the 📹 video version of this chapter:So let’s get started!What is Reinforcement Learning?In order to understand what is reinforcement learning, let’s start with the big picture',\n",
       "  '. The goal in this chapter is to give you solid foundations.If you prefer, you can watch the 📹 video version of this chapter:So let’s get started!What is Reinforcement Learning?In order to understand what is reinforcement learning, let’s start with the big picture.The big pictureThe idea behind Reinforcement Learning is that an agent (an AI) will learn from the environment by interacting with it (through trial and error) and receiving rewards (negative or positive) as feedback for performing actions.Learning from interaction with the environment comes from our natural experiences.For instance, imagine you put your little brother in front of a video game he never played, a controller in his hands, and let him alone.Your brother will interact with the environment (the video game) by ing the right button (action). He got a coin, that’s a +1 reward. It’s positive, he just understood that in this game he must get the coins.But then, he es right again and he touches an enemy, he just died -1 reward.By interacting with his environment through trial and error, your little brother just understood that in this environment, he needs to get coins, but avoid the enemies.Without any supervision, the child will get better and better at playing the game.That’s how humans and animals learn, through interaction. Reinforcement Learning is just a computational approach of learning from action',\n",
       "  '.Without any supervision, the child will get better and better at playing the game.That’s how humans and animals learn, through interaction. Reinforcement Learning is just a computational approach of learning from action.A formal definitionIf we take now a formal definition:Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.But how Reinforcement Learning works?The Reinforcement Learning FrameworkThe RL ProcessThe RL Process: a loop of state, action, reward and next stateTo understand the RL process, let’s imagine an agent learning to play a platform game:A big thanks to Felix for this updated illustrationOur Agent receives state S0 from the Environment — we receive the first frame of our game (environment).Based on that state S0, the agent takes an action A0 — our agent will move to the right.Environment transitions to a new state S1 — new frame.Environment gives some reward R1 to the agent — we’re not dead (Positive Reward +1).This RL loop outputs a sequence of state, action and reward and next state.The goal of the agent is to maximize its cumulative reward, called the expected return',\n",
       "  '.Environment transitions to a new state S1 — new frame.Environment gives some reward R1 to the agent — we’re not dead (Positive Reward +1).This RL loop outputs a sequence of state, action and reward and next state.The goal of the agent is to maximize its cumulative reward, called the expected return.The reward hypothesis: the central idea of Reinforcement LearningWhy the goal of the agent is to maximize the expected return?Because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).That’s why in Reinforcement Learning, to have the best behavior, we need to maximize the expected cumulative reward.(Optional) Markov PropertyYou’ll see in papers that the RL process is called the Markov Decision Process (MDP).We’ll talk again the Markov Property in the next chapters. But if you need to remember something today it is just that Markov Property implies that our agent needs only the current state to make its decision what action to take and not the history of all the states and actions he took before.Now let’s dive a little bit on all this new vocabulary:Observations/States SpaceObservations/States are the information our agent gets from the environment. In the case of a video game, it can be a frame (a screenshot), in the case of the trading agent, it can be the value of a certain stock etc',\n",
       "  '.Now let’s dive a little bit on all this new vocabulary:Observations/States SpaceObservations/States are the information our agent gets from the environment. In the case of a video game, it can be a frame (a screenshot), in the case of the trading agent, it can be the value of a certain stock etc.There is a differentiation to make between observation and state:State s: is a complete description of the state of the world (there is no hidden information). In a fully observed environment.In chess game, we receive a state from the environment since we have access to the whole check board information.With a chess game, we are in a fully observed environment, since we have access to the whole check board information.Observation o: is a partial description of the state. In a partially observed environment.In Super Mario Bros, we only see a part of the level close to the player, so we receive an observation.In Super Mario Bros, we are in a partially observed environment, we receive an observation since we only see a part of the level.In reality, we use the term state in this course but we will make the distinction in implementations.Action SpaceThe Action space is the set of all possible actions in an environment.The actions can come from a discrete or continuous space:Discrete space: the number of possible actions is finite',\n",
       "  '.In reality, we use the term state in this course but we will make the distinction in implementations.Action SpaceThe Action space is the set of all possible actions in an environment.The actions can come from a discrete or continuous space:Discrete space: the number of possible actions is finite.Again, in Super Mario Bros, we have only 4 directions and jump possibleIn Super Mario Bros, we have a finite set of actions since we have only 4 directions and jump.Continuous space: the number of possible actions is infinite.A Self Driving Car agent has an infinite number of possible actions since he can turn left 20°, 21°, 22°, honk, turn right 20°, 20,1°…SourceTaking this information into consideration is crucial because it will have importance when we will choose in the future the RL algorithm.Rewards and the discountingThe reward is fundamental in RL because it’s the only feedback for the agent. Thanks to it, our agent knows if the action taken was good or not.The cumulative reward at each time step t can be written as:The cumulative reward is equal to the sum of all rewards of the sequence.Which is equivalent to:However, in reality, we can’t just add them like that. The rewards that come sooner (at the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.Let say your agent is this small mouse that can move one tile each time step, and your opponent is the cat (that can move too)',\n",
       "  '. The rewards that come sooner (at the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.Let say your agent is this small mouse that can move one tile each time step, and your opponent is the cat (that can move too). Your goal is to eat the maximum amount of cheese before being eaten by the cat.As we can see in the diagram, it’s more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).As a consequence, the reward near the cat, even if it is bigger (more cheese), will be more discounted since we’re not really sure we’ll be able to eat it.To discount the rewards, we proceed like this:We define a discount rate called gamma. It must be between 0 and 1.The larger the gamma, the smaller the discount. This means our agent cares more the long term reward.On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more the short term reward (the nearest cheese).2. Then, each reward will be discounted by gamma to the exponent of the time step.As the time step increases, the cat gets closer to us, so the future reward is less and less probable to happen.Our discounted cumulative expected rewards is:Type of tasksA task is an instance of a Reinforcement Learning problem. We can have two types of tasks: episodic and continuous.Episodic taskIn this case, we have a starting point and an ending point (a terminal state)',\n",
       "  '.Our discounted cumulative expected rewards is:Type of tasksA task is an instance of a Reinforcement Learning problem. We can have two types of tasks: episodic and continuous.Episodic taskIn this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and New States.For instance think Super Mario Bros, an episode begin at the launch of a new Mario Level and ending when you’re killed or you’re reach the end of the level.Beginning of a new episodeContinuous tasksThese are tasks that continue forever (no terminal state). In this case, the agent has to learn how to choose the best actions and simultaneously interacts with the environment.For instance, an agent that do automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop him.Exploration/ Exploitation tradeoffFinally, before looking at the different methods to solve Reinforcement Learning problems, we must cover one more very important topic: the exploration/exploitation trade-off.Exploration is exploring the environment by trying random actions in order to find more information the environment.Exploitation is exploiting known information to maximize the reward.Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, we can fall into a common trap',\n",
       "  '.Exploitation is exploiting known information to maximize the reward.Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, we can fall into a common trap.Let’s take an example:In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation).But if our agent does a little bit of exploration, it can discover the big reward (the pile of big cheese).This is what we call the exploration/exploitation trade off. We need to balance how much we explore the environment and how much we exploit what we know the environment.Therefore, we must define a rule that s to handle this trade-off. We’ll see in future chapters different ways to handle it.If it’s still confusing think of a real problem: the choice of a restaurant:Source: Berkley AI CourseExploitation: You go everyday to the same one that you know is good and take the risk to miss another better restaurant.Exploration: Try restaurants you never went before, with the risk of having a bad experience but the probable opportunity of an amazing experience',\n",
       "  '.Exploration: Try restaurants you never went before, with the risk of having a bad experience but the probable opportunity of an amazing experience.The two main approaches for solving RL problemsNow that we learned the RL framework, how do we solve the RL problem?In other , how to build a RL agent that can select the actions that maximize its expected cumulative reward?The Policy π: the agent’s brainThe Policy π is the brain of our Agent, it’s the function that tell us what action to take given the state we are. So it defines the agent behavior at a given time.Think of policy as the brain of our agent, the function that will tells us the action to take given a stateThis Policy is the function we want to learn, our goal is to find the optimal policy π*, the policy that maximizes expected return when the agent acts according to it. We find this π* through training.There are two approaches to train our agent to find this optimal policy π*:Directly, by teaching the agent to learn which action to take, given the state is in: Policy-Based Methods.Indirectly, teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states: Value-Based Methods.Policy-Based MethodsIn Policy-Based Methods, we learn a policy function directly.This function will map from each state to the best corresponding action at that state. Or a probability distribution over the set of possible actions at that state',\n",
       "  '.Policy-Based MethodsIn Policy-Based Methods, we learn a policy function directly.This function will map from each state to the best corresponding action at that state. Or a probability distribution over the set of possible actions at that state.As we can see here, the policy (deterministic) directly indicates the action to take for each step.We have two types of policy:Deterministic: a policy at a given state will always return the same action.action = policy(state)Stochastic: output a probability distribution over actions.policy(actions | state) = probability distribution over the set of actions given the current stateGiven an initial state, our stochastic policy will output a probability distributions over the possible actions at that state.Value based methodsIn Value based methods, instead of training a policy function, we train a value function that maps a state to the expected value of being at that state.The value of a state is the expected discounted return the agent can get if it starts in that state, and then act according to our policy.“Act according to our policy” just means that our policy is “going to the state with the highest value”.Here we see that our value function defined value for each possible state.Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal',\n",
       "  \".Here we see that our value function defined value for each possible state.Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.The “Deep” in Reinforcement LearningWait… you spoke Reinforcement Learning, but why we speak Deep Reinforcement Learning?Deep Reinforcement Learning introduces deep neural networks to solve Reinforcement Learning problems — hence the name “deep.”For instance, in the next article, we’ll work on Q-Learning (classic Reinforcement Learning) and then Deep Q-Learning both are value-based RL algorithms.You’ll see the difference is that in the first approach, we use a traditional algorithm to create a Q table that s us find what action to take for each state.In the second approach, we will use a Neural Network (to approximate the q value).Schema inspired by the Q learning notebook by UdacityIf you are not familiar with Deep Learning you definitely should watch the MIT Intro Course on Deep Learning (Free)MIT Deep Learning 6.S191MIT's introductory course on deep learning methods with applications to computer vision, natural language processing…introtodeeplearning.comThat was a lot of information, if we summarize:Reinforcement Learning is a computational approach of learning from action\",\n",
       "  \".S191MIT's introductory course on deep learning methods with applications to computer vision, natural language processing…introtodeeplearning.comThat was a lot of information, if we summarize:Reinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment by interacting with it through trial and error and receiving rewards (negative or positive) as feedback.The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected cumulative reward.The RL process is a loop that outputs a sequence of state, action, reward and next state.To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long term future reward.To solve an RL problem, you want to find an optimal policy, the policy is the “brain” of your AI that will tell us what action to take given a state. The optimal one is the one who gives you the actions that max the expected return.There are two ways to find your optimal policy:By training your policy directly: policy-based methods.By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods\",\n",
       "  '.There are two ways to find your optimal policy:By training your policy directly: policy-based methods.By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.Finally, we speak Deep RL because we introduces deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based) hence the name “deep.”Congrats on finishing this chapter! That was the biggest one, and there was a lot of information.That’s normal if you’re still feel confuse with all these elements. This was the same for me and for all people who studied RL.Take time to really grasp the material before continuing. It’s important to master these elements and having a solid foundations before entering the fun part: creating AI that plays video games.Naturally, during the course, we’re going to use and deeper explain again these but it’s better to have a good understanding of them now before diving into the next chapters.In the next chapter, we’re going to learn our first RL algorithm Q-Learning and dive deeper into the value-based methods.You’ll train your first RL agent: a taxi Q-Learning agent that will need to learn to navigate in a city to transport its passengers from a point A to a point B. This will be fun.If you liked my article, please click the 👏 below as many times as you liked the article so other people will see this here on Medium',\n",
       "  '. This will be fun.If you liked my article, please click the 👏 below as many times as you liked the article so other people will see this here on Medium. And don’t forget to follow me on Medium, on Twitter, and on Youtube.See you next time,Keep learning, stay awesome,Deep Reinforcement Learning Course v2.0:Chapter 1: Introduction to Deep Reinforcement LearningChapter 2, Part 1: Q-Learning with Taxi-v3Chapter 2, Part 2: Q-Learning with Taxi-v3Machine LearningAIArtificial IntelligenceReinforcement LearningDeep Learning----6Written by Thomas Simonini5.3K ·220 Developer Advocate 🥑 at Hugging Face 🤗| Founder Deep Reinforcement Learning class 📚 https://bit.ly/3QADz2Q |',\n",
       "  'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | MediumAnalytics Vidhya·Analytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comIntroduction to Reinforcement Learning (RL) in PyTorchHarsh Panchal21 min read·Aug 26, 2021--6Photo by Alina Grubnyak on UnsplashRecap of Supervised LearningSo far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In supervised learning, we are given some sort of training data consisting of input/output pairs, with the goal being to be able to predict the output given some new inputs after learning the model. For example, we previously looked at a Convolutional Neural Network (CNN) classification model for MNIST; given a training set of 60000 digit images and corresponding digit labels (e.g. ‘5’), we learned a model that was capable of predicting the digit label of new MNIST images. In order words, something like (but not exactly) this:Image by authorWhat if we want to learn how to perform more complex behaviors, where data collection can be expensive? How do you teach a robot to walk? Self-driving cars? How do you defeat human champions in the game of Go?Reinforcement LearningEnter Reinforcement Learning',\n",
       "  '. In Reinforcement Learning, our model (commonly referred to as an agent in this context) interacts with an environment by taking actions 𝑎a and receives some sort of feedback from the environment in the form of a reward 𝑟. In this sense, reinforcement learning algorithms learn by experience. We call the trajectory of going from start to finish of a task an episode, and often our agent will learn by undergoing many episodes.Image by authorMany reinforcement learning algorithms are modeled as Markov Decision Processes (MDPs). In these settings, we have a concept of a state 𝑠, which encapsulates the situation of the agent (e.g. location, velocity). From each state 𝑠𝑡, the agent takes an action 𝑎𝑡, which results in a transition from one state 𝑠𝑡 to another 𝑠𝑡+1. In many settings, there is stochasticity in this transition, meaning that there’s is a distribution over 𝑠𝑡+1 conditioned on 𝑠𝑡 and 𝑎𝑡. Often, several of these states are considered episode-ending, after which the agent can no longer make any transitions or collect any more reward. These correspond to states such as reaching the final goal, a game concluding, or falling of a cliff. In the end, our goal is to learn a policy 𝜋 or a mapping from states to actions.In an MDP, we assume that we can always tell which state 𝑠𝑡 our agents is in. However, this isn’t always the case. Sometimes, all we have access to are observations 𝑜𝑡 that provide information the state 𝑠𝑡st, but enough to precisely pinpoint the exact one',\n",
       "  '.In an MDP, we assume that we can always tell which state 𝑠𝑡 our agents is in. However, this isn’t always the case. Sometimes, all we have access to are observations 𝑜𝑡 that provide information the state 𝑠𝑡st, but enough to precisely pinpoint the exact one. We call such settings Partially Observable Markov Decision Processes (POMDPs). Imagine for example a Roomba being trained to navigate a living room with RL. From its infrared and mechanical “bump” sensors, it receives partial information (𝑜𝑡ot) as to where it might be, but not a definitive location (𝑠𝑡). Operating as a POMDP adds a whole layer of complexity to RL algorithms. For the rest of day though, we’ll focus on MDPs, as their much simpler and easier to use to teach basic concepts.A simple MDP exampleImage by authorIn the above example, we can see the 3 possible states for the agent as 𝑠0, 𝑠1, and 𝑠2, with 2 actions 𝑎0 and 𝑎1 available from each state. We can see that each action doesn’t lead to a deterministic transition to the next stage, as shown by multiple paths from each action. Note that each of the outcomes of action is labeled with a small black number between 0 and 1. This denotes the probability of that outcome (which state we end up at) given the action; as these are probabilities, the sum of the probabilities of arriving at each of the next states 𝑠𝑡+1 given a previous state 𝑠𝑡st and selected action 𝑎𝑡 is 1',\n",
       "  '. This denotes the probability of that outcome (which state we end up at) given the action; as these are probabilities, the sum of the probabilities of arriving at each of the next states 𝑠𝑡+1 given a previous state 𝑠𝑡st and selected action 𝑎𝑡 is 1.ObjectiveThe goal of the agent is to maximize the total reward 𝑅R it can receive over a number of steps. It is important to ensure the reward actually captures the true goal we want the agent to achieve. The agent will dutifully attempt to maximize the objective it is given, without any considerations to any implicit objectives that a human may desire. There are more than a few (amusing) anecdotes of RL agents learning undesirable behaviors by exploiting some aspect of the reward function. As such, defining this reward requires special care.One countermeasure commonly deployed by RL researchers is the concept of discounted rewards. This is done with a multiplicative term 𝛾: a reward 𝑇 step in the future is discounted as 𝛾𝑇𝑟𝑇. Using discounting encourages the agent to finish the task sooner rather than later, a common implicit criterion. With discounting then, the RL agent’s goal is to maximize:Image by authorThis is far from the complete solution to making our rewards accurately capture our desired objectives, but achieving higher rewards sooner rather than later is an almost universal preference, so we almost always add it. Designing a good reward function can be art is highly dependent on the task',\n",
       "  '. Designing a good reward function can be art is highly dependent on the task.Reinforcement Learning as Supervised Learning?At first, this doesn’t seem too different from the supervised methods we’ve looked at before, and some natural questions might arise:Why can’t we just treat RL as a supervised task? Why can’t we use the reward (or rather, the negative of the reward) as our supervised loss?Unlike in supervised learning, in reinforcement learning, we often don’t have a pre-apportioned dataset to learn from. In some problems set-ups, we may have examples of other agents (oftentimes humans) performing the desired task, but these aren’t necessarily optimal examples of how to maximize the reward, which is what we want to learn. In most RL settings, we don’t have any examples of state-action trajectories beyond what our agent experiences through trial-and-error, which are even more suboptimal.Open AI GymBefore we dive any deeper into implementing reinforcement learning models, first we need an environment. Remember, the goal is to learn an agent that can interact with an environment in the way we want, so we need something that our agent can interact with and receive rewards from. In robotics, this is often the real world (or some set-up in the real world). However, it is oftentimes cheaper and quicker to first test our algorithms in simulated settings',\n",
       "  \". In robotics, this is often the real world (or some set-up in the real world). However, it is oftentimes cheaper and quicker to first test our algorithms in simulated settings. There are a number of tasks that are popular benchmarks for the reinforcement learning community, such as cart pole, mountain car, or Atari 2600 games. In the spirit of accelerating progress and promoting openness in the research community, Open AI has very nicely coded up Open AI Gym, which has implementations of many of these environments for public use. We will be using these environments, as it allows us to focus on the algorithms themselves, instead of worrying implementing each problem setting ourselves.To use it, we first need to download and install it. Make sure you’re in your PyTorch environment first!# If you environment isn't currently active, activate it:# conda activate pytorchpip install gymOnce it’s installed, we can import it like any other Python module:import gymFrozenLake (a Grid World)Let’s start with a simple environment: FrozenLake. Here’s the official description from OpenAI gym:Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you’ll fall into the freezing water\",\n",
       "  '. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you’ll fall into the freezing water. At this time, there’s an international frisbee shortage, so it’s absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won’t always move in the direction you intend.A visualization of FrozenLake as a grid world:Image by authorAt the start of an episode, we begin in the upper left corner (S). Our goal is to move ourselves to the lower right corner (G), avoiding falling into the holes (H). Icy water is cold.In reinforcement learning , each of the 16 locations on the grid is a state, and action is attempting to move in one of four directions (left, down, right, up). Each move will result in the agent’s state changing from 𝑠𝑡 to 𝑠𝑡+1 as it changes location unless it attempts to move in the direction of a wall, which results in the agent’s state not changing (the agent doesn’t move). We receive a positive reward of “+1” for reaching the goal (G), discounted according to how long it took. While there is not a negative reward for falling into a hole (H), the agent still pays a penalty in the sense that falling into the hole is episode-ending and therefore prevents it from receiving any reward',\n",
       "  '. While there is not a negative reward for falling into a hole (H), the agent still pays a penalty in the sense that falling into the hole is episode-ending and therefore prevents it from receiving any reward. We want to learn a policy 𝜋 that takes us from our starting location (S) to the goal (G) in as few steps as possible.To really establish what we are trying to accomplish here, it’s worth debunking a few common initial misconceptions:Knowledge of the states and transition probabilities: From the top-down view, your first thought might be to plot out a path from the start to the finish, just as you would with a maze. However, this view is provided to us the algorithm designers so we can visualize the problem at hand. The agent learning the task does not get this prior knowledge; all we are to tell it is that there are going to be 16 states and 4 possible actions from each state. A more proper analogy would be if I blindfolded you and dropped you in the middle of a frozen lake, and told you your state (location) every time you decided to take a step in one of four directions, then set off fireworks when you stepped on the frisbee.Knowledge of the goal (reward): In OpenAI’s official description of the environment, you (the agent) know what you’re hoping to accomplish: You want to retrieve the frisbee while avoiding falling through the ice. The agent does not know this',\n",
       "  '.Knowledge of the goal (reward): In OpenAI’s official description of the environment, you (the agent) know what you’re hoping to accomplish: You want to retrieve the frisbee while avoiding falling through the ice. The agent does not know this. Rather, it learns the goal by experiencing rewards (or penalties), and the algorithm updates its policy such that it will be more (or less) likely to do those actions again. Note that this means that if an agent never experiences certain rewards, it won’t know they exist.Prior knowledge of pathfinding, physics, etc.: As a human, even if you haven’t solved this task before, you still bring a tremendous amount of prior knowledge to this problem. For example, you know the shortest path to a destination is a line. You know that North, South, East, and West, are directions and that going North and then South brings you back to where you already were. You know ice is slippery. You know icy water is cold. You know being in icy cold water is bad. It’s important to keep in mind that our agent will begin knowing none of these things; its initial policy is essentially picking actions completely at random. By the end of the training, it still won’t know what abstract concepts like “North/South,” “cold,” or “slippery” mean, but it will have (hopefully) learned a good policy that allows it to complete the goal',\n",
       "  \". By the end of the training, it still won’t know what abstract concepts like “North/South,” “cold,” or “slippery” mean, but it will have (hopefully) learned a good policy that allows it to complete the goal.Interacting with FrozenLakeThis example is simple enough that we could code the environment and its interface ourselves fairly easily, but OpenAI has already done it, and we’d like to focus on the algorithm of solving it as much as possible. We can create an instantiation of FrozenLake in a single line of code:In [ ]:env = gym.make('FrozenLake-v0')Open AI Gym environments provide a mechanism to observe the state of the environment, and since FrozenLake is an MDP (as opposed to POMDP), the observation is the state itself. For FrozenLake, there are 16 grid locations on the map, meaning we have 16 states. We can confirm this by looking at the size of the observation_space attribute for the environment we just created.In [ ]:env.observation_spaceOur agent will interact with this environment causing its state to change. For FrozenLake, we have 4 options, each corresponding to attempting to step in a particular direction: [Left, Down, Right, Up]. We can confirm this by looking at the size of action_space of our environment.In [ ]:env.action_spaceBefore interacting with the environment, we have to first reset it to initialize it. Resetting also returns an observation of the first state after reset\",\n",
       "  '. We can confirm this by looking at the size of action_space of our environment.In [ ]:env.action_spaceBefore interacting with the environment, we have to first reset it to initialize it. Resetting also returns an observation of the first state after reset. In FrozenLake, we always start in the upper left corner, which corresponds to state 0. As such, we see the reset() command returning 0.In [ ]:env.reset()We can visualize the FrozenLake environment by calling render(). In more complex tasks this will actually add frames to a video showing our agent\\'s progress, but for FrozenLake, it just prints out a text representation, with the highlighted character showing our agent\\'s current location. We can see that we started in the upper-left corner, on the \"S,\" as promised.In [ ]:env.render()Now, let’s try moving. One thing to keep in mind is that the original FrozenLake environment is “slippery.” Because of the ice, if you try to go in one direction, you end up with a 1/3 chance of going in the direction you meant and the two adjacent directions each. For example, if we try going right, we have equal probabilities of slipping and going up and down instead. This makes things a little more complicated, so for now, let’s first turn off the stochasticity and make this a deterministic transition instead. We do this by registering a new type of environment, and then instantiating a copy of the said environment, making sure to reset it first.In [ ]:# Non-slippery versionfrom gym.envs',\n",
       "  '. We do this by registering a new type of environment, and then instantiating a copy of the said environment, making sure to reset it first.In [ ]:# Non-slippery versionfrom gym.envs.registration import registerregister( id=\\'FrozenLakeNotSlippery-v0\\', entry_point=\\'gym.envs.toy_text:FrozenLakeEnv\\', kwargs={\\'map_name\\' : \\'4x4\\', \\'is_slippery\\': False},)env = gym.make(\\'FrozenLakeNotSlippery-v0\\')env.reset()We advance time in an OpenAI environment with the step() method, which takes as argument an action. Let\\'s trying moving right, which corresponds to the action 2. Notice that the output is a tuple of four elements: the next observation (object), the reward (float), whether or not the episode is done (boolean), and a dictionary of information (dict) that may be useful for debugging (this dict shouldn\\'t be used in the final algorithm itself).In [ ]:env.step(2)Next, let’s render() to visualize what happened. Observe that this particular environment prints out the action we took in parentheses up top, in this case \"(Right)\", and then shows the result of that action. Notice that while most of the time, we succeed in going in the direction we want to, occasionally we slip on the ice and go in a direction we didn\\'t intend.In [ ]:env.render()We can keep doing this as many times as we want. Since we’re in Jupyter, we can just keep running the same cell (making small edits to change our action).Notice that once we fall into a hole, the episode is over, and we can no longer do anything',\n",
       "  '.In [ ]:env.render()We can keep doing this as many times as we want. Since we’re in Jupyter, we can just keep running the same cell (making small edits to change our action).Notice that once we fall into a hole, the episode is over, and we can no longer do anything. The same is true after reaching the goal.In [ ]:env.step(0)env.render()Before we get into any RL, let’s see how random actions perform in this environment:In [ ]:env.reset()done = Falsewhile not done: env.render() action = env.action_space.sample() _, _, done, _ = env.step(action)Hm. Not great. Alright, so clearly picking random steps isn’t very likely to take us to the goal. It’s apparent just from looking at the map that there’re much better policies that we can learn. How are we going to do so?Q-learningThere are many algorithms that we can use, but let’s choose Q-learning, which we covered earlier today. Remember, in Q-learning (and SARSA, it turns out), we’re trying to learn the Q values for the states in our system.The Q value for a policy 𝜋 is a function of the state 𝑠s and action 𝑎a and is defined as the :Image by authorIntuitively, the Q value is the total reward (including discounting) that the agent will gain if it takes action 𝑎a from state 𝑠 and then follows policy 𝜋 for the rest of the episode. As one might expect, if Q is known exactly, the agent will attain the highest reward from 𝑠s if the policy 𝜋 is to pick the 𝑎a with the highest Q value',\n",
       "  '. As one might expect, if Q is known exactly, the agent will attain the highest reward from 𝑠s if the policy 𝜋 is to pick the 𝑎a with the highest Q value.Okay, so if we know the Q values for the system, then we can trivially find the optimal policy. So what are the Q values of the system? Well, in the beginning, we don’t know, but we can try to learn them through experience. This is where Q-learning comes in. Q-learning iteratively updates the Q values in the way:Image by authorNotice that Q-learning is an off-policy method, in the sense that you don’t actually learn from the trajectory you actually took (otherwise it’d be SARSA). Instead, we learn from the greedy transition, i.e. the best action we know how to take.And that’s it! We run our agent through many episodes, experiencing many 𝑠𝑡→𝑎𝑡→𝑠𝑡+1 transitions and rewards, and just like that, we eventually learn a good Q function (and thus a good policy). Now, of course, there are a bunch of small details and tweaks to make this work in practice, but we’ll get to those later.Q-learning in FrozenLakeFrozenLake is a very simple setting, one that we would call a toy problem. With only 16 states and 4 actions, there are only 64 state-action pairs possible (16x4=64), less if we account for the goal and the holes being episode ending (for simplicity though, we won’t). With these few state-action pairs, we can actually solve this problem tabularly',\n",
       "  '. With only 16 states and 4 actions, there are only 64 state-action pairs possible (16x4=64), less if we account for the goal and the holes being episode ending (for simplicity though, we won’t). With these few state-action pairs, we can actually solve this problem tabularly. Let’s set up a Q table, and initialize the Q-values for all state-action pairs to zeros. Note that while we could, we’re actually not going to need PyTorch in this example; PyTorch’s autograd and neural network libraries are unnecessary here, as we’re only going to be modifying a table of numbers. Instead, we’ll use a Numpy array to store the Q table.In [ ]:import numpy as np#Initialize table with all zeros to be uniformQ = np.zeros([env.observation_space.n, env.action_space.n])A few hyperparameters we’re going to set:alpha: learning rate for the Q functiongamma: discount rate for future rewardsnum_episodes: number of episodes (trajectories from start to goal/hole) our agent will learn fromWe’re also going to store our rewards in an array called rs.In [ ]:# Learning parametersalpha = 0.1gamma = 0.95num_episodes = 2000# array of reward for each episoders = np.zeros([num_episodes])Now for the bulk of the algorithm itself. Notice that we’re going to loop through the process num_episodes times, resetting the environment each time. At each step, we take the action with the highest Q value for our current state, with some randomness added in (especially at the beginning) to encourage exploration',\n",
       "  '. Notice that we’re going to loop through the process num_episodes times, resetting the environment each time. At each step, we take the action with the highest Q value for our current state, with some randomness added in (especially at the beginning) to encourage exploration. After each action, we update our Q table greedily based on the reward experienced and the next best action. We also make sure to update our state, rinse, and repeat. We continue taking actions in an episode until it is, storing the final total reward for the episode.In [ ]:for i in range(num_episodes): # Set total reward and time to zero, done to False r_sum_i = 0 t = 0 done = False #Reset environment and get first new observation s = env.reset() while not done: # Choose an action by greedily (with noise) from Q table a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i/10+1))) # Get new state and reward from environment s1, r, done, _ = env.step(a) # Update Q-Table with new knowledge Q[s,a] = (1 - alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:])) # Add reward to episode total r_sum_i += r*gamma**t # Update state and time s = s1 t += 1 rs[i] = r_sum_iHow did we do? Let’s take a look at the rewards that we saved. We can plot the reward versus the episode number, and hopefully, we’ll see some sort of an increase over time. RL performance can be extremely noisy, so let’s instead plot a moving average.In [ ]:## Plot reward vs episodesimport matplotlib',\n",
       "  '. We can plot the reward versus the episode number, and hopefully, we’ll see some sort of an increase over time. RL performance can be extremely noisy, so let’s instead plot a moving average.In [ ]:## Plot reward vs episodesimport matplotlib.pyplot as plt# Sliding window averager_cumsum = np.cumsum(np.insert(rs, 0, 0)) r_cumsum = (r_cumsum[50:] - r_cumsum[:-50]) / 50# Plotplt.plot(r_cumsum)plt.show()Pretty good. We might also be interested in how often our agent actually reached the goal. This won’t account for how quickly the agent got there (which might also be of interest), but let’s ignore that for now. To prevent us from being overwhelmed by data points, let’s bucket the values into 10 stages, printing out how many episodes of each stage resulted in finding the goal.In [ ]:# Print number of times the goal was reachedN = len(rs)//10num_Gs = np.zeros(10)for i in range(10): num_Gs[i] = np.sum(rs[i*N:(i+1)*N] > 0) print(\"Rewards: {0}\".format(num_Gs))Our RL agent does a really good job at navigating the FrozenLake when its moves are deterministic, but after all, this is supposed to be FrozenLake, so where’s the fun without the slipperiness? Let’s go back to the original environment and see how the agent does.In [ ]:env = gym.make(\\'FrozenLake-v0\\')#Initialize table with all zeros to be uniformQ = np.zeros([env.observation_space.n, env.action_space.n])# Learning parametersalpha = 0.1gamma = 0.95num_episodes = 2000# array of reward for each episoders = np',\n",
       "  '.In [ ]:env = gym.make(\\'FrozenLake-v0\\')#Initialize table with all zeros to be uniformQ = np.zeros([env.observation_space.n, env.action_space.n])# Learning parametersalpha = 0.1gamma = 0.95num_episodes = 2000# array of reward for each episoders = np.zeros([num_episodes])for i in range(num_episodes): # Set total reward and time to zero, done to False r_sum_i = 0 t = 0 done = False #Reset environment and get first new observation s = env.reset() while not done: # Choose an action by greedily (with noise) from Q table a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i/10+1))) # Get new state and reward from environment s1, r, done, _ = env.step(a) # Update Q-Table with new knowledge Q[s,a] = (1 - alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:])) # Add reward to episode total r_sum_i += r*gamma**t # Update state and time s = s1 t += 1 rs[i] = r_sum_i## Plot reward vs episodes# Sliding window averager_cumsum = np.cumsum(np.insert(rs, 0, 0)) r_cumsum = (r_cumsum[50:] - r_cumsum[:-50]) / 50# Plotplt.plot(r_cumsum)plt.show()# Print number of times the goal was reachedN = len(rs)//10num_Gs = np.zeros(10)for i in range(10): num_Gs[i] = np.sum(rs[i*N:(i+1)*N] > 0) print(\"Rewards: {0}\".format(num_Gs))Much harder. However, we can see that the model does eventually learn something.PyTorch in RLHey, not bad. However, while the previous example was fun and simple, it was noticeably lacking any hint of PyTorch',\n",
       "  '.sum(rs[i*N:(i+1)*N] > 0) print(\"Rewards: {0}\".format(num_Gs))Much harder. However, we can see that the model does eventually learn something.PyTorch in RLHey, not bad. However, while the previous example was fun and simple, it was noticeably lacking any hint of PyTorch.We could have used a PyTorch Tensor to store the Q table, but that\\'s not any better than using a NumPy array. PyTorch\\'s true utility comes from building neural networks and calculating/applying gradients automatically, which learning the Q table didn\\'t need.Continuous domainsIn our previous example, we mentioned that with only 16 discrete states and 4 actions/states, the Q table only needed to hold 64 values, which is very manageable. However, what if the state or action space is continuous? You could discretize it, but then you have to pick a resolution, and your state-action space could explode exponentially. Treating these binned states or actions as completely different states is also ignoring that two consecutive bins are likely very similar in the needed policy. You can learn these relationships, but doing so is horribly sample inefficient.Instead of learning a Q table then, perhaps a Q function would be more appropriate. This function would take in a state and action as an input and return a Q value as an output. The Q function may be very complex, but as we’ve learned over the past few days, neural networks are very flexible and good for approximating arbitrary functions',\n",
       "  \". This function would take in a state and action as an input and return a Q value as an output. The Q function may be very complex, but as we’ve learned over the past few days, neural networks are very flexible and good for approximating arbitrary functions. Deep Q Networks take such an approach.Cart PoleLet’s look at the cart pole problem next. In this setting, we have a pole attached to a hinge on a cart, with the goal being to keep the pole vertical as long as possible, without traveling too far along the rail. Because of gravity, the pole will fall unless the cart is exactly beneath the pole’s center of gravity. To prevent the pole from falling, the agent can apply a force of +1 or -1 to the cart to move it left and right along a track. The agent receives a reward of +1 for every timestamp the pole remains vertical; the game ends when the pole falls past 15 degrees from vertical or the cart moves more than 2.4 units away from the center. We’re going to somewhat arbitrarily call “success” achieving a reward of +200; alternatively, the agent needs to avoid the aforementioned failure conditions for 200 ticks.Image by authorFirst, let’s create an instance of the cart pole environment:In [ ]:env = gym.make('CartPole-v0')Again, we can look at the observation_space for this environment. Also similar to FrozenLake, since this version of the cart pole is an MDP (as opposed to POMDP), the observation is the state itself\",\n",
       "  \".make('CartPole-v0')Again, we can look at the observation_space for this environment. Also similar to FrozenLake, since this version of the cart pole is an MDP (as opposed to POMDP), the observation is the state itself. We can see that the states for cart pole have 4 dimensions, which correspond to [cart position, cart velocity, pole angle, pole angular velocity]. Importantly, notice these states are continuous values.In [ ]:env.observation_spaceWe can look at them action_space again too. In cart pole, there are two actions available to the agent: [apply force left, apply force right]. We can see this by examining the action_space attribute:In [ ]:env.action_spaceResetting the environment returns our first observations, which we can see has 4 values corresponding to the 4 previously mentioned state variables.In [ ]:env.reset()Before we get into any reinforcement learning, let’s see how we perform actions within the environment.In [ ]:done = Falsewhile not done: env.render() action = env.action_space.sample() _, _, done, _ = env.step(action)Okay, so clearly choosing a random action at every time step doesn’t really achieve our goal of keeping the pole vertical. We’re going to need something smarter.Let’s close that rendering window. We do this with close(). Note that gym renderings can be a little finicky, especially on Windows; either close() or restarting your Jupyter kernel may be necessary to close the rendered window.In [ ]:env\",\n",
       "  '. We’re going to need something smarter.Let’s close that rendering window. We do this with close(). Note that gym renderings can be a little finicky, especially on Windows; either close() or restarting your Jupyter kernel may be necessary to close the rendered window.In [ ]:env.close()Cart pole is actually a fairly simple problem (it’s very low dimensional), and so there are simpler ways to do this, but since we’ve been having so much fun with deep learning, let’s use a neural network. Specifically, let’s build a DQN that uses Q-learning to learn how to balance the pole. We’re going to give our DQN agent 1000 episodes to try and reach the goal of 200 ticks.There are a lot of small details that go into making these models work well, so instead of going through it piece by piece, the full code:In [ ]:# Based on: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/from collections import dequeimport randomimport mathimport gymimport numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fclass DQN(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(4, 24) self.fc2 = nn.Linear(24, 48) self.fc3 = nn.Linear(48, 2)def forward(self, x): x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.relu(x) x = self.fc3(x) return xclass DQNCartPoleSolver: def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0',\n",
       "  \".fc1(x) x = F.relu(x) x = self.fc2(x) x = F.relu(x) x = self.fc3(x) return xclass DQNCartPoleSolver: def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False): self.memory = deque(maxlen=100000) self.env = gym.make('CartPole-v0') if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True) self.gamma = gamma self.epsilon = epsilon self.epsilon_min = epsilon_min self.epsilon_decay = epsilon_log_decay self.alpha = alpha self.alpha_decay = alpha_decay self.n_episodes = n_episodes self.n_win_ticks = n_win_ticks self.batch_size = batch_size self.quiet = quiet if max_env_steps is not None: self.env._max_episode_steps = max_env_steps# Init model self.dqn = DQN() self.criterion = torch.nn.MSELoss() self.opt = torch.optim.Adam(self.dqn.parameters(), lr=0.01)def get_epsilon(self, t): return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))def preprocess_state(self, state): return torch.tensor(np.reshape(state, [1, 4]), dtype=torch.float32) def choose_action(self, state, epsilon): if (np.random.random() <= epsilon): return self.env.action_space.sample() else: with torch.no_grad(): return torch.argmax(self.dqn(state)).numpy()def remember(self, state, action, reward, next_state, done): reward = torch.tensor(reward) self.memory\",\n",
       "  '.float32) def choose_action(self, state, epsilon): if (np.random.random() <= epsilon): return self.env.action_space.sample() else: with torch.no_grad(): return torch.argmax(self.dqn(state)).numpy()def remember(self, state, action, reward, next_state, done): reward = torch.tensor(reward) self.memory.append((state, action, reward, next_state, done)) def replay(self, batch_size): y_batch, y_target_batch = [], [] minibatch = random.sample(self.memory, min(len(self.memory), batch_size)) for state, action, reward, next_state, done in minibatch: y = self.dqn(state) y_target = y.clone().detach() with torch.no_grad(): y_target[0][action] = reward if done else reward + self.gamma * torch.max(self.dqn(next_state)[0]) y_batch.append(y[0]) y_target_batch.append(y_target[0]) y_batch = torch.cat(y_batch) y_target_batch = torch.cat(y_target_batch) self.opt.zero_grad() loss = self.criterion(y_batch, y_target_batch) loss.backward() self.opt.step() if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decaydef run(self): scores = deque(maxlen=100)for e in range(self.n_episodes): state = self.preprocess_state(self.env.reset()) done = False i = 0 while not done: if e % 100 == 0 and not self.quiet: self.env.render() action = self.choose_action(state, self.get_epsilon(e)) next_state, reward, done, _ = self.env.step(action) next_state = self.preprocess_state(next_state) self.remember(state, action, reward, next_state, done) state = next_state i += 1scores.append(i) mean_score = np',\n",
       "  \".quiet: self.env.render() action = self.choose_action(state, self.get_epsilon(e)) next_state, reward, done, _ = self.env.step(action) next_state = self.preprocess_state(next_state) self.remember(state, action, reward, next_state, done) state = next_state i += 1scores.append(i) mean_score = np.mean(scores) if mean_score >= self.n_win_ticks and e >= 100: if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100)) return e - 100 if e % 100 == 0 and not self.quiet: print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))self.replay(self.batch_size) if not self.quiet: print('Did not solve after {} episodes 😞'.format(e)) return eif __name__ == '__main__': agent = DQNCartPoleSolver() agent.run() agent.env.close()Reinforcement learning can be kind of noisy. In some sense, it depends on your agent “lucking” into the right behavior so that it can learn from it, and occasionally one can get stuck in a bad rut. Even if your agent fails to “solve” the problem (i.e. reach 200 ticks), you should still see the mean survival time mostly climbing as the agent experiences more episodes. You may need to re-run learning a couple of times for the agent to reach 200 ticks.Once you have reached this sentence you have gone through all the steps for introduction to Reinforcement Learning (RL) in PyTorchHere is a summary of your accomplishment today:1. Supervised learning2. Reinforcement Learning3. Open AI Gym4\",\n",
       "  '.Once you have reached this sentence you have gone through all the steps for introduction to Reinforcement Learning (RL) in PyTorchHere is a summary of your accomplishment today:1. Supervised learning2. Reinforcement Learning3. Open AI Gym4. FrozenLake (a Grid World)5. PyTorch in RL6. Cart PoleMachine LearningReinforcement LearningPytorch----6Published in Analytics Vidhya76K ·Last published Sep 1, 2025Analytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comWritten by Harsh Panchal30 ·2 Python | Machine Learning | Data science enthusiast.',\n",
       "  'Reinforcement Learning: A Survey Abstract This paper surveys the field of reinforcement learning from a computer-science per- spective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning. Reinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last five to ten years, it has attracted rapidly increasing interest in the machine learning and artificial intelligence communities',\n",
       "  '. Reinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last five to ten years, it has attracted rapidly increasing interest in the machine learning and artificial intelligence communities. Its promise is beguiling—a way of programming agents by reward and punishment without needing to specify how the task is to be achieved. But there are formidable computational obstacles to fulfilling the promise. This paper surveys the historical basis of reinforcement learning and some of the current work from a computer science perspective. We give a high-level overview of the field and a taste of some specific approaches. It is, of course, impossible to mention all of the important work in the field; this should not be taken to be an exhaustive account. Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment. The work described here has a strong family resemblance to eponymous work in psychology, but differs considerably in the details and in the use of the word “reinforcement.” It is appropriately thought of as a class of problems, rather than as a set of techniques. There are two main strategies for solving reinforcement-learning problems. The first is to search in the space of behaviors in order to find one that performs well in the environment',\n",
       "  '.” It is appropriately thought of as a class of problems, rather than as a set of techniques. There are two main strategies for solving reinforcement-learning problems. The first is to search in the space of behaviors in order to find one that performs well in the environment. This approach has been taken by work in genetic algorithms and genetic programming, ©1996 AT Access Foundation and Morgan Kaufmann Publishers. All rights reserved. KAELBLING, LITTMAN, & Moore as well as some more novel search techniques . The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world. This paper is devoted almost entirely to the second set of techniques because they take advantage of the special structure of reinforcement-learning problems that is not available in optimization problems in general. It is not yet clear which set of approaches is best in which circumstances. The rest of this section is devoted to establishing notation and describing the basic reinforcement-learning model. Section 2 explains the trade-off between exploration and exploitation and presents some solutions to the most basic case of reinforcement-learning problems, in which we want to maximize the immediate reward. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them',\n",
       "  '. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them. Section 4 considers some classic model-free algorithms for reinforcement learning from delayed reward: adaptive heuristic critic, TD(A) and Q-learning. Section 5 demonstrates a continuum of algorithms that are sensitive to the amount of computation an agent can perform between actual steps of action in the environment. Generalization—the cornerstone of mainstream machine learning research—has the potential of considerably aiding reinforcement learning, as described in Section 1.1 Reinforcement-Learning Model In the standard reinforcement-learning model, an agent is connected to its environment via perception and action, as depicted in Figure 238 REINFORCEMENT LEARNING: A SURVEY Formally, the model consists of e a discrete set of environment states, S; e adiscrete set of agent actions, A; and e aset of scalar reinforcement signals; typically {0,1}, or the real numbers. The figure also includes an input function J, which determines how the agent views the environment state; we will assume that it is the identity function (that is, the agent perceives the exact state of the environment) until we consider partial observability in Section An intuitive way to understand the relation between the agent and its environment is with the following example dialogue',\n",
       "  \". Environment: You are in state Agent: T'll take action Environment: You received a reinforcement of 7 units. You are now in state Agent: T'll take action Environment: You received a reinforcement of -4 units. You are now in state Agent: T'll take action The agent’s job is to find a policy 7, mapping states to actions, that maximizes some long-run measure of reinforcement. We expect, in general, that the environment will be non-deterministic; that is, that taking the same action in the same state on two different occasions may result in different next states and/or different reinforcement values. This happens in our example above: from state 65, applying action 2 produces differing rein- forcements and differing states on two occasions. However, we assume the environment is stationary; that is, that the probabilities of making state transitions or receiving specific reinforcement signals do not change over time.! Reinforcement learning differs from the more widely studied problem of supervised learn- ing in several ways. The most important difference is that there is no presentation of in- put/output pairs. Instead, after choosing an action the agent is told the immediate reward and the subsequent state, but is not told which action would have been in its best long-term interests. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally\",\n",
       "  '. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally. Another difference from supervised learning is that on-line performance is important: the evaluation of the system is often concurrent with learning. 239 KAELBLING, LITTMAN, & Moore Some aspects of reinforcement learning are closely related to search and planning issues in artificial intelligence. AI search algorithms generate a satisfactory trajectory through a graph of states. Planning operates in a similar manner, but typically within a construct with more complexity than a graph, in which states are represented by compositions of logical expressions instead of atomic symbols. These A] algorithms are less general than the reinforcement-learning methods, in that they require a predefined model of state transitions, and with a few exceptions assume determinism. On the other hand, reinforcement learning, at least in the kind of discrete cases for which theory has been developed, assumes that the entire state space can be enumerated and stored in memory—an assumption to which conventional search algorithms are not tied. 1.2 Models of Optimal Behavior Before we can start thinking about algorithms for learning to behave optimally, we have to decide what our model of optimality will be. In particular, we have to specify how the agent should take the future into account in the decisions it makes about how to behave now',\n",
       "  '.2 Models of Optimal Behavior Before we can start thinking about algorithms for learning to behave optimally, we have to decide what our model of optimality will be. In particular, we have to specify how the agent should take the future into account in the decisions it makes about how to behave now. There are three models that have been the subject of the majority of work in this area. The finite-horizon model is the easiest to think about; at a given moment in time, the agent should optimize its expected reward for the next h steps: h EQ ri) : t=0 it need not worry about what will happen after that. In this and subsequent expressions, r, represents the scalar reward received t steps into the future. This model can be used in two ways. In the first, the agent will have a non-stationary policy; that is, one that changes over time. On its first step it will take what is termed a h-step optimal action. This is defined to be the best action available given that it has h steps remaining in which to act and gain reinforcement. On the next step it will take a (h — 1)-step optimal action, and so on, until it finally takes a 1-step optimal action and terminates. In the second, the agent does receding-horizon control, in which it always takes the h-step optimal action. The agent always acts according to the same policy, but the value of h limits how far ahead it looks in choosing its actions. The finite-horizon model is not always appropriate',\n",
       "  '. In the second, the agent does receding-horizon control, in which it always takes the h-step optimal action. The agent always acts according to the same policy, but the value of h limits how far ahead it looks in choosing its actions. The finite-horizon model is not always appropriate. In many cases we may not know the precise length of the agent’s life in advance. The infinite-horizon discounted model takes the long-run reward of the agent into ac- count, but rewards that are received in the future are geometrically discounted according to discount factor +, (where 0 < 7 < 1): oo BD yr t=0 We can interpret y in several ways. It can be seen as an interest rate, a probability of living another step, or as a mathematical trick to bound the infinite sum. The model is conceptu- ally similar to receding-horizon control, but the discounted model is more mathematically tractable than the finite-horizon model. This is a dominant reason for the wide attention this model has received. 240 REINFORCEMENT LEARNING: A SURVEY Another optimality criterion is the average-reward model, in which the agent is supposed to take actions that optimize its long-run average reward: h lim BEY n) . hoo +=0 Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1',\n",
       "  '. hoo +=0 Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1 . One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount of reward in the initial phases and the other of whic does not. Reward gained on any initial prefix of the agent’s life is overshadowed by the long-run average performance. It is possible to generalize this model so that it takes into account both the long run average and the amount of initial reward than can be gained. In the generalized, bias optimal model, a policy is preferred if it maximizes the long-run average and ties are broken by the initial extra reward. Figure 2 contrasts these models of optimality by providing an environment in whic changing the model of optimality changes the optimal policy. In this example, circles represent the states of the environment and arrows are state transitions. There is only a single action choice from every state except the start state, which is in the upper left and marked with an incoming arrow. All rewards are zero except where marked. Under a finite-horizon model with h = 5, the three actions yield rewards of +6.0, +0.0, and +0.0, so the first action should be chosen; under an infinite-horizon discounted model with 7 = 0.9, the three choices yield +16.2, +59.0, and +58',\n",
       "  '. All rewards are zero except where marked. Under a finite-horizon model with h = 5, the three actions yield rewards of +6.0, +0.0, and +0.0, so the first action should be chosen; under an infinite-horizon discounted model with 7 = 0.9, the three choices yield +16.2, +59.0, and +58.5 so the second action should be chosen; and under the average reward model, the third action should be chosen since it leads to an average reward of + carefully in any application. The finite-horizon model is appropriate when the agent’s lifetime is known; one im- portant aspect of this model is that as the length of the remaining lifetime decreases, the agent’s policy may change. A system with a hard deadline would be appropriately modeled this way. The relative usefulness of infinite-horizon discounted and bias-optimal models is still under debate. Bias-optimality has the advantage of not requiring a discount parameter; however, algorithms for finding bias-optimal policies are not yet as well-understood as those for finding optimal infinite-horizon discounted policies. 1.38 Measuring Learning Performance The criteria given in the previous section can be used to assess the policies learned by a given algorithm. We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use. e Eventual convergence to optimal. Many algorithms come with a provable guar- antee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992)',\n",
       "  '. We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use. e Eventual convergence to optimal. Many algorithms come with a provable guar- antee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992). This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau 241 KAELBLING, LITTMAN, & Moore +2 Finite horizon, h=4 +10 Infinite horizon, y=0.9 OOO0-0-00\" Average reward at 99% of optimality may, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate. e Speed of convergence to optimality. Optimality is usually an asymptotic result, and so convergence speed is an ill-defined measure. More practical is the speed of convergence to near-optimality. This measure begs the definition of how near to optimality is sufficient. A related measure is level of performance after a given time, which similarly requires that someone define the given time. It should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accu- racy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework , there is a learning period during which mistakes do not count, then a performance period during which they do',\n",
       "  '. In the latter, expected future predictive accu- racy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework , there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance. That is usually an inappropriate view for an agent with a long existence in a complex environment. In spite of the mismatch between embedded reinforcement learning and the train/test perspective, Fiechter (1994) provides a PAC analysis for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views. Measures related to speed of learning have an additional weakness. An algorithm that merely tries to achieve optimality as fast as possible may incur unnecessarily large penalties during the learning period. A less aggressive strategy taking longer to achieve optimality, but gaining greater total reinforcement during its learning might be preferable. e Regret. A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret (Berry & Fristedt, 1985). It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain',\n",
       "  '. This measure is known as regret (Berry & Fristedt, 1985). It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain. 242 REINFORCEMENT LEARNING: A SURVEY 1.4 Reinforcement Learning and Adaptive Control Adaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algo- rithms for improving a sequence of decisions from experience. Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and ac- tions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory. A very common formulation of cost functions in adaptive control are quadratic penalties on deviation from desired state and action vectors. Most importantly, although the dynamic model of the system is not known in advance, and must be esti- mated from data, the structure of the dynamic model is fixed, leaving model estimation as a parameter estimation problem. These assumptions permit deep, elegant and powerful mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive control algorithms. One major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the roblems of exploration, we treat a very simple case in this section',\n",
       "  '. One major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the roblems of exploration, we treat a very simple case in this section. The fundamental issues and approaches described here will, in many cases, transfer to the more complex instances of reinforcement learning discussed later in the paper. The simplest possible reinforcement-learning problem is known as the k-armed bandit problem, which has been the subject of a great deal of study in the statistics and applied mathematics literature (Berry & Fristedt, 1985). The agent is in a room with a collection of k gambling machines (each called a “one-armed bandit” in colloquial English). The agent is ermitted a fixed number of pulls, h. Any arm may be pulled on each turn. The machines do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal machine. When arm ? is pulled, machine 7 pays off 1 or 0, according to some underlying probability parameter p;, where payoffs are independent events and the p;s are unknown. What should the agent’s strategy be? This problem illustrates the fundamental tradeoff between exploitation and exploration',\n",
       "  '. When arm ? is pulled, machine 7 pays off 1 or 0, according to some underlying probability parameter p;, where payoffs are independent events and the p;s are unknown. What should the agent’s strategy be? This problem illustrates the fundamental tradeoff between exploitation and exploration. The agent might believe that a particular arm has a fairly high payoff probability; should it choose that arm all the time, or should it choose another one that it has less information about, but seems to be worse? Answers to these questions depend on how long the agent is expected to play the game; the longer the game lasts, the worse the consequences of prematurely converging on a sub-optimal arm, and the more the agent should explore. a There is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt (1985). We use the term “action” to indicate the agent’s choice of arm to pull. This eases the transition into delayed reinforcement models in Section Section 2.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case. 243 KAELBLING, LITTMAN, & Moore Section 2',\n",
       "  '.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case. 243 KAELBLING, LITTMAN, & Moore Section 2.2 presents three techniques that are not formally justified, but that have had wide use in practice, and can be applied (with similar lack of guarantee) to the general case. 2.1 Formally Justified Techniques There is a fairly well-developed formal theory of exploration for very simple problems. Although it is instructive, the methods it provides do not scale well to more complex problems. 2.1.1 DYNAMIC-PROGRAMMING APPROACH If the agent is going to be acting for a total of A steps, it can use basic Bayesian reasoning to solve for an optimal strategy (Berry & Fristedt, 1985). This requires an assumed prior joint distribution for the parameters {p;}, the most natural of which is that each p; is independently uniformly distributed between 0 and can be represented as a tabulation of action choices and payoffs: {n1, wi, na, W2,..., Mk, We} denotes a state of play in which each arm 7 has been pulled n; times with w; payoffs. We write V*(n1, wi,-.-,2k, WE) as the expected payoff remaining, given that a total of h pulls are available, and we use the remaining pulls optimally. If 0; nj; = h, then there are no remaining pulls, and V*(nj, wy,..., nz, we) = Ve (ny.wp..-',\n",
       "  '. We write V*(n1, wi,-.-,2k, WE) as the expected payoff remaining, given that a total of h pulls are available, and we use the remaining pulls optimally. If 0; nj; = h, then there are no remaining pulls, and V*(nj, wy,..., nz, we) = Ve (ny.wp..-.,npewp) = max; B Future payoff if agent takes action a, then acts optimally for remaining pulls = max; piV™ (ny, W;,---,2i +1, wie+1,---, MK. We)+ (1 = pi) V*(m1, Wi, ee ME 1, Wi, +. Me, WE) where p; is the posterior subjective probability of action 7 paying off given n;, w; and our prior probability. For the uniform priors, which result in a beta distribution, p; = The expense of filling in the table of V* values in this way for all attainable belief states is linear in the number of belief states times actions, and thus exponential in the horizon. 2.1.2 GITTINS ALLOCATION INDICES Gittins gives an “allocation index” method for finding the optimal choice of action at each step in k-armed bandit problems . The technique only applies under the discounted expected reward criterion. For each action, consider the number of times it has been chosen, n, versus the number of times it has paid off, w. For certain discount factors, there are published tables of “index values,” I(n,w) for each pair of n and w. Look up the index value for each action 7, I(nj,w;)',\n",
       "  '. For each action, consider the number of times it has been chosen, n, versus the number of times it has paid off, w. For certain discount factors, there are published tables of “index values,” I(n,w) for each pair of n and w. Look up the index value for each action 7, I(nj,w;). It represents a comparative measure of the combined value of the expected payoff of action i (given its history of payoffs) and the value of the information that we would get by choosing it. Gittins has shown that choosing the action with the largest index value guarantees the optimal balance between exploration and exploitation. 244 REINFORCEMENT LEARNING: A SURVEY a=0 a=1 KL0+—O+—0 +++ O40 O90 + 0 0-00 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=1 a=0 a=1 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N41 r=0 Because of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward (Salganicoff & Ungar, 1995). Unfortunately, no one has yet been able to find an analog of index values for delayed reinforcement problems. 2.1.3 LEARNING AUTOMATA A branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar (1989), which were originally described explicitly as finite state automata',\n",
       "  '. 2.1.3 LEARNING AUTOMATA A branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar (1989), which were originally described explicitly as finite state automata. The Tsetlin automaton shown in Figure 3 provides an example that solves a 2-armed bandit arbitrarily near optimally as N approaches infinity. It is inconvenient to describe algorithms as finite-state automata, so a move was made to describe the internal state of the agent as a probability distribution according to which actions would be chosen. The probabilities of taking different actions would be adjusted according to their previous successes and failures. An example, which stands among a set of algorithms independently developed in the mathematical psychology literature (Hilgard & Bower, 1975), is the linear reward-inaction algorithm. Let p; be the agent’s probability of taking action e When action a; succeeds, Di t= pita(l—p) Pj (= py— ap; for 7 #2 e When action a; fails, p; remains unchanged (for all j). This algorithm converges with probability 1 to a vector containing a single 1 and the rest 0’s (choosing a particular action with probability 1). Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making a small (Narendra & Thathachar, 1974). There is no literature on the regret of this algorithm. 245 KAELBLING, LITTMAN, & Moore 2',\n",
       "  '. Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making a small (Narendra & Thathachar, 1974). There is no literature on the regret of this algorithm. 245 KAELBLING, LITTMAN, & Moore 2.2 Ad-Hoc Techniques In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun (1992) has surveyed a variety of these techniques. 2.2.1 GREEDY STRATEGIES The first strategy that comes to mind is to always choose the action with the highest esti- mated payoff. The flaw is that early unlucky sampling might indicate that the best action’s reward is less than the reward obtained from a suboptimal action. The suboptimal action will always be picked, leaving the true optimal action starved of data and its superiority never discovered. An agent must explore to ameliorate this outcome. A useful heuristic is optimism in the face of uncertainty in which actions are selected greedily, but strongly optimistic prior beliefs are put on their payoffs so that strong negative evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrar- ily small',\n",
       "  '. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrar- ily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the ez- ploration bonus in Dyna , curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993). 2.2.2 RANDOMIZED STRATEGIES Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random. Some versions of this strategy start with a large value of p to encourage initial exploration, which is slowly decreased. An objection to the simple strategy is that when it experiments with a non-greedy action it is no more likely to try a promising alternative than a clearly hopeless alternative. A slightly more sophisticated strategy is Boltzmann exploration. In this case, the expected reward for taking action a, E.R(a) is used to choose an action probabilistically according to the distribution (ER(a)/T PO Sea PROT The temperature parameter T can be decreased over time to decrease exploration. This method works well if the best action is well separated from the others, but suffers somewhat when the values of the actions are close',\n",
       "  '. This method works well if the best action is well separated from the others, but suffers somewhat when the values of the actions are close. It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care. 2.2.3 INTERVAL-BASED TECHNIQUES Exploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling’s interval estimation algorithm (1993b) stores statistics for each action a;: w; is the number of successes and n; the number of trials. An action is chosen by computing the upper bound of a 100-(1—a)% 246 REINFORCEMENT LEARNING: A SURVEY confidence interval on the success probability of each action and choosing the action with the highest upper bound. Smaller values of the a parameter encourage greater exploration. When payoffs are boolean, the normal approximation to the binomial distribution can be used to construct the confidence interval (though the binomial should be used for small n). Other payoff distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods (Box & Draper, 1987), which are used for comparing multiple treatments (for example, fertilizers or drugs) to determine which treatment (if any) is best in as small a set of experiments as possible. 2',\n",
       "  '. 2.3 More General Problems When there are multiple states, but reinforcement is still immediate, then any of the above solutions can be replicated, once for each state. However, when generalization is required, these solutions must be integrated with generalization methods (see section 6); this is straightforward for the simple ad-hoc methods, but it is not understood how to maintain theoretical guarantees. Many of these techniques focus on converging to some regime in which exploratory actions are taken rarely or never; this is appropriate when the environment is stationary. However, when the environment is non-stationary, exploration must continue to take place, in order to notice changes in the world. Again, the more ad-hoc techniques can be modified to deal with this in a plausible manner (keep temperature parameters from going to 0; decay the statistics in interval estimation), but none of the theoretically guaranteed methods can be applied. In the general case of the reinforcement learning problem, the agent’s actions determine not only its immediate reward, but also (at least probabilistically) the next state of the environment. Such environments can be thought of as networks of bandit problems, but the agent must take into account the next state as well as the immediate reward when it decides which action to take. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account',\n",
       "  '. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account. The agent will have to be able to learn from delayed reinforcement: it may take a long sequence of actions, receiving insignificant reinforcement, then finally arrive at a state with high reinforcement. The agent must be able to learn which of its actions are desirable based on reward that can take place arbitrarily far in the future. 3.1 Markov Decision Processes Problems with delayed reinforcement are well modeled as Markov decision processes (MDPs). An MDP consists of e@ aset of states S, e a set of actions A, 247 KAELBLING, LITTMAN, & Moore e a reward function R:S x A> ®R, and e astate transition function T : S x A — TI(S), where a member of II(S) is a probability istribution over the set S (i.e. it maps states to probabilities). We write T(s, a, s’) for the probability of making a transition from state s to state s’ using action a. The state transition function probabilistically specifies the next state of the environment as a function of its current state and the agent’s action. The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994)',\n",
       "  '. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994). Although general MDPs may have infinite (even uncountable) state and action spaces, we will only discuss methods for solving finite-state and finite-action problems. In section 6, we discuss methods for solving problems with continuous input and output spaces. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in MDP environments, we will ex- plore techniques for determining the optimal policy given a correct model. These dynamic programming techniques will serve as the foundation and inspiration for the learning al- gorithms to follow. We restrict our attention mainly to finding optimal policies for the infinite-horizon discounted model, but most of these algorithms have analogs for the finite- horizon and average-case models as well. We rely on the result that, for the infinite-horizon discounted model, there exists an optimal deterministic stationary policy . We will speak of the optimal value of a state—it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy',\n",
       "  '. We will speak of the optimal value of a state—it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy. Using 7 as a complete decision policy, it is written V*(s)= max E (>: on) t=0 This optimal value function is unique and can be defined as the solution to the simultaneous equations sles V*(s) = max (n a+7>> renner) WseS, (1) which assert that the value of a state s is the expected instantaneous reward plus the expected discounted value of the next state, using the best available action. Given the optimal value function, we can specify the optimal policy as x*(s) = argmax | R(s,a) +7 Ss T(s,a,8\\')V*(s\") “ ES 3.2.1 VALUE ITERATION One way, then, to find an optimal policy is to find the optimal value function. It can be determined by a simple iterative algorithm called value iteration that can be shown to converge to the correct V* values (Bellman, 1957; Bertsekas, 1987). 248 REINFORCEMENT LEARNING: A SURVEY initialize V(s) arbitrarily loop until policy good enough loop for s€S loop for aE A Q(s,4) = R(s,a) +7 Dyes Ts, 4, 8)V (6% V(s) := max, Q(s, a) end loop end loop It is not obvious when to stop the value iteration algorithm. One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function (Williams & Baird, 1993b)',\n",
       "  '. One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function (Williams & Baird, 1993b). It says that if the maximum difference between two successive value functions is less than ¢, then the value of the greedy policy, he policy obtained by choosing, in every state, the action that maximizes the estimated discounted reward, using the current estimate of the value function) differs from the value function of the optimal policy by no more than 2ey/(1— 7) at any state. This provides an effective stopping criterion for the algorithm. Puterman (1994) discusses another stopping criterion, based on the span semi-norm, which may result in earlier termination. Another mportant result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged . And in practice, the greedy policy is often optimal long before the value function has converged. = Value iteration is very flexible. The assignments to V need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value of every state gets updated infinitely often on an infinite run. These issues are treated extensively by Bertsekas (1989), who also proves convergence results. Updates based on Equation 1 are known as full backups since they make use of infor- mation from all possible successor states',\n",
       "  '. These issues are treated extensively by Bertsekas (1989), who also proves convergence results. Updates based on Equation 1 are known as full backups since they make use of infor- mation from all possible successor states. It can be shown that updates of the form Qs, a) = Qls,a) +a(r +7 maxQ(s!,a) — Q(s,a)) can also be used as long as each pairing of a and s is updated infinitely often, s’ is sampled from the distribution T(s, a, s’), r is sampled with mean R(s,a) and bounded variance, and the learning rate a is decreased slowly. This type of sample backup is critical to the operation of the model-free methods discussed in the next section. The computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions. Com- monly, the transition probabilities T(s, a, s’) are sparse. If there are on average a constant number of next states with non-zero probability then the cost per iteration is linear in the number of states and linear in the number of actions. The number of iterations required to reach the optimal value function is polynomial in the number of states and the magnitude of the largest reward if the discount factor is held constant. However, in the worst case the number of iterations grows polynomially in 1/(1— y), so the convergence rate slows considerably as the discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b). 249 KAELBLING, LITTMAN, & Moore 3.2',\n",
       "  \". However, in the worst case the number of iterations grows polynomially in 1/(1— y), so the convergence rate slows considerably as the discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b). 249 KAELBLING, LITTMAN, & Moore 3.2.2 Poticy ITERATION The policy iteration algorithm manipulates the policy directly, rather than finding it indi- rectly via the optimal value function. It operates as follows: choose an arbitrary policy 7’ loop wisn compute the value function of policy 7: solve the linear equations V,(s) = R(s,7(s)) + ¥ Nores T(s, 7(8), 8)Vi(s’) improve the policy at each state: n'(s) := argmax, (R(s, a) + 7 Dees T(s, a, 8’) Vz(s')) until t= 7’ The value function of a policy is just the expected infinite discounted reward that will be gained, at each state, by executing that policy. It can be determined by solving a set of linear equations. Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. If it can, we change the policy to take the new action whenever it is in that situation. This step is guaranteed to strictly improve the performance of the policy. When no improvements are possible, then the policy is guaranteed to be optimal. Since there are at most |A||s| distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations (Puter- man, 1994)\",\n",
       "  '. When no improvements are possible, then the policy is guaranteed to be optimal. Since there are at most |A||s| distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations (Puter- man, 1994). However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP (Littman et al., 1995b). 3.2.3 ENHANCEMENT TO VALUE ITERATION AND POLicy ITERATION In practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems. Puterman’s modified policy iteration algorithm (Puterman & Shin, 1978) provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of V,. Instead of finding an exact value for V,, we can perform a few steps of a modified value-iteration step where the policy is held fixed over successive iterations. This can be shown to produce an approximation to V, that converges linearly in Several standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration',\n",
       "  '. This can be shown to produce an approximation to V, that converges linearly in Several standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution . State aggre- gation works by collapsing groups of states to a single meta-state solving the abstracted problem (Bertsekas & Castafion, 1989). 250 REINFORCEMENT LEARNING: A SURVEY 3.2.4 COMPUTATIONAL COMPLEXITY Value iteration works by producing successive approximations of the optimal value function. Each iteration can be performed in O(|A||S|?) steps, or faster if there is sparsity in the ransition function. However, the number of iterations required can grow exponentially in he discount factor ; as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of O(|A]|.$|?+|S|?) can be prohibitive. There is no known tight worst-case bound available or policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some ractictioners',\n",
       "  '.$|?+|S|?) can be prohibitive. There is no known tight worst-case bound available or policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some ractictioners . Linear programming is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D’Epenoux, 1963; Hoffman & Karp, 1966). An advantage of this approach is that commercial-quality inear-programming packages are available, although the time and space requirements can still be quite high. From a theoretic perspective, linear programming is the only known algorithm that can solve MDPs in polynomial time, although the theoretically efficient algorithms have not been shown to be efficient in practice. In the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model. The model consists of knowledge of the state tran- sition probability function T(s, a, s’) and the reinforcement function R(s,a). Reinforcement learning is primarily concerned with how to obtain the optimal policy when such a model is not known in advance. The agent must interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy. At this point, there are two ways to proceed. e Model-free: Learn a controller without learning a model',\n",
       "  '. The agent must interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy. At this point, there are two ways to proceed. e Model-free: Learn a controller without learning a model. e Model-based: Learn a model, and use it to derive a controller. Which approach is better? This is a matter of some debate in the reinforcement-learning community. A number of algorithms have been proposed on both sides. This question also appears in other fields, such as adaptive control, where the dichotomy is between direct and indirect adaptive control. This section examines model-free learning, and Section 5 examines model-based meth- ods. The biggest problem facing a reinforcement-learning agent is temporal credit assignment. How do we know whether the action just taken is a good one, when it might have far- reaching effects? One strategy is to wait until the “end” and reward the actions taken if the result was good and punish them if the result was bad. In ongoing tasks, it is difficult to know what the “end” is, and this might require a great deal of memory. Instead, we will use insights from value iteration to adjust the estimated value of a state based on 251 KAELBLING, LITTM. — 4 AN, & Moore T’4 the immediate reward and the estimated value is known as temporal difference methods (Sutt: temporal-difference learning strategies for the d 4',\n",
       "  '. Instead, we will use insights from value iteration to adjust the estimated value of a state based on 251 KAELBLING, LITTM. — 4 AN, & Moore T’4 the immediate reward and the estimated value is known as temporal difference methods (Sutt: temporal-difference learning strategies for the d 4.1 Adaptive Heuristic Critic and TD(\\\\) The adaptive heuristic critic algorithm is an ada Sutton, & Anderson, 1983) in which the value mented by solving a set of linear equations, but TD(0). A block nents: a critic (la reinforcement-lear rithms, modified acting to maximiz v, that is computed by the critic. The critic us learn to map states to their expected discounted is the one currently instantia We can see the analogy wi o deal with multiple states a: value function V, i new policy 7’ tha however, both components op: can be guaranteed to converge and Baird explored the convergence properties for that po maximizes the new value fun o the optimal pol call “incremental variants of iagram for this approach is gi beled AHC), and a reinforcemen ning component can be an instance of any of the k-armed bandit algo- e instantaneous reward, it will ed in the RL comp h modified policy i working in alternation. The policy 7 implemented by RL is fixed and t cy. Now we fix the critic and let the RL com olicy iteration” (Williams & Baird, 1993a). he adaptive heuristic critic. of the next state. This class of algorithms ‘on, 1988). We will consider two different iscounted infinite-horizon model',\n",
       "  '. The policy 7 implemented by RL is fixed and t cy. Now we fix the critic and let the RL com olicy iteration” (Williams & Baird, 1993a). he adaptive heuristic critic. of the next state. This class of algorithms ‘on, 1988). We will consider two different iscounted infinite-horizon model. tive version of policy iteration (Barto, -function computation is no longer imple- is instead computed by an algorithm called iven in Figure be acting to maximize es val being executed eration if we imagine these components e critic learns the onent learn a plementations, ction, and so on. In most im erate simultaneously. Only the alternating implementation icy, under appropriate conditions. Williams of a class of AHC-related algorithms they It remains to explain how the critic can learn the value of a policy. We define (s, a, r,s’) to be an experience tuple summarizing a single t ransition in the environment. Here s is the agent’s state before the transition, a is its choice of action, r the instantaneous reward it receives, and s’ its resulting state. The value o algorithm which uses the update V(s): a policy is learned using Sutton’s TD(0) rule V(s) Whenever a state s is visited, its estimated val since r is the instantaneous reward received and occurring next state',\n",
       "  \". The value o algorithm which uses the update V(s): a policy is learned using Sutton’s TD(0) rule V(s) Whenever a state s is visited, its estimated val since r is the instantaneous reward received and occurring next state. This is analogous to the sa: VV (s')—V(s)) ue is updated to be closer to r+ yV(s‘), V(s’) is the estimated value of the actually mple-backup rule from value iteration—the only difference is that the sample is drawn from the real world rather than by simulating a known model. The key idea is that r + yV(s’ 252 is a sample of the value of V(s), and it is REINFORCEMENT LEARNING: A SURVEY more likely to be correct because it incorporates the real r. If the learning rate a is adjusted properly (it must be slowly decreased) and the policy is held fixed, TD(0) is guaranteed to converge to the optimal value function. The TD(0) rule as presented above is really an instance of a more general class of algorithms called TD(X), with 4 = V(u) = V(u) ta(rt+yV(s') —V(s)je(u) , but it is applied to every state according to its eligibility e(u), rather than just to the immediately previous state, s. One version of the eligibility trace is defined to be t : _ lifs=s e(s) = Say! *S sq , where 35,5, = { 0 otherwise k=1 The eligibility of a state s is the degree to which it has been visited in the recent past; when a reinforcement is received, it is used to update all the states that have been recently visited, according to their eligibility\",\n",
       "  '. When \\\\ = 0 this is equivalent to TD(0). When \\\\ = 1, it is roughly equivalent to updating all the states according to the number of times they were visited by the end of a run. Note that we can update the eligibility online as follows: e(s) i= yAe(s)+1 if s= current state yAe(s) otherwise It is computationally more expensive to execute the general TD(X), though it often converges considerably faster for large \\\\ (Dayan, 1992; Dayan & Sejnowski, 1994). There has been some recent work on making the updates more efficient (Cichosz & Mulawka, 1995) and on changing the definition to make T D(A) more consistent with the certainty-equivalent method (Singh & Sutton, 1996), which is discussed in Section 5.4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins’ Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation. Let Q*(s,a) be the expected discounted reinforcement of taking action ain state s, then continuing by choosing actions optimally. Note that V*(s) is the value of s assuming the best action is taken initially, and so V*(s) = max, Q*(s, a). Q*(s, a) can hence be written recursively as Q*(s,a) = R(s,a) +7 Ss T(s,a, 8’) max Q*(s’, a’). ES e Note also that, since V*(s) = max, Q*(s,a), we have x*(s) = argmax, Q*(s,a) as an optimal policy',\n",
       "  \". Q*(s, a) can hence be written recursively as Q*(s,a) = R(s,a) +7 Ss T(s,a, 8’) max Q*(s’, a’). ES e Note also that, since V*(s) = max, Q*(s,a), we have x*(s) = argmax, Q*(s,a) as an optimal policy. Because the Q function makes the action explicit, we can estimate the Q values on- line using a method essentially the same as TD(0), but also use them to define the policy, 253 KAELBLING, LITTMAN, & Moore because an action can be chosen just by taking the one wit current state. The Q-learning rule is Qs.) = Qls,a) + alr +7 maxQ(s'a! where (s,a,r,s’) is an experie each state an infinite number of Q values will converge with probability 1 to Q* (Watkins, Jordan, & Singh, 1994). Q-le more than one step previously, When the Q values are ne the agent to act greedily, the maximum Q value for the — Q(s.4)) nce tuple as described earlier. If each action is executed in a is decayed appropriately, the 989; Tsitsiklis, 1994; Jaakkola, arning can also be extended to update states that occurred . as in TD(A) (Peng & Williams, 1994). y converged to their optimal values, it is appropriate for imes on an infinite run and ar. aking, in each situation, the action with the highest @ value. During learning, however, there is a difficult exploitation versus exploration trade-off to be made. There are no good, forma pt one of the standard practice is to ado AHC architectures seem to It can be hard to get components converge togethe is, that the Q values will con level\",\n",
       "  '. During learning, however, there is a difficult exploitation versus exploration trade-off to be made. There are no good, forma pt one of the standard practice is to ado AHC architectures seem to It can be hard to get components converge togethe is, that the Q values will con level. ly justified approaches to this problem in the general case; ad hoc methods discussed in section 2.ifficult to work with than Q-learning on a practical e relative learning rates right in AHC so that the two In addition, Q-learning is exploration insensitive: that verge to the optimal values, independent of how the agent be more t Tr. behaves while the data is being collected (as long as all state-action pairs are tried often enough). This means that, in Q-learning, the details learning algorithm. For t most effective model-free however, address any of oO ese go a. although the exploration-exploitation issue must be addressed the explora ion strategy will not affect the convergence of the -learning is the most popular and seems to be the earning from delayed reinforcement. It does not, reasons, Q rithm for he issues involved in generalizing over large state and/or action spaces. In addition, it may converge qui e slowly to a good policy. 4.3 Model-free Learning With Average Reward As described, Q-learning can be applied to discounted infinite-horizon MDPs',\n",
       "  '. It does not, reasons, Q rithm for he issues involved in generalizing over large state and/or action spaces. In addition, it may converge qui e slowly to a good policy. 4.3 Model-free Learning With Average Reward As described, Q-learning can be applied to discounted infinite-horizon MDPs. It can also be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset. Schwartz (1993) examine framework. Although his R-le some MDPs, severa problem they wish to solve th Q-learning (Mahade reward policies. Mahadevan ( a reinforcement-learning persp In particu (and some dynamic cies. Jaakkola, Jor researchers have found the average-reward van, 1994). With that in mind, researchers have studied the problem o ar, he showed that existing reinforcement-learning alg programming algorithms) do not always an and Singh (1995) described an average-reward learning algorithm the problem of adapting Q-learning to an average-reward arning algorithm seems to exhibit convergence problems for criterion closer to the true an a discounted criterion and therefore prefer R-learning to learning optimal average- 996) surveyed model-based average-reward algorithms from ective and found several difficulties with existing algorithms. orithms for average reward roduce bias-optimal poli- with guaranteed convergence properties',\n",
       "  '. orithms for average reward roduce bias-optimal poli- with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the expected uture reward for each state as the agent moves through the environment. In 254 REINFORCEMENT LEARNING: A SURVEY addition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new textbook (1995). Although this recent work provides a much needed theoretical foundation to this area of reinforcement learning, many important problems remain unsolved. The previous section showed how it is possible to learn an optimal policy without knowing the models T(s, a, s’) or R(s,a) and without even learning those models en route. Although many of these methods are guaranteed to find optimal policies eventually and use very little computation time per experience, they make extremely inefficient use of the data they gather and therefore often require a great deal of experience to achieve good performance. In this section we still begin by assuming that we don’t know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are especially important in applications in which computation is considered to be cheap and real-world experience costly. 5',\n",
       "  '. In this section we still begin by assuming that we don’t know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are especially important in applications in which computation is considered to be cheap and real-world experience costly. 5.1 Certainty Equivalent Methods We begin with the most conceptually straightforward method: first, learn the T and R functions by exploring the environment and keeping statistics about the results of each action; next, compute an optimal policy using one of the methods of Section There are some serious objections to this method: e It makes an arbitrary division between the learning phase and the acting phase. e How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data than a system that interleaves experience gathering with policy-building more tightly (Koenig & Simmons, 1993). See Figure 5 for an example. e The possibility of changes in the environment is also problematic. Breaking up an agent’s life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment changes',\n",
       "  \". e The possibility of changes in the environment is also problematic. Breaking up an agent’s life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment changes. A variation on this idea is certainty equivalence, in which the model is learned continually through the agent’s lifetime and, at each step, the current model is used to compute an optimal policy and value function. This method makes very effective use of available data, but still ignores the question of exploration and is extremely computationally demanding, even for fairly small state spaces. Fortunately, there are a number of other model-based algorithms that are more practical. 5.2 Dyna Sutton’s Dyna architecture exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than 255 KAELBLING, LITTMAN, & Moore the certainty-equivalence approach. It simultaneously uses experience to build a model (T and R), uses experience to adjust the policy, and uses the model to adjust the policy. Dyna operates in a loop of interaction with the environment. Given an experience tuple (s,a,s',r), it behaves as follows: e Update the model, incrementing statistics for the transition from s to s’ on action a and for receiving reward r for taking action a in state s. The updated models are T and R\",\n",
       "  \". Given an experience tuple (s,a,s',r), it behaves as follows: e Update the model, incrementing statistics for the transition from s to s’ on action a and for receiving reward r for taking action a in state s. The updated models are T and R. e Update the policy at state s based on the newly updated model using the rule Qls.a) = Risa) +7 Do F(s,0,8!) maxQls'.a’) . which is a version of the value-iteration update for Q values. e Perform k additional updates: choose k state-action pairs at random and update them according to the same rule as before: Q(sp, ag) :=R(sp, ag) + + oT (sn, ap, 8’) max Q(s', a’). e Choose an action a’ to perform in state s’, based on the Q values but perhaps modified by an exploration strategy. The Dyna algorithm requires about f times the computation of Q-learning per instance, but this is typically vastly less than for the naive model-based method. A reasonable value of & can be determined based on the relative speeds of computation and of taking action. Figure 6 shows a grid world in which in each cell the agent has four actions (N, S, E, W) and transitions are made deterministically to an adjacent cell, unless there is a block, in which case no movement occurs. As we will see in Table 1, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational effort, however\",\n",
       "  '. As we will see in Table 1, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational effort, however. 256 REINFORCEMENT LEARNING: A SURVEY Steps before Backups before convergence convergence Q-learning 531,000 531,000 Dyna 62,000 3,055,000 prioritized sweeping 28,000 1,010,000 257 KAELBLING, LITTMAN, & Moore 5.3 Prioritized Sweeping / Queue-Dyna Although Dyna is a great improvement on previous methods, it suffers from being relatively undirected. It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the “interesting” parts of the state space. These problems are addressed by prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams, 1993), which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail. The algorithm is similar to Dyna, except that updates are no longer chosen at random and values are now associated with states (as in value iteration) instead of state-action pairs (as in Q-learning). To make appropriate choices, we must store additional information in the model. Each state remembers its predecessors: the states that have a non-zero transition probability to it under some action. In addition, each state has a priority, initially set to zero',\n",
       "  '. To make appropriate choices, we must store additional information in the model. Each state remembers its predecessors: the states that have a non-zero transition probability to it under some action. In addition, each state has a priority, initially set to zero. Instead of updating & random state-action pairs, prioritized sweeping updates k states with the highest priority. For each high-priority state s, it works as follows: e Remember the current value of the state: Vou = V(s). e Update the state’s value V(s) = max (i. al+y>oT(s, 4, vie) 3! e Set the state’s priority back to e Compute the value change A = |V,7aq — V(s)|. Use A to modify the priorities of the predecessors of s. If we have updated the V value for state s’ and it has changed by amount A, then the immediate predecessors of s’ are informed of this event. Any state s for which there exists an action a such that T(s,a,s’) # 0 has its priority promoted to A - T(s,a,s’), unless its priority already exceeded that value. The global behavior of this algorithm is that when a real-world transition is “surprising” (the agent happens upon a goal state, for instance), then lots of computation is directed to propagate this new information back to relevant predecessor states. When the real- world transition is “boring” (the actual result is very similar to the predicted result), then computation continues in the most deserving part of the space',\n",
       "  '. When the real- world transition is “boring” (the actual result is very similar to the predicted result), then computation continues in the most deserving part of the space. Running prioritized sweeping on the problem in Figure 6, we see a large improvement over Dyna. The optimal policy is reached in about half the number of steps of experience and one-third the computation as Dyna required (and therefore about 20 times fewer steps and twice the computational effort of Q-learning). 258 REINFORCEMENT LEARNING: A SURVEY 5.4 Other Model-Based Methods Methods proposed for based methods as well. RTDP (real-time model-based method t of the state-space that he agent is trying to By taking into accoun without necessarily vis’ solving MDPs given a model can be used in the context of model- ynamic programming) (Barto, Bradtke, & Singh, 1995) is another at uses Q-learning to concentrate computational effort on the areas the agent is most likely to occupy. It is specific to problems in which achieve a particular goal state and the reward everywhere else is the start state, it can find a short path from the start to the goal, iting the rest of the state space. The Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman, 994) exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one',\n",
       "  '. The Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman, 994) exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent’s current state and the goal state, if there is one. States that are not in the envelope are summarized by a single “out” state. The planning process is an alternation between finding an optimal policy on the approximate MDP and to the envelope. Action may take place in parallel with planning, in states are also pruned out of the envelope. adding useful states which case irrelevan All of the previous discussion has tacitly assumed that it is possible to enumerate the state and action spaces and store tables of values over them. Except in very small environments, his means impractical memory requirements. It also makes inefficient use of experience. In a large, smooth state space we generally expect similar states to have similar values and sim- actions. Surely, therefore, there should be some more compact representation han a table. Most problems will have continuous or large discrete state spaces; some wil have large or continuous action spaces. The problem of learning in large spaces is addresse: hrough generalization techniques, which allow compact storage of learned information an er of knowledge between “s: ilar optima. rans imilar” states and actions',\n",
       "  '. The problem of learning in large spaces is addresse: hrough generalization techniques, which allow compact storage of learned information an er of knowledge between “s: ilar optima. rans imilar” states and actions. The large literature of genera. ues from inductive concept learning can be applied to reinforcement learning niques often need to be tailored to specific details of the problem. In the following sections, we explore the application of standar unction-approximation techniques, adaptive resolution models, and hierarchical methods o the problem of reinforcement learning. T he s ization techni . However, tec. p D. he reinforcement-learning architectures and algorithms discussed above have include orage of a variety of mappings, including S — A (policies), S > R (value functions), Sx A— * (Q functions and rewards), S x A > S (deterministic transitions), and S x Ax § => [0,1] (transition probabilities). Some of these mappings, such as transitions an immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervise learning that support noisy training examples. Popular techniques include various neural- network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMAC , and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods',\n",
       "  '. Popular techniques include various neural- network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMAC , and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods. Other mappings, especially the policy 259 KAELBLING, LITTMAN, & Moore mapping, typically need specialized algorithms because training sets of input-output pairs are not available. 6.1 Generalization over Input A reinforcement-learning agent’s current state plays a central role in its selection of reward- maximizing actions. Viewing the agent as a state-free black box, a description of the current state is its input. Depending on the agent architecture, its output is either an action selection, or an evaluation of the current state that can be used to select an action. The problem of deciding how the different aspects of an input affect the value of the output is sometimes called the “structural credit-assignment” problem. This section examines approaches to generating actions or evaluations as a function of a description of the agent’s current state. The first group of techniques covered here is specialized to the case when reward is not delayed; the second group is more generally applicable. 6.1.1 IMMEDIATE REWARD When the agent’s actions do not influence state transitions, the resulting problem becomes one of choosing actions to maximize immediate reward as a function of the agent’s current state',\n",
       "  '. 6.1.1 IMMEDIATE REWARD When the agent’s actions do not influence state transitions, the resulting problem becomes one of choosing actions to maximize immediate reward as a function of the agent’s current state. These problems bear a resemblance to the bandit problems discussed in Section 2 except that the agent should condition its action selection on the current state. For this reason, this class of problems has been described as associative reinforcement learning. The algorithms in this section address the problem of learning from immediate boolean reinforcement where the state is vector valued and the action is a boolean vector. Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4. CRBP_ Thecomplementary reinforcement backpropagation algorithm (Ackley & Littman, 1990) (CRBP) consists of a feed-forward network mapping an encoding of the state to an encoding of the action. The action is determined probabilistically from the activation of the output units: if output unit ¢ has activation y;, then bit ¢ of the action vector has value 1 with probability y;, and 0 otherwise. Any neural-network supervised training procedure can be used to adapt the network as follows. If the result of generating action a is r = 1, then the network is trained with input-output pair (s,a). If the result is r = 0, then the network is trained with input-output pair (s,@), where @ = (1— a,..',\n",
       "  '. If the result of generating action a is r = 1, then the network is trained with input-output pair (s,a). If the result is r = 0, then the network is trained with input-output pair (s,@), where @ = (1— a,...,1— ay). The idea behind this training rule is that whenever an action fails to generate reward, CRBP will try to generate an action that is different from the current choice. Although it seems like the algorithm might oscillate between an action and its complement, that does not happen. One step of training a network will only change the action slightly and since the output probabilities will tend to move toward 0.5, this makes action selection more random and increases search. The hope is that the random distribution will generate an action that works better, and then that action will be reinforced. ARC The associative reinforcement comparison (ARC) algorithm is an instance of the AHc architecture for the case of boolean actions, consisting of two feed- 260 REINFORCEMENT LEARNING: A SURVEY forward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units. In the simplest case, the entire system learns only to optimize immediate reward',\n",
       "  '. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units. In the simplest case, the entire system learns only to optimize immediate reward. First, let us consider the behavior of the network that learns the policy, a mapping from a vector describing s to a Q or The adjustment for the output unit is, in the simplest case, e=r(a—1/2) , where the first factor is the reward received for taking the most recent action and the second encodes which action was taken. The actions are encoded as 0 and 1, so a—1/2 always has the same magnitude; if the reward and the action have the same sign, then action 1 will be made more likely, otherwise action 0 will be. As described, the network will tend to seek actions that given positive reward. To extend this approach to maximize reward, we can compare the reward to some baseline, b. This changes the adjustment to © =(r—d)a~1/2) | where 6 is the output of the second network. The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications (Anderson, 1986; Barto et al., 1983; Lin, 1993b; Sutton, 1984). REINFORCE Algorithms Williams studied the problem of choosing ac- tions to maximize immedate reward. He identified a broad class of update rules that per- form gradient descent on the expected reward and showed how to integrate these rules with backpropagation',\n",
       "  '., 1983; Lin, 1993b; Sutton, 1984). REINFORCE Algorithms Williams studied the problem of choosing ac- tions to maximize immedate reward. He identified a broad class of update rules that per- form gradient descent on the expected reward and showed how to integrate these rules with backpropagation. This class, called REINFORCE algorithms, includes linear reward-inaction (Section 2.1.3) as a special case. The generic REINFORCE update for a parameter w;; can be written Aw = a4j(r - badger Ina) where a;; is a non-negative factor, r the current reinforcement, 6;; a reinforcement baseline, and g; is the probability density function used to randomly generate actions based on unit activations. Both a;; and b;; can take on different values for each w;;, however, when a;; is constant throughout the system, the expected update is exactly in the direction of the expected reward gradient. Otherwise, the update is in the same half space as the gradient but not necessarily in the direction of steepest increase. Williams points out that the choice of baseline, 6 convergence speed of the algorithm. ij, can have a profound effect on the Logic-Based Methods Another strategy for generalization in reinforcement learning is to reduce the learning problem to an associative problem of learning boolean functions. A boolean function has a vector of boolean inputs and a single boolean output',\n",
       "  '. ij, can have a profound effect on the Logic-Based Methods Another strategy for generalization in reinforcement learning is to reduce the learning problem to an associative problem of learning boolean functions. A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive 261 KAELBLING, LITTMAN, & Moore the generalization process (Kaelbling, 1994b); the other searches the space of syntactic descriptions of functions using a simple generate-and-test method (Kaelbling, 1994a). The restriction to a single boolean output makes these techniques difficult to apply. In very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output. In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The CASCADE method (Kaelbling, 1993b) allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort. 6.1',\n",
       "  '. The CASCADE method (Kaelbling, 1993b) allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort. 6.1.2 DELAYED REWARD Another method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning. Here, a function approximator is used o represent the value function by mapping a state description to a value. Many reseachers have experimented with this approach: Boyan and Moore (1995) used local memory-based methods in conjunction with value iteration; Lin (1991) used backprop- agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992, 995) used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich (1995) used backpropagation and T D(A) to learn good strategies for job-shop scheduling. Although there have been some positive examples, in general there are unfortunate in- eractions between function approximation and the learning rules. In discrete environments here is a guarantee that any operation that updates the value function (according to the Bellman equations) can only reduce the error between the current value function and the optimal value function. This guarantee no longer holds when generalization is used',\n",
       "  '. In discrete environments here is a guarantee that any operation that updates the value function (according to the Bellman equations) can only reduce the error between the current value function and the optimal value function. This guarantee no longer holds when generalization is used. These issues are discussed by Boyan and Moore (1995), who give some simple examples of value unction errors growing arbitrarily large when generalization is used with value iteration. Their solution to this, applicable only to certain classes of problems, discourages such diver- gence by only permitting updates whose estimated values can be shown to be near-optimal via a battery of Monte-Carlo experiments. Thrun and Schwartz (1993) theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the “max” operator in the definition of the value function. Several recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show how the appro- priate choice of function approximator can guarantee convergence, though not necessarily to the optimal values. Baird’s residual gradient technique provides guaranteed convergence to locally optimal solutions. Perhaps the gloominess of these counter-examples is misplaced',\n",
       "  '. Baird’s residual gradient technique provides guaranteed convergence to locally optimal solutions. Perhaps the gloominess of these counter-examples is misplaced. Boyan and Moore (1995) report that their counter-examples can be made to work with problem-specific hand-tuning despite the unreliability of untuned algorithms that provably converge in discrete domains. Sutton (1996) shows how modified versions of Boyan and Moore’s examples can converge successfully. An open question is whether general principles, ideally supported by theory, can help us understand when value function approximation will succeed. In Sutton’s com- 262 REINFORCEMENT LEARNING : A SURVEY parative experiments with Boyan and Moore’s counter-examples, he changes four aspects of the experiments: the task specifications. generalization. iteration. There are intuitive reasons to believe that the fourth factor is more careful research is needed. Adaptive Resolution Models the environment into regions of states that can be considered t: learning and generating actions. Without detailed is very difficult to know what granularity or placement of par use adaptive resolution; during the course of learning, artition is constructed that is appropriate to the problem is overcome in a Decision Trees va. In environments ued variables, it is possible to learn compact decision trees for representing Q values. The methods tha hat are charac es uniformly in state space, ories',\n",
       "  '. In environments ued variables, it is possible to learn compact decision trees for representing Q values. The methods tha hat are charac es uniformly in state space, ories. articularly important, but In many cases, what we would like to do is partition e same for the purposes of rior knowledge of the environment, it itions is appropriate. This environment. erized by a set of boolean or discrete- works as fol G-learning algorithm (Chapman & Kk hat no partitioning is necessary an if it were one state. In parallel with input bits: it asks the question whe states in which 6 = aelbling, 1991), tries to learn this process, i her there is some bit 6 in hat the Q values for states in which 6 = 1 are significantly is found, it is used to spl Q values for gathers statistics based on individua e leaves. This method was able to learn very smal ows. It starts by assuming the entire environment as the state description such ifferent from Q values for it the decision tree. Then ; re game environment and or dealing with partial cannot, however, acqui (such as those needed Variable Resolution enables conventional resentations of the Q function in noisy state attributes. It outperformed Q-learning with backpro was used by McCallum (1995 observability re partitions in which attribu o solve parity problems). Dynamic Programming ynamic programming to be he presence of an overwhel to learn behaviors in a complex driving-simulator',\n",
       "  '. It outperformed Q-learning with backpro was used by McCallum (1995 observability re partitions in which attribu o solve parity problems). Dynamic Programming ynamic programming to be he presence of an overwhel to learn behaviors in a complex driving-simulator. I ming number of irrelevant, agation in a simple video- (in conjunction with other techniques es are only significant in combination The VRDP algorithm (Moore, 1991 performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimension- ality. A kd-tree (simi regions. The coarse regions are refined into detailed space which are predic ning “mental trajectories” through state space. This algorithm ed to be important. This no of problems for which disadvantage of requiri ull high-resolution arrays wo ng a guess at an initially vali 263 ar to a decision tree) is used to parti tion state space into coarse regions, but only in parts of the state ion of importance is obtained by run- proved effective on a number uld have been impractical. It has the trajectory through state-space. (a) KAELBLING, LITTMAN, & Moore (b) (c) Start Goal NF i i inal fo EE Bs The point robot must find a path from y of the barrier lines. (b) The path taken by rial. It begins with intense exploration to find a blem. route out of the almost entirely enclosed start region',\n",
       "  '. (a) KAELBLING, LITTMAN, & Moore (b) (c) Start Goal NF i i inal fo EE Bs The point robot must find a path from y of the barrier lines. (b) The path taken by rial. It begins with intense exploration to find a blem. route out of the almost entirely enclosed start region. Having eventually reached a sufficiently high resolution, it discovers the gap and proceeds gree ily towards the goal, only to be temporarily blocked by the goal’s barrier region. (c) The second trial. PartiGame Algorithm Moore’s PartiGame algorithm is another solution to the problem of learning to achieve goal configurations in deterministic high-dimensional continuous s aces by learning an adaptive-resolution model. It also divides the environment into cells; but in each cell, the actions available consist of aiming at the neighboring cells (this aiming problem sta incremental is accomplished by a local controller, which must be provided as ement). The graph of cell transitions is solved for shortest paths in an online manner, but a minimax criterion is used to detect when a group of cells is art of the too coarse to prevent movement between obstacles or to avoid limit cycles. The offending cells are spli choose appropria An important fea it also struc the agent wi small local c ures 1 ini ang to higher resolution',\n",
       "  \". The offending cells are spli choose appropria An important fea it also struc the agent wi small local c ures 1 ini ang to higher resolution. Eventually, the environment is divided up jus e actions for ach exploration of s ially try someth: es when all the Figure 7a shows a two-dimens' of a robot using second trial, star This is a very than a minute. T limits its applica methods. ed from a slight fast algorithm, | e restriction of bility, however. ture is that, as well as reducing memory and computational re enough to ieving the goal, but no unnecessary distinctions are made. uirements, ate space in a multi-resolution manner. Given a failure, ing very different to rectify the failure, and only resort to ualitatively different strategies have been exhausted. ional continuous maze. Figure 7b shows the performance he PartiGame algorithm during the very first trial. Figure 7c shows the y different position. earning policies in spaces of up to nine dimensions in less he current implementation to deterministic environments McCallum (1995) suggests some related tree-structured 264 REINFORCEMENT LEARNING: A SURVEY 6.2 Generalization over Actions The networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well\",\n",
       "  '.2 Generalization over Actions The networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well. In cases such as this when actions are described combinatorially, it is important to generalize over actions to avoid keeping separate statistics for the huge number of actions that can be chosen. In continuous action spaces, the need for generalization is even more pronounced. When estimating Q values using a neural network, it is possible to use either a distinct network for each action, or a network with a distinct output for each action. When the action space is continuous, neither approach is possible. An alternative strategy is to use a single network with both the state and action as input and Q value as the output. Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value (Baird & Klopf, 1993). Gullapalli has developed a “neural” reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience. When the chosen actions are not performing well, the variance is high, resulting in exploration of the range of choices',\n",
       "  '. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience. When the chosen actions are not performing well, the variance is high, resulting in exploration of the range of choices. When an action performs well, the mean is moved in that direction and the variance decreased, resulting in a tendency to generate more action values near the successful one. This method was successfully employed to learn to control a robot arm with many continuous degrees of freedom. 6.3 Hierarchical Methods Another strategy for dealing with large state spaces is to treat them as a hierarchy of learning problems. In many cases, hierarchical solutions introduce slight sub-optimality in performance, but potentially gain a good deal of efficiency in execution time, learning time, and space. Hierarchical learners are commonly structured as gated behaviors, as shown in Figure 6.3.1 FEUDAL Q-LEARNING Feudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external environment. Its actions consist of commands that 265 KAELBLING, LITTMAN, & Moore it can give to the low-level learner. When the master generates a particular command to the slave, it must reward the slave for taking actions that satisfy the command, even if they do not result in external reinforcement',\n",
       "  '. Its actions consist of commands that 265 KAELBLING, LITTMAN, & Moore it can give to the low-level learner. When the master generates a particular command to the slave, it must reward the slave for taking actions that satisfy the command, even if they do not result in external reinforcement. The master, then, learns a mapping from states to commands. The slave learns a mapping from commands and states to external actions. The set of “commands” and their associated reinforcement functions are established in advance of the learning. This is really an instance of the general “gated behaviors” approach, in which the slave can execute any of the behaviors depending on its command. The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels. 6.3.2 COMPOSITIONAL Q-LEARNING Singh’s compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of condi- tions in sequential order. The achievement of the conditions provides reinforcement for the elemental tasks, which are trained first to achieve individual subgoals. Then, the gating function learns to switch the elemental tasks in order to achieve the appropriate high-level sequential goal',\n",
       "  '. The achievement of the conditions provides reinforcement for the elemental tasks, which are trained first to achieve individual subgoals. Then, the gating function learns to switch the elemental tasks in order to achieve the appropriate high-level sequential goal. This method was used by Tham and Prager (1994) to learn to control a simulated multi-link robot arm. 6.3.3 HIBRARCHICAL DISTANCE TO GOAL Especially if we consider reinforcement learning modules to be part of larger agent archi- tectures, it is important to consider problems in which goals are dynamically input to the learner. Kaelbling’s HDG algorithm (1993a) uses a hierarchical approach to solving prob- lems when goals of achievement (the agent should get to a particular state as quickly as possible) are given to an agent dynamically. The HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned (a priori, but more recent work addresses the case of learning the partition) into a set of regions whose centers are known as “landmarks.” If the agent is 266 REINFORCEMENT LEARNING: A SURVEY office currently in the same region as the goal, then it uses low-level actions to move to the goal. If not, then high-level information is used to determine the next landmark on the shortest path from the agent’s closest landmark to the goal’s closest landmark. Then, the agent uses low-level information to aim toward that next landmark',\n",
       "  '. If not, then high-level information is used to determine the next landmark on the shortest path from the agent’s closest landmark to the goal’s closest landmark. Then, the agent uses low-level information to aim toward that next landmark. If errors in action cause deviations in the path, there is no problem; the best aiming point is recomputed on every step. In many real-world environments, it will not be possible for the agent to have perfect and complete perception of the state of the environment. Unfortunately, complete observability is necessary for learning methods based on MDPs. In this section, we consider the case in which the agent makes observations of the state of the environment, but these observations may be noisy and provide incomplete information. In the case of a robot, for instance, it might observe whether it is in a corridor, an open room, a T-junction, etc., and those observations might be error-prone. This problem is also referred to as the problem of “incomplete perception,” “perceptual aliasing,” or “hidden state.” In this section, we will consider extensions to the basic MDP framework for solving partially observable problems. The resulting formal model is called a partially observable Markov decision process or POMDP. 7.1 State-Free Deterministic Policies The most naive strategy for dealing with partial observability is to ignore it. That is, to reat the observations as if they were the states of the environment and try to learn to behave',\n",
       "  '. 7.1 State-Free Deterministic Policies The most naive strategy for dealing with partial observability is to ignore it. That is, to reat the observations as if they were the states of the environment and try to learn to behave. Figure 9 shows a simple environment in which the agent is attempting to get to he printer from an office. If it moves from the office, there is a good chance that the agent will end up in one of two places that look like “hall”, but that require different actions for getting to the printer. If we consider these states to be the same, then the agent cannot ossibly behave optimally. But how well can it do? The resulting problem is not Markovian, and Q-learning cannot be guaranteed to con- verge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate (Chrisman & 267 KAELBLING, LITTMAN, & Moore Littman, 1993). It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations. Unfortunately, when the environment is not Markovian, the transition probabilities depend on the policy being executed, so this new policy will induce a new set of transition probabilities. This approach may yield plausible results in some cases, but again, there are no guarantees',\n",
       "  '. Unfortunately, when the environment is not Markovian, the transition probabilities depend on the policy being executed, so this new policy will induce a new set of transition probabilities. This approach may yield plausible results in some cases, but again, there are no guarantees. It is reasonable, though, to ask what the optimal policy (mapping from observations to actions, in this case) is. It is NP-hard (Littman, 1994b) to find this mapping, and even the best mapping can have very poor performance. In the case of our agent trying to get to the printer, for instance, any deterministic state-free policy takes an infinite number of steps to reach the goal on average. 7.2 State-Free Stochastic Policies Some improvement can be gained by considering stochastic policies; these are mappings from observations to probability distributions over actions. If there is randomness in the agent’s actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan (1995) have developed an algorithm for finding locally-optimal stochastic policies, but finding a globally optimal policy is still NP hard. In our example, it turns out that the optimal stochastic policy is for the agent, when in a state that looks like a hall, to go east with probability 2 — /2 % 0.6 and west with probability /2 —1 % 0. 7',\n",
       "  '. In our example, it turns out that the optimal stochastic policy is for the agent, when in a state that looks like a hall, to go east with probability 2 — /2 % 0.6 and west with probability /2 —1 % 0. 7.3 Policies with Internal State The only way to behave truly effectively in a wide-range of environments is to use memory of previous actions and observations to disambiguate the current state. There are a variety of approaches to learning policies with internal state. Recurrent Q-learning One intuitively simple approach is to use a recurrent neural net- work to learn Q values. The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain “history features” to predict value. This approach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin & Mitchell, 1992; Schmidhuber, 1991b). It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems. Classifier Systems Classifier systems (Holland, 1975; Goldberg, 1989) were explicitly developed to solve problems with delayed reward, including those requiring short-term memory. The internal mechanism typically used to pass reward back through chains of decisions, called the bucket brigade algorithm, bears a close resemblance to Q-learning. In spite of some early successes, the original design does not appear to handle partially ob- served environments robustly',\n",
       "  '. The internal mechanism typically used to pass reward back through chains of decisions, called the bucket brigade algorithm, bears a close resemblance to Q-learning. In spite of some early successes, the original design does not appear to handle partially ob- served environments robustly. Recently, this approach has been reexamined using insights from the reinforcement- learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems (Dorigo & Bersini, 1994). Cliff and Ross (1994) start with Wilson’s zeroth- 268 REINFORCEMENT LEARNING: A SURVEY evel classifier system and add one and two-bit memory registers. They find hat, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments. Dorigo and Colombetti applied classifier systems to a moderately complex problem of learning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti, 994). Finite-history-window Approach One way to restore the Markov property is to allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell (1992) used a fixed-width finite history window to learn a pole balancing task. McCallum (1995) describes the “utile suffix memory” which learns a variable-width window hat serves simultaneously as a model of the environment and a finite-memory policy',\n",
       "  '. Lin and Mitchell (1992) used a fixed-width finite history window to learn a pole balancing task. McCallum (1995) describes the “utile suffix memory” which learns a variable-width window hat serves simultaneously as a model of the environment and a finite-memory policy. This system has had excellent results in a very complex driving-simulation domain (McCallum, 995). Ring (1994) has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations. POMDP Approach Another strategy consists of using hidden Markov model (HMM) echniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Monahan, 1982). Chrisman (1992) showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum (1993), also gave heuristic state- splitting rules to attempt to learn the smallest possible model for a given environment. The resulting model can then be used to integrate information from the agent’s observations in order to make decisions. Figure 10 illustrates the basic structure for a perfect-memory controller',\n",
       "  '. The resulting model can then be used to integrate information from the agent’s observations in order to make decisions. Figure 10 illustrates the basic structure for a perfect-memory controller. The component on the left is the state estimator, which computes the agent’s belief state, b as a function of the old belief state, the last action a, and the current observation Now we are left with the problem of finding a policy mapping belief states into action. This problem can be formulated as an MDP, but it is difficult to solve using the techniques described earlier, because the input space is continuous. Chrisman’s approach (1992) does not take into account future uncertainty, but yields a policy after a small amount of com- putation. A standard approach from the operations-research literature is to solve for the 269 KAELBLING, LITTMAN, & Moore optimal policy (or a close approximation thereof) based on its representation as a piecewise- linear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations (Cassandra et al., 1994; Littman, Cassandra, & Kaelbling, 1995a). One reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act',\n",
       "  '., 1994; Littman, Cassandra, & Kaelbling, 1995a). One reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act. But it is unsurprising that it has also been used by a number of researchers as a practical computational tool for constructing autonomous systems that improve themselves with experience. These applications have ranged from robotics, to industrial manufacturing, to combinatorial search problems such as computer game playing. Practical applications provide a test of the efficacy and usefulness of learning algorithms. They are also an inspiration for deciding which components of the reinforcement learning framework are of practical importance. For example, a researcher with a real robotic task can provide a data point to questions such as: e How important is optimal exploration? Can we break the learning period into explo- ration phases and exploitation phases? e What is the most useful model of long-term reward: Finite horizon? Discounted? Infinite horizon? e How much computation is available between agent decisions and how should it be used? e What prior knowledge can we build into the system, and which algorithms are capable of using that knowledge? Let us examine a set of practical applications of reinforcement learning, while bearing these questions in mind. 8',\n",
       "  '. 8.1 Game Playing Game playing has dominated the Artificial Intelligence world as a problem domain ever since he field was born. Two-player games do not fit into the established reinforcement-learning ramework since the optimality criterion for games is not one of maximizing reward in the ace of a fixed environment, but one of maximizing reward against an optimal adversary (minimax). Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games (Littman, 1994a) and many researchers have used reinforcement earning in these environments. One application, spectacularly far ahead of its time, was Samuel’s checkers playing system . This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning. More recently, Tesauro (1992, 1994, 1995) applied the temporal difference algorithm o backgammon. Backgammon has approximately 107° states, making table-based rein- orcement learning impossible. Instead, Tesauro used a backpropagation-based three-layer 270 REINFORCEMENT LEARNING: A SURVEY Training Hidden Results Games Units Basic Poor TD 1.0 300,000 80 Lost by 13 points in 51 games TD 2.0 800,000 40 Lost by 7 points in 38 games TD 2.1 1,500,000 80 Lost by 1 point in 40 games games against the top human professional players',\n",
       "  '.0 300,000 80 Lost by 13 points in 51 games TD 2.0 800,000 40 Lost by 7 points in 38 games TD 2.1 1,500,000 80 Lost by 1 point in 40 games games against the top human professional players. A backgammon tournament involves playing a series of games for points until one player reaches a set target. TD-Gammon won none of these tournaments but came sufficiently close that it is now considered one of the best few players in the world. neural network as a function approximator for the value function Board Position > Probability of victory for current player. Two versions of the learning algorithm were used. The first, which we will call Basic TD- Gammon, used very little predefined knowledge of the game, and the representation of a board position was virtually a raw encoding, sufficiently powerful only to permit the neural network to distinguish between conceptually different positions. The second, TD-Gammon, was provided with the same raw state crafted features of backgammon board information supplemented by a number of hand- positions. Providing hand-crafted features in this manner is a good example of how inductive biases from human knowledge of the task can be supplied to a learning algorithm. The training of both learning algorit was achieved by constant self-play. No greedily chose the move with the larges ms required several months of computer time, and exploration strategy was used—the system always expected probability of victory',\n",
       "  '. The training of both learning algorit was achieved by constant self-play. No greedily chose the move with the larges ms required several months of computer time, and exploration strategy was used—the system always expected probability of victory. This naive explo- ration strategy proved entirely adequate for this environment, which is perhaps surprising given the considerable work in the reinforcement-learning literature which has produced numerous counter-examples to show that greedy exploration can lead to poor learning per- formance. Backgammon, however, has is followed, every game is guaranteed information is obtained fairly frequent’ wo important properties. Firstly, whatever policy o end in finite time, meaning that useful reward y. Secondly, the state transitions are sufficiently stochastic that independent of the policy, all states will occasionally be visited—a wrong initial value function has little danger of starving us from visiting a critical part of state space from which important information could be obtained. The results (Table 2) of TD-Gammon are impressive. It has competed at the very top level of international human play. Basic TD-Gammon played respectably, but not at a professional standard. 271 Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess',\n",
       "  '. 271 Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess . It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 8.2 Robotics and Control In recent years there have been many robotics and control applications that have used reinforcement learning. Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway. The juggling robot learned a world model from experience, which was generalize o unvisited states by a function approximation scheme known as locally weighte regression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). Between each trial, a form of dynamic programming specific to linear control policies and locally linear ransitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design (Sage & White, 1977). 272 REINFORCEMENT LEARNING: A SURVEY earned. ical reinforcement learning, an unthinka nals called progress estimators were use This was achieved in a robust manner he estimators, but had the freedom to Secondly, control was decentralized. Eac. without explicit communication with the ot rofi quantized into a small number of discre of the Q-learned policies were almost as he job',\n",
       "  '. Eac. without explicit communication with the ot rofi quantized into a small number of discre of the Q-learned policies were almost as he job. hree examples: goo bly high dimensional in which the robots from the induc ive bias h robot learned its own policy ers. Thirdly, s as a simple hand-crafte ching task (Crites & Bar boxes for extended periods of time. Box-pushing is a well-known difficult robotics roblem, characterized by immense uncertainty in the results of actions. Q-learning was used in conjunction with some novel clustering techniques designed to enable a higher-dimensional input than a tabular approach would have permitted. The robot earned to perform competitively with the performance of a human-programmed so- ution. Another aspect of this work, mentioned in Section 6.3, was a breakdown of the monolithic task description into a set of lower level tasks to be pre-programmed e viewpoint of theoret- state space, containing many dozens of degrees of freedom. Four mobile robots traveled within an enclo- sure collecting small disks and transporting them to a destination region. There were hree enhancements to the basic Q-learning algorithm. Firstly, pre-programmed sig- to break the monolithic task into subtasks. were not forced to use hey provided. independently ate space was brutally e states according to values o ber of pre-programmed boolean features of the underlying sensors. The performance a small num- controller for 0, 1996)',\n",
       "  '. Firstly, pre-programmed sig- to break the monolithic task into subtasks. were not forced to use hey provided. independently ate space was brutally e states according to values o ber of pre-programmed boolean features of the underlying sensors. The performance a small num- controller for 0, 1996). The works for function approximation and ess than the best alternative algorithm reinforcement learning by one of the ble numbers of non-identical products. e The mean weight of all containers produced by a shift must not be below the manufacturer’s declared weight W. 273 KAELBLING, LI TTMAN, & Moore e The number of containers below the declared weight must be less than P%. e No containers may be produced below weight W’. Such tasks are controlled by machinery which operates according to various setpoints. Conventional practice is that setpoints are chosen by human operators, but this choice is not easy as it is task constraints. The task was posed ependent on the The dependency is current product characteristics and the current often difficult to model and highly non-linear. as a finite-horizon Markov decision task in which the state of the system is a function of the product characteristics, the amount of time remaining in the production so far. The system regression was ing was used to ma information was ob typically with wast. deployed successful Some interesting aspects of practical rein examples',\n",
       "  '. The system regression was ing was used to ma information was ob typically with wast. deployed successful Some interesting aspects of practical rein examples. The mos necessary to supplement plying extra knowledge comes a Sup the system is subse these, a knowledge-fr the finite lifetime of What forms did linearity for the jugg shif was discretized i use intain an optimal age reduced by a y in several factor: striking is that in al he fundamental al uently less aw ee approach woul. he robots. his pre-programmed ing robot’s policy, no the two mobile-robo the @ values which assumed loca ionally used a manual dimensions and so required correspon addi sumption of local pie in the amoun T ysis of lear e exploration s o judge were able to plora T strategies mir yet all prove Finally, it They where ion. were al The earn well wit e packaging task use rors theoretically op adequate. is also worth considering the compu very different, which indicates that t various reinforcement learning algorithms do indeed hav juggler needed to make very fast decisions with had long periods (30 seconds and more) between each he consis examples, while ly € y discr ing and the mean was ained. In simulate a price: onomous. ized state space. T age and percent below declared in the shift nto 200,000 discrete states and local weighted to learn and generalize a transition model. Prioritized sweep- value function as each new piece of transition experiments the savings were considerable, actor of ten',\n",
       "  '. ized state space. T age and percent below declared in the shift nto 200,000 discrete states and local weighted to learn and generalize a transition model. Prioritized sweep- value function as each new piece of transition experiments the savings were considerable, actor of ten. Since then the system has been ies within the United States. orcement learning come to light from these cases, to make a real system work it proved gorithm with extra pre-programmed knowledge. more human effort and insight is required and But it is also clear that for tasks such as have achieved worthwhile performance within knowledge take? It included an assumption of a manual breaking up of the task into subtasks for box-pusher also used a clustering technique for tent @ values. The four disk-collecting robots e packaging example had far fewer y weaker assumptions, but there, too, the as- cewise continui ning yin t ata required. trategies are inter to profitably gree ex te) imal (bu y esting too. T experiment. loration—alway: ptimism in t e transition model enabled massive reductions e juggler used careful statistical anal- However, both mobile robot applications 8 exploiting without deliberate ex- e face of uncertainty. None of these t computationally intractable) exploration, and of these experiments. utational demands of ational regimes iffering com ean array of differing applications',\n",
       "  '. None of these t computationally intractable) exploration, and of these experiments. utational demands of ational regimes iffering com ean array of differing applications. ow latency between each hit, but rial to consolidate the experiences e collected on the previous trial and to perform the more aggressive computation necessary to produce a new reactive controller on the next trial. T e box-pushing robot was meant to 274 REINFORCEMENT LEARNING: A SURVEY operate autonomously for hours and so had to make decisions with a uniform length control cycle. The cycle was sufficiently long for quite substantial computations beyond simple Q- learning backups. The four disk-collecting robots were particularly interesting. Each robot had a short life of less than 20 minutes (due to battery constraints) meaning that substantial number crunching was impractical, and any significant combinatorial search would have used a significant fraction of the robot’s learning lifetime. The packaging task had easy constraints. One decision was needed every few minutes. This provided opportunities for fully computing the optimal value function for the 200,000-state system between every control cycle, in addition to performing massive cross-validation-based optimization of the transition model being learned. A great deal of further work is currently in progress on practical implementations of reinforcement learning',\n",
       "  '. A great deal of further work is currently in progress on practical implementations of reinforcement learning. The insights and task constraints that they produce will have an important effect on shaping the kind of algorithms that are developed in future. There are a variety of reinforcement-learning techniques that work effectively on a variety of small problems. But very few of these techniques scale well to larger problems. This is not because researchers have done a bad job of inventing learning techniques, but because it is very difficult to solve arbitrary problems in the general case. In order to solve highly complex problems, we must give up tabula rasa learning techniques and begin to incorporate bias that will give leverage to the learning process. The necessary bias can come in a variety of forms, including the following: shaping: The technique of shaping is used in training animals (Hilgard & Bower, 1975); a teacher presents very simple problems to solve first, then gradually exposes the learner to more complex problems. Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up , and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995). local reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local',\n",
       "  '. local reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local. In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly . imitation: An agent can learn by “watching” another agent perform the task . For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel . problem decomposition: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very power- ful technique for biasing learning. Most interesting examples of robotic reinforcement learning employ this technique to some extent (Connell & Mahadevan, 1993). reflexes: One thing that keeps agents that know nothing from learning anything is that they have a hard time even finding the interesting parts of the space; they wander 275 lea: KAELBLING, LITTMAN, & Moore around at random never getting near the goal, or they are always “killed” immediately. These problems can be ameliorated by programming a set of “reflexes” that cause the agent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto, Grupen, & Connolly, 1994)',\n",
       "  '. These problems can be ameliorated by programming a set of “reflexes” that cause the agent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto, Grupen, & Connolly, 1994). These reflexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn. Recent work by Millan (1996) explores the use of reflexes to make robot learning safer and more efficient. With appropriate biases, supplied by human programmers or teachers, complex reinforcement- rning problems will eventually be solvable. There is still much work to be done and many interesting questions remaining for learning techniques and especially regarding methods for approximating, decomposing, and incorporating bias into problems. Acknowledgements T to anks to Marco Dorigo and three anonymous reviewers for comments that have helped improve this paper. Also thanks to our many colleagues in the reinforcement-learning community who have done this work and explained it to us. 93 in Leslie Pack Kaelbling was supported in part by NSF grants IRI-9453383 and IRI',\n",
       "  'arXiv:2312.14925v2 [cs.LG] 30 Apr 2024 A Survey of Reinforcement Learning from Human Feedback Timo Kaufmann timo.kaufmann@ifi.lmu. de IMU Munich, MCML Munich Paul Weng paul.weng@duke. edu Duke Kunshan University Viktor Bengs viktor. bengs@ifi.lmu.de IMU Munich, MCML Munich Eyke Hiillermeier eyke@ifi.lmu.de IMU Munich, MCML Munich Abstract Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interac- tion. This positioning offers a promising avenue to enhance the performance and adaptabil- ity of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model’s ca- pabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader per- spective, examining the diverse applications and wide-ranging impact of the technique',\n",
       "  '. While recent focus has been on RLHF for LLMs, our survey adopts a broader per- spective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic rela- tionship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to pro- vide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research. Contents 1 Introduction 3 1.1 Why Human Feedback... 2.4 Reinforcement Learning from Human Feedback .. 2... 20. 2.5 Active Learning... 2... ee 14 Feedback 15 3.1 Attributes of Feedback Types... 1 Introduction In reinforcement learning (RL), an agent traditionally navigates through an environment and attempts to make optimal decisions (i.e., action choices) through a process of trial and error. Whether a decision is optimal or not is determined solely by reward signals. These signals have to be defined by a system designer based on measurements of the agent’s performance, ensuring that the learning agent receives the nec feedback to learn the correct behavior. Designing a reward function, however, is challenging. Indeed, suc is hard to formally define and measure in many applications',\n",
       "  '. Designing a reward function, however, is challenging. Indeed, suc is hard to formally define and measure in many applications. Beyond that, a sparse signal of success may not be well suited for agent learning — resulting in the need for reward shaping , where the reward signal is transformed into one that is more suitable for learning. This often makes the reward signal more susceptible to spurious correlations, however — behaviors that are rewarded because they are usually correlated with the true objective but are not valuable in themselves. This ultimately cumulates in the issue of reward hacking , where learning agents exploit reward-specific loopholes to achieve undesired outcomes while still generating high rewards. n response to these challenges, reinforcement learning from human feedback (RLHF) has emerged as a ractically meaningful alternative that introduces a critical human-in-the-loop component to the standard RL learning paradigm. In a nutshell, RLHF differs from RL in that the objective is defined and iteratively refined by the human in the loop instead of being specified ahead of time. This approach not only has the otential to overcome the limitations and issues of classical RL methods but also has potential benefits for agent alignment, where the agent’s learning goals are more closely aligned with human values, promoting ethically sound and socially responsible AI systems',\n",
       "  '. This approach not only has the otential to overcome the limitations and issues of classical RL methods but also has potential benefits for agent alignment, where the agent’s learning goals are more closely aligned with human values, promoting ethically sound and socially responsible AI systems. RLHF has seen a number of successful applications, advances in methodology, and theoretical insights since he last comparable survey . The applications span various domains, including large anguage model (LLM) fine-tuning , image generation , continuous con- rol , games , and robotics (Hejna & Sadigh, 2022). At the same ime, there have been a lot of developments in the methodology since the last comparable survey . Examples of methodological developments include fusing multiple feedback types to leverage their relative strengths (see Section 3.5), enhancing query efficiency through active learning and active query syn- hesis (see Section 4.1.1), incorporating psychological insights to improve the quality of human feedback (see Section 4.2.1), using techniques such as meta-learning to quickly adapt learned preferences to new tasks using prior data (see Section 5.5.1), and using available preference data more efficiently through approaches such as data augmentation and semi-supervised learning (see Section 5.5.2)',\n",
       "  '.2.1), using techniques such as meta-learning to quickly adapt learned preferences to new tasks using prior data (see Section 5.5.1), and using available preference data more efficiently through approaches such as data augmentation and semi-supervised learning (see Section 5.5.2). Finally, there have been some achievements with regard to theoretical results for the field of RLHF (Section 7), providing new insights but also new questions for the fundamental mathematical problems underlying the modeling of the learning scenario in RLHF. In this survey, we, therefore, discuss the current state of affairs with regard to the ongoing research in RLHF, classify the current approaches as well as concisely describe their main characteristics, and give a brief overview of the application areas. In the remainder of this section, we will start by discussing the motivation (Section 1.1) and origins (Section 1.2) of RLHF as well as the scope of this survey (Section 1.3) and conclude by outlining the contents of the following sections (Section 1.5). 1. In conventional RL, the agent’s objective is defined by a reward function that it aims to maxi- mize (Sutton & Barto, 2018)',\n",
       "  '.1) and origins (Section 1.2) of RLHF as well as the scope of this survey (Section 1.3) and conclude by outlining the contents of the following sections (Section 1.5). 1. In conventional RL, the agent’s objective is defined by a reward function that it aims to maxi- mize (Sutton & Barto, 2018). Specifying this reward function can be challenging, particularly in complex domains: What would be a suitable reward function for a robot assisting humans in a household environ- ment or for autonomous vehicles navigating through a busy urban environment? Moreover, even reward functions that initially seem well-defined can lead to unexpected behaviors due to distributional shifts or over-optimization, raising practical and safety concerns. Learning the agent’s objective from human feedback circumvents reward engineering challenges and fosters robust training, with the reward function dynamically refined and adjusted to distributional shifts as the agent learns. Interactive Feedback vs. Demonstrations The field of inverse RL aims to infer reward functions rom human demonstrations (Arora & Doshi, 2021). While this can partially resolve reward engineering challenges, it faces inherent difficulties: (i) It is generally not possible to robustly identify rewards from demonstrations (Cao et al',\n",
       "  '. While this can partially resolve reward engineering challenges, it faces inherent difficulties: (i) It is generally not possible to robustly identify rewards from demonstrations (Cao et al., 2021a; Mindermann & Armstrong, 2018), (ii) it is only applicable in scenarios where good demonstrations can be obtained, (iii) it struggles to outperform the demonstrator, and (iv) humans often do not demonstrate the behavior they would prefer a machine to adopt . nteractive feedback, by contrast, can use active queries to differentiate between human preferences and irrelevant noise, is much easier to provide than demonstrations, does not require near-optimal performance rom the human evaluators, and elicits preferences on the behavior that a human would prefer from the machine. Interactive feedback can also be used to complement demonstrations, in which case it can be used o shape and refine capabilities learned through initial training, like behavioral cloning, thereby preventing overfitting to demonstrated behavior . Avoiding Reward Engineering Reward engineering in RL presents significant challenges, as accurately specifying reward functions is notoriously difficult (Amodei et al., 2016; Knox et al., 2023). These challenges can be mitigated by utilizing human feedback, which enables training agents for tasks that are hard to define manually and helps avoid safety issues arising from misaligned rewards',\n",
       "  '., 2016; Knox et al., 2023). These challenges can be mitigated by utilizing human feedback, which enables training agents for tasks that are hard to define manually and helps avoid safety issues arising from misaligned rewards . Safety issues related to a misalignment between the agent’s and the human’s objectives are studied as the AI alignment roblem , in particular agent alignment and value alignment . Excessive optimization for poorly specified rewards often leads to unintended behaviors. Examples of such ehaviors include exploiting flaws in the environment simulation for higher rewards (Lehman et al., 2020; Baker et al., 2020) or engaging in more general reward hacking , where the behavior maximizes the specified reward but deviates from the intended objective. This is evident in cases where agents ‘ocus on intermediate rewards without achieving the actual goal (Clark & Amodei, 2016) or prematurely exit a game to avoid negative rewards . The root of these issues is that the reward ‘unction does not properly reflect the actual learning task. While these issues may seem trivial in game- ike environments, their implications are far more serious in safety-critical contexts such as healthcare an autonomous driving. In these settings, it is crucial to prevent misaligned reward functions from leading to harmful outcomes, like a care robot causing injury or a self-driving car jeopardizing road safety',\n",
       "  '. In these settings, it is crucial to prevent misaligned reward functions from leading to harmful outcomes, like a care robot causing injury or a self-driving car jeopardizing road safety. RLHF presents a promising approach to enhance alignment by enabling agents to learn rom human feedback, which is often more closely aligned with the true objective than manually specifie rewards. Nonetheless, the effectiveness of RLHF in resolving these alignment issues is debated . Examples of possible pitfalls raised in this debate are that the agent may be incentivized to manipulate he human teacher to provide feedback that is easier to optimize (Armstrong et al., 2020; Carroll et al., 2023) or that the agent may learn to exploit errors in human judgement . We refer the intereste reader to the survey by Casper et al. (2023) for a more detailed discussion of these issues. Despite this debate, RLHF represents an important early step towards aligning agents with human values and serves as a foundation to build on to further improve agent alignment. 1.2 The Origins of RLHF Learning behavior from human feedback has long been studied as a subfield of RL, but methods and termi- nology have evolved over time. Early methods focused on learning directly from human rewards (Knox, 012; Isbell et al., 2001; Knox & Stone, 2009), action advice , or action critique udah et al., 2010). Notable approaches in this area include TAMER (Knox & Stone, 2009; Warnell et al',\n",
       "  '. Early methods focused on learning directly from human rewards (Knox, 012; Isbell et al., 2001; Knox & Stone, 2009), action advice , or action critique udah et al., 2010). Notable approaches in this area include TAMER (Knox & Stone, 2009; Warnell et al., 018), which interprets human feedback as samples of the optimal action-value function, and the later iS) ya COACH (MacGlashan et al., 2017; Arumugam et al., 2019), which interprets human feedback in a policy- dependent way, i.e., as samples of the advantage function. This survey, however, focuses on more indirect approaches to inferring the objective from human feedback. Reinforcement learning from human feedback (RLHF) in its modern guise has its origin in the setting of preference-based reinforcement learning (PbRL) as introduced independently by Akrour et al. (2011) and Cheng et al. (2011). The original idea of preference-based reinforcement learning (PbRL) is to infer the objective from qualitative feedback, such as pairwise preferences between behaviors or between actions given Feedback Type PbRL SSRL RLHF Binary trajectory comparisons Trajectory rankings State preferences Action preferences Binary critique Scalar feedback Corrections Action advice mplicit feedback Natural language ww KK RRQ LS ww RK KKK KO NNN N NANA states, instead of quantitative feedback in the form of numerical rewards. The term RLHF was coined as an alternative later on (Askell et al., 2021; Ouyang et al',\n",
       "  '. The term RLHF was coined as an alternative later on (Askell et al., 2021; Ouyang et al., 2022; OpenAI, 2022), though initially referring to he same concept of learning behavior from relative feedback. Disentangling PbRL and RLHF is challenging due to their overlapping use in the literature. For instance, Christiano et al. (2017) themselves are using the term PbRL, yet are often cited as a seminal reference for RLHF (Daniels-Koch & Freedman, 2022; Ouyang et al., 2022). This indicates the interchangeability of hese terms. Practically, RLHF is often associated with reward modeling and deep RL, while PbRL is often inked to direct policy optimization in traditional RL settings. This is underlined by Jeon et al. (2020), who characterize PbRL as limited to direct policy learning from preferences. This is in contrast with other sources, however, who include reward learning within the scope of PbRL (Christiano et al., 2017; Wirth et al., 2017). Despite the overlapping and sometimes conflicting usage, RLHF is increasingly viewed as a generalization of PbRL. While both involve human feedback to define RL objectives, PbRL primarily focuses on relative feedback, such as binary comparisons and rankings. RLHF not only includes these aspects but also extends to a wider range of feedback types (Metz et al., 2023; Yuan et al., 2024). Table 1 gives an exemplary overview of our interpretation of these terms',\n",
       "  '. RLHF not only includes these aspects but also extends to a wider range of feedback types (Metz et al., 2023; Yuan et al., 2024). Table 1 gives an exemplary overview of our interpretation of these terms. Another concept, semi-supervised reinforcement learning (SSRL), introduced by Christiano (2016) and dis- cussed by Amodei et al. (2016), refers to an RL setting where an agent receives feedback on a subset of its experiences. The initial discussions of SSRL focused on absolute feedback on subsets of the agent’s experi- ences, making the concept complementary to PbRL. In contrast to PbRL and RLHF, the term SSRL seems to be used less in the recent literature. In our work, we adopt the viewpoint that RLHF is a broader category that encompasses various approaches where human feedback is used to define the objective of an RL agent. In this definition, RLHF encompasses both PbRL and SSRL. As the definitions and distinctions between these terms are not universally agreed upon, these distinctions are based on our interpretation of the current predominant usage of these terms in the literature. 1. This section outlines the criteria guiding our selection of approaches in the realm of RLHF. We focus on works that rely on a reward model as the sole source of information about the objective. This reward model should then be learned in an interactive, online, scalable, and asynchronous manner. The following will describe each of these criteria in more detail',\n",
       "  '. We focus on works that rely on a reward model as the sole source of information about the objective. This reward model should then be learned in an interactive, online, scalable, and asynchronous manner. The following will describe each of these criteria in more detail. Reward Modeling We focus on approaches that learn a reward model from human feedback and then use this model to train a policy. Although it is possible to directly optimize a policy from human feedback , thereby performing RLHF without reward learning, this approach was almost not practiced for a long time and has only recently gained renewed interest especially in the on domain of language model fine-tuning (see Section 6.3). The decomposition into reward learning and policy training offers many conceptual and practical benefits. Among those benefits are the direct applicability of supervised learning techniques for the reward model and the possibility of evaluating the reward model in isolation. In addition to that, the decomposition naturally leads to a form of semi-supervised learning, enabling the agent to use labeled episodes for reward model training while leveraging unlabelled episodes to refine its behavior and explore the environment. Human Defined While there are many approaches that include humans in the RL loop, in this survey, we focus on approaches where human feedback is the only source of truth about the objective',\n",
       "  '. Human Defined While there are many approaches that include humans in the RL loop, in this survey, we focus on approaches where human feedback is the only source of truth about the objective. This excludes approaches to reward shaping, feature engineering, and other forms of human guidance that are supplementary to a given objective. Interactive and Online We also put an emphasis on providing feedback in an interactive, online manner. This excludes imitation learning, learning from demonstration, and pure inverse RL. While we do not directly cover inverse RL in this survey, combinations of inverse RL methods with interactive improvements of the reward function are in scope and employed by some of the surveyed methods. See Sections 3.3 and 5.5.1 for a discussion of those approaches. Scalable and Asynchronous We focus on works in which the human is included in the loop, but the agent is not blocked by the human’s feedback, and the human does not need to be present continuously. This distinguishes RLHF from more direct methods of incorporating a human into the RL loop, and we believe that this is key for practicality and efficiency. In addition to these criteria, we mainly focus on works published after 2017 since earlier works are surveyed by Wirth et al. (2017). Nevertheless, some works from this period are revisited from time to time in order to elaborate on certain concepts that are still state of the art or have significantly shaped it',\n",
       "  '. (2017). Nevertheless, some works from this period are revisited from time to time in order to elaborate on certain concepts that are still state of the art or have significantly shaped it. Note that while there has been a recent forus on RLHF for LLMs, LLMs are not the primary focus of this work. Instead, we cover RLHF in a broader context, focusing on control applications as well as discussing the implications of RLHF for fine-tuning generative models such as LLMs. 1.4 Prior Surveys Based on the criteria mentioned in the previous section, we will first differentiate our survey from other surveys in marginally related subject areas sharing the common theme of human-in-the-loop RL. Then, we will describe the differences between our survey and previous surveys or survey-like articles that exist within the RLHF field. 1.4. Human participation in machine learning (ML), particularly in guiding machine learners, is a much-studied scenario. This field, commonly referred to as human-in-the-loop ML, can be further divided into subfields based on various criteria, e.g., the ones detailed in Section 1. Human-in-the-Loop Learning from human feedback falls into the domain of human-in-the-loop ML. Wu et al. (2022) survey human-in-the-loop ML in general. They also cover some applications of RLHF (for LLMs in particular) but do not give a detailed overview. Retzlaff et al',\n",
       "  '. Human-in-the-Loop Learning from human feedback falls into the domain of human-in-the-loop ML. Wu et al. (2022) survey human-in-the-loop ML in general. They also cover some applications of RLHF (for LLMs in particular) but do not give a detailed overview. Retzlaff et al. (2024) provide a similar overview over human-in-the-loop RL in particular, focusing on human involvement in RL on a more abstract level than our work and not covering RLHF in detail. Similarly broad in scope, Najar & Chetouani (2021) study the setting of RL with human advice, which they define as ‘teaching signals that can be communicated by the teacher to the learning system without executing the task’ While this setting subsumes RLHF, the broad generality limits the depth in which their survey can cover RLHF approaches. Interactive RL RLHF can be considered a sub-field of interactive RL, which studies RL algorithms that learn in interaction with humans. This interaction can take the form of feedback defining an objec- Reference Topic Reward Human Interactive Sealab le Modelling Defined and Online Async. Wu et al. (2022) Human-in-the-loop ML x x x x Retzlaff et al. (2024) Human-in-the-loop RL (v) (Vv) v (Vv) cet) Chetouani RL with human advice (v) (Vv) v (Vv) Lin et al. (2020a) Social feedback x (Vv) v x Poole & Lee (2024) RL from brain signals x v v x Cruz & Igarashi (2020) nteractive RL for HCI x x v x Osa et al. (2018) mitation learning x v x x Arora & Doshi (2021) nverse RL v v x v Bignold et al',\n",
       "  '. (2020a) Social feedback x (Vv) v x Poole & Lee (2024) RL from brain signals x v v x Cruz & Igarashi (2020) nteractive RL for HCI x x v x Osa et al. (2018) mitation learning x v x x Arora & Doshi (2021) nverse RL v v x v Bignold et al. (2021) Assisted RL x x (Vv) x Luketina et al. (2019) Language-informed RL x x x x Zhang et al. (2021) Human guidance x v v x Ji et al. (2023b) AI Alignment v x v v Liu et al. (2023c) LLM applications x x x x Ours RLHF v v v v tive, resulting in the RLHF setting, but can also, e.g., be used to drive exploration or speed up the agent’s learning process. Cruz & Igarashi (2020) survey interactive RL from an human-computer interaction (HCI) viewpoint, exploring various ways humans can influence RL agents, with a particular focus on reward definition based on human feedback, without a predefined environmental reward function. Due to the breadth of their survey, they do not cover many works in this area. The survey by Lin et al. (2020a) centers on interactive RL using human social cues, like gestures and spoken language, but does not cover the reward modeling aspect. Similarly, the study by Poole & Lee (2024) examines RL with direct feedback from human brain signals, such as through brain-computer interfaces, also not focusing on reward modeling. Demonstrations Learning from demonstrations, in the form of behavior cloning and inverse RL (Arora & Doshi, 2021), shares the goal of RLHF to learn behavior from human input',\n",
       "  '. Demonstrations Learning from demonstrations, in the form of behavior cloning and inverse RL (Arora & Doshi, 2021), shares the goal of RLHF to learn behavior from human input. In contrast to RLHF, however, it requires demonstrations of the desired behavior instead of feed- back, and these demonstrations are usually not provided interactively and online. This limits their applications and also their final performance due to the availability of near-optimal demonstrations. Nonetheless, imitation and demonstration can be a useful component of an RLHF system but are not the main focus of this survey. However, we will discuss the intersection between these fields in some parts whenever necessary. Assisted RL Bignold et al. (2021) review the field of assisted RL, where an agent may receive external information (for example, from a human) that aids it in action selection. While updates to the reward function are one of the possible effects of advice in this setting (in addition to action selection or modifications of the agent’s internal state), it is usually assumed that an initial reward function is given and the extent of the updates is limited to reward shaping or supplementary reward signals. In contrast to RLHF, the external information does not define the task but only helps the agent in achieving it. Closely related to this, Luketina et al. (2019) survey RL assisted by natural language',\n",
       "  '. In contrast to RLHF, the external information does not define the task but only helps the agent in achieving it. Closely related to this, Luketina et al. (2019) survey RL assisted by natural language. In addition to this assistance setting, they also discuss approaches that infer a language-conditioned that the aspect is not covered. Reference (Focus) Beyond Label Wirth et al. (2017) (preference-based RL) x v) Abdelkareem et al. (2022) x x (recent advances of PbRL) Jeon et al. (2020) V x (feedback modelling) Casper et al. (2023) V x (open issues in RLHF) Fernandes et al. (2023). ; v v (language generation) Metz et al. (2023) (feedback types) v v Yuan et al. (2024) (feedback types) v v Ours (fundamentals, recent V V advances, and trends) RM Comparisons Collection Training Theory Bevdhamarks x (V) Vv) (Vv) x K x (V) K (V) K K K v v v reward function. However, they discuss this setting rather briefly and use techniques from inverse RL and not RLHF. Guidance In their survey on human guidance, Zhang et al. (2021) delve into various aspects related to RLHF. Although they touch on aspects such as r of their work. Instead, their main focus lies on ex, involve the learning of a reward model. eward learning, i AI Alignment Ji et al. (2023b) provide a general overview of AI alignment the objectives of an intelligent system with those o in some detail. As AI alignment is a very broad fie! into as much depth on the topic of RLHF as we do its human opera: here. Applications Liu et al',\n",
       "  '. eward learning, i AI Alignment Ji et al. (2023b) provide a general overview of AI alignment the objectives of an intelligent system with those o in some detail. As AI alignment is a very broad fie! into as much depth on the topic of RLHF as we do its human opera: here. Applications Liu et al. (2023c) give an overview of current applications of as ChatGPT and GPT- ly enjoys a lot o: s a broader pers is not the primary emphasis loring more immediate approaches that do not , Le., the challenge of aligning ors. This survey covers RLHF Id, however, the article nevertheless does not go RLHF methods for LLMs such f attention, it is only one spe- pective, examining the diverse applications and impact of RLHF encompassing application areas beyond LLMs. 1.4. There have been previous surveys or survey-like articles that are closely related to RLHF. Table 3 gives a brief overview of how these articles differ from ours, which we will explain in more detail below. Preference-Based RL Previous surveys in the domain of RLHF often focus on PbRL, where feedback is limited to binary preferences (see Section 1.2). An illustrative example of this is the survey by Wirth et al. (2017), which is a direct precursor to our work. In contrast to our work, they concentrate on binary preferences for trajectories and primarily survey methods that learn policies without deriving a reward model',\n",
       "  '.2). An illustrative example of this is the survey by Wirth et al. (2017), which is a direct precursor to our work. In contrast to our work, they concentrate on binary preferences for trajectories and primarily survey methods that learn policies without deriving a reward model. Since then, the reward-modeling approach has become dominant in the field, and other approaches have extended RLHF to new feedback types. Abdelkareem et al. (2022) give another more recent literature review of PbRL. While this review focuses on reward modeling and includes some recent work, it is far less comprehensive than our review, as many aspects are only touched upon and partly overlap with those of Wirth et al. (2017). Feedback Types Although not a survey per se, Jeon et al. (2020) propose reward-rational implicit choice as a unifying framework to comprehend many previous studies in PbRL and RLHF. To illustrate its generality, they overview different feedback types used in previous work and explain how they fit into their framework. The concurrent works by Metz et al. (2023) and Yuan et al. (2024), which are also not strictly surveys, propose frameworks for studying user interaction and interface design for multiple feedback types. As part of their work, they provide a classification of feedback types and a brief overview of RLHF approaches. Metz et al',\n",
       "  '. (2023) and Yuan et al. (2024), which are also not strictly surveys, propose frameworks for studying user interaction and interface design for multiple feedback types. As part of their work, they provide a classification of feedback types and a brief overview of RLHF approaches. Metz et al. (2023) have a stronger focus on the feed- back interface and on learning from multiple feedback types simultaneously, discussing properties o: feedback types and proposing a standard encoding for them. On the other hand, Yuan et al. (2024) also include an offline RLHF benchmark and have a stronger focus on the reward learning aspect, focusing on the entire learning pipeline. Nevertheless, many facets of RLHF are not dealt with a all in those studies, as they are not primarily survey articles. Our survey has a broader scope and, therefore, has more extensive coverage, going beyond the study of feedback types and discussing more recent work. Domain-Specific Fernandes et al. (2023) focuses on human feedback for langauge generation. As a resul of their focus, their survey is less comprehensive than this work but discusses some language-specific aspects that do not fall into our scope, such as using feedback models at generation time. Open Problems Casper et al. (2023) provide a detailed overview of the open questions and limitations o: RLHF with a particular focus on aspects of security, governance, and transparency',\n",
       "  '. Open Problems Casper et al. (2023) provide a detailed overview of the open questions and limitations o: RLHF with a particular focus on aspects of security, governance, and transparency. In their article, reward modeling is also covered, as is human feedback, which goes beyond preference comparisons, but other aspects, such as theoretical approaches or an overview of existing benchmarks, are no included. Thus, it can be seen as a supplementary article that is ideal for further reading once being familiarized with the topic through our survey. All in all, our survey can be seen as the canonical continuation of Wirth et al. (2017), which examines the evolution of the field of PbRL to the more modern and general field of RLHF. This includes a thorough description of the basics as well as an in-depth discussion of current advances and trends in the field. 1.5 Outline In the next section, we begin with an introduction to the basics by revisiting the most important concepts from the standard RL setting, which are also naturally important in RLHF (Section 2). We then dive into the RLHF topic by outlining the most studied scenario of reward model learning from pairwise preferences. Using this introductory and illustrative example scenario, we explain the basic framework of RLHF alongside its three main components of (human) feedback, label collection (feedback acquisition), and reward model learning. These three main components will essentially form the structure of our survey',\n",
       "  '. These three main components will essentially form the structure of our survey. In Section 3, we turn our attention to the human feedback component and provide an overview of the different types of feedback as well as their key attributes. The important concepts in terms of label collection are then explained in Section 4, followed by learning the reward model in Section 2 Preliminaries In this section, we recall the basic setting and the most important concepts of RL and RLHF. In the course of this review, we will fix the notation that will be used throughout the survey. We first introduce what is probably the most studied RLHF scenario, i.e., learning a reward model from binary trajectory comparisons. Based on this introductory and illustrative example scenario, we explain the basic framework of RLHF with its main components and briefly discuss the respective roles of these components in the learning process. We will also briefly touch on active learning, which strongly connects to the feedback collection component. Notations For any integer n € N, we denote by [n] the set {1,2,...,n}. For any set S, A(S) denotes the set of probability distributions over S. We use P(£) for denoting the probability of some event E, while E[X] is used to denote the expected value of a random variable X. In some cases, we will write Ep|-] or similar variants to emphasize that the distribution for the expected value is governed by the probability distribution P € A(S)',\n",
       "  '. We use P(£) for denoting the probability of some event E, while E[X] is used to denote the expected value of a random variable X. In some cases, we will write Ep|-] or similar variants to emphasize that the distribution for the expected value is governed by the probability distribution P € A(S). Moreover, we will write X ~ P if a random variable X is distributed according to a probability distribution P. 2.1 Reinforcement Learning Reinforcement learning (RL) (Sutton & Barto, 2018) is the setting of learning behavior from rewarded in- teraction with an environment. Such a learning environment is formalized as an Markov decision process (MDP), which is a model for sequential decision-making. In an MDP, an agent iteratively observes its current state, takes an action that causes the transition to a new state, and finally receives a reward that depends on the action’s effectiveness. Formally, an MDP is defined as a tuple (S,.A, P, R,do,) where « S isa set of states (the state space), « Aisa set of actions (the action space), « P: Sx A- A(S) is a transition function (the transition dynamics), « R: Sx A-— Risa reward function, « do € A(S) is a distribution over initial states, « and 7 € [0,1] is a discount factor. The transition function P defines the dynamics of the environment: For any state s and action a, the value P(s,a)(s’), also sometimes denoted P(s’ | s, a), is the probability of reaching the state s’ after executing a in s',\n",
       "  '. The transition function P defines the dynamics of the environment: For any state s and action a, the value P(s,a)(s’), also sometimes denoted P(s’ | s, a), is the probability of reaching the state s’ after executing a in s. In light of this, we will also refer to the transition function sometimes simply as the transition dynamics. For a given state and action, the transition is conditionally independent of all previous states and actions, which is known as the Markov property and the reason for the naming as an MDP. The value R(s,a) € R provides an immediate evaluation after performing action a in state s, which is also called the (instantaneous) reward. It is also quite possible that the instantaneous reward is 0 for some states, and one only receives a reward in specific states, for example, in so-called terminal states for which the transition function is zero. When both the state space S and the action space A are finite, we call the MDP a tabular MDP. In an MDP, an H-step trajectory 7 is a sequence of H € N\\\\{0} pairs of state-action ending in a terminal state. Formally, it is given by 7 = (80, a0, $1,41,---, 8H). Given tp > 0 and H’ < H, we can define a segment F = (Sty; Uo, $1941) to 4+1,-++, 8H\"), Which refers to a continuous sequence of steps within a larger trajectory',\n",
       "  '. Formally, it is given by 7 = (80, a0, $1,41,---, 8H). Given tp > 0 and H’ < H, we can define a segment F = (Sty; Uo, $1941) to 4+1,-++, 8H\"), Which refers to a continuous sequence of steps within a larger trajectory. A trajectory 7’s return R(r) is the accumulated (discounted) rewards collected along this trajectory: A-1 R(r) = YF 7\" R(sn,an)- ” h=0 Note that we here use the same notation for the return and the reward function, but both have different signatures (trajectory vs. state-action pair). We can also define the return R(c) of a segment o in a similar manner. The return is well defined even if the horizon H is infinite as long as y < 10 Environment Query qi Reward Model me | abeler Label |; Agent HEGscia Environment Dynamics Sate wo bjective Reward ron Agent Action az (b) RLHF with reward modeling (a) The standard RL setting. A policy specifies how to select actions in a state, either deterministically or stochastically. In the former case, a policy is simply a mapping 7 : S + A from states to actions, while in the latter, it is a mapping a: S& —» A(A) from states to probability distributions over actions. Since the deterministic case is a special case of the stochastic one, we assume the latter case in the following. The basic interaction loop is depicted in Fig. la: The agent chooses an action a; ~ 7(s;) based on its policy and the current state. As a consequence, the environment transitions into the new state si,1 ~ P(s:, at), governed by the transition dynamics',\n",
       "  '. The basic interaction loop is depicted in Fig. la: The agent chooses an action a; ~ 7(s;) based on its policy and the current state. As a consequence, the environment transitions into the new state si,1 ~ P(s:, at), governed by the transition dynamics. The agent observes this new state and the reward rz41 ~ R(s, a), after which this interaction cycle is restarted. In this setting, the RL agent aims at learning a policy that maximizes the expected return I (a) = Edo,p.x[R(7)], where the expectation is with respect to policy 7, transition function P, and initial distribution do. To solve this problem, two families of RL approaches have been considered: model-based RL and model-free RL. The methods in the first family learn a model (i-e., P, R) of the underlying MDP to help solve the RL problem, while the methods in the second directly try to obtain a good policy without learning an MDP model. The second family can be further decomposed into two main categories: value-based methods and policy search methods. In deep RL, both value functions and policies are approximated with neural networks. Value-based methods (e.g., DQN and its variants (Mnih et al., 2015; Hessel et al., 2018)) aim at learning the Q-function Q* of an optimal policy. The Q-function of a policy 7 is defined by: H-1 Qx(8,@) = Ep bs a) , h=0 where so = s, and aj = a and in the expectation, an ~ 7(- | sn) as well as s, ~ P(- | 8n—1,an—1) for h € [H — 1]',\n",
       "  '., 2015; Hessel et al., 2018)) aim at learning the Q-function Q* of an optimal policy. The Q-function of a policy 7 is defined by: H-1 Qx(8,@) = Ep bs a) , h=0 where so = s, and aj = a and in the expectation, an ~ 7(- | sn) as well as s, ~ P(- | 8n—1,an—1) for h € [H — 1]. A policy can be naturally designed from a Q-function by choosing an action in a greedy manner in each state: m(s) = argmax, Q(s,a). Note that for a deterministic optimal policy 7* it holds that I(x*) = Ea[Q*(s,7*(s))]- Similar to the action-value function Q, we can also define the state-value function H-1 V,(s) = Ep. bs 7\"R(8n; Gn) | so = | . h=0 11 ts value for some state s is the expected return when starting in that state and then always using the policy a. It is related to the Q-function by means of Vi(s) = Eawn(s) (Qx(s,4)] for any state s € S. In contrast, policy search methods directly aim at finding a good policy in some parametrized policy space. The most data-efficient algorithms in this class of methods follow an actor-critic scheme where both an actor (i.e., a policy) and a critic (i-e., usually its Q-value function) are learned at the same time. Typical represen- ative methods here are PPO , TD3 , or SAC . RL algorithms can further be classified as either on-policy or off-policy. In an on-policy algorithm, such as PPO, only the recently generated transitions are used for training',\n",
       "  '. Typical represen- ative methods here are PPO , TD3 , or SAC . RL algorithms can further be classified as either on-policy or off-policy. In an on-policy algorithm, such as PPO, only the recently generated transitions are used for training. In contrast, in an off-policy algorithm, such as DQN (or its variants), TD3, or SAC, the agent can be updated with transitions not necessarily generated by its current policy. While on-policy training is usually more stable, off-policy training enables more data-efficient learning by reusing samples from a replay buffer that stores past transitions. 2.2 Preference-Based MDPs In contrast to standard RL as described in the previous section, RLHF does not assume that a reward signal is available. It instead assumes the existence of an oracle (e.g., human labeler) that can provide information about the reward in a specific indirect manner. More precisely, in the RLHF, the agent can make queries qi to the oracle, which in practice means asking for human feedback, and in response, the agent receives a abel l;, which in general gives a hint about the reward. In principle, the query can be made asynchronously o the actual conventional RL cycle. See Fig. 1b for an illustration. In the most common setting, the oracle can compare two (segments of) trajectories, but various other cases have been considered, as we shall see later on. For the former case, RLHF is based on the setting of preference-based MDPs (Gilbert et al., 2017; Wirth et al',\n",
       "  '. See Fig. 1b for an illustration. In the most common setting, the oracle can compare two (segments of) trajectories, but various other cases have been considered, as we shall see later on. For the former case, RLHF is based on the setting of preference-based MDPs (Gilbert et al., 2017; Wirth et al., 2017), which can be defined as an MDP model without reward function, but where comparisons of trajectories are available. 2.3 Reward Learning RLHF approaches can be divided into two categories, depending on whether a utility-based approach is used ‘or reward modeling or an alternative criterion that is detached from a utility concept is used (Gilbert et al., 2016; Gilbert & Weng, 2016; Wirth et al., 2017). Most works fall into the first category, on which this overview focuses. Such approaches assume a human-dependent utility function that can be used as a reward ‘unction in order to apply standard RL methods. Next, we will describe the commonly used approach for reward learning for the common setting of binary trajectory comparisons. The prevalent approach to learning a utility function from observations of pairwise comparisons is based on the Bradley-Terry model (Bradley & Terry, 1952), which stipulates a probabilistic model for the oracle (human labeler): 1 1+ exp(R(72) — R(11))’ P(T1 + m2) = where > means “preferred to” and R(r) corresponds to the utility (i-e',\n",
       "  '., return in the context of RL) of N 1 max] 1+ exp(Ru(73) — Ry(t})) ° 12 In the context of RL, since Ry(r) = jZg 7! Ru(sn,an), (2) can then directly be used to train a function approximator (e.g., single or ensemble of neural network) to approximate R. This entire modeling approach accommodates the case of a noisy or unreliable oracle, in which case the Bradley-Terry model can be understood as the generative model of the answers from the oracle (or labels provided by the human labeler). When the oracle is reliable, more direct methods based on preference elici- tation to recover the reward function have been studied (Regan & Boutilier, 2009; 2011; Weng & Zanuttini, 2013; Gilbert et al., 2015; Sadigh et al., 2017; Wilde et al., 2018). In this survey, we will focus on the general case where the oracle may be noisy. Note that in contrast to the typical way of preference learning, the learned reward function is used to train an RL agent and not directly to compare trajectories. This discrepancy in the objective function in the reward learning part and how the learned rewards are used may lead to suboptimal policies (Lindner et al., 2021b). 2.4 Reinforcement Learning from Human Feedback In the RLHF setting as illustrated in Fig. 1b, the learning agent needs to solve an RL task without having access to a reward function. To this end, the agent usually simultaneously learns an approximation of the reward function (via the assumed utility function) and an RL policy',\n",
       "  '. 1b, the learning agent needs to solve an RL task without having access to a reward function. To this end, the agent usually simultaneously learns an approximation of the reward function (via the assumed utility function) and an RL policy. Therefore, a generic RLHF algorithm consists of repeating two phases: (1) reward learning and (2) RL training. The first phase can itself be decomposed into two main steps: (i) generate queries to ask the oracle, (ii) train a reward function approximator with the answers provided by the oracle. The RL training part is more conventional and is usually directly based on running a deep RL algorithm using the currently trained reward function approximator. Algorithm 1 Generic RLHF Algorithm in an Actor-Critic Scheme 1: Initialize parameters 6 (policy), @ (critic), and y (reward) 2: Initialize replay buffer B with randomly-generated trajectories 3: fori =1,...,N do 4: // Reward learning Generate queries from B Update D with answers to queries from the oracle Update ~ using D (e.g., to maximize Eq. (2)) // RL training Update B with new trajectories generated with 7 10: Update 6 (actor) using B and Ry ll: Update ¢ (critic) using B and Ry 12: end for on This basic generic algorithm is summarized in Algorithm 1, where an off-policy actor-critic scheme is assumed to be used for the RL training part, but other RL policy learning approaches can, of course, also be used here',\n",
       "  '. For an on-policy algorithm, such as PPO , only the recently generated transitions are used for training. For a DQN-like algorithm, lines 9 to 11 would be replaced by a loop where transitions are generated by a behavior policy based on the current estimate of the Q-function (e.g., e-greedy algorithm) and the Q network is updated using mini-batches of transitions sampled from the replay buffer D. An efficient RLHF algorithm needs to overcome several difficulties which are specific to this setting: « The oracle may provide various types of feedback (see Section 3). The questions of what information some given feedback provides and how observed feedback can be exploited need to be answered (see Section 5). ¢ Informative queries need to be generated to minimize the efforts of the oracle, which is crucial when it is a human (see Section 4). Active learning techniques (see next subsection) can be adapted to face this challenge. 13 ¢ The RL agent is actually trained in a non-stationary environment since the reward approximator is concurrently updated. The RL training part needs, therefore, to account for this factor, e.g., using non-vanishing learning rates (see Section 6). ¢ There is also the question of how the agent’s performance can be meaningfully evaluated, especially if the reward function is not known (see Section 8)',\n",
       "  '. The RL training part needs, therefore, to account for this factor, e.g., using non-vanishing learning rates (see Section 6). ¢ There is also the question of how the agent’s performance can be meaningfully evaluated, especially if the reward function is not known (see Section 8). ¢ Collecting feedback directly from humans introduces its own challenges, such as the question of a suitable user interface and the associated issues of delay between query and feedback observation, or the feedback variability and reliability (see Section 4). This may explain why many studies evaluate novel RLHF algorithms with simulated feedback. A standard RL algorithm can be run in the RL training part, as done in most previous work in RLHF (although this may not be the best approach). This suggests that any improvements in a standard deep RL method (e.g., auxiliary losses , planning in learned model , curriculum learning , or data augmentation (Laskin et al., 2020; Lee et al., 2020; Lin et al., 2020b)) may potentially be transferred to the RLHF setting. In addition, most previous work in RLHF directly uses trajectories stored in replay buffer D to synthesize queries. An interesting research direction to explore in RLHF would be to specifically generate trajectories in order to be able to synthesize more informative queries (instead of only generating trajectories that are beneficial for RL training)',\n",
       "  '. An interesting research direction to explore in RLHF would be to specifically generate trajectories in order to be able to synthesize more informative queries (instead of only generating trajectories that are beneficial for RL training). This would lead to tackle a novel exploration-exploitation dilemma: Shall we visit state-action pairs that may be bad but may help better learn the reward function, or shall we visit state-action pairs that we currently think are good? This is further discussed in Section 5.5.In RLHF, since the oracle is a human or a group of humans, reducing the number of queries is crucial to limit the labeling cost. Therefore, the reward learning part requires techniques similar to those proposed in active learning, which we recall next. 2.5 Active Learning In active learning , the task is to strategically select data points for labeling to minimize the amount of labeled data required to achieve a desired level of performance, particularly valuable in scenarios like RLHF where labeling is costly. Unlike batch learning, where labeled data is predetermined, active learning empowers the learner to actively select the most informative unlabeled instances for labeling, maximizing the learning process with limited labeled data',\n",
       "  '. Unlike batch learning, where labeled data is predetermined, active learning empowers the learner to actively select the most informative unlabeled instances for labeling, maximizing the learning process with limited labeled data. We will only briefly introduce the active learning task here and then discuss the strategies for creating informative queries considered thus far in Section For RLHF with pairwise comparisons, this setting can be formally described as follows. Suppose there is a set of N pairs of trajectories {(7{,73) | 7 = 1,...,.N}, where each pair (r/,75) can be interpreted as an unlabeled instance. To efficiently learn a reward function to explain observed pairwise comparisons, an agent can select a set of unlabeled pairs (possibly a singleton) to query an oracle to obtain their labels. At a high level, the main idea in active learning is to query data points to quickly reduce the epistemic (i.e., reducible) uncertainty about the predictions of the learned model, although other aspects can be important, such as the representativeness of the queried data points (see Wang et al. (2023a) for a survey). Two main representations are considered to describe this epistemic uncertainty: either using an ensemble of models or using a Bayesian representation. In both cases, a first basic approach selects queries using uncertainty- based criteria in order to focus on instances with high prediction uncertainty as measured by, e.g',\n",
       "  '. In both cases, a first basic approach selects queries using uncertainty- based criteria in order to focus on instances with high prediction uncertainty as measured by, e.g., variance or entropy computed over predictions. In contrast to the first approach, where the selection criteria are instance-based, a second approach considers criteria that may depend on all instances. Possible options are, for instance, expected model change, expected error reduction, or density-weighted uncertainty-based criteria. Here, the weights in the expectation or density allow us to take into account the distributional information about the instances and, therefore, to focus on the higher-density regions. 14 3 Feedback Agent A St Reward Model Feedback mechanisms are fundamental to the success o: Environment Dynamics ction ag State s¢41) Query gi _—___ Ee oton Label |; the RLHF loop discussed in this section. f any RL system. In the standard setting as described in Section 2.1, the RL agents expect feedback in the form of scalar immediate rewards. These rewards are most commonly determined by a hand-engineered reward function, which can be used to evaluate any state-action combination. As discussed in Section 1',\n",
       "  '. In the standard setting as described in Section 2.1, the RL agents expect feedback in the form of scalar immediate rewards. These rewards are most commonly determined by a hand-engineered reward function, which can be used to evaluate any state-action combination. As discussed in Section 1.1, it is desirable to allow humans to refine behavior interactively through feedback instead of requiring the While a human could, in principle, assign rewards to e role of the reward function, this is usually impractical effort required to provide rewards on a sufficiently reg to that, directly integrating human rewards into the which impedes the learning pace while waiting for hum: numeric state-action rewards, which is challenging to In contrast to directly rewarding each of the agent m to pre-specify a reward function. ach of the agent’s actions directly, thereby taking the ‘or multiple reasons. The main challenge is the human ular basis, i.e., at least once per episode. In addition RL loop would require these rewards immediately, an feedback. Finally, the standard RL setting expects rovide in a consistent manner. ’s actions, RLHF as discussed in this survey (see Section 1.3) harnesses indirect and asynchronous feedback methods. Such methods avoid the challenges of immediate numeric rewards and are also better al improved learning progress and human user experience',\n",
       "  '. ’s actions, RLHF as discussed in this survey (see Section 1.3) harnesses indirect and asynchronous feedback methods. Such methods avoid the challenges of immediate numeric rewards and are also better al improved learning progress and human user experience. igned with human interaction patterns, resulting in A feedback type is a kind of interaction in which a human conveys some information about their preferences. Examples include pairwise comparisons and direct cri iques. This section is concerned with the many ways in which human feedback can be expressed and used. Several previous works have already studied and sorted feedback types by listing the most common ones (Jeon et al., 2020; Yuan et al., 2024) and discussing their attributes and dimensions (Metz et al., 2023; Lindner & El-Assady, 2022). The attributes and classes described in this section build upon this prior work and can be considered a synthesis and extension of it. The remainder of this section will start by discussing relevant attributes of feedback types that can be used to classify them (Section 3.1). We will then discuss c common classes and examples of interactive feedback types (Section 3.2) as well as some non-interactive types that can serve as initializations (Section 3.3). 3',\n",
       "  '.1). We will then discuss c common classes and examples of interactive feedback types (Section 3.2) as well as some non-interactive types that can serve as initializations (Section 3.3). 3.1 Attributes of Feedback Types Feedback types may differ on many dimensions, some of which relate to the way feedback is given (arity, involvement), others to the form of the query instance i it is given on (granularity, abstraction), and yet others to features of the human interaction (intent, explicitness). The attributes we discuss in this section are based on the framework proposed by Metz et al. (2023). We have adjusted and added terminology where i clarity, generalized the distinction between relative and of co-generative involvement and literal intent. Furthermore, we systematically analyze a set of exemp feedback types in the next section, expanding on the ini by Metz et al. (2023). In the following, we will introduce each of the six attributes in more detail. Arity This attribute describes whether a single instan aids absolute feedback to ‘arity’, and added the categories ary itial examination of a smaller range of abstract classes ce is evaluated in isolation (unary) or relative to other instances (binary, n-ary). Unary feedback is often convenient for detailed and descriptive feedback but lacks any grounding and therefore puts feedback',\n",
       "  '. Unary feedback is often convenient for detailed and descriptive feedback but lacks any grounding and therefore puts feedback. Non-unary feedback always has an a great burden on the human to provide consistent implicit grounding but relates to the instances being comparable. While n-ary feedback, such as rankings, can provide more information than binary fee back, it also puts a higher cognitive burden on the labeler. Involvement The labeler may either passively observe an instance, actively generate it, or coactively par- Granularity Feed tici labelers since it does not require the ability to demonstra at the most informative examples wi h active learning tec! ate in its generation (co-generation). Passive involvement poses the smallest challenge to the e the task. It can also easily be directed hniques. Unfortunately, passive feedback often cannot match the information density of active feedback. It is, therefore, common to combine bot. then refine it from passive a human can share contro: makes it possible to direc with the the human’s attention to the h types to first initialize the reward model from (possi eedback. Between these two extremes is co-generative feedback, in which ess demanding than active feedback and most informative samples, but it is still agent. This more taxing than purely passive involvement. differ back may also from whole episode recor ings over on t',\n",
       "  '. Between these two extremes is co-generative feedback, in which ess demanding than active feedback and most informative samples, but it is still agent. This more taxing than purely passive involvement. differ back may also from whole episode recor ings over on t. artial segments to bly very suboptimal) can be active feedback and he granularity of the instances being evaluated. This ranges eedback on individual steps (i.e., states, actions, or state-action pairs). A more coarse-grained granularity has the advantage of giving humans more context and getting problems. is often impractica as demonstrated by Guan queries within an episode that we only classi: is compatible with paper uses entire e or te pisodes. Finer-grained feedback is much easier to learn ious for humans to provide. I et al. (2021) who propose to use queries on step granularity, but batch 0 provide additional context and make it easier to give feedback. Note fy a type of feedback as “episode” granularity if it requires entire episodes. If it partial segments as well, we classify it as “segment” even if is also possible to s Abstraction This describes whether feedback is given directly on raw instances, e.g., (see granu easier to for a human to ma that this features used as in (see invo Explicitness Humans may communica Intent of actions directed at other pur information is often much more erences. The assumed human intent can uts for the reward model. Ty poses',\n",
       "  '.g., (see granu easier to for a human to ma that this features used as in (see invo Explicitness Humans may communica Intent of actions directed at other pur information is often much more erences. The assumed human intent can uts for the reward model. Ty poses. While exp readi e important for instructive, or descriptive in their explicit feedbac! feedback. Evaluative, instructive, an descriptive a reward function, whereas literal feedback is a by] reward function directly. Descriptive a partial reward function. The context of a particular instance The distinction between litera They argue that humans with arity) or on abstract features of the instances. While feature-level learn from, extracting useful features is challenging. In some contexts, i ke abstract judgments rather than more intuitive instance-level judgments. Note always refers to the level of abstraction that the user sees, icit information is easiest to eedback for larger sections of behavior but also poses credit assignment rom and simplifies credit assignment, but rike a compromise, he discussed source behavior recordings information can be may also be harder which may differ from the pes of feedback that depend on active generation lvement), such as improvements, generally work on a raw instance level. e explicitly for the purposes of feedback or implicitly as a side-effect learn from, implicit y available and can possibly communicate more detailed pref- eedback processing',\n",
       "  '. e explicitly for the purposes of feedback or implicitly as a side-effect learn from, implicit y available and can possibly communicate more detailed pref- eedback processing. A human may be evaluative, , while they are generally literal in their implicit eedback is pedagogical in nature, aiming to teach product of a human actor’s efforts to optimize the feedback is often given on the level of the task, e.g., through other types of intent, in contrast, are generally given within the of be havior. and pedagogical feedback was introduced by Milli & Dragan (2020). edagogical intent (i.e., evaluative, instructive, or descriptive) may act differently compared to humans with literal intent. Even though they find that assuming the (wrong) literal intent can still lead to know this intent to choose the right human model (see Section 5.1.5). 3. better reward inference, it still indicates that it can be important to Even though the concrete types of feedback used in the literature are rarely exactly the same, they can generally be sorted into a set of common classes. We will describe a selection of those classes, their defining 16 be used to learn a representation prior to learning a reward model. When * is specified for an attribute, this indicates that it is not a defining feature of the class and may vary in different instantiations. Class Granularity Involvement Arity Abstr. Intent Expl',\n",
       "  '. When * is specified for an attribute, this indicates that it is not a defining feature of the class and may vary in different instantiations. Class Granularity Involvement Arity Abstr. Intent Expl. Critique + Observe Unary + Evaluative — Explici Comparisons + Observe 2+ + Evaluative — Explici e Inter-Temporal Segment: Observe Unary + Evaluative — Explici = 4 Proxy Rewards Episode Observe + Feature Descriptive Explici & | Social Behavior Segment: Observe Unary Instance — Litera Implici Improvements Episode Co-generative Unary Instance + Natural Language Observe Unary + Descriptive Explici a, J E-Stops Episode Observe Unary Instance — Litera Implici BR Importance + Observe + + Descriptive Explici a Feature Traces Segment: Active Unary Instance Descriptive Explici cc | Similarity Queries + Observe Ternary Descriptive Explici attributes, and examples of concrete instances in the literature in the following. Table 4 gives an overview of the classes and their attributes as described in Section 3.3.2.1 Primary Feedback Classes This section introduces common feedback classes which can be used on their own to learn a reward model. The classes are critique, comparisons, inter-temporal feedback, proxy rewards, social behavior, improvements, and natural language. Critique Critique is arguably the most direct type of feedback. In this setting, the human expresses their preference by directly critiquing an instance of agent behavior, often in the form of binary feedback',\n",
       "  '. Critique Critique is arguably the most direct type of feedback. In this setting, the human expresses their preference by directly critiquing an instance of agent behavior, often in the form of binary feedback. Note that critique, as considered in this survey, is distinct from directly supplying a reward signal since it is given in an asynchronous and indirect manner (see Section 1.3). The defining features of critique are that the human passively observes the behavior (involvement), gives feedback on a single instance (arity), and does so explicitly (explicitness) with an evaluative intent. The feedback may be given for any granularity and on any level of abstraction. There are many examples of critique feedback in the literature. Xiao et al. (2020) employ binary feedback on individual state and action pairs. Although they learn a shaping reward that complements an environment reward signal, the same technique could be used without environment reward. Huang et al. (2023) extend his to multi-label feedback, allowing the user to distinguish between a regular good or bad action and a erminal action that achieves the goal or fails to do so. They map these classes to scalar values and then learn a reward model by regression. Wang et al. (2020) present an approach to learning a reward model from noisy critiques in the form of human physiological signals (brain signals) using active querying. In contrast to this action-level feedback, Fu et al. (2018b); Singh et al',\n",
       "  '. Wang et al. (2020) present an approach to learning a reward model from noisy critiques in the form of human physiological signals (brain signals) using active querying. In contrast to this action-level feedback, Fu et al. (2018b); Singh et al. (2019) rely on binary outcome success labels. Fu et al. (2018b) introduce the basic approach, which Singh et al. (2019) extend by moving to an off-policy setting and including online queries, thereby reducing the reliance on many positive examples by interactively correcting false positives. n addition to learning the main reward function, critique can also be used for safety evaluation. Cosner et al. (2022) train a secondary reward model focused on safety from binary action critiques. This is in addition o the main reward model, which is trained from comparisons in their approach. Note that this secondary 17 safety model could, in principle, be trained with any of the feedback types discussed here, using methods identical to the ones used for reward learning. Comparisons Binary comparisons and rankings are among the most common types of feedback. The defining features of comparisons are that the human passively observes the behavior (involvement), gives relative feedback on multiple instances (arity), and does so explicitly (explicitness) with an evaluative in- tent. It is most commonly given on a segment (granularity), but other granularities are possible in princi- ple',\n",
       "  '. It is most commonly given on a segment (granularity), but other granularities are possible in princi- ple . Similarly, comparisons are commonly requested on an instance level (abstraction), but this is not a requirement. Comparisons were first used for direct policy learning (Akrour et al., 2011; Cheng et al., 2011), but were later extended to the reward-learning setting (Wirth et al., 2016; Christiano et al., 2017). The most com- mon setting relies on pairwise comparisons of trajectory segments, but comparisons of individual states or actions were also considered in early PbRL works and com- parisons can even be extended to more abstract trajectory features . This basic setting has been extended and modified in various ways. To reduce noise in the labels, it is common to extend the binary choice by giving the labelers the option to avoid the hard choice and in- stead indicate incomparability, uncertainty, or perceived similarity . This option is c commonly interpreted as “equally preferable”, e.g., as if each trajectory had an equal probability of being preferred in the preference predictor. It is alco common, however, to additionally provide an “incomparable” option which results in simply omitting the query from the data set (Christiano et al., 2017; Ibarz et al., 2018). In contrast to this, Verma et al. (2023) explicitly state that they do not allow for the equally preferred option, arguing that these cases are rare enough not to matter very much',\n",
       "  '., 2017; Ibarz et al., 2018). In contrast to this, Verma et al. (2023) explicitly state that they do not allow for the equally preferred option, arguing that these cases are rare enough not to matter very much. Another line of research suggests that more precision in the expression of pairwise preferences, such as softening the hard binary choice to scalar feedback indicating the strength of a preference , can be beneficial for preference learning. Other extensions change the pairwise setting to choices among larger choice sets or even full rankings (Myers et al., 2021; Brown et al., 2020; Ouyang et al., 2022). Jain et al. (2015) compare different kinds of re-ranking feedback (select first which is better than top, choose best from top-5, choose best from random set). Their study suggests that the first two are roughly equivalent, while the latter is much less informative. Askell et al. (2021) evaluate binary comparisons as well as ranking, but finds that ranking performs better. Ziegler et al. (2020) note that for language tasks, a larger choice set can amortize he time needed for a labeler to get acquainted with the context necessary to understand a query. Basu et al. (2019) propose to use hierarchical queries, i.e., a sequence of pairwise comparisons that build up on each other. Pairwise comparisons are generally easier to supply than ratings, even when the feedback is given by a ‘oundation model (Wang et al., 2024b)',\n",
       "  '. Basu et al. (2019) propose to use hierarchical queries, i.e., a sequence of pairwise comparisons that build up on each other. Pairwise comparisons are generally easier to supply than ratings, even when the feedback is given by a ‘oundation model (Wang et al., 2024b). While absolute feedback is generally dependent on a policy serving as an implicit baseline , the explicit baseline in pairwise comparisons can be more easily controlled. Therefore, comparisons provide many benefits over absolute ratings, such as reduced bias, inconsistencies, and subjectivity (Yannakakis & Martinez, 2015). On the flip-side, comparisons convey relatively little information per label. Tien et al. (2023) study the weaknesses of pairwise-comparison-based reward learning. Since the amount of information provided for each label is small, these models are prone ‘o causal confusion, i.e., misattributing reward to noise and misidentifying the reward function. Inter-Temporal Feedback One limitation of trajectory comparisons is that they require a set of roughly comparable trajectories. In many real-world environments, starting conditions or even the agent’s current task may vary between episodes. In these cases, it is hard for human labelers to compare trajectories from these episodes, limiting the usefulness of comparison feedback. One way to remedy this limitation is to provide feedback within a single trajectory',\n",
       "  '. In these cases, it is hard for human labelers to compare trajectories from these episodes, limiting the usefulness of comparison feedback. One way to remedy this limitation is to provide feedback within a single trajectory. Instead of comparing a set of instances with each other as in regular comparative feedback, inter-temporal feedback conveys relative judgments over different states in time within a single instance. The defining features of inter-temporal feedback are that it is given explicitly (explicitness) on segment (granularity) while passively observing (involvement) a single instance (arity) of the agent’s behavior with evaluative intent. It is most commonly given on raw instances, but any level 18 of abstraction is possible in principle. There are two main ways to convey this feedback: Reward sketching and inter-temporal preferences. Reward sketching, as introduced by Cabi et al. (2020), involves users sketching a visual representation of the reward function over time. This type of feedback, which can be given by sketching a graph with the mouse while watching a behavior recording, provides intuitive, per-timestep reward annotations. Rahtz et al. (2022) also adopted this approach, referring to it as “one of the highest-bandwidth feedback mechanisms currently available”. Inter-temporal preferences were introduced by Abramson et al. (2022)',\n",
       "  '. Rahtz et al. (2022) also adopted this approach, referring to it as “one of the highest-bandwidth feedback mechanisms currently available”. Inter-temporal preferences were introduced by Abramson et al. (2022). In this setting, humans give feedback on multiple points of a trajectory, indicating whether an agent makes progress towards or regresses from a goal. This is then interpreted as preferences relative to the other labeled and unlabelled points. The authors note that one potential downside of this feedback type is that labelers may tend to give preferences on short-term actions that are easy to judge, failing to communicate long-horizon preferences. Cui & Niekum (2018) propose a similar type of feedback, in which humans segment a trajectory into good and bad parts. This makes it possible to derive many state-action labels from a few segmentation points. Proxy Rewards Proxy rewards are partial or inaccurate reward functions that convey information about the task the agent is supposed to complete but may not induce optimal behavior. This form of feedback does not generally refer to any particular behavior instance but instead gives global direction for the entire task. However, in line with our selection criteria (Section 1.3), we only consider proxy reward feedback that is interactive and online within the context of one or multiple observations to fill holes in the initial description',\n",
       "  '. However, in line with our selection criteria (Section 1.3), we only consider proxy reward feedback that is interactive and online within the context of one or multiple observations to fill holes in the initial description. The defining features of proxy reward feedback is that the labeler passively observes the agent’s behavior (involvement) and gives feedback explicitly (explicitness) on a feature-level (abstraction) with descriptive intent (intent). Proxy reward feedback may be given with respect to a single or multiple instances (arity), although it generally refers to multiple instances. It is most commonly given on an pisode (granularity), but other granularities are possible in principle. @ The work by He & Dragan (2021) exemplifies this form of feedback by using proxy reward functions, i.e., preliminary reward functions that might not cover all edge cases, to guide the agent toward learning the actual reward function. Alternatively, Mindermann et al. (2018); Hadfield-Menell et al. (2017b) suggest querying about the reward function. They allow users to choose from a set of understandable, linear proxy rewards or to specify which features are more critical in the linear reward structure. In a related setting, Guan et al. 2023) lets the user specify changes to a current symbolic reward function in the form of changes to target attributes (e.g., walking speed)',\n",
       "  '. In a related setting, Guan et al. 2023) lets the user specify changes to a current symbolic reward function in the form of changes to target attributes (e.g., walking speed). Social Behavior Humans give rich implicit social feedback in the form of facial reactions and gestures when interacting with agents. The defining attributes of this type of feedback are that it is given implic- itly (explicitness) on passively observed (involvement) segments (granularity) with respect to a single instance (arity) and literal intent. Cui et al. (2021) propose a framework to learn reward functions from such social behavior. They suggest a two-phase training setup. In the first phase, they ground the implicit feedback by use of incentives, i.e., they incentivize humans to have a known objective. After learning a mapping from feedback to reward, they use regular RL techniques to learn a policy. Note that the learned reward function can be seen as conditional on human implicit feedback and, therefore, they require a human in the loop throughout training. Improvements Improvements are a form of feedback in which the human improves on the agent’s behavior, either by intervening as the agent acts or by providing a corrected behavior after the agent acts. To improve an episode, it is usually necessary to observe the entire episode (granularity) at the instance level (abstraction). In this type of feedback, the human both observes and demonstrates behavior, resulting in co-generative involvement',\n",
       "  '. To improve an episode, it is usually necessary to observe the entire episode (granularity) at the instance level (abstraction). In this type of feedback, the human both observes and demonstrates behavior, resulting in co-generative involvement. Improvements generally relate to a single reference trajectory being improved (unary arity), although an improvement could also be interpreted as a binary comparison between the improved and the non-improved trajectory. Improvements are most commonly provided explicitly with instructive intent. 19 We distinguish between post-fac o improvements, calling them corrections, and improvements made while the agent is acting, calling them interventions. The key difference is that the uncorrected trajectory is available in the case of corrections, while it can only be estimated in the case of interventions. Interventions can be considered to user share autonomy to reach a common goal learn form the occurrence of an int Luo et al. (2024) by directly inferring nega‘ and the other is to learn from the data is then used for behavior c not directly used as feedback. In formats: Either by overtaking cont agent’s movements. Losey et al. (2 method that was later extended (Losey & O’Malley, 2018) risk-sensitive deployment. Li et al. physical corrections without needing to wait until the trajectory is comp! be an ins ervention itself that the prior ive rewards from inter corrected trajectory. The la‘ ntify challenging situations reward model training',\n",
       "  '. Li et al. physical corrections without needing to wait until the trajectory is comp! be an ins ervention itself that the prior ive rewards from inter corrected trajectory. The la‘ ntify challenging situations reward model training. T addition rol from t (022) proposes to learn o the above distinction, a reward mo to incorpora his setting t (2021) further extend ance of the shared autonomy setting since the agent and the . There are two main ways to leverage interventions: One is to behavior was suboptimal, as done, e.g., by ventions without learning a reward model, ter set: (2022), who ask humans to intercede on agent failure. They use this where the agent is the weakest and to ide: oning an ing is studied by Abramson et al. o collect targeted demonstrations for their evaluations. The gathered he fact that a correction occurred is interventions can come in two main he agent to correct its behavior or by physically correcting the del from such physical corrections, a te uncertainty for active learning and o learn from a sequence of correlated eted. The correction case is closely related to the setting of coactive learning (Shivaswamy & Joachims, 2015), in which a learning system repeatedly proposes a solution which a user may correct to reach a common goal. Jain et al. (2015) treat corrections as a demonstration whi le Jeon et al',\n",
       "  '. The correction case is closely related to the setting of coactive learning (Shivaswamy & Joachims, 2015), in which a learning system repeatedly proposes a solution which a user may correct to reach a common goal. Jain et al. (2015) treat corrections as a demonstration whi le Jeon et al. (2020) propose (but do not evaluate) an alternative interpretation of inferring implicit preferences from comparisons by assuming the corrected trajectory is preferred over the original one. Natural Language Natural language is a versatile form of feedback that can be used to convey a wide range of information. It cannot on! ly be used to express preferences, but also to suggest concrete changes. Natural language feedback may be given on any granularity, at any level of abstraction. Its defining features are that it is given explicit While much wor! instructions in a non-interactive se language is used as feedback, not natural language. As an example o: where one agen’ Language may be interpreted as a policy (involvement) with descrip learning a language-conditioned policy, regardless of whether or not (responsible for solving problems) is controlled by t (responsible for setting tasks) is controlled by the human. ly (explicitness) in the context of tive intent. ting (Hermann et al., 2017; Nair e or task definition. Note, however, this, Abramson et al. (2022) use R. orm of feedback in the reward-ratio where an utterance implies a set o: can choose an w human to provide 2022b)',\n",
       "  '. ly (explicitness) in the context of tive intent. ting (Hermann et al., 2017; Nair e or task definition. Note, however, this, Abramson et al. (2022) use R. orm of feedback in the reward-ratio where an utterance implies a set o: can choose an w human to provide 2022b). An alternative way rom the language. rom language feed Language feedback is demonstrated in language feedback terance that maximizes the probability of a desired act specific which leads to pragmatic reasoning). This can be used to infer he utterances they did provide. A similar approac! f trajectories compatible with the w a single (arity) observed behavior or k in language for RL focuses on the problem of learning a policy that follows natural-language al., 2021), we focus on works where that RLHF can be a useful tool for the feedback itself is in the form of LHF in an interactive agents setting, he learned policy and another agent nal choice setting , terance (grounding) and the human ion (ie., the human is assumed to be a reward which may have caused the 20 h is followed, e.g., by (Sumers et al., of incorporating language feedback is by extracting more structured forms of feedback An example is to use sentiment analysis to extract information about the reward function ack, as proposed by Sumers et al. (2021). can also be interpreted by an LLM directly, without learning a reward model. This the concurrent works by Ma et al. (2024) and Xie et al',\n",
       "  '. (2021). can also be interpreted by an LLM directly, without learning a reward model. This the concurrent works by Ma et al. (2024) and Xie et al. (2024) who propose to learn a symbolic reward model in the form of a language-model written piece of (Python) code. They use natural o improve the reward model based on observations of the agent’s behavior induced by he previous version of the reward model. In addition to the way language feedback is used, the way it is elicited can also have an impact on the learning process. Sumers et al. (2022a) study the relative merits of natural language instructions and descriptions of the desired outcome. They find that instructions tend to work better for low-autonomy settings, while descriptions are more effective in high-autonomy settings. 3.2.2 Supplementary Classes Merge this with representation-specific, highlight distinction direct feedback about preferences vs feedback about representation in section introduction This section introduces two feedback classes, e-stops and importance, that can be used in conjunction with a primary class to learn a reward model. These classes are not sufficient to learn a reward model on their own, but can supplement a primary feedback type. E-Stops Emergency stops (e-stops) are an active type of feedback. In this type of feedback, the human may intervene with the agent’s behavior by stopping it, i.e., they may choose to stop the agent’s current trajectory at any point',\n",
       "  '. E-Stops Emergency stops (e-stops) are an active type of feedback. In this type of feedback, the human may intervene with the agent’s behavior by stopping it, i.e., they may choose to stop the agent’s current trajectory at any point. This is closely related to interventions but, in contrast o those, e-stops do not suggest an alternative action. The defining features of e-stops are that the human oth passively observes the agent’s behavior (involvement), gives absolute feedback on a single instance (arity) on the instance level (abstraction) and does so implicitly (explicitness) as a side-effect of regular interaction. The intent is literal due to the implicit nature. For the purposes of intervention, the human usually observes the full episode (granularity). Due to the small amount of infrequent information they rovide, e-stops should only be considered as a supplementary feedback type. E-stops are primarily intended to prevent bad behavior and only implicitly convey information about the correct behavior. This interaction and the arising incentives have been formalized in the form of the “off switch game” by Hadfield-Menell et al. (2017a). Jeon et al. (2020) propose to interpret this as a form of reward-rational feedback, where the ‘off’ choice maps to the trajectory with the robot remaining still after he off switch has been triggered (see Section 5.1.1). Kahn et al. (2021) demonstrate that a robot can learn o navigate using such feedback',\n",
       "  '. (2020) propose to interpret this as a form of reward-rational feedback, where the ‘off’ choice maps to the trajectory with the robot remaining still after he off switch has been triggered (see Section 5.1.1). Kahn et al. (2021) demonstrate that a robot can learn o navigate using such feedback. Importance Another form of supplementary feedback may come in the form of importance labels, com- municating which parts of the observation are important for the objective. Its defining features are that he importance information itself does not contribute towards generating behavior samples (observed in- volvement), is of descriptive intent, and is given explicitly (explicitness). Granularity, arity, and abstraction may vary depending on the primary feedback type. Since importance feedback needs a base ask with respect to which the importance is defined, it cannot be used on its own but is rather a supple- mentary type of feedback. One way to convey this information is by labeling salient parts of a visual input. This is explored by Guan et al. (2021), who augment pairwise comparisons with manually annotated visual saliency maps, in- forming the algorithm which parts of the visual input contributed to the decision. They leverage these an- notations for data augmentation by assuming that random perturbations to irrelevant (non-salient) regions do not impact the human preferences. Basu et al',\n",
       "  '. They leverage these an- notations for data augmentation by assuming that random perturbations to irrelevant (non-salient) regions do not impact the human preferences. Basu et al. (2018) take an even more direct approach by combining comparative feedback with direct feature queries, i.e., asking the user which feature is important for inferring he reward. 3.2.3 Representation-Specific Classes While the previous classes of feedback types are all aimed at directly learning a reward function, there are also classes of feedback types that do not directly learn a reward function but rather help to learn a better representation. Feature Traces While many approaches either rely on hard-coded features or learn a model entirely end- to-end, it is also possible to actively elicit new features from human feedback. Feature traces were proposed by Bobu et al. (2022) as an approach to actively learn new relevant features. This type of feedback relies on 21 a human operator to demonstrate a behavior in which a certain feature of interest, such as the distance to a sensitive object, monotonically increases or decreases. They make it possible to extend the set of features once the current set can no longer adequately explain the human feedback supplied through another type of feedback',\n",
       "  '. They make it possible to extend the set of features once the current set can no longer adequately explain the human feedback supplied through another type of feedback. The defining characteristics of feature traces are that they are of descriptive (intent) and explicitly (explicitness) given in an active manner (involvement) for a single (arity) segment (granularity) on an instance-level abstraction. Feature traces are strongly related to inter-temporal preferences (Section 3.2.1) since both types rely on changes in feature or reward values in the course of a single trajectory. Bobu et al. (2022) propose to learn from feature traces by leveraging a Bradley-Terry model to learn the feature values, similar to other approaches that use such a model to learn reward values. Similar to importance feedback, feature traces rely on another type of feedback to actually make use of the learned features and is, therefore, a purely supplementary form of feedback. For instance, Bobu et al. (2022) use intervention feedback to train a reward model on the set of features derived using feature traces. Similarity Queries Similarity queries are a feedback type aimed at learning a representation conforming to a notion of similarity and difference in the trajectory space. That aim is closely aligned with that of feature queries, though the actual queries are more similar to comparisons',\n",
       "  '. Similarity Queries Similarity queries are a feedback type aimed at learning a representation conforming to a notion of similarity and difference in the trajectory space. That aim is closely aligned with that of feature queries, though the actual queries are more similar to comparisons. The queries consist of triples of trajectories, with one anchor and two alternatives, for which the human has to decide which pair is more similar. Responses to similarity queries are given on observed behavior (involvement) with ternary arity, descriptive intent, and explicit feedback, while the granularity and abstraction may vary. This type of feedback was first introduced by Bobu et al. (2023), who used it to learn representations for reward learning. 3.3 Initializations Some modes of communicating reward functions are not interactive nor online and, therefore, do not directly fit within the scope of this survey (Section 1.3). Since these are often used to initialize a reward function for later refinement with some of the previously discussed interactive feedback types, they are still worth mentioning. Initializations are most commonly given by examples of successful task completions, either in the form of terminal or goal states , expert demonstrations (Ibarz et al., 2018; Fu et al., 2018a; Palan et al., 2019; Lee et al., 2021b; Buyik et al., 2022a; Abramson et al., 2022; Huang et al., 2023), labeled demonstrations , or ranked demonstrations',\n",
       "  '., 2018; Fu et al., 2018a; Palan et al., 2019; Lee et al., 2021b; Buyik et al., 2022a; Abramson et al., 2022; Huang et al., 2023), labeled demonstrations , or ranked demonstrations . It is even possible to infer some human preferences by the state of the environment alone, e.g., assuming the parts of the initial environment that are under the user’s control are largely set up in accordance with the user’s preferences (Shah et al., 2019; Lindner et al., 2021a). Since offline initialization is not the main focus of our survey, we do not cover these works in detail. We refer the interested reader to literature on inverse RL (Arora & Doshi, 2021) for further details on learning from demonstrations in particular. 3.4 Choice of Feedback Type The best feedback type is not always clear and may depend on the task, the user, or the agent. It may also change over time as the agent learns more about the task. Table 4 may serve as a starting point to select a set of feedback types which may be applicable based on the possible user interaction and expertise, e.g., y the desired granularity, level of abstraction or involvement. This choice may be informed, among other eatures, by the task complexity or time horizon: Jain et al. (2015) compare purely passive (re-ranking based) with active (slight trajectory improvements) feedback and conclude that the former is usually preferred by humans when the action space is large while the latter is preferred for complex tasks. Similarly, Sumers et al',\n",
       "  '. (2015) compare purely passive (re-ranking based) with active (slight trajectory improvements) feedback and conclude that the former is usually preferred by humans when the action space is large while the latter is preferred for complex tasks. Similarly, Sumers et al. (2023) empirically demonstrate that language is a more effective teaching modality than demonstrations for complex concepts. The relevance of the time horizon is highlighted in the work by Sumers et al. (2022b), which suggests that instructive feedback is more efficient for short time horizons and descriptive feedback or longer horizons. n addition to these static choices, Section 4.1.2 will discuss how to choose a feedback type adaptively. It is also possible to let the user choose the feedback type, as demonstrated by Jain et al. (2015) who let the user 22 choose between re-ranking and improvement. Jeon et al. (2020) propose to use this choice of feedback type itself as a source of information (“meta-choice”). 3.5 Combination of Feedback Types In addition to using any of the previously described feedback types in isolation, combining multiple feedback types is both possible and often advantageous. There are three main ways to combine feedback types: (a) a two-phase setup, consisting of initialization and refinement, (b) integrating a primary feedback type with a supplementary one; and (c) merging multiple primary feedback types',\n",
       "  '. There are three main ways to combine feedback types: (a) a two-phase setup, consisting of initialization and refinement, (b) integrating a primary feedback type with a supplementary one; and (c) merging multiple primary feedback types. The two-phase setup can be used to either initialize the reward model from offline data or to learn a representation that improves later reward learning. A common approach involves using one feedback type, typically demonstrations (see Section 3.3), to initialize the reward model, subsequently fine-tuning this model with another type, such as comparisons. This method is exemplified by Ibarz et al. (2018), who combined demonstrations and pairwise comparisons. For a more detailed discussion on feedback types suitable for initialization, refer to Section 3. Combining a primary feedback type with a supplementary one can be beneficial to make the most out of the available user-interactions. Supplementary feedback, while not sufficient for reward model training by itself, can often be collected cheaply or as a side-effect of other interactions, making it a valuable addition to the rimary feedback type. Refer to Section 3.2.2 for a discussion on supplementary feedback types. Finally, combining multiple primary feedback types can be beneficial to capitalize on the strengths of each. For instance, Koppol et al',\n",
       "  '. Refer to Section 3.2.2 for a discussion on supplementary feedback types. Finally, combining multiple primary feedback types can be beneficial to capitalize on the strengths of each. For instance, Koppol et al. (2020) combine informative and demanding queries with less-informative and less-demanding ones to achieve a balance between cognitive load and informativeness. Similarly, to en- hance expressivity, Mehta & Losey (2023) combines demonstrations, pairwise comparisons, and corrections, allowing users to select their preferred type of feedback. 4 Label Collection Environment Agent Reward Model Action az Dynamics $4 State sei Query gi Labeler Label |; In this section, we explain how preference data can be collected for training a reward model, independent of the specific type of feedback used. We start by overviewing the active learning problem posed by query selection, e.g., how queries can be generated for a given query type, and how even the type of feedback itself can be selected. Following this, we discuss issues arising in such human-computer interaction. 4.1 Active Learning We first discuss query generation and selection for a given feedback type, then the extension of these tech- niques to the choice of feedback types. 23 References Factors Uncertainty On-policy Query Trajectory Query Query Data Simplicity Quality Diversity — Cost Daniel et al',\n",
       "  '. 4.1 Active Learning We first discuss query generation and selection for a given feedback type, then the extension of these tech- niques to the choice of feedback types. 23 References Factors Uncertainty On-policy Query Trajectory Query Query Data Simplicity Quality Diversity — Cost Daniel et al. (2014) Probability of improvement x x x x x Expected improvement x x x x x Upper confidence bound x x x x x Christiano et al. (2017) Ensemble variance x x x x x Sadigh et al. (2017) Volume removal x x x x x Biyik et al. (2024) Volume removal x x x v v Wilde et al. (2018) Feasible space reduction x x v x x Ibarz et al. (2018) Random x x x x x Mindermann et al. (2018) Information gain x x x x x Cui & Niekum (2018) nformation gain v x v x x Racca et al. (2019) Entropy x v x x x Biyik et al. (2019) Mutual information x v x x x Biyik et al. (2020) nformation gain x x x x x Reddy et al. (2020) Ensemble averaged KL- x x v v x divergence to mean output x x x x x Novoseller et al. (2020) Dueling posterior sampling x x v x x Wilde et al. (2020; 2021) Vlaximum regret x x x x x Lee et al. (2021b) Ensemble-averaged entropy x x x x x Lindner et al. (2021b) nformation gain v x x x x Katz et al. (2021) Posterior sampling x x v x x Myers et al. (2023) Expected value of information x x x x x Biyik et al. (2024) Mutual information x x x v x Dwaracherla et al. (2024) Double Thompson sampling x x v x x Hu et al. (2024) Random v x x x x 4.1',\n",
       "  '. (2021b) nformation gain v x x x x Katz et al. (2021) Posterior sampling x x v x x Myers et al. (2023) Expected value of information x x x x x Biyik et al. (2024) Mutual information x x x v x Dwaracherla et al. (2024) Double Thompson sampling x x v x x Hu et al. (2024) Random v x x x x 4.1.1 Query Generation and Selection One core problem that needs to be tackled in RLHF is that of learning about the human’s preferences. This problem shares some simi arities with the active learning setting since the agent can actively query a human teacher about those preferences. However, in contrast to standard active learning, which usually assumes a supervised learning set ing, in RLHF, the agent needs to solve this pro lem in the context of RL. This means that it can both influence the distribution of the data (i.e., the transitions) and decide which data should be labeled. As the RL agent is trained and its learned policy changes, the trajectories it generates will naturally evolve. Most work directly uses natively, the agent can a! RL training), possib! Liu et al. (2023b)). This Algorithm 1) so tha via a criterion, usual. acquisition function y with a learned transi ind of active generation of queries can possibly | n order to efficiently learn a suitable reward it can quickly learn a existing work in RLHF uses an acquisition he trajectories ob ained during RL training for rajectories, candidate queries are often randomly generated',\n",
       "  '. However, efficient approach is to generate queries by lso generate trajectories specifically to be used ion model to limit the sampling cost (e.g., Reddy et al. (2020); model, the agent must genera good strategy using those queries. This selection is performed y called acquisition function, which allows the queries to be compared. Although most ‘unction that provides some measure of uncertainty (e.g., about he learned rewards), which is arguably one of the most important factors in active learning, an efficient (see Table 5 for works dedicated to improving query selection) may need to include references learning. Using such or pairwise comparison, a more exploiting preference transitivity . Alter- ‘or querying (not necessarily for ead to more informative ones. e and select queries (Line 5 from various additional other aspects, such as: on-policy data, query simplicity, trajectory quality, query diversity, 1 We follow the terminology used in Bayesian active learning. 24 or query cost, which will be discussed one by one in the following. As a side note, interestingly, as highlighted by Habibian et al. (2022), the queries asked by an RL agent may also reveal its current reward learning stage. Uncertainty This factor usually corresponds to epistemic uncertainty (Hiillermeier & Waegeman, 2021), which represents how uncertain the agent is about the ground-truth reward function',\n",
       "  '. (2022), the queries asked by an RL agent may also reveal its current reward learning stage. Uncertainty This factor usually corresponds to epistemic uncertainty (Hiillermeier & Waegeman, 2021), which represents how uncertain the agent is about the ground-truth reward function. Epistemic uncertainty can be contrasted to aleatoric uncertainty, which describes inherent stochasticity in the system. While the latter cannot be fully eliminated, the former can be reduced with addi- tional queries. Uncertainty is usually one of the most important aspects to consider when deciding which query to ask. It is usually represented either as a probability distribution (i-e., belief) in Bayesian approaches or using an ensemble of reward networks to approximate this belief. However, other representations are also possible, e.g., using the recently-proposed epistemic neural network . With a belief representation, various classic acquisition functions have been considered in the Bayesian framework, such as the probability of improvement, the expected improvement, or the upper confidence bound. For instance, Daniel et al. (2014) compare those three criteria in a robotic domain and observe that the latter yield the best performance, but at a high cost in terms of number of queries, while the first asks the fewest number of queries, but with a slightly lower asymptotic per- formance. Other alternative criteria have been considered, such as volume removal (Sadigh et al., 2017; Basu et al',\n",
       "  '. Other alternative criteria have been considered, such as volume removal (Sadigh et al., 2017; Basu et al., 2018; 2019) or information gain (Mindermann et al., 2018; Biyik et al., 2019; 2020). The volume removal criterion uses the minimum volume of the hypothesis set removed by an answer to a query as the acquisition function and has been shown to be effective in practice. However, Biyik et al. (2019) show that volume removal has uninformative global optima and argue that the practical effectiveness is due to non-convexity leading to local optima that are informative. They also show that optimizing for information gain has the same computational complexity while avoiding this flaw. One drawback of these Bayesian approaches is that they require maintaining a distribution over reward functions (e.g., using Gaussian processes or simpler probability distributions, such as Gaussian distribution, but with a linear reward model) and, there- fore, may not be suitable for more complex domains due to the computational complexity or the strong assumption about the reward model. When using an ensemble instead of a direct belief representation, these criteria for epistemic un- certainty reduction correspond to measures of disagreement within the ensemble. Previous cri- teria could possibly be applied, but one popular candidate is the variance of the ensemble out- puts (Lee et al., 2021b; Metcalf et al., 2023; Gleave & Irving, 2022) or equivalently its standard deviation',\n",
       "  '. Previous cri- teria could possibly be applied, but one popular candidate is the variance of the ensemble out- puts (Lee et al., 2021b; Metcalf et al., 2023; Gleave & Irving, 2022) or equivalently its standard deviation . The average entropy of the ensemble outputs (e.g., when assuming a Bradley-Terry model for the human answers) has also been used (Lee et al., 2021b;a; Park et al., 2022). However, note that it does not quantify epistemic uncertainty but rather the aleatoric uncertainty in the human’s answers, as provided by the response model of the human. Therefore, this criterion may not be suitable in the RLHF setting since it amounts to focusing on the queries for which an answer is expected to be the most random (according to the Bradley-Terry model). By definition of this model, the segments in the pairwise comparisons are the most similar in terms of returns and are, therefore, the hardest to answer for the human. In contrast to those acquisition functions that lead to deterministic query selection, sampling-based approaches have also been studied, from pure random selection to Thompson sampling and its variants (Novoseller et al., 2020; Dwaracherla et al., 2024). Re- cently, in the context of fine-tuning LLMs with pairwise comparison queries, Dwaracherla et al. (2024) shows that using the relatively novel epistemic neural network , dou- ble Thompson sampling (Wu & Liu, 2016), which naturally favors better elements to be compared, performs well experimentally',\n",
       "  '. (2024) shows that using the relatively novel epistemic neural network , dou- ble Thompson sampling (Wu & Liu, 2016), which naturally favors better elements to be compared, performs well experimentally. In addition to epistemic uncertainty, one may also take the outcomes into account to select queries, that is consider utilities (i.e., returns or expected returns in RL). In a Bayesian setting, this leads to acquisition functions such as expected value of information or information gain over return differences (Lindner et al., 2021b), while in a non-Bayesian setting, the notion of regret, which measures the difference of performance between a policy optimal for the ground-truth reward function and a policy optimal for a learned reward function, can be used . Finally, it should be mentioned that naturally approaches with use uncertainty as the main criterion for label collection (e.g., No for more details). heoretical guarantees usually also voseller et al. (2023), see Section 7 On-Policy Data Only focusing on uncertainty is likely insufficient or inefficient in RL because the previous methods may focus on choosing queries to identify the reward function as precisely as possible uniformly on the whole state-action space. However, it may be important to favor more on-policy trajectories to guarantee the relevance of the generated queries for the current policy',\n",
       "  '. However, it may be important to favor more on-policy trajectories to guarantee the relevance of the generated queries for the current policy. Indeed, improving reward learning in state-action regions that may never be visited with the current policy would lead to wasteful queries (Lindner et al., 2021b). One simple approach to ensure that the data is more on-policy is by simply sampling from the current policy (Cui & Niekum, 2018) or favoring more recently-generated trajectories (Eberhard et al., 2022; Hu et al., 2024). Query Simplicity Selecting queries only based on their informativeness may lead to queries that are hard for a human to answer, which is, for example, the case for the average entropy. The ease of answering a query is important to alleviate the cognitive load of the human oracle. Some work specifically takes this aspect into account, for instance, by considering the similarity of consecutive queries or the information gain. For this latter criterion, Buyik et al. (2019) show that in contrast to volume removal, it naturally leads to queries that are easier to answer for a human because information gain can be increased when the uncertainty in the human answer is lower. Trajectory Quality Most approaches directly use the trajectories generated during RL training. Especially early in training, these can be very bad with respect to the ground-truth reward function. In addition to that, they can be irrelevant or even contradictory for a given task',\n",
       "  '. Trajectory Quality Most approaches directly use the trajectories generated during RL training. Especially early in training, these can be very bad with respect to the ground-truth reward function. In addition to that, they can be irrelevant or even contradictory for a given task . Building queries on such trajectories may lead to unnatural queries for a human to respond to, such as comparing a very bad trajectory with an irrelevant one. Katz et al. (2021) measure trajectory quality by optimizing over sampled reward functions. Similarly, Cui & Niekum (2018) generate trajectories using op imal policies for reward functions sampled from the current Bayesian belief. Query Diversity When asking many queries (in batch, in sequence), the diversity o the queries becomes especially crucial to avoid asking redundant queries. Most work (Christiano e al., 2017; Lee et al., 2021b; Verma & Me calf, 2024) follows a very myopic approach: Queries are often selected from a Query Cost The cost usually randomly-generated set of potential queries, and sequences of queries are not really coordi- nated. While some work exists that specifically tackles the selection of a batch of diverse queries (Biyik & Sadigh, 2018; Bryik et al., 2024), the latter is rarely considered due to its computational intractability. Indeed, planning ahead a sequence of queries would amount to solving a sequential decision-making problem under uncertainty over a combinatorial action space (i.e',\n",
       "  '., 2024), the latter is rarely considered due to its computational intractability. Indeed, planning ahead a sequence of queries would amount to solving a sequential decision-making problem under uncertainty over a combinatorial action space (i.e., the set of pos- sible queries). For diverse batch querying, previous work considered using clustering methods such as k-medoids (Biyik & Sadigh, 2018) or more recently determinantal point processes, which define probability distributions that promote diversity . of generating queries may also be an important factor if the interaction of the human is live since it may not be practical to let the human wait before showing any queries . In that case, it may be more important to quickly show some relatively good queries instead of computing the most informative ones. Although this factor may not translate directly into an acquisition function, it may influence the choice of the acquisition function and its implementation in a given problem. Since various different acquisition functions have been considered, some effort (Lee et al., 2021b;a) has been made to compare them. Generally speaking, uncertainty-based criteria (e.g., variance or average entropy) seem to often perform better empirically compared to random selection, a query diversity-based 26 criterion alone or combined with an uncertainty-based criterion. In addition, Eberhard et al',\n",
       "  '. Generally speaking, uncertainty-based criteria (e.g., variance or average entropy) seem to often perform better empirically compared to random selection, a query diversity-based 26 criterion alone or combined with an uncertainty-based criterion. In addition, Eberhard et al. (2022) em- pirically observe that a variance-based criterion performs better than a selection method only based on trajectory recency. Surprisingly, random selection has been shown to perform competitively in some cases (Christiano et al., 2017; Ibarz et al., 2018). Thus, a better understanding of which acquisition function should be preferred in which situation or domain is still an open question. In addition, combinations of different criteria have naturally also been evaluated. For instance, Reddy et al. (2020) use four acquisition functions (high uncertainty, high novelty, high reward, low reward) in parallel. This approach has also been validated in a 3D environment . A more sophisticated approach consists of considering a portfolio of acquisition functions and learning to select them using a multi-armed bandit approach . Various extensions to the basic setting have also been investigated. In the context of multiple human labelers, the issue of selecting reliable teachers to query arises (Daniels-Koch & Freedman, 2022)',\n",
       "  '. Various extensions to the basic setting have also been investigated. In the context of multiple human labelers, the issue of selecting reliable teachers to query arises (Daniels-Koch & Freedman, 2022). Assuming all teachers have the same preferences, this can be modeled for pairwise comparisons by incorporating a rationality coefficient 6 into a Bradley-Terry model and estimating this factor: N 1 . max] T+ exp(3(Ru (td) — Ru)’ ®) where a higher 6 corresponds to a more reliable human (see Section 5.1.2). The setting in which this assumption does not hold, i.e., the labeler’s reward functions differ (a setting already considered in in- verse RL (Choi & Kim, 2012)), has also been studied recently (Siththaranjan et al., 2024; Xue et al., 2023a; Dong et al., 2024; Myers et al., 2021; Bakker et al., 2022). Interestingly, a noisy oracle may sometimes pro- vide more information than a completely reliable oracle. For instance, in Eq. (3), the frequency of erroneous answers given by the noisy oracle is related to how much a segment is preferred to the other one (Xu et al., 2020; Chan et al., 2021). In contrast, only a binary preorder over segments can be inferred from the answers of a reliable and deterministic oracle, which may not be enough to recover the true reward function. Another notable recent work is that by Ellis et al. (2024) who raise the issue of identifiability of the ground- truth reward function: many reward functions result in the same optimal behaviors',\n",
       "  '. Another notable recent work is that by Ellis et al. (2024) who raise the issue of identifiability of the ground- truth reward function: many reward functions result in the same optimal behaviors. The authors propose a framework that enables the generation of acquisition function for various definitions of reward similarity, such as the one discussed in Section 5.4.1.2 Adaptive Choice of Feedback Type In addition to selecting queries within a given feedback type, it is also possible to actively select the feedback type itself . The best choice of feedback type can depend on many factors, such as human rationality as well as task-dependent factors, some of which may change during the labeling process. Ghosal et al. (2023) formalize this setting as one in which we try to select a feedback design (or feedback type) x out of the space of possible designs 4 such that the expected information gain over the distribution of reward functions is maximized for the next human response. Concretely, the goal is to choose a feedback design by means of r= argmax Eo, ~P(en|2) [Dx (P(O | cn, x) | P())], rE where c;, is the human response to a query defined by x and P(6) is the prior distribution over reward functions. Ghosal et al. (2023) find that the most informative feedback type depends on the (type-dependent) rationality of the human labeler (see Section 4.2.1). More precisely, it is shown that the most informative feedback depends on the rationality factor, e.g',\n",
       "  '. Ghosal et al. (2023) find that the most informative feedback type depends on the (type-dependent) rationality of the human labeler (see Section 4.2.1). More precisely, it is shown that the most informative feedback depends on the rationality factor, e.g., while demonstrations are more informative than comparisons when the human is highly rational, comparisons should be preferred in less-rational settings. Given that this rationality might change due to factors such as fatigue or an individual labeler’s capabilities, this suggests that adaptively adjusting the feedback type during the labeling process may be worthwhile. Further study of this relationship is a promising area for future work. 27 4. This section explores the label collection process, which follows after query selection. This task intersects with several related disciplines, especially within the social sciences, as it encompasses the design of interactions to facilitate informative query responses. A prominent field in this area is psychometrics , which focuses on measuring psychological attributes, including preferences. Similarly, survey research is dedicated to developing techniques for gathering information from individuals via surveys. Human- computer interaction plays a significant role as well, investigating the design of user interfaces tailored for preference elicitation',\n",
       "  '. Similarly, survey research is dedicated to developing techniques for gathering information from individuals via surveys. Human- computer interaction plays a significant role as well, investigating the design of user interfaces tailored for preference elicitation . Moreover, preference label collection is also necessary for discrete choice experiments within health economics , where it is used for the assessment of service values. 4.2.1 Psychology-Aware Preference Elicitation Understanding human psychology is essential for effective preference elicitation in RLHF systems. Human decision-making is complex, often diverging from traditional rational choice models due to cognitive, social, and emotional factors. This complexity is exemplified by phenomena like fatigue, which can affect the reliability of choices based on the order of queries. This section overviews these phenomena, exploring how constructive preferences, biases, framing effects, and social interactions shape the observed choices. Recognizing and addressing these psychological underpinnings is key to developing more accurate and reliable systems. In this section, we will discuss various psychological phenomena, such as cognitive biases and response biases, and related effects (fallacies, biases, heuristics, psychological phenomena impacting decision- making processes), which may falsify labels by adding systematic bias or noise',\n",
       "  '. In this section, we will discuss various psychological phenomena, such as cognitive biases and response biases, and related effects (fallacies, biases, heuristics, psychological phenomena impacting decision- making processes), which may falsify labels by adding systematic bias or noise. Preference learning methods typically assume the existence of inherent, stable preferences that can be elicited through querying. Contrary to this assumption, psychological research, such as the work by Lichtenstein & Slovic (2006), indicates that preferences are often constructed during the elicitation pro- cess and may vary with the method of elicitation or over time. This suggests that the feedback type not only affects elicitation’s effectiveness but also shapes preferences. Systematic biases, noise, and other psy- chological factors may influence observed choices, challenging the traditional models of human choice used o infer latent utilities (see Section 5.1). The elicitation method, query presentation, and context thus play a critical role in shaping measured preferences, compounded by cognitive biases and irrationalities. The influence of psychological phenomena on preference learning has been well-documented in the litera- ure, especially within the context of explicit preference elicitation for recommender systems. For instance, Tran et al. (2021) provide a thorough discussion of the relationship between psychology and recommender systems. Similarly, Atas et al',\n",
       "  '. For instance, Tran et al. (2021) provide a thorough discussion of the relationship between psychology and recommender systems. Similarly, Atas et al. (2021) review how preference construction is influenced by cognitive biases, personality traits, and emotional states in recommender systems, discussing effects like serial position, fram- ing, anchoring, choice overload, and preference visibility. In a more specialized discussion, Mandl et al. (2011) focus on cognitive biases in the context of consumer decision-making and its interaction with rec- ommender systems. Mandl et al. (2011) specifically address cognitive biases in consumer decision-making in interaction with recommender systems. Finally, Kaufmann et al. (2023) link these psychological aspects to RLHF, discussing the common practice of using synthetic instead of real human feedback for algorithm evaluation and highlighting the limitations of that approach. They further discuss challenges posed by real human feedback, many of which are related to the concepts discussed in the following paragraphs, as well as the opportunities provided by integrating psychological insights into RLHF systems. Constructive preferences are closely related to framing effects, which refer to changes in elicited preferences based on how tasks or alternatives are described, even when these descriptions are essentially equivalent',\n",
       "  '. Constructive preferences are closely related to framing effects, which refer to changes in elicited preferences based on how tasks or alternatives are described, even when these descriptions are essentially equivalent. For example, presenting a choice as a loss versus a gain can lead to different decisions despite identical outcomes. Moreover, serial position effects, commonly known as primacy and recency effects, also play a significant role. These effects describe the tendency for the beginning and end of an experience to influence subjective experience disproportionately. This phenomenon becomes particularly relevant in scenarios like video choices, where the initial or concluding segments might disproportionately affect preferences. Atas et al. (2021) discuss both of these effects in the context of recommender systems. 28 Ordering effects pose another challenge in preference elicitation, where the sequence of queries can affect responses. Day et al. (2012) outline several factors contributing to these effects: institutional learning, changing preferences, and varying levels of cognitive effort. Institutional learning involves gaining familiarity with the task and feedback type, which can enhance labeler’s expertise and, consequently, the accuracy of their responses. However, due to the constructive nature of preferences, their preferences may evolve during the elicitation process, leading to changing preferences',\n",
       "  '. However, due to the constructive nature of preferences, their preferences may evolve during the elicitation process, leading to changing preferences. This evolution might also be influenced by anchoring effects, where previously seen instances bias responses. Furthermore, cognitive effort levels can fluctuate due to factors like fatigue or boredom. This is closely related to choice overload, a form of fatigue from excessive choices, as discussed by Atas et al. (2021) and bounded rationality, as explored by Chen et al. (2013). In such scenarios, labelers might opt out of making a choice when overwhelmed by options. Bounded rationality refers to the limitations in human decision-making capabilities, particularly when processing large amounts of information, which aligns with the concept of choice overload. To address these challenges, studies like Biyik et al. (2019) and Zhang et al. (2022) propose methods to reduce cognitive effort in responding to queries. Buyik et al. (2019) focus on posing queries that are straightforward for humans to answer, while Zhang et al. (2022) enhance the human evaluation process by presenting queries in a user-friendly format. When multiple labelers collaborate on the same task in preference elicitation, as is studied, e.g., by Barnett et al. (2023) and Daniels-Koch & Freedman (2022), this may lead to another set of biases if they have the opportunity to exchange information',\n",
       "  '. When multiple labelers collaborate on the same task in preference elicitation, as is studied, e.g., by Barnett et al. (2023) and Daniels-Koch & Freedman (2022), this may lead to another set of biases if they have the opportunity to exchange information. This exchange may either be direct or indirect through observing the system’s predictions, which are based on the other labeler’s feedback. Such interactions can affect their preferences through several mechanisms, as identified by Atas et al. (2021): anchoring effects, ransfer of emotional states, and conflict avoidance. Anchoring effects, for instance, occur when a labeler’s choices are influenced by the knowledge of others’ preferences or system predictions, a phenomenon also discussed under the term preference visibility. This bias can lead labelers to align their preferences with the anchors they are exposed to, which is a significant consideration in recommender systems. Understanding hese biases is crucial for designing RLHF systems that mitigate the influence of labeler interactions on reference construction. The effects previously discussed stem from systemic biases in preference expression. In addition to these iases, choices may also be affected by noise. This is commonly discussed under the term stochastic rational- ity, where an agent’s behavior is rational with respect to an unobserved random state. The reward-rational implicit choice framework, as introduced by Jeon et al',\n",
       "  '. In addition to these iases, choices may also be affected by noise. This is commonly discussed under the term stochastic rational- ity, where an agent’s behavior is rational with respect to an unobserved random state. The reward-rational implicit choice framework, as introduced by Jeon et al. (2020), addresses this by integrating a rationality ‘actor 3 into the human choice model (see Eq. (3)). This factor’s impact has been further examined by Ghosal et al. (2023) through synthetic experiments and user studies, demonstrating that accurately estimat- ing this type-dependent rationality coefficient can enhance learning performance and guide feedback type selection (see Section 4.1.2). However, a practical method for estimating this factor remains a challenge. While Ghosal et al. (2023) use calibration feedback with a known latent utility function for estimation, such an approach is not feasible for most tasks. In a related study, Daniels-Koch & Freedman (2022) investigate a scenario with multiple teachers, focusing on the agent’s ability to select the most knowledgeable or rational eacher. Therefore, developing more advanced methods to estimate this factor, along with understanding its variability due to factors like fatigue or other ordering effects, presents a vital area for future research in reference elicitation Finally, the quality of human feedback is biased towards factors that are easy to judge. Hosking et al',\n",
       "  '. Hosking et al. (2024) demonstrates that in the case of LLM fine-tuning, humans tend to favor assertiveness over factuality, since he latter is hard to judge without external assistance or resources. A similar phenomenon was observed in he control setting by Amodei et al. (2017), where the agent learned a behavior that looked good only from he camera angle that the human labelers had access to. ncorporating psychological insights into the preference-learning components of RLHF systems is essential for optimizing their efficacy. A key area of focus should be research aimed at mitigating biases and harnessing cognitive aspects of preference formation. For instance, designing user interfaces that minimize framing effects and developing algorithms that account for ordering and serial positioning are crucial steps. In this realm, Metz et al. (2023) and Yuan et al. (2024) each propose a configurable user interface for studying various feedback types and their combinations. Additionally, the study by Krening & Feigh (2018) on the impact of feedback type, such as binary critiques versus action advice, on task performance and labeler 29 satisfaction highlights the significant role of feedback type in preference elicitation. Furthermore, the work of Pommeranz et al. (2012) in user-interaction design underlines the importance of having an expressive feedback type to increase user engagement',\n",
       "  '. Furthermore, the work of Pommeranz et al. (2012) in user-interaction design underlines the importance of having an expressive feedback type to increase user engagement. The integration of these research findings into RLHF systems points to a clear need for a more multidisci- plinary approach. Drawing insights from related fields like behavioral economics and psychology can provide valuable methodologies and perspectives. Addressing irrational choice patterns and enhancing the quality of human feedback remain critical challenges. As we continue to develop and refine these systems, the fo- cus should be on creating robust frameworks that align learning processes with human behavior, effectively managing the inherent complexity and variability of human feedback. 4.2.2 Importance of Researcher-Labeler Agreement High-quality labels are important for the final policy in an RLHF process. Early work on fine-tuning language models using RLHF noticed a mismatch between the researcher’s goals and the (paid) labeler’s actual labels (researcher-labeler disagreement). Ziegler et al. (2020) note that researchers agreed with each other about 60% of the time (on 4-way comparisons, where random choice would result in 25% agreement), while agreeing with labelers only 38% or 46% of the time (depending on the task). Stiennon et al. (2020) attempt to reduce these disagreements by maintaining a hands-on relationship with the labelers and thereby ensuring high researcher-labeler agreement',\n",
       "  '. Stiennon et al. (2020) attempt to reduce these disagreements by maintaining a hands-on relationship with the labelers and thereby ensuring high researcher-labeler agreement. Concretely, they provide on-boarding with detailed instructions, keep an open channel of communication between researchers and labelers, and give feedback to the labelers. They evaluate the researcher-labeler agreement and reach an agreement rate of 77% + 2%. Perfect labels are often impossible due to the inherently subjective nature of the task. Returning to the example given by Stiennon et al. (2020), different researchers agreed with each other in only 73% + 4% of the cases. Ouyang et al. (2022) also report the agreement rates on a different task (instruction fine-tuning instead of summarization) and find that labelers agree with each other in 72.6 + 1.5% of the time, after a screening procedure that, amongst others, selects labelers that agree with researcher labels. Preferences can additionally be inconsistent between feedback types, as demonstrated by the finding of Bansal et al. (2024), which shows that preferences inferred from ratings and rankings significantly disagree for both human and AI annotators. The importance of quality does not trump the importance of quantity, however. Indeed, Stiennon et al. (2020) note that excluding low-confidence samples from the data set generally did not help with reward model training',\n",
       "  '. The importance of quality does not trump the importance of quantity, however. Indeed, Stiennon et al. (2020) note that excluding low-confidence samples from the data set generally did not help with reward model training. This indicates that even though quality is important, a larger quantity is still generally better. o The scale of the labeled data set required for effective training and refinement varies widely, impacting the quality of the resulting models. Studies have shown a broad range in data set sizes, from tens of labels in smaller studies to hundreds in more complex scenarios . Larger- scale applications may require thousands or even millions of labels , each bringing its own challenges in ensuring label accuracy and consistency. This variability in data set size underscores the need for rigorous label quality control measures across different scales. In smaller data sets, each label carries more weight, making accuracy and precision critical. Conversely, in larger data sets, the challenge lies in maintaining consistency and mitigating systematic biases that might emerge from he sheer volume of data. Similarly, the labeling setting varies in the surveyed works, from author-provided feedback , over small in-person studies , to larger remote studies . Each setting rovides unique challenges to ensure high-quality labels. Various works have suggested measures to improve label quality',\n",
       "  '. Similarly, the labeling setting varies in the surveyed works, from author-provided feedback , over small in-person studies , to larger remote studies . Each setting rovides unique challenges to ensure high-quality labels. Various works have suggested measures to improve label quality. Hagendorff & Fabi (2022) discuss the ossible failure modes of the labeling task in more detail, for example, discussing systematic biases and conflicting motivation, and propose concrete changes to the training and evaluation methodology to alleviate hese. Glaese et al. (2022) suggest providing labelers with multiple natural language rules and collecting reference labels for each rule individually to improve label quality. This is related to Bai et al. (2022b), who propose to generate feedback automatically based on such a set of rules and a language model. 30 5 Reward Model Training Environment Dynamics In this section, we delve deeper into the process of reward model learning, which we briefly touched on in Section 2. 5.1 Human Feedback Model The basic premise underlying the majority of approaches in RLHF is that human feedback is directly related to the reward function to be learned. To this end, the human feedback must first be captured in a sound mathematical framework that establishes the connection to the reward function. On a high level, one can break down (almost) all feedback types in Section 3',\n",
       "  '. To this end, the human feedback must first be captured in a sound mathematical framework that establishes the connection to the reward function. On a high level, one can break down (almost) all feedback types in Section 3.2 to a choice scenario: The human chooses one specific feedback option (label) from an available (possibly infinite) pool of possible feedback options (choice sets). Here, the query that is made specifies the explicit contents of the choice set, e.g., if the query is to compare two trajectories, then the choice set consists of all possible outcomes for these two trajectories. Assuming that human choices are not always optimal, one obtains a fruitful mathematical framework when focusing on the probability P(c is chosen|C) , (4) where C is the set of possible choices and c € C one explicit choice. For the RLHF scenario, where the agent asks queries g; and the human gives labels 1; as feedback (see Section 2.4), the choice set is specified by a function of the query. Formally, C = m(q) for some mapping m that maps a query gq to the set of all possible candidate labels extractable from q for the specific feedback type. For example, if the query is to rank a finite number of trajectories, then the choice set is the set of all possible rankings that can occur for the trajectories involved',\n",
       "  '. For example, if the query is to rank a finite number of trajectories, then the choice set is the set of all possible rankings that can occur for the trajectories involved. With this view, we can therefore place (4) in the RLHF context and write P (label 1 is provided | m(q)) (5) for the probability that a human labeler returns a label | from all possible candidate labels that can be extracted from a given query g. We explain next how this probability can be modeled and discuss various related modeling questions (e.g., human rationality, multiple humans, or Markov assumption). One could also recover the noiseless scenario if the latter probability distribution is degenerated for all possible candidate label sets. 5.1.1 Boltzmann Distribution Human choice models as in (4) have been studied for a long time in various scientific fields such as psychol- ogy , economics , or behavioral science . Accordingly, there are many different choice models to resort to for (5), which, in some cases, are the same models, just under ?This point of view goes back to the work of Jeon et al. (2020). 31 different names. A popular class of such human choice models assumes every choice option c to be equipped with a (latent) utility u., which the human perceives in a perturbed way. This perturbation is modeled by means of perturbation random variables €, that perturb the utility in an additive way, so that (4) becomes P(c is chosen|C) = P (« = argmax Uc + «)',\n",
       "  '., which the human perceives in a perturbed way. This perturbation is modeled by means of perturbation random variables €, that perturb the utility in an additive way, so that (4) becomes P(c is chosen|C) = P (« = argmax Uc + «) . (6) ceC The translation for the RLHF setting for (5) is then accordingly P (label 1 is provided | m(q)) = P ( =argmax w+ «) ; (7) lem(q) and we shall now stick to the RLHF translation from now on. These probabilities depend on the specific distributional assumptions that are made on the perturbation variables that only for specific cases lead to a closed-form of the right-hand sides of the latter equations. When stipulating a standard Gumbel distribution for the perturbations, one always obtains a closed form that is proportional to the exponential utility of the provided label: P (label / is provided | m(q)) « exp(ur) . (8) This is known as the Boltzmann distribution that also appears in a perhaps slightly modified version in various different subfields of ML and statistics. When restricting to discrete (choice) sets for m(q), this distribution is also known as the multinomial logit model or Gibbs measure , and as the Bradley-Terry model (Bradley & Terry, 1952) when the choice sets consist of pairs. All of these also have a link to the Plackett-Luce model (Luce, 1959; Plackett, 1975), which is a probability distribution on the space of total orders or rankings (see Alvo & Yu (2014) for details). This model is often used for various reasons',\n",
       "  '. All of these also have a link to the Plackett-Luce model (Luce, 1959; Plackett, 1975), which is a probability distribution on the space of total orders or rankings (see Alvo & Yu (2014) for details). This model is often used for various reasons. A particularly compelling reason is the closed analytic form, which in turn makes it possible to obtain a closed form for the gradient with respect to the utilities. Another reason is that this model satisfies Luce’s axiom of choice , which requires the probability of choosing an option from a pool of choice options not being affected by the presence or absence of other options in the pool. In this way, coherent decision-making is ensured, which, however, might be challenged as humans are likely not making fully rational decisions (see Section 4.2.1). Jeon et al. (2020) show that the usage of the Boltzmann distribution is justified by the principle of maximum entropy. More precisely, they show that it is the maximum entropy distribution over choices for a so-called satisficing human decision maker, i.e., one who is making in expectation a choice with an optimal reward up to some slack € > To build the bridge between reward learning and the modeling of human feedback, the Boltzmann distribution can be used by assuming that the utilities can be represented as a function of the reward function, usually featuring the return of a trajectory',\n",
       "  '. More specifically, one assumes a grounding function G that maps choice options (or labels) to the set of distributions over trajectories and sets the utility of a label / as w= E,raw[R(7)) - ®) Note that uj depends on the return R, so that we also may use u;(R) to emphasize this dependency. For the common case of pairwise trajectory comparisons, where for two trajectories 7,,72 we obtain for the possible labels | € {71 > 72,71 ~ T2} the respective utility by using the projection onto the preferred trajectory as the grounding function G. Accordingly, the utility of the label represents essentially the utility of the preferred trajectory of that label, i.e., 71 or 72 in this case. As another example consider the case of e-stops feedback (see Section 3.2.2). Here, the possible labels | provided by the user are STOP; and CONT encoding the stopping at time ¢ or the continuation of a trajectory. For the grounding function, one can define aw = { 1 = CONT, ToTe---T, |= STOP;, where 32 feedback type. It is worth noting that one can also easily find a grounding function for the feedback type of a (partial) order over trajectories as considered, for instance, by Myers et al. (2021). Moreover, one can generalize this modeling approach by using (partial) segments instead of trajectories. Although this general human feedback model has been much in use and shown to be useful for the sake of hu- man alignment, it is not without its critics (see Lindner & El-Assady (2022) or Section 3.2',\n",
       "  '. Although this general human feedback model has been much in use and shown to be useful for the sake of hu- man alignment, it is not without its critics (see Lindner & El-Assady (2022) or Section 3.2.1 in Casper et al. (2023)). This has led to different adaptions of the general model based on the Boltzmann distribution that will be discussed in the following. Moreover, we will also concisely review other human feedback models that have been in use besides the Boltzmann distribution, discuss relevant work on the consequences or robustness of human feedback model misspecification, and highlight contributions on varying the standard assumptions on the nature of the human feedback. 5.1. The Boltzmann distribution in (8) can be extended by a rationality coefficient 3 € [0,00) that reflects the precision of the human labeler?: P (label 1 is provided | m(q)) = P ( =argmax 6 uy + «) x exp(3- uw). (10) lem(q) The higher 6, the more (10) resembles a pointmass distribution modeling a highly rational human labeler (decision-maker) that is always able to identify the option the with highest utility, while the lower 8, the more (10) resembles a uniform distribution modeling a highly irrational human labeler (decision-maker) acting purely at random. Without this extension, the commonly considered Boltzmann distribution (or Bradley-Terry model in the common case of pairwise comparisons) in (8) assumes a rationality coefficient of . Ghosal et al',\n",
       "  '. Without this extension, the commonly considered Boltzmann distribution (or Bradley-Terry model in the common case of pairwise comparisons) in (8) assumes a rationality coefficient of . Ghosal et al. (2023) show in their experiments that the estimation of this coefficient can indeed positively influence reward learning. For the estimation, however, a calibration reward function is needed, as the rationality coefficient is otherwise not identifiable (Bengs & Hiillermeier, 2020). Similar findings are shown y Daniels-Koch & Freedman (2022), who model the rationality coefficient as a query-dependent function hat might differ for the human labelers (see Section 5.1.6). Another alternative to the rationality coefficient for representing irrational humans is achieved by introducing a query-independent error probability . To be more precise, it is assumed that the human labeler only adheres to the Boltzmann distribution in (8) in 90% of cases and otherwise makes completely random decisions. This formulation is similar to Huber’s contaminated model (Mu & Xiong, 2023). 5.1.3 Alternative Utility Notions Knox et al. (2024) show that the Boltzmann model does not generally lead to an identifiable reward function using (9) by presenting three concrete scenarios for which identification is not possible. The root cause of he non-identifiability is the usage of a trajectory’s return as the utility in (9)',\n",
       "  '. (2024) show that the Boltzmann model does not generally lead to an identifiable reward function using (9) by presenting three concrete scenarios for which identification is not possible. The root cause of he non-identifiability is the usage of a trajectory’s return as the utility in (9). They, therefore, suggest using a trajectory’s regret as an alternative, which provably leads to identifiable rewards. A trajectory’s regret is the negated sum of the optimal policy’s advantage over each state-action pair in he trajectory. Empirically, it has been shown that this modification improves the alignment of the learned strategy with human preferences. The downside of this alternative is that regret depends on the unknown optimal policy. Recently, it has also been suggested to consider @Q-values of a human policy as the utili- ies , while Holladay et al. (2016) used differences of cost functions that depend on the available choice set and the human’s uncertainty. 5.1.4 Human Feedback Models Beyond Boltzmann While the human feedback model based on the Boltzmann distribution is the most widely used model nowadays, other models have also been considered in the literature. In particular, for the probability in (4) ’This can be achieved by multiplying the utilities u, with 33 other models such as the Thurstone model (Wilson et al., 2012; Kupcsik et al., 2018; Biyik et al., 2020), the ridge-noise model , the binary model or mixed forms thereof have been considered',\n",
       "  '. In particular, for the probability in (4) ’This can be achieved by multiplying the utilities u, with 33 other models such as the Thurstone model (Wilson et al., 2012; Kupcsik et al., 2018; Biyik et al., 2020), the ridge-noise model , the binary model or mixed forms thereof have been considered. Of these models, only the Thurstone model has a similar interpretation as the Boltzmann distribution based on perturbed utilities, only differing in the distribution of the perturbance random variables. Link functions Another possibility, which is particularly popular in theoretical work on RLHF (see Section 7), is the use of other functions on the right-hand sides of Eq. (8) than the exponential function. The concept is primarily used for pairwise comparisons of trajectories. It essentially states that the probability of the result of a pairwise comparison between two trajectories is the difference of their utility values under a so-called link function. More specifically, let ¢ = {1,72} be the query to compare the trajectories 7, and 72, then, assuming a link function @ : R — [0,1], one models the probability in (5) for J representing a preference for 7, as P (label / is provided | m(q)) = P (71 > 72 |m({71, 72})) = ®(ur, — ury) - (11) For | representing a preference for 72, one proceeds similarly',\n",
       "  '. The minimal assumptions on the link functions are that (i) it is (strictly) monotonically increasing to take into account that trajectories with higher utilities will also have a higher chance to be picked; (ii) (x) = 1 — &(—z) to ensure that P (7 > 72|m({t1,72})) = 1—P (m1 ~ 72| m({71, 72})). Note that the second property implies ®(x) = 1/2 so that trajectories with the same utility also have the same chance of being selected. Any cumulative distribution function of a symmetric continuous random variable fulfills these two conditions. The two most common link functions that both fulfill the conditions are the linear link function given by ®(x) = max{0,min{1,1/2- (1+ 2)}} and the logistic link function given by 1 (x) = 1+ exp(—2) * Both are cumulative distribution functions: The linear link function is the cumulative distribution function of a continuous uniform distribution on [0,1]. In contrast, the logistic link function is the cumulative distribution function of a logistic distribution with location parameter 0 and scale parameter Two-Staged Choice Model Bobu et al. (2020b) propose the Limiting Errors due to Similar Selections (LESS) model that is inspired by the attribute rule model suggested by Gul et al. (2014)',\n",
       "  '. (2020b) propose the Limiting Errors due to Similar Selections (LESS) model that is inspired by the attribute rule model suggested by Gul et al. (2014). It assumes a feature map for trajectories and a (similarity) function mapping trajectory features and trajectories to integers and uses a two-stage process for modeling the human feedback (or choice): First, choosing a trajectory feature according to the Boltzmann distribution and then a trajectory with the (logarithmic) similarity functions as the utilities within the Boltzmann distribution. Their experiments show that this model can capture human feedback more appropriately than the standard Boltzmann distribution. Generative Model Abramson et al. (2022) evaluate the usage of a generative model for learning from human preferences. More specifically, instead of assuming some underlying utility as in the Bradley-Terry model, they attempt to train a model to generate the human feedback (inter-temporal preferences in this case, see Section 3.2.1) and directly interpret this feedback as reward. However, they found that this empirically does not perform as well as the inter-temporal Bradley-Terry model. 34 5.1.5 Misspecification The human feedback model may be misspecified in various ways. Milli & Dragan (2020) investigate the roblem of misspecifying the nature of human feedback that can be either literal or pedagogical',\n",
       "  '. 34 5.1.5 Misspecification The human feedback model may be misspecified in various ways. Milli & Dragan (2020) investigate the roblem of misspecifying the nature of human feedback that can be either literal or pedagogical. The former means that the human gives targeted feedback for solving the actual RL problem, while the latter means hat the human gives targeted feedback that is deemed helpful for the learner. They show theoretically and empirically that the case of a learner assuming a pedagogical feedback with an actual literal human always erforms worse than the reversed case, i.e., a learner assuming a literal feedback with an actual pedagogical human. A related question is studied by Freedman et al. (2021), namely, what if the learner makes incorrect assump- ions about the choices from which the human selects its feedback? They consider different types of such choice set misspecification and show that depending on the type of misspecification, the performances might vary drastically, even leading to no losses at all in some specific cases. n the field of inverse RL, the general question of the robustness of reward learning in terms of a misspec- ified human feedback model is theoretically investigated by Skalse & Abate (2023)',\n",
       "  '. n the field of inverse RL, the general question of the robustness of reward learning in terms of a misspec- ified human feedback model is theoretically investigated by Skalse & Abate (2023). It turns out that the optimality model is not robust with respect to any misspecification, the Boltzmann model is robust for quite a range of specific misspecifications, and the degree of robustness of the maximal causal entropy model lies etween the latter two. Even though these results are primarily derived for inverse RL, they also have similar immediate implications for RLHF. 5.1.6 Diverse Preferences One potential issue with the RLHF framework is that it does not specify whose preferences to align to. It is common to request feedback from multiple labelers in a crowd-sourcing setting, in which case the different labelers may disagree. There are two main ways to deal with this challenge: Either trying to learn each labeler’s preference separately, or trying to learn a model of the group’s mean preference. Bakker et al. (2022) investigate the first option by proposing to learn multiple reward functions, which can then be aggregated in arbitrary manners and even be utilized to find consensus among people with different preferences. The second is more commonly used, however: Xue et al',\n",
       "  '. Bakker et al. (2022) investigate the first option by proposing to learn multiple reward functions, which can then be aggregated in arbitrary manners and even be utilized to find consensus among people with different preferences. The second is more commonly used, however: Xue et al. (2023a) learn a single reward ‘unction from multiple humans who may give diverse and inconsistent feedback, aiming to stabilize learning in spite of these inconsistencies using regularization, a consistency constraint, and ensembling. Similarly, Chhan et al. (2024) try to estimate the correct preference for pairwise trajectories directly by combining he users’ expressed preference labels instead of assuming individual reward functions. As a middle-ground etween the single reward function and multiple ones, Myers et al. (2021) propose to learn a multimodal reward function that captures multiple people’s preferences and use a mixture model of Plackett-Luce models ‘0 represent the feedback more accurately. With a stronger focus on the active retrieval of human feedback, Freedman et al. (2023) model the problem of selecting a suitable human labeler as a variant of the multi-armed bandit problem (Lattimore & Szepesvari, 2020), which they call hidden-utility bandit',\n",
       "  '. With a stronger focus on the active retrieval of human feedback, Freedman et al. (2023) model the problem of selecting a suitable human labeler as a variant of the multi-armed bandit problem (Lattimore & Szepesvari, 2020), which they call hidden-utility bandit. In this variant, the agent has in each decision round the choice between two options: (i) drawing a bandit arm, then receiving a hidden arm-dependent utility, and finally observing an item, or (ii) querying a human to observe a preference between two sampled items and incurring a human-specific query cost. The feedback mechanism of all human teachers is modeled via a same Boltzmann distribution, differing only in their known individual rationality coefficients. The same modeling of human feedback is also considered by Barnett et al. (2023), who, however, use a Bayesian approach to determine which person should be queried in order to obtain the most informative feedback in expectation. aniels-KochDANIELS-KOCH & Freedman (2022) investigate the rationality coefficient already considered in the previously mentioned work and model it as a query-dependent function that might differ for the human labelers. 5.1.7 Relaxation of the Markov Assumption Most works assume that the human feedback is given based on a latent Markovian reward model, ice., the return of a trajectory tT decomposes into a sum of independent rewards over state-action pairs (see (1)). Early et al',\n",
       "  '. 5.1.7 Relaxation of the Markov Assumption Most works assume that the human feedback is given based on a latent Markovian reward model, ice., the return of a trajectory tT decomposes into a sum of independent rewards over state-action pairs (see (1)). Early et al. (2022) relax this assumption by dropping the need for the Markov property, such that the instan- taneous reward might depend on hidden states. Similarly, Kim et al. (2023) avoid the Markov assumption by utilizing a transformer as the preference model. A similar effect may be achieved by learning a state representation with a recurrent network in which the rewards are Markov, similar to the approach taken by Hafner et al. (2023), but we are not aware of any work exploring this. Abramson et al. (2022) work in a non-Markovian setting as well by utilizing memory-augmented networks for both the policy and the reward model. 5.2 Utility Learning After choosing a human model to relate feedback to utilities, we can use the observed feedback to recover the latent utilities. This utility learning can be reduced to a standard supervised learning problem and, therefore, is commonly solved with the techniques of empirical risk minimization or Bayesian approaches, both of which will be discussed in the following. 5.2.1 Empirical Risk Minimization The most prevalent variant for learning the reward function, already been presented in Section 2.3, is a special case of empirical risk minimization',\n",
       "  '. 5.2.1 Empirical Risk Minimization The most prevalent variant for learning the reward function, already been presented in Section 2.3, is a special case of empirical risk minimization. The general approach of empirical risk minimization for reward function learning, assuming an underlying human feedback model with utilities as in (9) is to find the minimizer of £(R:D) => €(w(R)- ti, m(qy)) (12) where D = {(l;, awn, is the given data set of observed label and query pairs, f: R x Q x C is a suitable loss function with Q being the set of all possible label sets, and u.(R) denoting the utility (depending on the return R) of the possible labels for the given label-query pair 1;,m(q;). As an illustration, consider the common case of pairwise trajectory comparisons where queries are pairs of trajectories q; = {ri rit, and labels l; € {7} > 73,7} < 73} = m(qi) are the human’s preference over the two trajectories. For a given query qi, we then obtain (2) as a special case of (12) by using the loss function: (u.(R), li, m(qi)) 1 ~ [es (; + exp(Um(q:)\\\\: (R) _ Tm) 1 ~ es (; + exp(E,nG(m(q:)\\\\t) R()] — =n) , where in the case of pairwise comparison, u.(R) = (wi;(R), Um(q,)\\\\1,(R)) and the grounding function G is the projection onto the preferred trajectory. This is the negative log-likelihood for the Boltzmann distribution for the observational pair (1;, m(qi)). For the entire learning process, a model class R is assumed for the reward function R',\n",
       "  '.(R) = (wi;(R), Um(q,)\\\\1,(R)) and the grounding function G is the projection onto the preferred trajectory. This is the negative log-likelihood for the Boltzmann distribution for the observational pair (1;, m(qi)). For the entire learning process, a model class R is assumed for the reward function R. This model class is usu- ally a parameterized class of functions, such as, for example, the class of linear reward functions R= {Ry(s,a) = ¥d(s,a) | (s,a) €S x AW ERY, where @ : S x A > R?@ is some state-action feature mapping. This entails that good features are known in advance such that rewards can be expressed as a linear combination of those features. Using such linear models may lead to reward model misspecification. Studying this setting, Bobu et al. (2020a) propose to adapt the hyperparameter 6 in (3) to account for this issue. Since the assumption of a linear reward model may be too strong in practice, most recent work is based on non-linear models, especially using differentiable models, but other cases have been investigated as well. In the latter case, for instance, decision trees have been considered to learn an interpretable reward model (see Section 5.2.4). In the former case, simple multilayer perceptron (MLP) has naturally been considered, but more recent deep learning architectures are more commonly used in the recent literature',\n",
       "  '.2.4). In the former case, simple multilayer perceptron (MLP) has naturally been considered, but more recent deep learning architectures are more commonly used in the recent literature. Thus, especially with partially observable domains, the reward network may be composed of a state-action encoder followed 36 by fully connected layers. For instance, Abramson et al. (2022) combine ResNet blocks for image processing, a learnable embedding table, a multi-modal transformer, LSTMs, and MLPs. Besides, Kim et al. (2023) utilize a Transformer-based architecture , motivated by the observation that rewards are often non-Markovian. In addition to the usual empirical risk in (12), it is also typical, as in supervised ML, to add a regularization function to prevent overfitting: N L(Ry;D) = S° e(w, (Ray). m(qi)) + AvP), (13) i=l where \\\\,.: V > Rx is a regularization function defined on the parameter space V of the underlying reward model class. For instance, Christiano et al. (2017) simply use L2 regularization and also consider dropout in some domains. Recently, Verma & Metcalf (2024) propose to define a more complex regularization term, which consists in biasing the learned rewards to be proportional to an approximate state importance provided by a trained Transformer-based forward model. The main supervised loss to train the reward model can also be augmented with additional losses corre- sponding to auxiliary tasks to avoid overfitting and improve generalizability',\n",
       "  '. The main supervised loss to train the reward model can also be augmented with additional losses corre- sponding to auxiliary tasks to avoid overfitting and improve generalizability. For instance, Abramson et al. (2022) use a behavior cloning loss and add a policy head to the reward network, thereby preventing that the reward model drifts too far from its initialization from the policy. Metcalf et al. (2023) design a reward model using state-action representations trained to be temporally consistent via self-supervised learning. On a related note, the scalar preference optimization problem has been extended to a multidimensional one by Zhong et al. (2024) to represent diverse human preferences and Marta et al. (2023) for query efficiency. 5.2.2 Bayesian Approach As is generally the case in supervised ML, there is also the variant of using Bayesian modeling for learning a target object instead of the (empirical) minimization of a loss function. To this end, one starts with a prior distribution p over the parameter space of the reward function that is updated in light of the data set D by means of Bayes theorem: P(b|D) x Ly(D) - oh), where Ly(D) = [][, P(t; is provided | m(q;), %) is the likelihood of the data under the assumed human feedback model with reward function Ry. Such an approach is used for pairwise trajectory comparisons, for instance, by Schoenauer et al. (2014) for the noisy-ridge model or by Sadigh et al',\n",
       "  '. Such an approach is used for pairwise trajectory comparisons, for instance, by Schoenauer et al. (2014) for the noisy-ridge model or by Sadigh et al. (2017) for the Boltzmann distribution as the human feedback model. In inverse RL, such Bayesian approaches have been considered as well (see Section 4.3 in Arora & Doshi (2021)). Instead of assuming that the reward functions are parameterized, one can use the reward functions directly as a parameter class and use a prior distribution over them. This could, for example, be a Gaussian process as initially considered by Kupcsik et al. (2018) for pairwise trajectory comparisons and adapted in later works (Buyik et al., 2020; Cosner et al., 2022). Here, again, it is worth mentioning that such considerations have also been made in inverse RL before (see Section 4.3 in Arora & Doshi (2021)). 5.2.3 Partial Identifiability A crucial question when it comes to learning the reward function is whether the reward function can be identified at all. If two reward functions induce exactly the same human feedback model, the reward func- tion is called partially identifiable or ambiguous. Skalse et al. (2023) study this topic for the Boltzmann distribution as the underlying human feedback model when demonstrations (inverse RL) or pairwise tra- jectory preferences are given as feedback. For demonstrations, this question has also been examined in other works (Ng & Russell, 2000; Dvijotham & Todorov, 2010; Kim et al., 2021; Cao et al., 2021a)',\n",
       "  '. For demonstrations, this question has also been examined in other works (Ng & Russell, 2000; Dvijotham & Todorov, 2010; Kim et al., 2021; Cao et al., 2021a). Ona related note, Ellis et al. (2024) tackle this identifiability issue by considering suitable acquisition functions (see Section 4.1.1). 37 5.2.4 Interpretability The field of explainable artificial intelligence (XAI) has emerged in recent years to improve the transparency and explainability of models or even to enable them in the first place. Roughly speaking, the aim is to resort to more interpretable methods or provide explanations for both experts and non-experts, shedding light on why a certain input in a (black box) model leads to a certain result. Explanations can take different forms, as can he ways to ensure the transparency of models, and for a detailed overview, we refer to Barredo Arrieta et al. (2020). It is worth noting that the field has grown so extensively over the years that even dedicated overviews ‘or the field of interpretable and explainable RL are by now available (Puiutta & Veith, 2020; Glanois et al., 2022; Qing et al., 2023; Milani et al., 2023). For the branch of RLHF, the existing works are quite sparse and mostly limited to using tree models as ransparent and explainable models for learning the reward function (Bewley & Lécué, 2022; Bewley et al., 2022; Kalra & Brown, 2022; Bewley et al., 2024; Kalra & Brown, 2023)',\n",
       "  '., 2023). For the branch of RLHF, the existing works are quite sparse and mostly limited to using tree models as ransparent and explainable models for learning the reward function (Bewley & Lécué, 2022; Bewley et al., 2022; Kalra & Brown, 2022; Bewley et al., 2024; Kalra & Brown, 2023). Another way to realize explainabil- ity within RLHF suggested by Zhang & Kashima (2023) is to learn simultaneously the reward function and he importance of states using a weight network. Assuming that for (long) trajectories, only a few states are important for the preference outcome, their framework can be used to select samples for explainability urposes. Moreover, a perturbation analysis is suggested to evaluate explanations in a quantitative manner using the learned state importance weights. 5.2.5 Online Improvements Christiano et al. (2017) demonstrate that it is important to improve the reward model online, a finding that has been confirmed by subsequent works such as the one by Gao et al. (2023), which empirically demonstrates that overoptimization of a reward model trained offline leads to performance degradation. Without online improvements, issues of overoptimization of an imperfect reward model may occur. Abramson et al. (2022) give an example of this: They attempt to fine-tune an agent initialized with behavioral cloning with an engineered reward function and find that it fails to generalize and actually worsens the performance',\n",
       "  '. Abramson et al. (2022) give an example of this: They attempt to fine-tune an agent initialized with behavioral cloning with an engineered reward function and find that it fails to generalize and actually worsens the performance. They also compare RLHF with a reward model trained offline with iterative improvement and find that iterative improvement leads to better performance, even sometimes exceeding human performance. This is related to issues posed by the approximate nature of the reward model in general, discussed in further detail in Section 6.1, but improving reward model accuracy, in general, is not sufficient: McKinney et al. (2022) further show the interdependence of the reward model and the policy, demonstrating that reward models trained online together with a policy may not be effective when a completely new policy is trained. Solutions to the problems of overoptimization and interdependence can take different forms: One is to update the reward model online with sufficient frequency using notably more on-policy queries (see Section 4.1.1), another is to improve the reward model, e.g., by leveraging ensembles or by modifying the training procedure to place additional emphasis on challenging examples , and a third, discussed in Section 6.1, is to add constraints to the policy training. 5.2.6 Learning from Multiple Feedback Types As discussed in Section 3.5, it is often desirable to combine several feedback types',\n",
       "  '.1, is to add constraints to the policy training. 5.2.6 Learning from Multiple Feedback Types As discussed in Section 3.5, it is often desirable to combine several feedback types. This requires extensions of the learning process to incorporate different sources of feedback. Learning from multiple feedback types can be achieved by pre-processing the feedback, assuming common latent factors, or by using the feedback types for distinct purposes. The first approach is demonstrated by , who infer preferences from demonstrations, allowing them to treat both types of feedback equally in the learning pipeline. In the style of the sec- ond approach, Jeon et al. (2020) proposes the unified framework of reward-rational choice, which allows for interpreting many forms of human feedback as Boltzmann-rational choices and, through this common framework, enables combination and adaptive selection of feedback types. Finally, different types of feed- back can be used for entirely different purposes, such as one for objective learning and another for safety- constraints or for representation learning (see Section 3.2.3). 38 Since multiple sources of reward information may conflict, it is important to consider how to combine them. Krasheninnikov et al. (2021) study several possible strategies of combining several reward functions in this setting, relating it to multi-task inverse RL',\n",
       "  '.2.3). 38 Since multiple sources of reward information may conflict, it is important to consider how to combine them. Krasheninnikov et al. (2021) study several possible strategies of combining several reward functions in this setting, relating it to multi-task inverse RL. Note that this challenge of conflicting sources of reward relates tightly to challenges posed when receiving diverse preferences from different labelers, as discussed in Section 5.1.5.2.7 Offline Reward Learning There is a recent trend towards offline RLHF, where both the reward model and the policy are trained offline. The offline setting is also frequently considered in RLHF theory (Section 7). Early approaches in this area (Kim et al., 2023; Shin et al., 2023) first generate queries from an offline dataset of behaviors, gather human responses, train a reward model from the resulting preference data, and then leverage offline RL algorithms to derive a policy. We do not cover these works in detail, since this survey primarily focuses on the interactive and online setting (see Section 1.3). Nonetheless, the offline setting is particularly useful for evaluating novel approaches, e.g., for active query selection, using offline datasets. We refer the interested readers to Section 8.4 for a discussion of available datasets. 5',\n",
       "  '.3). Nonetheless, the offline setting is particularly useful for evaluating novel approaches, e.g., for active query selection, using offline datasets. We refer the interested readers to Section 8.4 for a discussion of available datasets. 5.3 Evaluating Learned Reward Functions A central question when it comes to learning the reward function is how to evaluate the learned reward function and how reward functions can be compared with each other. For this purpose, different approaches are available, e.g.: Rollout Method In inverse RL, a common method for evaluation is the rollout method (Ng et al., 1999; Fu et al., 2018a; Ibarz et al., 2018; Brown et al., 2019). In this approach, one first learns an optimal policy for the learned reward function and then estimates the value of this policy for online trajectory rollouts using the known ground-truth reward function. This approach can be transferred to RLHF as well. In many cases, however, especially in safety-critical areas such as medicine or autonomous driving, such online rollouts cannot be executed. Off-policy Evaluations When online rollouts are not possible, so-called off-policy evaluations, which esti- mate the value of the optimal policy on the basis of an available data set, may be considered. For coping with biases or large variances due to policy mismatch, approaches using importance sam- pling , regression- or classification-based methods (Paduraru, 2013; Le et al., 2019; Irpan et al',\n",
       "  '. For coping with biases or large variances due to policy mismatch, approaches using importance sam- pling , regression- or classification-based methods (Paduraru, 2013; Le et al., 2019; Irpan et al., 2019), or combinations of these (Jiang & Li, 2016) have been proposed. The problem with these approaches, however, is that the traces of the explicit sources of error through policy learning or reward learning are blurred, and that they require access to the ground-truth rewards. Distance Functions Yet another alternative, which has been advanced in the seminal paper by Gleave et al. (2022a), is using a suitable distance function for reward functions. Suitable here means that two reward functions, which differ only by certain transformations such as potential shaping (Ng & Russell, 2000) or positive scaling, should have zero distance if these transformations do not change the policy ranking with regard to the expected return. For this purpose, Gleave et al. (2022a) present a pseudometric, called Equivalent-Policy Invariant Comparison (EPIC) distance, that is determined in three steps: First, mapping two reward functions to a so-called canonical- ization form that is invariant to transformations of the latter kind. Second, normalizing these canonicalization forms by means of a specific weighted L2-norm whose weights are determined by a distribution over the transitions. Finally, the EPIC distance is the weighted L2-norm distance of the normalized canonicalization forms',\n",
       "  '. Second, normalizing these canonicalization forms by means of a specific weighted L2-norm whose weights are determined by a distribution over the transitions. Finally, the EPIC distance is the weighted L2-norm distance of the normalized canonicalization forms. Even if some attractive properties, above all a Lipschitz continuity in terms of the EPIC distance of two reward functions for the difference of the value functions of the induced optimal policies is shown, this distance has its shortcomings. One of these is that the canonicalization form used by EPIC distance does not encode sufficient knowledge about the underlying transition dynamics, which might lead to unreliable distances when evaluating reward functions on physically non-realizable 39 transitions. To this end, Wulfe et al. (2022) propose the Dynamics-Aware Reward Distance (DARD), which uses a slightly different form of canonicalization but restricts the evaluation of the reward ‘unctions to transitions that are approximately physically feasible. unctional parameters fulfill certain requirements, then the result properties, e.g., being a pseudometric that is zero if and only if particular, these metrics retain the flexibility of DARD (in terms o Visual and Human Inspection For an evaluation by visual inspection,',\n",
       "  '. unctional parameters fulfill certain requirements, then the result properties, e.g., being a pseudometric that is zero if and only if particular, these metrics retain the flexibility of DARD (in terms o Visual and Human Inspection For an evaluation by visual inspection, . Recently, EPIC-like distances and STAndardised Reward Comparison (STARC) metrics , which are entire classes of pseudometrics on the space of all reward unctions were proposed that generalize the three-step approach underlying the EPIC distance (and DARD) by parameterizing each of the steps. Specifically, the canonicalization function in the first step, the normalization in the second, and the metric in the third step are kept variable. If these three ing distance has some appealing he two reward functions induce the same ordering of policies or imply upper and lower bounds on value function differences. In specifying transition dynamics), while at the same time preserving the theoretical justification of EPIC. enner & Gleave (2021) propose a method for preprocessing reward functions by transforming them into simpler but equivalent reward unctions for better interpretability. Related to this and the ro reward function learned can also be evaluated by a human (or ex of the agent on the target task',\n",
       "  '. enner & Gleave (2021) propose a method for preprocessing reward functions by transforming them into simpler but equivalent reward unctions for better interpretability. Related to this and the ro reward function learned can also be evaluated by a human (or ex of the agent on the target task. Besides, in the context of LLM lout method, the quality of the pert) by examining the behavior Is, datasets have been proposed and specifically designed to evaluate the (in)consistency of learned reward models with respect to semantic changes of prompts . 5.4 Reward Model Inputs Besides the feedback type, another factor is the modality of the reward consists of the agent’s observations and actions. Observations can range from true state to high dimensional inputs (e.g., images), while actions can range from discrete finite actions to continuous actions. For instance, many typical RL benchmarks are in the continuous control domain (e.g., MuJoCo simulate robotics tasks) with true state representations and simple discrete actions. In such problems, Christiano et al. (2017) train reward models from these inputs. When no compact state representation is available, raw images are often used in control tasks, which makes the learning of rewards more challenging since the setting becomes partially observable and the reward function is generally not Markov with respect to the observations. In sv of approximating a true state with a sequence of frames is often employed',\n",
       "  '. In sv of approximating a true state with a sequence of frames is often employed. This approach is used, for instance, by Christiano et al. (2017) to train reward models on the Atari only observations as inputs, one can resort to recurrent models or Transformer-base models . More recently, many applications of RLHF are in the natural language processing (NLP) domain. In these settings, the policy takes natural language as both input and output while the reward model takes it as inpw model input data. This usually uch cases, the conventional trick benchmark suite. When taking (see, e.g., the work by Ouyang et al. (2022)). Naturally, more complex scenarios (e.g., with both language and vision inputs ) have also been studied. 5.5 Increasing Feedback Efficiency Maximizing feedback efficiency is vital in RLHF due to the high cost of human feedback. This section delves into methods that enhance learning from limited human feedback. We discuss methods that leverage prior offline data, methods that use (partially unlabelled) data more efficiently, more informative data. 5.5.1 Using Prior Data and methods that aim to gather There are often large amounts of prior data available at little or no additional cost. While this data generally was generated for other tasks, many basic human preferences are the same for various tasks and can often 40 even be extracted from completely unsupervised data such as text corpora',\n",
       "  '. While this data generally was generated for other tasks, many basic human preferences are the same for various tasks and can often 40 even be extracted from completely unsupervised data such as text corpora. By leveraging this prior data, we can greatly reduce the amount of feedback necessary to learn the current task’s objective. We explore various methods, including meta- and transfer learning, leveraging foundation models, reward model initialization, preference model pretaining, and supervised representation learning. Meta- and Transfer Learning Meta- and transfer learning techniques in reward model training exploit the commonalities across different objectives, facilitating quick adaptation to new tasks. Ren et al. (2022) develop a broadly applicable meta-reward model, pre-trained on a diverse set of tasks to capture a wide range of preference patterns, enabling efficient adaptation to new tasks with fewer examples. Xie et al. (2018) use a similar meta-learning approach to build a goal classifier across multiple visuomotor tasks. Closely related to these meta-learning approaches, Hejna & Sadigh (2022) integrate few-shot learning principles, optimizing their approach for scenarios where only a few examples are available for adapting to new tasks. In the domain of transfer learning, Liu et al. (2023a) explore zero-shot transfer of preferences, a method that enables adapting preferences without additional data from the new task',\n",
       "  '. In the domain of transfer learning, Liu et al. (2023a) explore zero-shot transfer of preferences, a method that enables adapting preferences without additional data from the new task. In a different vein, but closely related to meta- and transfer learning, Mendez et al. (2018) tackle the lifelong inverse RL problem, focusing on inferring reward functions for multiple tasks over time, which involves knowledge transfer between tasks. Collectively, these studies underscore the potential of meta- and transfer learning in enhancing the efficiency and applicability of reward models in RLHF. Leveraging Foundation Models Foundation models, i.e., large models trained on large amounts of often unlabeled data, can acquire significant knowledge about basic human preferences. A language model trained to predict the next token in a text corpus, for example, may learn to complete the sentence ‘Frank was mad that his vacuum robot broke the vase,’ thereby learning that humans prefer non-destructive behavior. These learned preferences can then be leveraged in RLHF approaches. For instance, Kwon et al. (2023) propose to use LLM as a source of rewards. Du et al. (2023) is another example, where a success detector is trained using a pre-trained vision-language model (Flamingo). Their approach utilizes a data set of trajectories with binary success labels, employing a non-interactive training method',\n",
       "  '. Du et al. (2023) is another example, where a success detector is trained using a pre-trained vision-language model (Flamingo). Their approach utilizes a data set of trajectories with binary success labels, employing a non-interactive training method. Reward Model Initialization It is often beneficial to initialize the reward model with parameters from a model trained on a related task. This strategy is particularly common in language model fine-tuning, where self-supervised pretraining is a common practice. In such scenarios, it becomes logical to use these pre-trained models for initializing not just the policy but also the reward model. This methodology is adopted by Askell et al. (2021) and Ouyang et al. (2022). Specifically, Ouyang et al. (2022) use a pretrained language model for the reward model, opting for a smaller model relative to the policy to mitigate unstable learning. Notably, while they apply supervised fine-tuning to the policy before the RLHF phase, the reward model is initialized directly from the language model without any preliminary fine-tuning. This approach’s applicability extends beyond language models to other areas. A notable example is Abramson et al. (2022), who, in the control domain, begin by training a policy through contrastive self-supervised learning and ehavioral cloning. They then add an MLP head to the policy for the prediction of cumulative rewards. Reward Model Pretraining Reward model pretraining (Askell et al., 2021; Bai et al',\n",
       "  '. (2022), who, in the control domain, begin by training a policy through contrastive self-supervised learning and ehavioral cloning. They then add an MLP head to the policy for the prediction of cumulative rewards. Reward Model Pretraining Reward model pretraining (Askell et al., 2021; Bai et al., 2022a) leverages rior offline data to pretrain the preference model before training it on policy samples. Askell et al. (2021) note that in the case of language models, noisy preference data can be readily obtained from sources such as rated Reddit comments, preferred Stack Overflow answers, and reverted Wikipedia edits. They leverage this as a pretraining step to increase data efficiency. This is in addition to regular language model pretraining, as discussed in the previous paragraph. A similar approach could be applied to control in case prior data and some means of inferring preferences, such as human corrections, are available. Even if no inferred preferences are available, Verma & Kambhampati (2023a) show that it can be beneficial to pre-train the preference model to predict close to constant reward on an initial set of trajectories. This avoids excessive fitting of the policy to random initialization differences in the reward function. Supervised Representation Learning A compact representation that captures all relevant information for human preferences while minimizing noise can greatly enhance preference learning efficiency',\n",
       "  '. Supervised Representation Learning A compact representation that captures all relevant information for human preferences while minimizing noise can greatly enhance preference learning efficiency. It may also generalize better than a representation learned end-to-end as part of the preference learning task, which 41 may contain spurious correlations. Bobu et al. (2022) address this by proposing the learning of features hrough explicit human feedback using feature traces. Feature traces (see Section 3.2.3) involve human labelers explicitly teaching relevant features one by one by demonstrating behavior in which the feature monotonically increases or decreases. This method directly aligns the learned representation with human- identified features, enhancing preference learning efficiency but requiring detailed human input. However, eature traces require labelers to be able to identify and articulate relevant features, which can be challenging. Bobu et al. (2023) offer an alternative approach with their Similarity-based Implicit Representation Learn- ing (SIRL) method. SIRL learns representations from similarity queries (see Section 3.2.3), where human labelers provide feedback on whether behaviors are similar or different concerning the features that matter o them. This method captures a broader range of human notions of similarity without needing explicit eature knowledge, thus reducing the cognitive load on human labelers',\n",
       "  '.2.3), where human labelers provide feedback on whether behaviors are similar or different concerning the features that matter o them. This method captures a broader range of human notions of similarity without needing explicit eature knowledge, thus reducing the cognitive load on human labelers. In summary, while both approaches emphasize human feedback’s centrality in representation learning, they differ in their methods of gathering his feedback. The feature traces used by Bobu et al. (2022) require specific feature knowledge, whereas SIRL used by Bobu et al. (2023) utilizes more intuitive similarity assessments, potentially offering a more user-friendly way to capture human preferences. These diverse methods of utilizing prior data demonstrate the potential for enhancing data efficiency in RLHF, enabling more effective learning from limited human feedback. 5.5.2 Using Data More Efficiently Beyond the application of prior data, several techniques can enhance the efficiency of data utilization in training processes. This section will discuss a range of such methods, including self-supervised and semi- supervised training, as well as the integration of inductive biases and data augmentation strategies. These approaches are designed to make the most of the available human interactions and improve the final perfor- mance of RLHF models',\n",
       "  '. These approaches are designed to make the most of the available human interactions and improve the final perfor- mance of RLHF models. Self-Supervised Auxiliary Tasks Self-supervised training enhances data efficiency in reward model training by using unannotated data to capture information about the task. This technique extends beyond the scope of pretraining methods, as discussed in the prior section, to incorporating concurrent auxiliary tasks to maximize the utility of available data. A prevalent technique, as applied by Abramson et al. (2022), Brown et al. (2020), and Metcalf et al. (2023), involves adding self-supervised losses to enhance represen- tation learning for rewards. Abramson et al. (2022) implement a contrastive task where the reward net- work differentiates between observations that are consistent between multiple modalities and those that are not, blending this with preference learning loss and behavioral cloning. Brown et al. (2020) add mul- tiple auxiliary tasks such as inverse and forward dynamics modeling, temporal distance prediction, and variational autoencoder training. Similarly, Metcalf et al. (2023) use the self-predictive representations tech- nique to learn state representations that encode environmental dynamics, enabling a linear model to anticipate successor states, thereby forming an efficient basis for preference learning and significantly boosting sample efficiency',\n",
       "  '. (2023) use the self-predictive representations tech- nique to learn state representations that encode environmental dynamics, enabling a linear model to anticipate successor states, thereby forming an efficient basis for preference learning and significantly boosting sample efficiency. However, auxiliary losses for better representation learning are not the only approach to leverage self-supervised training. An alternate approach by Verma & Metcalf (2024) involves identifying important states using attention weights from a world model transformer and state im- portance estimates based on a preference predicting transformer. These estimates can aid credit assignment for observed preferences, further optimizing the training process. Semi-Supervised Training Semi-supervised training, blending labeled and unlabeled data, can leverage the unlabeled data to glean information about the task and the environment. This is most commonly done by generating pseudo-labels for the unlabeled data, either by leveraging model predictions or by making assumptions. The first approach is utilized by Cao et al. (2021b) and Zhan et al. (2021), which use generative models and GAN-based methods to mimic human preferences. Similarly, Park et al. (2022) expand their data set with high-confidence unlabeled samples based on the preference predictor’s evaluations. The second strategy, making assumptions to augment data, is showcased by Zhou & Xu (2020)',\n",
       "  '. Similarly, Park et al. (2022) expand their data set with high-confidence unlabeled samples based on the preference predictor’s evaluations. The second strategy, making assumptions to augment data, is showcased by Zhou & Xu (2020). They generate preference data by assuming that (i) human-written examples are better than model-written examples, (ii) human- written and model-written examples are indistinguishable amongst themselves, and (iii) generations of later model iterations are better than those of earlier ones. 42 Data Augmentation Data augmentation focuses on creating additional examples from existing labeled data. Temporal augmentation is particularly effective in RLHF, involving trajectory data. This is exemplified y Brown et al. (2019) and Park et al. (2022) who base their augmentation on the premise that preferences ‘or complete trajectories can be extrapolated to cropped segments, allowing the generation of multiple deriva- tive pairs from a single labeled trajectory pair. Park et al. (2022) additionally explore state modifications, such as random re-scaling and Gaussian noise addition, finding temporal cropping to be the most effective, with noise sometimes negatively impacting performance. In a similar vein, Verma & Kambhampati (2023b) ‘ocus on augmenting trajectories by concentrating on changing elements in observations and perturbing the other parts, based on the premise that movement indicates importance in image-based observations',\n",
       "  '. In a similar vein, Verma & Kambhampati (2023b) ‘ocus on augmenting trajectories by concentrating on changing elements in observations and perturbing the other parts, based on the premise that movement indicates importance in image-based observations. Com- plementing these methods, Abramson et al. (2022) employ augmentation by randomly altering instructions and language responses, thus creating artificial examples of non-preferred behavior. These diverse data aug- mentation methods collectively enhance the training data set, contributing to the increased robustness and efficacy of RLHF models. Relatedly, Meta-Reward-Net optimizes not only for the preference prediction accuracy of he learned reward function but also of the learned Q function in an actor-critic RL algorithm. This is beneficial since it avoids the phenomenon of confirmation bias, where one learned model (in this case the Q function) overfits to targets predicted by another model (the reward model). It is not strictly a data augmentation technique, but closely related in practice. 5.5.3 Gathering Better Data n addition to leveraging unlabeled data and using labels more efficiently, sample efficiency can be further increased by collecting more informative samples in the first place. This can either be achieved by selecting more informative samples from the experience buffer or by generating more informative experiences',\n",
       "  '. This can either be achieved by selecting more informative samples from the experience buffer or by generating more informative experiences. While selecting informative samples from the experience buffer is addressed under active learning (see Section 4.1.1), his section focuses on generating more informative experiences. While we are not aware of many works in this area, one possible approach involves steering the agent’s exploration towards regions of the state space where human feedback would be most beneficial. Liang et al. 2022) implement this by employing intrinsic motivation, driven by the estimated uncertainty of the reward model, to guide the agent’s exploration. This highlights the potential of not just using data more efficiently ut also generating data in a more targeted manner. 6 Policy Learning Agent Environment Dynamics Action at After learning a reward model, or, more commonly, interleaved with reward model learning, the next step is to train a policy that maximizes the expected accumulated reward. This section will algorithms for policy learning, which can be categorized into two main techniques: adaptation of conventional RL algorithms and direct policy optimization (DPO). 6.1 Adaptation of RL Algorithms Using the learned reward model, any standard RL algorithm (e.g., DQN, A3C, PPO, SAC) could potentially be applied to train a policy',\n",
       "  '. 6.1 Adaptation of RL Algorithms Using the learned reward model, any standard RL algorithm (e.g., DQN, A3C, PPO, SAC) could potentially be applied to train a policy. However, in the setting of RLHF, this direct application may suffer from two 43 issues: The non-stationary nature of the learned reward function in RLHF and the inaccuracy of intermediate reward models. We will discuss these issues and possible adaptations of RL algorithms to address them in the following. Non-Stationary Rewards RL algorithms are designed to learn a policy that maximizes the expected accumulated reward in an MDP framework, which assumes a stationary reward function. The RLHF setting violates that assumption by periodically updating the reward model, leading to a non-stationary reward ‘unction. Various works have empirically demonstrated that conventional RL algorithms can be applied nonetheless, with little to no modification. Christiano et al. (2017) argue that policy-gradient methods are better suited for non-stationary reward functions compared to value-based methods. They and various follow-up works successfully apply policy-gradient methods without any modifications in this setting. This approach has been picked up for language-model fine-tuning as well . Later works have shown that value-based methods (possibly in an actor-critic scheme) can also be effective in RLHF (Ibarz et al., 2018; Lee et al., 2021b; Park et al., 2022; Liu et al., 2022; Xue et al., 2023b)',\n",
       "  '. This approach has been picked up for language-model fine-tuning as well . Later works have shown that value-based methods (possibly in an actor-critic scheme) can also be effective in RLHF (Ibarz et al., 2018; Lee et al., 2021b; Park et al., 2022; Liu et al., 2022; Xue et al., 2023b). One rick to make value-based methods work is to use the reward model to relabel the experiences in the replay uffer whenever it is updated (Ibarz et al., 2018; Lee et al., 2021b). Similar to conventional RL, the use of such a replay buffer can greatly decrease the amount of environment interactions necessary for successful learning. As demonstrated by Gulcehre et al. (2023), the sample efficiency can be increased even further by using offline-RL techniques in a growing-batch RL setting, an offline-RL technique that iteratively increases he size of the dataset by policy rollouts while still being more sample-efficient than online RL. n addition to the basic RL approaches, there are also some policy learning approaches tailored specifically for RLHF. Wu et al. (2023) propose a policy gradient algorithm, called Pairwise Proximal Policy Optimization (P30), as an alternative to PPO, which avoids estimating the value function and at the same time is provably invariant with respect to equivalent rewards (unlike PPO). In a similar vein, Zhu et al',\n",
       "  '. Wu et al. (2023) propose a policy gradient algorithm, called Pairwise Proximal Policy Optimization (P30), as an alternative to PPO, which avoids estimating the value function and at the same time is provably invariant with respect to equivalent rewards (unlike PPO). In a similar vein, Zhu et al. (2023b) replace the KL-regularization of PPO by means of a squared error term of the logarithmic probabilities resulting in a seemingly more stable RL learner. Overoptimization of Approximate Rewards Since the learned reward model, which is only an ap- roximation of the true reward function, is used to train a policy, overoptimization or reward hacking can happen. Section 5.2.5 discusses the interdependence of the reward model and the policy in more detail as well as possible improvements from the reward model side, while here we focus on how to adapt policy training to cope with possibly inaccurate rewards in general. One approach is to regularize the policy so as not to diverge too much from human-given demonstrations. This is particularly common for language-model fine-tuning (Ouyang et al., 2022; Abramson et al., 2022), but Abramson et al. (2022) explores this for control as well. They found that this was important for some cases, in particular for deciding when to output language, but not for all. Going beyond KL-regularization, Moskovitz et al',\n",
       "  '., 2022; Abramson et al., 2022), but Abramson et al. (2022) explores this for control as well. They found that this was important for some cases, in particular for deciding when to output language, but not for all. Going beyond KL-regularization, Moskovitz et al. (2024) investigate several techniques of constrained RL to only maximize rewards up to a threshold while avoiding excessive deviation from a pre-trained policy. 6.2 Framing RLHF for Generative Models as a Bandit Problem So far, we have assumed that we ultimately want to solve a reinforcement learning problem represented by an MDP. However, especially with regard to the application of RLHF for the area of LLMs, there is now another simplified way of looking at the problem. Namely, as an instantiation of a (contextual) preference- based bandits problem , which can of course be modeled by the more general case of a Markoy decision process (MDP). In both cases, the focus is on the concept of tokens or rather sequences of tokens. However, in the MDP point of view, the state space S consists of all previous tokens and the prompt (represented as a sequence of tokens), while the action space A consists of all potential next tokens. A terminal state is often indicated here by the special token <eos> and trajectories are filled with this token until the maximum length H is reached',\n",
       "  '. A terminal state is often indicated here by the special token <eos> and trajectories are filled with this token until the maximum length H is reached. Moreover, the transition function P is degenerated (or deterministic) 44 with being one only for the state that is the concatenation of the current state and the taken action. A (latent) reward is only received at the end of the trajectory giving rise to a sparse feedback scenario. The (contextual) preference-based bandits view, on the other hand, naturally considers no state space and no transition function, but an action space consisting of all possible responses to a prompt (both represented as a sequence of tokens). Here, the prompt specifies the context for which at least two actions are executed and for which a qualitative comparison is observed as feedback. In bandit literature, this is also referred to as a “(multi-)duel”, coining the term dueling bandits. Thus, this point of view takes a trajectory-wise perspective, while the MDP point of view takes a token-wise perspective. Note that the bandit formulation considers an entire episode (response in the LLM context) as an action with a single associated reward, resulting in sparse feedback. This is in contrast to the standard RLHF formulation as it is often used in control settings, where it is assumed that the reward of a trajectory is composed of the sum of the rewards of individual steps, which allows the optimizer to distribute rewards densely as best fits the data',\n",
       "  '. This is in contrast to the standard RLHF formulation as it is often used in control settings, where it is assumed that the reward of a trajectory is composed of the sum of the rewards of individual steps, which allows the optimizer to distribute rewards densely as best fits the data. On an intuitive level, this leads to state-action pairs that often occur in preferred trajectories to be highly rewarded, without necessarily putting all reward on the terminal actions. In practice, this can lead to nicely-shaped reward functions , which cannot directly be achieved in the bandit setting. However, Chan et al. (2024) show how to take advantage of the predominantly used transformer architecture for the reward model in order to obtain a denser reward, even when assuming the bandit setting: More specifically, since the transformer architecture maintains attention weights in the last layer for each token, these can be used to attribute the overall reward signal to individual tokens. The main appeal of the bandit formulation is that it does not require exploration of the environment’s dynamics, since they are deterministic. It therefore enables simpler policy learning approaches, such as DPO or VPO , discussed in the following section. 6.3 Direct Policy Optimization The two-phase approach involving utility learning and policy optimization is not the only viable path to earning a policy from human feedback',\n",
       "  '. It therefore enables simpler policy learning approaches, such as DPO or VPO , discussed in the following section. 6.3 Direct Policy Optimization The two-phase approach involving utility learning and policy optimization is not the only viable path to earning a policy from human feedback. While we have previously discussed the case in which we learn a reward function from observed preferences by assuming a human feedback model, an emerging branch of the iterature is concerned with circumventing the reward-learning step and using preferences directly to address he actual RL problem. Concrete approaches are DPO , SLiC-HF , OPPO , DPPO , PRO , RSO (Liu et al., 2024b), or by formulating policy search as a zeroth-order optimization . Azar et al. (2023) introduce an objective called U-preference optimization (YPO) that unifies the objective functions in DPO and RLHF. More specifically, for a specific instantiation of Y, the objective in YPO recovers DPO and SLiC-HF. In addition, DPO has been further generalized to include diverse divergence constraints (Wang et al., 2024a). Besides, Hejna et al. (2024) propose contrastive preference learning based on a regret preference model instead of the usual one in RLHF. It is also possible to learn a Q function from human preferences directly, which implies a policy without the need for separate policy- and reward-model training',\n",
       "  '. (2024) propose contrastive preference learning based on a regret preference model instead of the usual one in RLHF. It is also possible to learn a Q function from human preferences directly, which implies a policy without the need for separate policy- and reward-model training . t is worth noting that approaches for directly learning the policy from preferences have been considered in the ast as well (Wilson et al., 2012; Fiirnkranz et al., 2012; Wirth & Fiirnkranz, 2013b;a; Busa-Fekete et al., 2014). In Sections 3.2.1 and 3.2.2 in the survey by Wirth et al. (2017), these methods are explained in more detail. Another recent trend in fine-tuning models with human feedback is to even manage it without the usage of RL. An alternative is based on supervised reward learning with new types of loss functions (Lee et al., 2023; Yuan et al., 2023) or a specific learning process (Dong et al., 2023; Korbak et al., 2023). There are also RL-free approaches that do not use a reward model to train a policy to execute natural-language instructions using a transformer architecture (Brohan et al., 2023; Yu et al., 2023). On a related note, Liu et al. (2024a) suggest a way how to convert human feedback to natural language sentences for the task of fine-tuning language models. 7 Theory The field of RLHF has recently made some progress in terms of theoretical results, which we will discuss in this section',\n",
       "  '., 2023). On a related note, Liu et al. (2024a) suggest a way how to convert human feedback to natural language sentences for the task of fine-tuning language models. 7 Theory The field of RLHF has recently made some progress in terms of theoretical results, which we will discuss in this section. First, we consider the contributions where the goal is to learn a provable (near) optimal policy both in an online and offline fashion or even in a way that falls in between. Then, we discuss and highlight recent contributions related to different theoretical aspects of RLHF, such as its relation to the standard reward-based RL. Tables 6 and 7 provide a concise overview of the results for the online or offline policy learning setting. Here, N¢(e,d) denotes the e-covering number of a set F under some metric d*. It is worth mentioning that (almost) all works have two standard assumptions, namely that the reward function is bounded and that the ground-truth reward, human feedback model, or transition dynamic is an element of the considered model space, respectively. 7.1 Policy Learning In the literature focusing on theoretical results, there is a distinction (similar to the distinction made in standard RL) between an offline and online setting. In the former, learning is based on a given fixed data set, usually previously collected through an interaction with the environment',\n",
       "  '. In the former, learning is based on a given fixed data set, usually previously collected through an interaction with the environment. In contrast, in the online environment, one interacts directly with the environment to learn from real-time feedback and continuously updates one’s strategies based on the feedback received, allowing the agent to learn and adapt as it engages with the environment. Accordingly, an important component of the online variant is the sampling procedure, ie., how the labels are selected. This is usually accomplished using an acquisition function that is based on uncertainty (see Section 4.1.1). Online Learning The first work dealing with the question of theoretical guarantees for learning an optimal policy from trajectory comparison feedback (see Section 3.2) in an online manner is by Novoseller et al. (2020). It laid the foundation for a paradigm subsequently embraced by many subsequent research endeavors: Adapting learning algorithms from the dueling or preference-based bandit literature (Yue & Joachims, 2009; Sui et al., 2018; Bengs et al., 2021) to the underlying situation with additional states. The preference based bandit problem can be viewed as a preference-based RL problem with one state, so state transition dynamics must be considered accordingly for a fruitful adaptation. It is worth mentioning that Jain et al. (2015) used a quite similar idea before for feedback in the form of corrections (see Section 3',\n",
       "  '. It is worth mentioning that Jain et al. (2015) used a quite similar idea before for feedback in the form of corrections (see Section 3.2) by resorting to the coactive learning setting (Shivaswamy & Joachims, 2012). Assuming the existence of a ground-truth context-trajectory scoring function and that the user’s feedback is informative, the Preference Perceptron algorithm by Shivaswamy & Joachims (2012) is used and analyzed in terms of its cumulative regret. Novoseller et al. (2020) suggest the Dueling Posterior Sampling (DPS), which is an adaptation of the self- sparring algorithm . It takes a Bayesian perspective on the problem and defines a Dirichlet rior on the dynamics and a Gaussian prior on the rewards that are subsequently updated, while the rajectories to be compared by the human labeler are chosen based on their (posterior) probability of being optimal’. Assuming a linear link function (see Section 5.1.4) as well as a tabular MDP, it is shown that DPS is (i) consistent, i.e., converges in distribution to the optimal policy, and (ii) achieves an asymptotic expected regret bound (see Table 6). Xu et al. (2020) combine dynamic programming and policy search with a black-box preference-based bandit gorithm for each state to design routines that return an almost optimal (a.k.a. e-optimal) policy with high robability®',\n",
       "  '. Xu et al. (2020) combine dynamic programming and policy search with a black-box preference-based bandit gorithm for each state to design routines that return an almost optimal (a.k.a. e-optimal) policy with high robability®. The first routine, called Preference-based Policy Search (PPS), requires access to a simulator, while the second routine, called Preference-based Exploration and Policy Search (PEPS), gets rid of this requirement by exploring the state space by means of an auxiliary synthetic reward function. By assuming hat the probability of one policy dominating another policy is bounded uniformly over all states from below y a multiplicative of their value function, they show generic upper bounds for both routines on the number 2 4The e-covering number is the minimum integer N such that there exists a subset F’ C F with |F’| = N, and for any f € F, there exists some f’ € F’ satisfying d(f, f’) < «. 5The latter probability is assessed by posterior sampling; a commonly used technique in the bandit literature used by so-called Thompson Sampling strategies, see Lattimore & Szepesvari (2020) for more details. 6This is a so-called PAC learning setting in which the goal of finding the/an optimal object is relaxed to finding a “good enough” object, usually specified by some distance measure on the object domain',\n",
       "  '. 6This is a so-called PAC learning setting in which the goal of finding the/an optimal object is relaxed to finding a “good enough” object, usually specified by some distance measure on the object domain. 46 Algorithm (Reference) Poste- Sampling Dueling rior (DPS) Algorithmic approach Leveraging Posterior Sampling from dueling bandits Assumptions Linear link function, tabular MDP Target(s) and goal(s) of learner Bayes regret mini- mization w.r.t. op- timal policy based on trajectory com- parison feedback Theoretical guarantee(s) Asymptotic regret rate: 0 (isi Viarreatan) Logistic Prefer- Leveraging MaxInP Logistic link function, Expected regret Transition dynamics: ence Reinforce- from contextual duel- tabular MDP, linear re- minimization w.r.t. ning (PbOP) proximation and general transition tion w.r.t. optimal +6 dy(F)T log (Ae (+ 4)) (Chen et al',\n",
       "  '., dynamic class Fp with policy based on ap ‘the ¢-infinit: Well 2022) finite /2-norm p-Eluder trajectory compar- ® ” ~ dimension d?)(p) and __ ison feedback (2) iv d,-’(p), respectively Preference-based = Dynamic program- Uniform dependence (e,5)-PAC for op- Simulator step bound Policy Search ming, policy search, of policy prefer- _ timal policy based o HOt1|s|W(|Al,</H,5/|S|) (PPS) (€,6)-PAC black-box ence_ probabilities on on trajectory com- = dueling bandit algo- value function differ- parison feedback Sample complexity bound oO (Fst a) rithm and simulator ences, tabular MDP, (€, 6)-PAC dueling bandit algorithm with W(K,e,d)e~* sample complexity for K arms Preference-based Similar to PPS, instead Same as PPS and_ (e,6)-PAC for op- Step complexity bound Exploration & of simulator using an _ stochastic triangle timal policy based 3 (sea </H,5/|S\\\\) Policy Search auxiliary synthetic re- inequality of trajectory on trajectory com- em (PEPS) ward function comparisons prefer- parison feedback Comparison complexity bound ences ° (2S stalee sisi) UCBVI-Planning Optimistic least- Binary rewards for (e,6)-PAC for op- Tabular MDP: (Kong & Yang, squares value iteration, state-action pairs timal policy based 2022) maximum information gain, value iteration based on _ pessimistic expected value func- tion estimation based on human re- sponse model f € Fy with bounded noise A > 0, compliant and tabular/linear MDP with dimension d on binary state- action reward feed- back = tog( HSA) ° ss',\n",
       "  'policy based 2022) maximum information gain, value iteration based on _ pessimistic expected value func- tion estimation based on human re- sponse model f € Fy with bounded noise A > 0, compliant and tabular/linear MDP with dimension d on binary state- action reward feed- back = tog( HSA) ° ss aisiAly H3|S|?|A| tog ( HSH AL ) ooo Linear MDP: 245 4 tog ( HUSIAL ° (= Pde, H ao) qo ~~ Preference-based Least-squares value it- General differentiable | Expected regret Expected regret bound: & Randomized eration with perturbed link function ®, linear — minimization w',\n",
       "  '.r.t. O(erat/ + VP. a3H5/24 Least-Squares state-action-wise re- MDP, linear rewards optimal policy 4 an7/2 411/23) Value Iteration ward model with d-dimensional fea- | and/or low trajec- . . (PR-LSVI) ture embedding of tra- tory comparison Comparison complexity bound: (Wu & Sun, jectories feedback com- o( (e+ Bmax)\" /e ) 2024) plexity steered by “= infec [— Rmax,Rmax] 6! (2) €€ (0,1) Algorithm for Iterative bilevel opti- Lipschitz assumptions Solving the bilevel | Convergence rate: Policy Alignment in Reinforcement Learning (A- PARL) mization via gradient descent based on an es- timated policy gradient on the objective func- tion, the reward func- tion, the parametric policy class, and con- vexity assumptions on the value function 47 optimization prob- lem oa/T) of pairwise trajectory comparisons (see Table 6). If these dominance probabilities have even more structural properties, such as fulfilling stoc! (2020); Bengs et al. (2021) A follow-up work by Saha et al. (2023) assumes a feature embedding of embedding of policies an essentially viewing the policy em (see Section 5.1.4), confidence se maximum likelihood estimate (M. ea traj ng (LP variance are used to samp based reinforcement learni taking the uncertainty regarding cases, i.e., known or unknown dyn: In contrast to previous wor unknown human feedback dimension’ (Russo & Van (PbOP) algorithm, which essentia‘ adap s the MaxInP algorithm (Saha, 202 eddings as the contexts',\n",
       "  '.e., known or unknown dyn: In contrast to previous wor unknown human feedback dimension’ (Russo & Van (PbOP) algorithm, which essentia‘ adap s the MaxInP algorithm (Saha, 202 eddings as the contexts. More precis LE), and the two policies with the hig ectory, respectively, to be compared. RL) is derived and also extended ti model he human feedback mode reward mode! rajec ertur rajec sampli: analyz ories 0; ories is eit in Recent optimi (ie, ( erized by ex] essence, the regret term coming from that al and transi hey derive lower bounds for the regret of any learning algorithm by reducing the once-per-episode-feedback RL problem to K-wise comparisons, where one obtains all (f ) pairwise comparisons he dynamics into account when const amics, upper bounds on the regret o: l considers tabular MDPs, Chen et a and unknown dynamics each from Roy, 2013). They propose and analyze the Preference-based O ly follows a similar design as LPbRL but uses least-s ions dynamics along with confi the PbRL problem. Fin: he human feedback model class Wu & Sun (2024) consider a similar learning scenario as Saha et al. (2023) but with t o keep the number of queries of trajectory comparisons low, which is a combination of two competing objectives also studied in the bandit Preference-based and Randomized Least-Squares Value Iteration (PR-LSVI) algorithm, which combines least-squares value iteration with a per minimization; a similar idea to CoLSTIM suggested for contextual dueling bandits',\n",
       "  '. More specifically, in each time step, the policy maximizing the value function of the pertur and the policy maximizing the later in the previous time steps are these two policies and computing their expected absolute reward difference (base bed state-action-based reward model) as a measure of uncertainty, preference queried if the uncertainty exceeds a certain threshold. Moreover, they a ng counterpart of this algorithm, the Preference-based Thompson Sampling (PbTS) algorit erms of Bayesian quantities. iterature . urbed state-action-based reward ly, Chakraborty et al. (2024) proposed a bilevel optimization problem that general zation problem for RLHF with trajectory feedback and the negative log-likelihood 2)). This problem, which they call PARL (Policy Alignment in Reinforcement Learning), is licitly taking into account the dependence on the data-collecting process a s for the expected scores of the policies are constructed based on t ally, they extend their ana hastic transitivity or stochastic triangle inequality (see Haddenhorst et al. ), then these upper bounds can be further refined. rajectories that gives rise to a feature ) for contextual dueling bandits by ely, assuming a logistic link function he hest uncertainty in terms of maximal In this way, the logistic preference- o the case of unknown dynamics by ructing the confidence sets. For both f LPbRL are shown (see Table 6).',\n",
       "  '. For both f LPbRL are shown (see Table 6). . (2022) consider the case of a general ‘unction classes with a finite Eluder timistic Planning uare estimates for ence sets based on them. Moreover, ysis to the case of ueried trajectories. In factor of VK. for AK many improves by a he additional objective For this purpose, they suggest the model with Gaussian noise for regret bed state-action-based “i ‘played’. By sampling on the eedback for these two so suggest a posterior hm, and tandard ‘unction izes the s as a loss charac- for the one leve optimal policy parameters at the other level. For this problem, A-PARL is proposed, which is shown to have an O(1/T) convergence rate under specific assumptions, where T is the number of iterations. Finally, for t dueling bandi bandits (Du he LLM training scenario, there are two perspectives on the problem (MDP vs. contextual ts) as mentioned in Section preference-based application. This k as a reverse-KL regularized contextual bandit problem. Typically the context is chosen externally, but Mehta et al. (2023) consider the learning variant in which the learning agent chooses the context as well. This variant is referred to as active contextual dueling bandits. TRoughly speaking, the Eluder dimension of a function class refers to the number of worst-case errors one must make to identify an unknown function from that class',\n",
       "  '. This variant is referred to as active contextual dueling bandits. TRoughly speaking, the Eluder dimension of a function class refers to the number of worst-case errors one must make to identify an unknown function from that class. 48 Algorithm (Reference) Pessimistic MLE Algorithmic approach Greedy _ policy Assumptions Logistic link func- Target(s) and goal(s) of learner High probability Theoretical guarantee(s) (Zhu et al',\n",
       "  '. 48 Algorithm (Reference) Pessimistic MLE Algorithmic approach Greedy _ policy Assumptions Logistic link func- Target(s) and goal(s) of learner High probability Theoretical guarantee(s) (Zhu et al., for pessimistic tion, linear reward ound for the oO Gas (eps ) 2023a) expected value function for a state- performance ™ function estima- pair feature embed- gap based on tion ding with some reg- _ trajectory-based ularity assumptions (and action- on weights, known ased) feedbac transition dynamics oF fline Re- Greedy policy General differen- High probability Transition dynamics: inforcemEnt for pessimistic tiable link function ound for the on weights, known model class entailing the value and re- ward function of the dynamic discrete choice model p-exponential decay, ° (aoe |Al(nRmax)*” y [oxivFoox¥7) p-polynomial decay, _ ati 1 = uta + patent d =population effective - sampling effective dimension LCBVI-Tabular- Offline (Kong & Yang, 2022) Maximum _in- formation gain for reward querying, value iteration based on pessimistic expected value function estima- tion for policy learning Binary rewards for state-action pairs based on human response model with bounded noise, com- pliant and tabular MDP 49 High probability bound for the performance gap based on binary state-action re- ward feedback Linear model class: o(4y /\\\\S\\\\log(|S|| A] Hn/5) Exe [Soy Manan) +0727] ) Np are numbers of visit time Offline Learning Zhu et al',\n",
       "  '. (2023a) study the performance of a greedy policy trained from a data set consisting of trajectory pairs along with the observed preference that is as: sumed to be generated by means of a Bradley-Terry model with linear rewards. For this purpose, different results with respect to the MLE of the Bradley-Terry model for different feedback scenarios are derived that are quite of independent interest. In particular, they show concentration inequalities of the MLE for trajectory-based comparison feedback and additionally its asymptotic normality for action-based comparison feedback that also holds for K-wise comparisons. Based on these, it is shown that the greedy policy using the feedback might fail while using a pessimistic MLE leads to minimax-rates gap®. The latter is also shown to be true in the case of trajectory-based fee MLE in the case of action-based with respect to the performance back. Technically, the pessimistic MLE is realized by taking the policy that has the largest pessimistic expected value function, i.e., the lowest realization of the value function within a hyperparameter-dependent confidence region around the MLE. Further results of independent interest are the inferred theoretical guaran’ ees for maximum entropy inverse RL and action-based inverse RL algorithms (Neu & Szepesvari, 2009). The simple model assumptions underlying (Zhu et al., 2023a) were then replaced by more sophisticated assumptions in some subsequent work',\n",
       "  '. The simple model assumptions underlying (Zhu et al., 2023a) were then replaced by more sophisticated assumptions in some subsequent work. The linear reward assumption has been replaced by more general reward function classes by Zhan et al. (2024a) and Li et al. (2023). In addi sider more general unknown human feedback models and construct the confidence regions for the pessimistic approach directly from the log-likelihood function. The resulting approach in terms of its performance gap, for which some problem-dependent coefficients, the per-step, per-trajectory, and transition concentrability coefficient, are introduced. On the basis o: rajectory-based and action-based comparison feedback are considered. he Iterative Data Smoothing (IDS) algorithm, which implicitly weights Zhu et al. (2023a) as these are based on the assumption of bounded utiliti Assuming a dynamic discrete choice model underlying the give: he per-trajectory concentrability coefficient should naturally appear in the bound on the performance gap. Moreover, the concentrability coefficient is shown to be upper bounded by the constant appearing in the special case of linear rewards considered by Zhu et al. (2023a). Finally, it is worth mentioning that both n follow-up work, Zhu et al. (2024) found overfitting as well as overoptimization issues of the MLE in the Boltzmann model for pairwise comparison feedback',\n",
       "  '. (2023a). Finally, it is worth mentioning that both n follow-up work, Zhu et al. (2024) found overfitting as well as overoptimization issues of the MLE in the Boltzmann model for pairwise comparison feedback. This can arise in particular if the observations of labels are strongly unbalanced and thus the utilities can become infinite. To overcome this problem, they propose heir frequency and their current likelihood. Note that these issues do not contradict the results shown by ion, Zhan et al. (2024a) also con- , called FREEHAND, is analyzed a lower bound, it is shown that observed labels appropriately by es (or rewards). n data set of observed trajectories (without explicitly observed preferences), Li et al. (2023) suggest the Dynamic-Choice-Pessimistic-Policy- Optimization (DCPPO) algorithm. It first estimates the reward model a linear function model class as well as a subset of a reproducing kernel Hi class. n both cases, the cost of ensuring label differential privacy is a multiplica using this assumption and then learns a policy in a (pessimistic) value iteration manner from the estimated reward model. In the case of a inear MDP and a known model class that entails both the value and the reward function of the dynamic discrete choice model, DCPPO is analyzed with respect to its performance gap',\n",
       "  '. In the case of a inear MDP and a known model class that entails both the value and the reward function of the dynamic discrete choice model, DCPPO is analyzed with respect to its performance gap. This is done for the case of Ibert space (RKHS) as the model Focusing on the estimation of the weight parameter in the Bradley-Terry model for the action-based feedback under label differential privacy conditions , Chowdhury & Zhou (2023) analyze two estimation rocedures, MLE and stochastic gradient descent (SGD), under similar assumptions as in Zhu et al. (2023a). tive factor. Reward collapse, a term introduced by Song et al. (2023), describes the issue when rank-based training methods for LLMs lead to the same reward distribution regardless of the prompts used in the final training steps. The authors show that this occurs because the rank-based approach does not adequately account for rompt-related information. To address this problem, the authors propose a family of utility functions as well as an optimization method that successfully creates prompt-dependent reward distributions, effectively mitigating the collapse of rewards during training. 8The expected difference between the optimal value function and the value function of the used policy. Blending Online and Offline Learning Kong & Yang (2022) study the problem of optimal policy learning from critique feedback (see Section 3.2), i.e., binary rewards for state-action pairs, with as few queries to the human as possible',\n",
       "  '. Blending Online and Offline Learning Kong & Yang (2022) study the problem of optimal policy learning from critique feedback (see Section 3.2), i.e., binary rewards for state-action pairs, with as few queries to the human as possible. They assume an underlying ground-truth human feedback model that leads to a positive evaluation for a state-action pair if it exceeds a specific threshold evaluated at that pair. In addition, the learning process consists of two phases: First, exploring the environment in an unsupervised manner, and then querying user feedback in an active reward learning phase to learn the human feedback model. This learning process is again analyzed in two variants: Either the exploration phase was performed externally, and a data set consisting of trajectories is provided (offline), or this data set is actively collected itself (online). For both variants, an active learning algorithm is proposed that essentially selects query points (state-action pairs) that provide the most information gain given the points already designated to be queried. For the online variant, an exploration strategy based on optimistic least- squares value iteration is also introduced for tabular or linear MDPs. In both variants, policy earning is carried out by a pessimistic value iteration with the empirical transitions and the estimated reward function, resulting in UCBVI-Planning (online) and LCBVI-Tabular-Offline (offline)',\n",
       "  '. In both variants, policy earning is carried out by a pessimistic value iteration with the empirical transitions and the estimated reward function, resulting in UCBVI-Planning (online) and LCBVI-Tabular-Offline (offline). Under the assumption of bounded noise (Massart & Nédélec, 2006) or low-noise assumption (Korba et al., 2017; Haddenhorst et al., 2021), bounds on the performance gap of both algorithms are derived. The question of the ideal experimental design for RLHF is addressed by Zhan et al. (2024b), in particular, how to separate the process of data acquisition (e.g., trajectories to be evaluated) from the process of retrieving human feedback to avoid constantly involving humans in the training loop. Assuming linear rewards, the Bradley-Terry model and either a transition oracle (e.g., available for tabular or low-rank MDPs) or a linear MDP they suggest the expeRimental dEsiGn for queryIng huMan prEference (REGIME) algorithm that first samples exploratory trajectories indented to be as informative as possible for learning the reward via MLE and then applies a greedy policy based on the reward learned by the latter. They explicitly show that REGIME requires less human feedback to be queried in order to output an e-optimal policy at the end than the approach by Saha et al. (2023). 7.2 Preference-Based vs',\n",
       "  '. They explicitly show that REGIME requires less human feedback to be queried in order to output an e-optimal policy at the end than the approach by Saha et al. (2023). 7.2 Preference-Based vs. Reward-Based Learning There have been some theoretical analyses regarding the question in how far, or if at all, preference-based feedback in the form of trajectory comparisons is more suitable compared to numerical feedback. Ji et al. (2023c) suggest a human rating model for this purpose in the numerical feedback case and analyze the LCB algorithm in order to compare it with the pessimistic MLE (Zhu et al., 2023a). It is shown hat under specific assumptions, LCB has a constant performance gap, while the preference-based pessimistic MLE under similar assumptions has a similar bound as in Table Wang et al. (2023b) provide reduction-based algorithms that can directly utilize state-of-the-art results in reward-based RL for RLHF with utility-based and general state-action and trajectory-based comparison feedback. They show, in general, how theoretical results of the underlying standard RL algorithm can be ranslated to theoretical results for the resulting preference-based RL algorithm. For some special cases, such as MDPs with finite Eluder dimension and utility-based preference feedback, the theoretical guarantees are explicitly derived using state-of-the-art RL algorithms that are qualitatively similar to explicit preference- ased RL algorithms. 7',\n",
       "  '. For some special cases, such as MDPs with finite Eluder dimension and utility-based preference feedback, the theoretical guarantees are explicitly derived using state-of-the-art RL algorithms that are qualitatively similar to explicit preference- ased RL algorithms. 7.3 Nash Learning from Human Feedback The majority of theoretical works use the modeling of (pairwise comparison) feedback by means of a link function (see Section 5.1.4). Even if this often leads to simpler derivations, this modeling has the decisive disadvantage that it imposes a transitivity of the human feedback that does not necessarily prevail in reality. In other words, it is quite possible that preference cycles can occur. For this reason, there is a new direction in theoretical work that dispenses with parametric modeling of the preference probability similar to Chen et al. (2022) but uses it to formulate a new learning objective. Specifically, the problem is considered from a game theory perspective, where two policies each propose a trajectory that should be highly preferred by the human user. Thus, the goal is to find a policy that suggests trajectories that are preferred to the trajectories of any other policy, i.e., a Nash equilibrium or a von Neumann winner. This learning variant was first considered by Wang et al. (2023b), who showed that the problem can be reduced to finding restricted Nash equilibria in a multi-agent RL problem (based on numerical rewards)',\n",
       "  '.e., a Nash equilibrium or a von Neumann winner. This learning variant was first considered by Wang et al. (2023b), who showed that the problem can be reduced to finding restricted Nash equilibria in a multi-agent RL problem (based on numerical rewards). For special situations of the latter problem, wrapper algorithms are proposed that have been shown to find the von Neumann winner with high probability. The learning problem was recently taken up and analyzed by Munos et al. (2023) and Ye et al. (2024) in a KL-regularization variant. While the former considers the online learning setting assuming a known preference model, the latter considers both online as well as offline learning settings and the preference model belonging to a finite function class. 8 Applications and Benchmarks The field of RLHF has advanced significantly in the last few years, with increasing interest driven by promi- nent applications. First and foremost are applications to large language models, exemplified by Chat- GPT . This section starts by providing a sample of such applications, showcasing how this technology is being utilized in fields as varied as robotics, language processing, image generation, and more. We will also delve into libraries that provide foundational support for RLHF research, enabling researchers and practitioners to experiment with and refine a range of approaches',\n",
       "  '. We will also delve into libraries that provide foundational support for RLHF research, enabling researchers and practitioners to experiment with and refine a range of approaches. We then explore a spectrum of benchmarks that have been developed to standardize and simplify the evaluation of new approaches, offering insights into their performance in different settings. Finally, and closely related to those benchmarks, we will discuss common evaluation practices. 8.1 Applications RLHF finds applications across various domains, showcasing its versatility in addressing complex and nuanced tasks. The most prominent application is ChatGPT , which is an example of an application in the domain of language models. Beyond that, however, applications extend across diverse domains such as control tasks, generative models, and recommender systems. This section provides an overview of notable works applying RLHF in different areas. Control and Interactive Environments There is a long history of using control environments as bench- mark tasks for RL. In addition to the breadth of available environments, control applications are of particular interest because tasks are often hard to specify. Christiano et al. (2017) demonstrated the effectiveness of RLHF in games as well as simulated continuous control tasks, matching the performance of RL agents trained on ground-truth rewards with a fraction of the feedback. Extending to robotics, Ding et al',\n",
       "  '. Christiano et al. (2017) demonstrated the effectiveness of RLHF in games as well as simulated continuous control tasks, matching the performance of RL agents trained on ground-truth rewards with a fraction of the feedback. Extending to robotics, Ding et al. (2023) trained a reward model for diverse tasks with a single robot, achieving human-like behavior. Kupcsik et al. (2018) applied RLHF for precise robot-to-human handovers. Similarly, Abramson et al. (2022) used RLHF in the Playhouse simulator, a platform for sensorimotor task training, and Milani et al. (2022) showcase an application in the context of the MineRL Basalt competition for Minecraft tasks. Recently, Dong et al. (2024) use RLHF to guide a diffusion-based planning model. Generative Models in Language and Imaging Generative models, i.e., models that generate new data instead of just predicting labels, can be framed as an RL setting in which a policy assembles the output through its actions. In the context of language models, this means that the language model is interpreted as a policy with tokens as actions. Using this reframing, we can use RLHF approaches to fine-tune generative models to produce preferred outputs. ChatGPT and GPT-4 are prime examples of language models fine-tuned using RLHF. These applications build on earlier work, such as by Ouyang et al. (2022), Ziegler et al. (2020) and Glaese et al. (2022). This method extends to text summariza- tion (Gao et al., 2018; 2020; Stiennon et al',\n",
       "  '. ChatGPT and GPT-4 are prime examples of language models fine-tuned using RLHF. These applications build on earlier work, such as by Ouyang et al. (2022), Ziegler et al. (2020) and Glaese et al. (2022). This method extends to text summariza- tion (Gao et al., 2018; 2020; Stiennon et al., 2020), dialogue summarization , and question answering . In image generation, Lee et al. (2023) and Xu et al. (2023) demonstrate the use of reward modeling for text-to-image tasks, while Pinto et al. (2023) and Kazemi et al. (2020) explore RLHF applications in broader computer vision tasks. Interestingly, in the context of LLMs, reward learning has also been expressed as density estimation instead of the supervised approach described in Section Recommender Systems In the context of recommender systems, Xue et al. (2023b) have shown the potential of RLHF in optimizing for long-term engagement. Although it is, in principle, possible to algorith- mically evaluate policies in this domain, these rewards are sparse. To combat this, Xue et al. (2023b) use RLHF to distill sparse, global feedback into a dense reward model. These diverse applications underscore RLHF’s adaptability and its growing importance in various techno- logical domains, paving the way for innovative solutions and enhanced human-computer interactions. 8',\n",
       "  '. (2023b) use RLHF to distill sparse, global feedback into a dense reward model. These diverse applications underscore RLHF’s adaptability and its growing importance in various techno- logical domains, paving the way for innovative solutions and enhanced human-computer interactions. 8.2 Supporting Libraries Several libraries have emerged that aim to provide a toolset for implementing and experimenting with RLHF and reward learning algorithms, contributing to the ease and efficiency of research and development. One notable example is the imitation library (Gleave et al., 2022b). It encompasses a collection of imitation and reward learning algorithms, including those introduced in the seminal work by Christiano et al. (2017). In the offline realm, Clean-Offline-RLHF provides implementations for offline RL algorithms with human feedback. Two other libraries, APReL (Biyik et al., 2022b) and POLAR , focus on the Bayesian setting. Buyik et al. (2022b) provide a specialized framework for preference-based reward earning with a focus on Bayesian methods. Meanwhile, Tucker et al. (2022) introduce a framework designed or Bayesian reward learning from multiple feedback types, including pairwise preferences, in MATLAB. Finally, the domain of language model fine-tuning, the tr1X library offers a toolkit specifically designed for language model training. It specializes in the fine-tuning of transformer-based language models, treating the language model as the policy in an RLHF setup',\n",
       "  '. Finally, the domain of language model fine-tuning, the tr1X library offers a toolkit specifically designed for language model training. It specializes in the fine-tuning of transformer-based language models, treating the language model as the policy in an RLHF setup. Due to the many interacting components and the human element in RLHF research, implementing new ideas and running experiments can be quite challenging. The discussed libraries reduce this challenge and make RLHF research more approachable to many researchers. 8. Due to the difficulty of reproducible evaluations without a ground-truth objective and with humans in the loop, benchmarks play an important role in advancing and evaluating RLHF approaches. Several benchmarks have been proposed, each focusing on different applications and challenges. One such benchmark is B-Pref (Lee et al., 2021a), which focuses on control tasks with synthetic feedback. B-Pref aims to provide simulated human feedback that captures some irrationalities, thereby coming closer o evaluation with real human feedback than other approaches. At the same time, by relying entirely on synthetic feedback, the results are reproducible and cost-effective to generate. In a similar vein, Freire et al. (2020) propose a set of environments designed to diagnose common problems in reward learning. These environments help in identifying and addressing the typical challenges that arise in RLHF scenarios',\n",
       "  '. In a similar vein, Freire et al. (2020) propose a set of environments designed to diagnose common problems in reward learning. These environments help in identifying and addressing the typical challenges that arise in RLHF scenarios. The offline RLHF setting is particularly well-suited for benchmarks, as it allows for the use of static datasets. Shin et al. (2023) evaluate pre-existing offline RL benchmarks for their suitability for RLHF evaluation, and find that many are ill-suited due to the simplicity of the required reward function. They do, however, identify a subset of these benchmarks together with their own addition for evaluation. While Shin et al. (2023) leverage synthetic rewards, Yuan et al. (2024) propose a dataset and benchmark for offline RLHF, including preference data. This helps to circumvent the challenges of synthetic feedback and benchmark reproducibility with real feedback. The MineRL BASALT competition (Shah et al., 2021b; Milani et al., 2022) gives a more application-driven benchmark with a complex environment. The competition proposes the challenge of solving tasks defined by natural language descriptions in Minecraft based on human feedback. Writing hand-engineered reward functions is very challenging in that setting, which makes it a good benchmark for methods based on human feedback. The competition is method-agnostic in principle, and non-RL approaches such as behavioral cloning are also considered',\n",
       "  '. Writing hand-engineered reward functions is very challenging in that setting, which makes it a good benchmark for methods based on human feedback. The competition is method-agnostic in principle, and non-RL approaches such as behavioral cloning are also considered. While the initial dataset consists of human demonstrations, the competition is agnostic for the feedback type, which may include demonstrations, comparisons, and others. The final evaluation is performed by humans through pairwise comparisons. In the domain of language modeling, Truthful QA serves as a benchmark that measures the truthfulness of models. Also, in the context of language models, Ramamurthy et al. (2023) introduce a set of pre-trained reward models, learned from human feedback, as benchmarks. These models serve as reference points for evaluating new RLHF techniques against established standards. Together, these benchmarks provide a diverse and comprehensive suite of tests that drive the development and refinement of RLHF methods, ensuring they are robust, effective, and capable of handling a wide range of real-world scenarios. 8.4 Datasets Due to its interactive and online nature, RLHF research often does not rely on static datasets. This is because the feedback is generally collected interactively and depends on the current policy',\n",
       "  '. 8.4 Datasets Due to its interactive and online nature, RLHF research often does not rely on static datasets. This is because the feedback is generally collected interactively and depends on the current policy. When the reward model is not refined iteratively, however, as is common practice for the related settings of LLM fine-tuning and offline RLHF, static datasets can be used. Such a static dataset can significantly simplify the development and evaluation of RLHF methods. Since language model fine-tuning is a popular application of RLHF and generally does not iteratively refine the reward model, many datasets have been developed for this purpose. Particularly notable are hh-rlhf (Bai et al., 2022a) and PKU-Safe-RLHF (Ji et al., 2023a), two datasets focusing on harm- less and helpful responses, the OpenAssistant datasets (oasst1, oasst2) , contain- ing not only response rankings but also ratings on various dimensions, the summarize_from_feedback dataset focusing on preferences over text summaries, the Stanford Human Pref- erences Dataset (SHP) , which is based on Reddit responses, the WebGPT dataset (webgpt_comparisons) , focused on long-form question answering and the HelpSteer (Wang et al., 2023c) dataset, which is not based on preferences but instead gives ratings on for 4 attributes (helpfulness, correctness, coherence, complexity) for each response',\n",
       "  '., 2023c) dataset, which is not based on preferences but instead gives ratings on for 4 attributes (helpfulness, correctness, coherence, complexity) for each response. Although static datasets are used more rarely in the control setting, some datasets have been developed for offline RLHF in this domain. Concretely, Yuan et al. (2024) propose the Uni-RLHF dataset and a benchmark ‘or offline RLHF while Kim et al. (2023) publish a dataset of real human preferences for typical offline RL asks (D4RL, Robosuite). 8.5 Evaluation Evaluating RLHF poses unique challenges, particularly in scenarios without precise ground-truth task spec- ifications. Evaluations generally focus on either the learned policy or the reward model, each shedding light on different aspects of system performance. Policy Evaluation Assessing learned behavior is crucial for the evaluation of an RLHF system. In domains with ground-truth rewards, these can be used for policy evaluation . However, many RLHF applications lack this clarity. Ouyang et al. (2022), for instance, evaluate the quality of language model responses by having labelers rate the output quality on a test set of prompts, highlighting the significance of human judgment in assessing model outputs. Jain et al. (2015) use direct Likert-scale scores for evaluations, including self-assessments by trainers and cross-evaluations by others. Losey et al',\n",
       "  '. Jain et al. (2015) use direct Likert-scale scores for evaluations, including self-assessments by trainers and cross-evaluations by others. Losey et al. (2022) extend this with a Likert-scale survey and free-form participant comments, comparing evaluations based on known true rewards with subjective experiences. Moreover, Abramson et al. (2022) employ a multi-stage evaluation scheme that includes scripted probe tasks, a standardized test suite evaluated by humans, and full interactive assessments, demonstrating the need for diverse and thorough evaluation methodologies in RLHF. Reward Model Evaluation Direct reward model evaluation complements policy assessment. While reward model accuracy is a more direct measure of preference-learning success, the ultimate goal is inducing effective policies. A perfectly accurate reward model is often not necessary to induce a good policy, which is the actual goal of RLHF. Therefore, both evaluation methods are ideally used in combination. Jain et al. (2015) also use a ranking loss method for test sets of trajectories, compared against expert evaluations with known Likert-scores. This approach provides quantitative measures of the reward model’s fidelity. n addition, Wilde & Alonso-Mora (2022) compare parameter-based and reward-based evaluation measures or learned reward functions, identifying strengths and weaknesses in both methods and contributing to a more nuanced understanding of reward model assessment in RLHF',\n",
       "  '. n addition, Wilde & Alonso-Mora (2022) compare parameter-based and reward-based evaluation measures or learned reward functions, identifying strengths and weaknesses in both methods and contributing to a more nuanced understanding of reward model assessment in RLHF. These approaches provide a quantitative measure of the reward model’s accuracy in reflecting human preferences and expert judgments. For a detailed discussion of reward model evaluation, also refer to Section 5.Policy- and reward model evaluation both offer insights into the performance of an RLHF approach. Ideally, oth measures should be combined to enable quick iteration and give insights into both the preference learning performance as well as the quality of the learned behavior. 9 Discussion and Conclusion n this survey, we have provided an overview of the current state of RLHF, highlighting its evolution from PbRL and examining its broad applications across various domains like control, natural language processing, and computer vision. While our survey captures the current state and many significant trends and advance- ments in RLHF, we acknowledge the rapid expansion of this field and the inevitable limitations in covering every extension and application in depth. We will discuss some of these extensions, open questions, and conclusions in this section. We have specifically focused on RLHF methods where a reward function is learned online from human feedback',\n",
       "  '. We will discuss some of these extensions, open questions, and conclusions in this section. We have specifically focused on RLHF methods where a reward function is learned online from human feedback. There have been some recent works that are outside of this scope and yet propose promising new methods to learn human-aligned objectives, such as offline learning of reward functions or privacy-preserving alignment based on differential privacy . An alternative approach to RLHF is to learn objectives from a pre-trained AI system instead of human feedback. This has been termed RL from AI feedback (RLAIF) and leverages foundation models as a source of preference and has shown successful for language-model fine-tuning (Bai et al., 2022b; Sun et al., 2024), generating intrinsic motivation for text-based games , as well as learning rewards (Wang et al., 2024b) or coding reward functions (Ma et al., 2024; Xie et al., 2024) for control tasks without any human involvement. Closely related is the setting of assisted evaluation, studied by Saunders et al. (2022), where a language model is used to generate critiques of language model outputs, thereby assisting human evaluators. Most work on RLHF implicitly assumes that tasks can be specified by maximization of expected accu- mulated scalar rewards. This assumption, called the reward hypothesis , is under ac- tive debate (Lambert, 2021; Vamplew et al., 2022; Bowling et al., 2023; Skalse & Abate, 2022) in the RL community',\n",
       "  '. Most work on RLHF implicitly assumes that tasks can be specified by maximization of expected accu- mulated scalar rewards. This assumption, called the reward hypothesis , is under ac- tive debate (Lambert, 2021; Vamplew et al., 2022; Bowling et al., 2023; Skalse & Abate, 2022) in the RL community. On a related note, Skalse & Abate (2024) investigate the sensitivity of inverse RL to this misspecification. Recent approaches in RLHF are, for instance, considering more complex objective func- tions, such as multi-objective frameworks involving non-linear aggregation of expected accumulated vector rewards . Many more extensions of RLHF are inspired by revisiting classic RL topics under the RLHF lens. This is exemplified by studies on exploration , reward feature learning , reward shaping , multi-task RL (Ouyang et al., 2022; Abramson et al., 2022; Myers et al., 2023), hierarchical RL , hindsight experience replay , risk-sensitive RL , safe RL (Dai et al., 2024; Cosner et al., 2022), fair RL , or continual learning . As discussed in Section 4.2, the intersection of RLHF with HCI also offers a fertile ground for future research, especially for refining feedback mechanisms. It is crucial to keep human psychology in mind when designing these systems and to learn from other related fields that already studied such issues extensively. n addition to those extensions, current RLHF methods also have challenges and limitations to be aware of',\n",
       "  '. It is crucial to keep human psychology in mind when designing these systems and to learn from other related fields that already studied such issues extensively. n addition to those extensions, current RLHF methods also have challenges and limitations to be aware of. Without assistance during feedback, it is limited by the tasks humans can reliably judge (Leike, 2022; Leike et al., 2018; Wu et al., 2021; Christiano et al., 2018). This challenge can be seen when trying to fine- une a LLM to give factual answers, where humans often prefer assertive, but wrong responses . As another limitation, current RLHF approaches often fail to learn the actual causes of human feed- back, learning correlations instead . The common separation between the policy and the ot ot reward model also limits how the agent can reason about its knowledge of the human preferences, a limitation hat may be lifted by tighter integration such as in the cooperative inverse RL setting (also called assistance games) (Hadfield-Menell et al., 2016; Shah et al., 2021a). Additionally, the RLHF framework is limited in how it can reason about its knowledge of human preferences Casper et al. (2023) offer a thorough analysis of further issues and limitations of RLHF, highlighting the practical constraints of current approaches. Adding o that, from a theoretical perspective, a primary challenge lies in further relaxing underlying assumptions',\n",
       "  '. (2023) offer a thorough analysis of further issues and limitations of RLHF, highlighting the practical constraints of current approaches. Adding o that, from a theoretical perspective, a primary challenge lies in further relaxing underlying assumptions. This requires striking a delicate balance: On the one hand, ensuring the assumptions are not overly restrictive 0 encompass a broad range of practical use cases, and on the other, maintaining the feasibility of theoretical guarantees for computationally efficient algorithms. Key questions in this context are whether it is possible o design algorithms that do not need to actively maintain a policy space and eliminate sub-optimal policies nor rely on a computation oracle. Recent work such as Wang et al. (2023b) or Wu & Sun (2024) give hope hat this may be possible. Although RLHF has significantly contributed to the advancements in LLMs and other areas of ML, it is a domain still in its infancy with many unanswered questions and inherent limitations. Despite and because of hese challenges, it is ripe for further advancements in theory and practice, hopefully resulting in even more robust algorithms making more efficient use of human feedback. It remains intriguing to what extent RLHF will continue to shape the fields of natural language processing, RL, robotics, AI alignment, and beyond in he future',\n",
       "  '. It remains intriguing to what extent RLHF will continue to shape the fields of natural language processing, RL, robotics, AI alignment, and beyond in he future. Acknowledgements We thank Tom Bewley, Andreea Bobu, Adam Gleave, Yannic Metz, Peter Stone, and Banghua Zhu for their feedback on earlier versions of this survey. This publication was supported by LMUexcellent, funded by the Federal Ministry of Education and Research (BMBF) and the Free State of Bavaria under the Excellence Strategy of the Federal Government and the Lander as well as by the Hightech Agenda Bavaria. This work has also been supported in part by the program of National Natural Science Foundation of China (No. 62176154) and a collaborative project funded by NetEase.',\n",
       "  '2308.14328v3 [cs.LG] 24 Feb 2025 1V ~arXi a © JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Reinforcement Learning for Generative AI: A Survey Yuanjiang Cao, Quan Z. Sheng, Member, IEEE, Julian McAuley, Lina Yao, Senior Member, IEEE, Abstract—Deep Generative AI has been an essential topic in the machine learning community for a long time, and it can impact a number of application areas like text generation and computer vision. The major paradigm for training a generative model is maximum likelihood estimation. This formulation successfully establishes the objective of generative tasks, while it cannot satisfy all the requirements that a user might expect from a generative model. Reinforcement learning has demonstrated its power and flexibility to inject new training signals such as human inductive bias to build a performant model. Thereby, reinforcement learning has become a trending research field and has stretched the limits of generative AI in both model design and application. It is reasonable to summarize advances in recent years with a comprehensive review. Although there have been surveys in different application areas recently, this survey aims to shed light on a high-level review that spans a range of application areas. We provide a rigorous taxonomy and make sufficient coverage on various models and applications, including the fast-developing large language model area',\n",
       "  '. We provide a rigorous taxonomy and make sufficient coverage on various models and applications, including the fast-developing large language model area. We conclude this survey by showing the potential directions that might tackle the limit of current models and expand the frontiers for generative AL Index Terms—reinforcement learning, LLMs, generative models, I. INTRODUCTION Recent years have witnessed tremendous progress in gen- erative AI, like variational autoencoders , autoregressive models (2), adversarial generative nets , diffusion models energy-based models and normalizing flows (6). The advancement of these models has brought the development of a broad range of applications, from neural language processing to image generation and scientific research. Particularly, the emergence of Large Language Models (LLM), like ChatGPT has changed the paradigm of industry and academia regarding how to develop the next generation of machine learning systems to bridge the gap toward general AI further. Another fast-developing area is Diffusion models which is the foundation for large models that generate high quality images, videos (8). and medical image analysis (9}. whose training requires large amounts of computing resources for performant image generation. Generative models also power scientific research in molecular design and optimization. Al- phaFold shows that protein structure can be effectively modeled by machine learning systems [11], (12}. Yuanjiang Cao and Quan Z',\n",
       "  '. Generative models also power scientific research in molecular design and optimization. Al- phaFold shows that protein structure can be effectively modeled by machine learning systems [11], (12}. Yuanjiang Cao and Quan Z. Sheng (Michael Sheng) are with Department of Computing, Macquarie University, Sydney, NSW, AUS. Email: yuan- jiang.cao@mg.edu.au, michael.sheng@mq.edu.au Julian McAuley is with University of California San Diego, California, USA. Email:jmcauley @eng.ucsd.edu Lina Yao is with CSIRO’s Data61 and University of New South Wales. Email: lina.yao@data61 .csiro.au Training generative model is one of the cornerstones of gen- erative AI research, which studies to design objective functions to guide the learning process. The major objective of gener- ative models is Maximum Likelihood Estimation (MLE), in other words, decreasing the Kullback-Leibler(KL) divergence between generated distribution and target data distribution. However, in some cases, human want more than what the MLE can provide. Taking conditional text generation as an example, for a text generator, we hope not only that it achieves good performance on texts that exist in the training dataset, but also that it can output texts that satisfy other desired properties like diversity, coherence, human-like, and moral considera- tions. The discrepancy between evaluation metrics and training objectives can decrease the quality of generated outputs',\n",
       "  '. The discrepancy between evaluation metrics and training objectives can decrease the quality of generated outputs. These desired properties for a text generator expose that the gap between distribution fitting and desired properties makes the maximum likelihood objective insufficient. The generalization of models connects to the limit of the Negative Log Likelihood (NLL) objective as well. In some applications of generative AI, we hope the model can cope with out-of-distribution inputs or explores out-of-distribution. For example, in novel molecule design, the goal of the learning process is to explore and generate unseen molecules instead of those in the dataset. A code summarizer or generator is expected to produce well- designed code for novel tasks instead of those in the dataset. To address the aforementioned limits, the reinforcement learning has been proposed as a useful optional training paradigm to improve the performance of generation models. Reinforcement learning is a training paradigm designed for learning from interaction . It has flexible objectives in terms of the reward function, in contrast to the distribution modeling objective of supervised learning and unsupervised learning. Furthermore, many generation problems can be re- defined as decision-making problems, creating the utility of RL methods on generation problems',\n",
       "  '. Furthermore, many generation problems can be re- defined as decision-making problems, creating the utility of RL methods on generation problems. Why this survey There are numerous existing surveys and reviews of the application of reinforcement learning scattered in various models (14) and application areas, including neural language processing [ {18}, large language model (19), code generation , speech processing , computer vision [23], [24], neural architecture search [25} drug discovery [27] . survey the area of RL applying on generative models, while it only investigates three perspectives in terms of objective design. In contrast, this work covers topics like using RL as a sampling method. For application, we explicitly list the JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15.1 Peaked Distribution 5.2 Exploration and Exploitation 15.3 Reward function design and multi objective optimization 5.4 Long-term Credit Assignment 5.5 Generalization 5.4 Model Enhancement and Control, 15.7 Human Alignment in LLM and foundation models ‘Text Summartzation Machine Translation Dialog system Human Value Alignment and Constraints | | S.1NLP ‘Text, Queries and Knowledge Graph Large Language Models RL for ther Applications in NLP ode Search Comment and Annotation Generation cout ceveaton, 87088 Goeraton Unit Test Generation Image Captioning Visual Question Answering Visual Dialog system 5',\n",
       "  '.1NLP ‘Text, Queries and Knowledge Graph Large Language Models RL for ther Applications in NLP ode Search Comment and Annotation Generation cout ceveaton, 87088 Goeraton Unit Test Generation Image Captioning Visual Question Answering Visual Dialog system 5.3 Computer Vision ‘Text-to-Image Generation (other Computer Vision Tasks Text-to-Speech Translation 5.4 Speech and Music Generation Melody Gene! Molecule Design Reaction Opti ston 55 Al for Science Micro-structure Get Design 5.6 Recommender System and Information Retrieval 5.7 Robotics Procedute Generation Robotics Simulation Optimization ‘Graph Generation |~ Exploratory Oata Analysis — *+ Semen Neu Tens eet Variational Autoencoder (VAE) Generative Adversarial Networks (GANS) 21 Generative Models | _Energy-Based Models (EBM) Autoregressive Models Normalizing Flows Markov Decision Process 2.2 Reinforcement Learning Methods | Model-based Methods Comparison Between Reinforcement Methods 2.3 Framing Generation tasks as Reinforcement Learning Problem ‘The generated varlable Is non- [5.1 Solving the Non-ditferentiable Learning _differentiable problems “The training objective ls non-cltferentiable Reward by Discriminator Reward by Hand-designed rules Reward and Divergence Reward by data-driven model 3.3 sampling Distributional Policy Gradient State and Action Design 15.4 Neural architecture Search ‘Sample Efficiency Fig',\n",
       "  '.3 sampling Distributional Policy Gradient State and Action Design 15.4 Neural architecture Search ‘Sample Efficiency Fig. 1: The Overview Structure of This Survey application area, making readers easier to access topics of their interests and also inspire new ideas in the model discussion part. We also cover niche application areas like procedure generation and graph generation. Our survey surpasses these previous surveys in terms of the selection criteria and the comprehensiveness of coverage in this area. We not only summarize lines of research across multiple areas but also organize theoretical works that aim to improve generative models by reinforcement learning tech- niques. For instance, we cover the recent advancement that the efficiency of Diffusion Model could be enhanced by shortcut fine-tuning motivated by one of the reinforcement learning method: policy gradient, as shown in Section [IM-B3] Scope and Paper selection criteria. This study offers a comprehensive analysis of the potential and obstacles asso- ciated with reinforcement learning, with a particular focus on deep reinforcement learning in the realm of generative AL The analysis encompasses their intrinsic capabilities, such as addressing non-differentiable learning issues, infusing gen- erative AI with innovative training signals, and advances in sampling and neural architecture search',\n",
       "  '. It also sheds light on the prevailing challenges, including peaked distribution, the conundrum of exploration versus exploitation, sparse reward scenarios, challenges in long-term credit allocation, and is- sues of generalization. Furthermore, the research delves into their practical applications across various domains, including Natural Language Processing (NLP), Computer Vision (CV), code synthesis, speech decoding, information extraction, rec- ommendations, robotics, and AI’s role in scientific endeavors. Emerging trends, such as the intricacies of reward function formulation, multi-objective optimization strategies, enhancing and controlling models, modeling human preferences, ensuring interoperability, and the integration of novel reinforcement learning techniques with Large Language Models (LLMs) and foundational models, are also meticulously explored. In application areas, this survey mainly focuses on se- quential generation, e.g. conditional text generation and code generation, but also contains less-mentioned vision tasks such as 3D point cloud completion. We select papers according to three selection criteria, impact, venues, and time. For classic models, we focus on high-quality works that are present in well-appreciated conferences and journals. For recent papers JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO',\n",
       "  \". We select papers according to three selection criteria, impact, venues, and time. For classic models, we focus on high-quality works that are present in well-appreciated conferences and journals. For recent papers JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 TABLE I: Notations in this Survey Notation Explanation x, the target random variable that a generative model aims to generate, D1, Liye In a sequence of variables, x; represents an element of « when «x represents a sequence. This is useful in sequence modelling. z latent variable t the time step in a sequence, often serve as an index St the state at time step ¢ in an MDP at the action at time step ¢ in an MDP. Tt the reward given by the environment at time step t in an MDP T a trajectory, a.k.a. a sequence of states, actions, and rewards (s9, 40, $1,171, ---Sn,Tn) Rt Rt = of, yr; the discounted accumulative return over a trajectory 7 , as shown aboy, is the discount factor that decreases the impact of future rewards at an exponential rate T the policy of the RL agent Vir (st) the value function of an agent given a state at time step t Qn (st, at) the value function of an agent that takes a state and an action as input pP(-),4() the probability distribution of a given variable Exvpz['] the expectation of some variable on the distribution of x Drx(allp) the KL Divergence between two distribution p and q on the same variable\",\n",
       "  '. D(-) the discriminator in GAN G(-) the generator in GAN that push the boundaries like large language models, we relaxed the criteria. For some small branches, we list all the Papers we can find. We propose this survey to systematically review the appli- cation of reinforcement learning in generative AI, including models and applications. Our contributions include « We perform a systematic and thorough examination of various directions within the field of generative models and problems using Reinforcement Learning. Recogniz- ing the multifaceted nature of these areas, we carefully organize and present an exhaustive review that encom- passes different methods, structures, and applications. « We developed a unified taxonomy, meticulously crafted to organize the extensive and varied literature in generative studies. This taxonomy not only classifies existing works according to common themes and methodologies but also draws attention to potential intersections and divergences within the field. e With a specialized focus on Reinforcement Learning (RL) methodologies within generative AI, our research provides a detailed exploration of the contemporary chal- lenges and avenues for development. We dissect the inherent complexities and hurdles in implementing RL methods and organize them in a manner that encapsulates the current state of the art. Furthermore, we identify sev- eral emerging directions that hold promise for innovative solutions. Il',\n",
       "  '. We dissect the inherent complexities and hurdles in implementing RL methods and organize them in a manner that encapsulates the current state of the art. Furthermore, we identify sev- eral emerging directions that hold promise for innovative solutions. Il. PRELIMINARY AND BACKGROUND In this section, we will provide preliminaries about the relevant concepts and models, in terms of generative models and reinforcement learning. A. Generative Models The topic of this survey is reinforcement learning applied in generative AI. Thus it is essential to provide a brief intro- duction to basic generative models, which lays the foundation for successive chapters. Before we dive into details, we list the most common notations throughout this survey in Table []] Generation is a broad research area that scatters in various application areas. We take three examples for readers to understand what is generation, then we introduce the formal definition of key concepts. The first example is a dialogue system like ChatGPT one of the most popular generative AI that covers more than 180 million users If we want to build a dialogue system, we collect a dataset consists of pairs of user prompts and human response. The goal is to train a model that takes a sequence of prompts that consists of symbols and then return a sequence of symbols as response. The symbol comprises of alphabet characters, math symbols, other specific symbols',\n",
       "  '. The goal is to train a model that takes a sequence of prompts that consists of symbols and then return a sequence of symbols as response. The symbol comprises of alphabet characters, math symbols, other specific symbols. The second example is text-to-image generation application like Midjourney , where users input a sequence of symbols and expect to get an image that satisfying the input. The third example is image-to-image translation. The user wants to transform a realistic image to other styles like an impressionistic image. The user needs to pipe the image and prompt, which is again a sequence of symbols, into the image generator. From above problem description, we can see that the goal of generation is typically requires to pipe the input variable y into the model, which could be in the form of a sequential form (y1, y2,..., Yn) and the model emits a variable x, which could be represented in the form of a sequence (21, %2,...,2m)- Given the input variable and output variable, it is easy to think that we can directly a probabilistic model p(x|y) via machine learning methods. Interestingly, this is generally infeasible for the mismatch between data we have and the task requirement. For example, we want to build a dialogue system, however, most text data we can get is acquiring from webs, these texts are not dialogue data, which means it cannot be used to learn p(2|y)',\n",
       "  \". Interestingly, this is generally infeasible for the mismatch between data we have and the task requirement. For example, we want to build a dialogue system, however, most text data we can get is acquiring from webs, these texts are not dialogue data, which means it cannot be used to learn p(2|y). Instead, we can train a model that models p(x), the distribution of the data we can get, then we can adjust the model to the task we need. In the dialogue JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 example, we can train a model with web dataset, then we can fine-tune the model based on the dialogue data we collected from interaction with human. For text-to-image generation, changing the learning objective from p(x) to p(xly) is also not difficult. For simplicity, we describes generative models whose goal is to learn p(x) in the following section. Given a dataset X composed of a set of samples {x'|1 < i > n}, we aim to use a generative model p(@) to generate a sample & that follow the true data distribution p(x), where x is the random variable that represents the true data sample, n the number of samples in the dataset. The x could be a scalar variable, a vector variable or a sequence of variables (1,22, ...,%n), Where the subscript means the ordered index in the sequence, x; means one data point of the sequence. Training and Inference The learning of a model contains two useful stages: training phase and inference stage. In training stage, a model is trained\",\n",
       "  '...,%n), Where the subscript means the ordered index in the sequence, x; means one data point of the sequence. Training and Inference The learning of a model contains two useful stages: training phase and inference stage. In training stage, a model is trained. Training a model requires an objective function or a loss function which reveals the goal the model optimizes towards. For example, language models typically use next word prediction as an objective. In this section, we introduces basic formulation of five generative models, and we use minimizing NLL as the objective function without additional description, which is supported by the maximum likelihood principle (35). The computation of NLL entails computing p(x), therefore for all generative models in this subsection, the formulation is about how to compute p(x). In inference stage, the model should generate a sample # given a latent variable z that is sampled from a given distribution like Gaussian Distribution. The inference stage is how a model is used to generate samples. Variational Autoencoder (VAE) [I], learns useful representations by reconstructing the input x with a latent variable z in consideration. Formally, we can decompose the distribution p(x) by the latent variable: va) = | plel2)o(e)ae. a) Inp(x) = Dex (4(z|2)|P(2)) + Eq(eje) np(a|z) 2) But the integral in Equation |1} is intractable in real-world applications',\n",
       "  '. Formally, we can decompose the distribution p(x) by the latent variable: va) = | plel2)o(e)ae. a) Inp(x) = Dex (4(z|2)|P(2)) + Eq(eje) np(a|z) 2) But the integral in Equation |1} is intractable in real-world applications. Therefore, an optional way is to approximate this conditional distribution by a simpler distribution g(a) with an evidence lower bound (ELBO) in Equation [2] In inference stage, VAE could sample following Equation sample a z from Gaussian Distribution, then pipe z into the decoder p(z|z) to compute the distribution, then sample a % from the distribution. Generative Adversarial Networks (GANs) BI are com- prised of a discriminator and a generator. The discriminator is trained to classify where samples come from, real datasets or generated. The generator aims to trick the discriminator. Therefore, the two networks form a zero-sum game which consequently pushes the output distribution of the generator to approximate the real distribution. Formally, the objective of GAN can be defined as: min Max Ex~paua(2) D(@) + Ez~p,(z)[L — D(G(z))|_ @) We can observe that generator could directly generates data samples by sampling a latent vector z and then pipe the vector into the generator. Energy-Based Models (EBM) represents any distri- bution density with an energy function by —E(x) Te (4) Tex p(x) = where E(x) is an energy function that outputs a density of the a certain sample x',\n",
       "  '. Energy-Based Models (EBM) represents any distri- bution density with an energy function by —E(x) Te (4) Tex p(x) = where E(x) is an energy function that outputs a density of the a certain sample x. It could be better to understand the energy function as a compatibility when the energy function is used to model a joint distribution E(x, y), the energy is lower when x and y are more compatible. For distribution p(x), we could treat E(a) as a density. EBMs do not pose constraint on tractability of the normalizing constant, making them more flexible [38]. It can be seen from this formulation that the denominator is an integral over the space Vol = Vo — Inpo(x) (5) = VoEe(x) + Vo in [ eo Bole\") (6) al WX = VoEo(x) — Ex-xpy VoEo(a ) (7) where the integral term is approximated by a Markov Chain Monte Carlo (MCMC) method that samples from the EBM, forming a contrastive objective (38). Sampling from EBM is not trivial as well. Researchers tend to use MCMC for sampling (38). fortunately, one line of research shows that it might take a few steps of sampling chain to get an acceptable sample [39]. Autoregressive Models (AR) decompose the probability distribution of a variable x into a sequence of variables by the chain rule: P(x) = p(#1, 2, ++, %n) = [][pG@ile:. w5G-1) (8) It can be naturally applied to sequence generation tasks like text generation and molecule design',\n",
       "  '. Autoregressive Models (AR) decompose the probability distribution of a variable x into a sequence of variables by the chain rule: P(x) = p(#1, 2, ++, %n) = [][pG@ile:. w5G-1) (8) It can be naturally applied to sequence generation tasks like text generation and molecule design. It is not so obvious that AR models could model an image by breaking the image into a sequence of patches, allowing AR to be used for image generation. The ordering of generation is fixed and cannot be changed during training and inference. The computational complexity of the sampling process is linear in the number of steps in the generation, and the sequential ordering makes par- allel computation difficult which leads to less efficiency. The training objective could minimize the negative log likelihood of p(x), and sampling process could generate tokens one by one, where tokens refer to x; in Equation} Although it seems the sequential generation process prevent parallel computing of generation and limits the scalability of AR, [40}-[42] etc. have been proposed to address the scalability and AR has become one of the most useful generative models. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Normalizing Flows [6], [43], [44] Besides autoregressive models, another representative generative model that is capable of direct optimization of NLL objective is Normalizing Flow',\n",
       "  '. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Normalizing Flows [6], [43], [44] Besides autoregressive models, another representative generative model that is capable of direct optimization of NLL objective is Normalizing Flow. Given a latent variable z of a tractable distribution p(z), normalizing flow can sample a data point x by transforming z through an invertible function f, forming x = f(z). Because the f(-) is an invertible function, the distribution of x could be computed by -1 Of (2) Oz In Equation I normalizing flows use the change of variable tule of distributions to get the distribution of x. This formula- tion allows fast sampling and inference by f(z). The constraint of invertible functions is strong that one step transformation is generally insufficient when the distribution of x is complex, which leads to a combination of multiple invertible transfor- mations: det p(x) = p(z) (9) det (10) Ui-1 Inp(e,) = Inp(a,) — S> In This formulation requires each transformation xz; = f;(xi—1) to efficiently compute Jacobian determinant as well as to be expressive and invertible. Capability of tractable sampling and inference makes training and inference tractable. During training, normalizing flows can compute p(x) directly by Equation [9] The sampling process has been explained above. Diffusion Model is a type of generative model that injects noise into the data and learn a reverse process to gener- ate samples',\n",
       "  \". During training, normalizing flows can compute p(x) directly by Equation [9] The sampling process has been explained above. Diffusion Model is a type of generative model that injects noise into the data and learn a reverse process to gener- ate samples. Here we briefly introduce one widely adopted diffusion model, Denoising Diffusion Probabilistic Models (DDPM). A DDPM model consists of two Markov chains, a forward chain q(xx,---,01\\\\v0) = [TfL a(ve|an—1) that perturbs the data sample with noises, and a backward chain po(%0,%1,---,UK) Tex q(xr—1|c~) that recover a sample from the data distribution given a random noise sam- ple. Note that for simplicity, we slightly abuse the subscription of x as a series of perturbing or denoising steps k € [1, K]. The 6 means the reverse process is parameterized by a neural network. The neural network is trained by minimizing the KL divergence between forward process and reverse process. Dex(q\\\\|pe) = E[-log po(xo)] (11) where the KL divergence is a upper bound of the NLL objective. Hu et al. links the KL divergence to the following loss function: Egv(1,K],ro~a(o)e~N' (0,1) ACK) lle — €0(2, KDI] 12) where ¢ is the Gaussian noise used in forward computation and A(k) is a positive weighting function that is necessary to link KL divergence to this formulation\",\n",
       "  \". Hu et al. links the KL divergence to the following loss function: Egv(1,K],ro~a(o)e~N' (0,1) ACK) lle — €0(2, KDI] 12) where ¢ is the Gaussian noise used in forward computation and A(k) is a positive weighting function that is necessary to link KL divergence to this formulation. Large Language Model is a language model that uses very large neural networks as models that typically have billons of parameters in order to train a generalized model for various applications. The famous ChatGPT [7] is a chatbot powered by the large language models. They are typically trained by two steps, the first step using a next word prediction objective, and the second step to use various methods to finetune the model in order to constrain the behaviors of the large models. B. Reinforcement Learning Methods Reinforcement learning is a computational approach to automating goal-directed learning and decision-making (13). In this section, we introduce the Markov Decision Process (MDP), a formulation that can be widely applied to rein- forcement learning problems. After formally establishing the problem, we introduce the categorization of reinforcement learning methods and the key details of these methods. 1) Markov Decision Process: Markov Decision Process is a classical formalization of sequential decision making | where actions have impacts on subsequent states and rewards\",\n",
       "  '. 1) Markov Decision Process: Markov Decision Process is a classical formalization of sequential decision making | where actions have impacts on subsequent states and rewards. The complete formulation model of an MDP contains the following five elements: e S is a set of states of the environment e Aisa set of actions of the agents e T:Sx A-— S is the transition probability distribution p(st41 se, a) e¢ RC R is the reward function that determines the goal of the agent ¢ 7 € [0,1] is the discount factor for cumulative reward computation The learning model and decision-maker is the agent, and the remaining elements outside the agent comprise the environ- ment. The experience of an agent is a sequence of interactions of discrete time steps ¢t = 0,1, 2,3,.... When an agent interacts with its environment, it observes a state s, at time step t emitted by the environment, decides the action a; based on the observation s, and responds to the environment. Given the state of the last time step s, and the action a;, the environment transitions to the state of the next time step s,;;1 and sends an immediate reward r;,, back to the agent. Without further specification, s; contains sufficient information for the agent to decide the best action. The agent learns by collecting new experience data and optimizing a policy 7 for action selection. We consider the finite episodic MDP where the state space and action space are finite. An episodic MDP has finite length of sequence',\n",
       "  '. The agent learns by collecting new experience data and optimizing a policy 7 for action selection. We consider the finite episodic MDP where the state space and action space are finite. An episodic MDP has finite length of sequence. A finite MDP has finite state space and action space. This is a realistic setting for generation tasks. For example, in de-novo molecular design, the action space is the set of all sub-part molecule embeddings, which includes atoms and bounds between atoms, and the state space is concatena- tion of actions, forming a string of aotms and bounds. Each molecule can be represented as a finite character sequence, therefore it is episodic. The atom space and the space of bounds between atoms is finite. Another example is image generation. The common approach is generate RGB pixels that have discrete and finite spaces. Given the limited size of an image, the state space and action space of image generation is finite as well. In an episodic MDP, the environment resets itself after transitioning T steps. The T-step sequence is called an episode. The goal of an agent is to achieve the highest cumulative rewards from the environment in an episode. At each time step t, the agent should select an action that achieves maximum of the cumulative rewards. The agent maps a state to an action by a deterministic policy or a probability distribution m(az|S,)',\n",
       "  '. At each time step t, the agent should select an action that achieves maximum of the cumulative rewards. The agent maps a state to an action by a deterministic policy or a probability distribution m(az|S,). Note that the cumulative reward is termed as a return at time step t, R, = foo YT ket which means that JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 RL Algorithms Related Works Value-based Q-learning [13], DON Soft Q-learning [47) (7) Policy-based Polic NFORCE radieni Actor-Critic iy TRPO (49) > (3). PPO (9 ABC [51] (51) bPG {52}, SAC {53} TABLE II: Model-free RL algorithms. Hybrid Methods the agent should consider future expected rewards instead of the reward in the current step. y is the discount factor that decreases the impact of future rewards at an exponential rate. Then the objective of an agent is formalized as nm” = argmax, E[R|7] (13) One critical assumption of an MDP compared to uncon- strained RL tasks is the Markov Property, where D(St-41; 741180; G0, $1; 1, wey Sty Ut) = p(St41,1t41|St, Gt). It guarantees that state s, and action a, determine the next state s;41 and r;, We select research branches that are related to the gen- eration literature. We go through classic models in the RL research, model-free RL and model-based RL. 2) Model-free Methods: We first describe Model-free RL. There are two main approaches: models based on value functions and models based on policy search',\n",
       "  '. We go through classic models in the RL research, model-free RL and model-based RL. 2) Model-free Methods: We first describe Model-free RL. There are two main approaches: models based on value functions and models based on policy search. Value functions are variants of the objective in Equation [13] This objective is an expectation over a sequence of rewards while the interaction unfolds step by step. At time step t, the agent’s policy is to optimize the objective in the current time step R;. Under a policy 7, the expectation of R; is the value function V\" (st) = Ex[Re|s¢] and Q*(s;, a) = Ex[Ri|s1, (s2)]- The value functions have particular recursive properties: V™(s,) Esiailti41 + YV7(se41)] and Q*(s;,a,) = Eosyy1 [rest + YQ* (St41, 7(S:41))] which are called the Bell- man Equation which makes it possible to train a model through one-step modeling. Bellman Equation could estimate the value at each time step. Compared to methods that esti- mates the value at the end of each episode, it could decreases the variance of the estimation and make it easier for the model to learn which action has the highest expected return. Bellman Equation leads to the classic Q-learning that has the following learning rules: Q™ (st, at) = Q* (st; ay) + alri — Q*(s1, az) (14) Classic model-free RL algorithms are listed in the Table [Il] DQN extends Q-learning with the neural approxima- tion of Q values',\n",
       "  '. Bellman Equation leads to the classic Q-learning that has the following learning rules: Q™ (st, at) = Q* (st; ay) + alri — Q*(s1, az) (14) Classic model-free RL algorithms are listed in the Table [Il] DQN extends Q-learning with the neural approxima- tion of Q values. It devises the experience replay method where an interaction history is collected, stored, and used to retrain the parameters of the Q function. This technique smooths the distribution of transitions, which can stablize the training process by randomly sampling from memory instead of correlated episodes sampled from recent states of environments. Different from value-based methods, Policy gradient or REINFORCE is a method that directly maximizes the objective by computing the gradient of the policy: VoE[R\\\\r] = E,~vx9[r(7) - Vo log x0(r)] O41 =O, +0r v(r) - Vo log (7) (15) r(r) - Vo log mo(7 > R, log 76 (az|52) t=1 where 7 is defined as a trajectory variable that includes states and actions. REINFORCE with baseline [13], is defined to sub- tract the value term with a baseline term. The baseline term can be any function, even a random term that takes the state as an input: Gi41 = 6; + a( Rt — b(st)) Vo log 76(s2) (16) where i represents the iteration of model parameter update, R, is the expected return term. b(s;) is the baseline. This results in an unbiased estimation of the policy with a reduced variance [13]',\n",
       "  '. b(s;) is the baseline. This results in an unbiased estimation of the policy with a reduced variance [13]. A natural choice for the baseline is the state value function V7(s,) that can capture the value fluctuations of different states. Actor Critic methods have a similar form as variance- reduced REINFORCE algorithm does, which is a branch of policy-based methods. The key difference is that the value function takes part in the value term of Equation which can increase the bias of estimation and accelerate learning by decreasing the variance, which is formulated by + yV™(se41) — V\"(st)) Vo log 76(se) (17) where the value term uses both a policy model 79(s) and an estimated value function V™(s) for bootstrapping. They can benefit from both value-based methods and policy-based method. On the one hand, the selection of actions for value functions requires the maximum values of all actions. When there is an infinite number of actions, it can be intractable to compute all the values. Policy methods can remedy this by action selection instead of value evaluation in value- based methods. On the other hand, policy models suffer from the high variance that can be alleviated by bootstrapping to accelerate learning. In actor critic method, another major innovation is the branch of Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization [50]',\n",
       "  '. On the other hand, policy models suffer from the high variance that can be alleviated by bootstrapping to accelerate learning. In actor critic method, another major innovation is the branch of Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization [50]. TRPO [49] introduces a trust region for the policy to update itself, in which the policy improvement is monotonically guaranteed theoretically. The trust region algorithm is used to compute the gradient of the following constrained optimization problem: m9 (als) Faxa(als) ene) [Dice (®.a(-|8) || toCls)] <6 where the objective function is the actor critic method that uses the trajectories of an old policy 7,,, and computes the expected return with an old Q function Qo,,,. The objective is constrained by an expectation of KL divergence between Orn. =O + (Tey max n ~ 0 Es PO o1a0U~ Toa (18) $.t. Eswpy old JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 the old policy and the new policy. This problem is estimated by a quadratic approximation solved by the conjugate gradient method. PPO uses a clipped surrogate objective to form a lower bound of the objective of TRPO. Replacing the complicated quadratic approximation, PPO utilizes only first- order optimization to solve a constrained problem: MAX Es~po,,.,0~T44 [MIN(Sfactored_poticy($, @)QGo1a(S+), clip(f factored_policy(8,@), 1 — €,1 + €)Qo',\n",
       "  '. Replacing the complicated quadratic approximation, PPO utilizes only first- order optimization to solve a constrained problem: MAX Es~po,,.,0~T44 [MIN(Sfactored_poticy($, @)QGo1a(S+), clip(f factored_policy(8,@), 1 — €,1 + €)Qo.14(8, @))] (19) —to(als)_ This formulation where ffactored_policy($,4) = Fa,,,als)\" substitutes the KL divergence constraint as a clip operation to control the divergence between 79,,, and 7@ that enables the algorithm to replace the complex optimization process. Another line of research in actor-critic models is to accel- erate training through parallel computation. The asynchronous advantage is actor-critic (A3C) is the representative example. The policy and value parameters are updated asyn- chronously. The model does not require a replay buffer because it runs multiple agents in parallel with different exploration policies that can stabilize training. It achieves good performance increases compared to DQN, Sarsa, and n-step Q. Apart from value-based and policy-based methods, hybrid methods tend to integrate the advantages of both methods to strike a balance. DPG and DDPG Silver et al. proposed the Deterministic Policy Gradient (DPG) algorithms. It allows a more efficient estimation of policy compared to policy gradient',\n",
       "  \". Apart from value-based and policy-based methods, hybrid methods tend to integrate the advantages of both methods to strike a balance. DPG and DDPG Silver et al. proposed the Deterministic Policy Gradient (DPG) algorithms. It allows a more efficient estimation of policy compared to policy gradient. DPG does not estimate the value Q” by the Monte Carlo method, it uses an actor-critic method to estimate the Q value and compute the gradient of actions directly by (20) a=r6(8) The difference between Monte Carlo based policy gradient and actor-critic is that the Monte Carlo method requires to estimate the expected reward of a state by averaging all returns of visits to a state. This requires to iterate from T' to 0, collect returns, and compute averages. Actor-critic is more easy to compute by the Bellman Equation. Lillicrap et al. extend DPG to Deep DPG (DDPG), which combines training techniques from DQN (46). DDPG incorporates experience replay and a soft target that updates the model parameters with a control variable to slow down the parameter changes. Soft Q-learning derives an energy-based policy for con- tinuous states and actions by modeling the policy distribution as a Boltzmann distribution. The reward function is modified to Re = Dypcg * (regtgs + H(r(-|se-4041))) Soft actor- critic (SAC) integrates the Soft Q-learning into actor-critic methods and optimizes towards the direction of rewards plus entropy of policy distributions\",\n",
       "  '. The reward function is modified to Re = Dypcg * (regtgs + H(r(-|se-4041))) Soft actor- critic (SAC) integrates the Soft Q-learning into actor-critic methods and optimizes towards the direction of rewards plus entropy of policy distributions. 3) Model-based Methods: In RL, models come from prior knowledge or learning. For example, for an agent playing Go, the rules of Go are fixed, thus it’s possible to get a perfect environment model by programming. Influential works like AlphaGo constructs a Monte-Carlo Tree for forward prediction. Sometimes it’s not feasible to get a perfect model. VJo = Esxaz | Vote(s) VaQ\"(s, a) In AlphaGo [55], policy learning is integrated with Monte- Carlo Tree search for the Go game. Generally, Monte-Carlo Tree search creates a tree whose nodes represent states in RL. The expansion of the tree is exploring actions and new states. Decision making is based on values on nodes. AlphaGo adds a policy network and a value network to the tree. During inference, nodes are explored and values are obtained along the tree structure. The parameters of the policy network and the value network are updated in training by policy gradient and mean squared error respectively. Actions are selected with three factors in consideration, a Q-value, a probability and a number of traversals. The Q-value is computed by mixing a state value and a reward collected from random fast rollouts',\n",
       "  '. Actions are selected with three factors in consideration, a Q-value, a probability and a number of traversals. The Q-value is computed by mixing a state value and a reward collected from random fast rollouts. AlphaZero focuses on self-play to learn a policy from scratch using an adapted version of AlphaGo models. Dyna Q-learning employs the model with domain knowledge as a predictor and data augmentor for the model- free policy, combining trial-and-error, a domain knowledge model, planning, and reactive execution into one algorithm. The dynamics model generates pseudo-experiences that are incorporated into policy training. 4) Comparison Between Reinforcement Methods: Given the fact that there are various types of RL methods, it could be difficult to select from them for a specific generation task. Generally, model-based RL requires to use a world model or learn one, in contrast to model-free methods. DQN is more suitable for discrete actions and policy gradients-based methods and actor-critic methods could handle continuous space. TRPO ie PPO enhance the stability for learning. DDPG [52] learns a deterministic policy. We suggest that researchers should start with basic algorithms like DQN or policy gradient, then investigate towards more complex algorithm training method. From the survey we observe that more works employ policy gradients than DQN. This could originate the characteristics of policy gradients and DQN',\n",
       "  '. From the survey we observe that more works employ policy gradients than DQN. This could originate the characteristics of policy gradients and DQN. For example, PPO is popular in fine-tuning a large language model, while a recent work shows that the performance of PPO could be matched by well RLOO, a well designed REINFORCE algorithm by investigating the components that works for large language model pre- training. This work argues that the PPO aims to improve stability of training for environments exploration with high variance in RL tasks, while it is not suitable for RLHF fine-tuning which has a relatively stable initial distribution. It replaces PPO with REINFORCEMENT algorithm without partial sequence reward learning which accelerates learning and achieves better performance. C. Framing Generation tasks as Reinforcement Learning Problem We use Figure [2a] to show how a generator could be framed as a reinforcement learning agent in applications. Reinforce- ment Learning problem is typically defined as a MDP as shown in Section [II-B] which is typically a sequential decision maker. therefore, we use the agent to generate a sequence @1,U2,23,...,%,. At time step t, the previously generated actions x, ..,%,_, and the task-specific context form the data JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Environment Task-specific Context Action (a) Reinforcement learning agent as generator',\n",
       "  '...,%,. At time step t, the previously generated actions x, ..,%,_, and the task-specific context form the data JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Environment Task-specific Context Action (a) Reinforcement learning agent as generator. The Signal Reward Function (b) Non-differentiable setting for generation action is integrated into the observation in the next models. The blue boxes refer to steps that might step. cause non-differentiable condition. Dashed lines represent the block of gradient and the green line show that RL uses signal to train the generator. Fig. 2: (a) Framing generation tasks as reinforcement learning framework; (b) The reward computation is non-differentiable. of the environment. For example, when the application is a visual question answering, 71, ...,;—1 are answer tokens, task specific context includes the image and the token sequence of the question. Ill. BENEFITS OF RL-BASED GENERATIVE MODELS A. Solving the non-differentiable learning problems One major use of reinforcement learning is that it can prop- agate gradients through non-differential modules. This extends the capability of neural networks because it allows the model to be trained when discrete modules exist in the computation pipeline. This characteristic is supplementary to supervised learning and unsupervised learning objectives, which both require a differentiable training pipeline. In this section, we introduce two classes of non-differentiable problems',\n",
       "  '. This characteristic is supplementary to supervised learning and unsupervised learning objectives, which both require a differentiable training pipeline. In this section, we introduce two classes of non-differentiable problems. 1) The generated variable is non-differentiable: Discrete values are prevalent in various generative applications such as computer vision, neural language processing, and molecule generation. In language applications and molecular design, elements of text and molecules are tokenized and embedded into high-dimensional space in order to capture a_ better representation. The tokens are discrete values or one-hot vectors. In computer vision, a common format of images is the RGB format which comprises discrete values of three color channels. Although it is feasible and easy to normalize the discrete values that are fed into a continuous generative model, transforming discrete values into continuous values leads to adverse effects such as weaker robustness (59). Reinforcement learning is a suitable tool for such problems. The policy gradient method is a widely adopted approach in the field of machine learning. The formulation lacks any explicit constraint governing the relationship between the gradient Vo7e(s) and the reward r(r). The utilization of a reward signal for policy training has been explored in previous works such as (60}-(62}, circuvmenting the differentiable requirement of supervised learning',\n",
       "  '. The utilization of a reward signal for policy training has been explored in previous works such as (60}-(62}, circuvmenting the differentiable requirement of supervised learning. For example, BGAN aims to tackle the inherent limitation of GANs in handling discrete data due to the absence of differentiable conditions. This is achieved by establishing a connection between policy gradient and the GAN objective. The data distribution is opti- mized by Monte-Carlo estimation,thereby mitigating variance in the policy gradient. In language modeling, [61] borrows the SeqGAN model into a visual dialog system. The generative agent outputs non-differentiable word sequences. Hence, policy gradient is leveraged as a means of facilitating knowledge transfer. Moreover, the reinforcement learning agent can control the training instead of being the generator. In this context, the control values can be discrete. For instance, in InfoNCF (62), the policy gradient is adopted to optimize the number of evaluation functions for normalizing flows in the latent space. 2) The training objective is non-differentiable: Apart from the generative variable, the training objective can be non- differentiable. Take policy gradient as an example, it allows directly injecting a non-differentiable objective as reward without further constraints. Widespread evaluation metrics for machine translation and text summarization are good examples',\n",
       "  '. Take policy gradient as an example, it allows directly injecting a non-differentiable objective as reward without further constraints. Widespread evaluation metrics for machine translation and text summarization are good examples. For example, MIXER proposes to address the exposure bias between the training and testing phase in sequence generation. Consequently, it uses test metrics, BLEU JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Environment Task-specific Input RL-based Generator Reward Function Hand-designed Rules Trained from data Fig. 3: RL can introduce new signals by flexible reward functions TABLE III: Methods to introduce new training signal What signals could be incorporated by RL Related Works Probability from a discriminator Task-specific Hand-designed Rules Distribution Discrepancy Train a reward model with human labelled data and ROUGE [66], to directly optimize the model. This section overlaps with section |I and we leave other related discussions later. B. Introducing new training signal As described in the last section, reinforcement learning methods can be employed for non-differentiable problems that exist in generation applications. In Section [IMI-AT] policy gradient has no requirements between reward and policy, in contrast to supervised learning or unsupervised learning, allowing more flexibility',\n",
       "  '. In Section [IMI-AT] policy gradient has no requirements between reward and policy, in contrast to supervised learning or unsupervised learning, allowing more flexibility. This flexibility exists in most re- inforcement learning methods in Section Thus it is straightforward to design useful reward functions as additional training objectives, which can incorporate various training signals into the generation process, making RL an influential approach in generation model and application. We demonstrate four major approaches in this section, as shown in Figure [3] 1) Reward by Discriminator: From the standpoint of train- ing signal, the discriminator component within the Generative Adversarial Network (GAN) architecture fulfills a role akin to that of a reward in the context of reinforcement learning. SeqGAN (63) first proposes to exploit this similarity by introducing a GAN to generate the sequence tokens. The reward signal is derived from the output probability, which serves as a discriminative measure between real and generated samples. Subsequent studies have expanded upon or altered the aforementioned framework through the utilization of a meticulously crafted discriminator the development of novel reward formulations (68). the implementation of actor- critic techniques (69), the incorporation of rank formulations , the utilization of multiple discriminators and other related modifications',\n",
       "  '. the implementation of actor- critic techniques (69), the incorporation of rank formulations , the utilization of multiple discriminators and other related modifications. Objective-reinforced GAN extends SeqGAN with domain-specific objectives to the reward to adapt the generated samples towards the domain-specific direction. Adversarial rewards are also employed in GRL that takes the differ- ence between two probabilities E,.p,,,, D(@) — Exxny D(2). MaskGAN replaces the baseline in REINFORCE by a critic, forming an actor-critic algorithm for text generation instead of a policy gradient in SeqGAN. It uses an in-filling task to train the agent and uses the probability of real words in the discriminator as the reward. SAL changes the reward function with comparison discriminators. The discriminators take a pair of samples (21,22) as input, which is collected from the current generated sample and previous ones. Three types of discriminators D‘>), D‘<), and D‘~) are defined to describe the relationship between the quality samples. Using coefficients for a better balance between exploration and exploitation has been taken into consideration. RankGAN follows the combination of GAN and RL, which substitutes the discriminator with a ranker and the reward is provided by a rank score estimated by the ranker given a sentence and a reference set and a comparison set',\n",
       "  '. RankGAN follows the combination of GAN and RL, which substitutes the discriminator with a ranker and the reward is provided by a rank score estimated by the ranker given a sentence and a reference set and a comparison set. The ranker is trained to work as a discriminator by increasing the score of sentences from the dataset and decreasing the score from the generator. Li et al. (98) introduce adversarial learning into the RL-based dialogue generation framework, where the reward is defined as the score produced by a discriminator on the dataset consisting of a human-generated response or auto-generated response. This method uses the RL framework to guide the generated texts by human text distribution. To prevent deteriorating the model quality, a teacher-forcing method that includes human supervision in discrimination training is adopted. ColdGAN [72] explores the exposure bias problem on the GAN-based model in the text generation tasks. It analyzes the impact of randomness on discriminators and finds that bad discriminators might mislead the generators to low-quality areas of the parameter space. Therefore, it integrates the constraints of old policies as an importance sampling strategy to balance the impact of the discriminator. It also proposes a policy merge method for a cautious generative process. MaliGAN [73] combines maximum likelihood with gradient descent. It hypothesizes that the discriminator is easy to learn and can get optimal results',\n",
       "  \". It also proposes a policy merge method for a cautious generative process. MaliGAN [73] combines maximum likelihood with gradient descent. It hypothesizes that the discriminator is easy to learn and can get optimal results. Under this assumption, the following equation JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 holds, 1 Z@) E, [r(D) log po(2)] where Z(0') = E,[r(D)] TextGAIL incorporates a contrastive discriminator where the input includes the previous sequence as well as real data or generated data. The discriminator is required to evaluate the relative realness between sequences. It also employs a PPO to decrease the variance of the RL training. 2) Reward by Hand-designed rules: Novel metrics or heuristic functions are intended to provide incentives for training, and as far as we can tell, the majority of RL algorithm implementations fall within this branch. Hand-designed rules could lead to non-differentiable objec- tives . Like MIXER [64], SCST defines the reward by the performance of the current model under the inference algorithm, including CIDEr [102], BLEU4 [65], ROUGEL [66], and METEOR [103}. Tac Tg incorporates type auxiliary guiding for code comment generation by rein- forcement learning. The Ton takes BLEU [65] (65) and ROUGE [66] as rewards. ReGen ] proposes to use reinforcement learning for non- ‘itfeouiabie evaluations like BLEU (65). METEOR [103], and chrF++ to guide text generation\",\n",
       "  '. The Ton takes BLEU [65] (65) and ROUGE [66] as rewards. ReGen ] proposes to use reinforcement learning for non- ‘itfeouiabie evaluations like BLEU (65). METEOR [103], and chrF++ to guide text generation. One line of research is incorporating testing-time metrics as a reward. This line overlaps the branch of non-differentiable objective when re oan metric is non-differentiable, such as ROUGE [64], [105j-(107] and BLEU [64], The utilization of vote. -time metrics also enhances models to combat the discrepancy between the training objective and testing objective. MIXER (64) proposes to address this problem in sequence generation by reinforcement learning. It applies REINFORCE to train an agent whose actions are next words given the current time step context. The reward is the test metric for neural language processing models. Wan et al. introduce an actor-critic algorithm to code summarization by leveraging the exploration feature in reinforcement learn- ing. Apart from testing-time metrics, task-specific reward func- tions are also popular. Li et al. propose to use a policy Ep, {log po(a)] = gradient with three considerations in dialogue generation, ease of answering, information flow, and semantic coherence. The ease of answering is measured by the negative log-likelihood NE >, log p(b|a) where b is a human constructed list of dull responses that contains sentences like “I don’t know what you are talking about”, and a is the generated sentence',\n",
       "  '. The ease of answering is measured by the negative log-likelihood NE >, log p(b|a) where b is a human constructed list of dull responses that contains sentences like “I don’t know what you are talking about”, and a is the generated sentence. The information flow is constructed by penalizing similarity be- tween two consecutive turns of the same agent. The semantic coherence is measured by the mutual information between this turn action and actions from previous turns. PETAL manages dialog systems by reinforcement learning. It studies dialog systems in real-world coffee order systems. The reward contains multiple items, representing personal reward and general reward. The personal reward reveals the interaction between the agent and the user, such as accepting or rejecting the agent’s suggestion. The global reward includes motivation for getting the user’s information, payment, and shortening the dialog. In modeling, it breaks down the value function into two parts, one standing for general value, and one for personal preference. This achieves a better transfer effect when a model is tested on new users. Zhao et al. employs RL to interact with a database in a dialog state tracking and management task, which is beyond the capability of supervised learning on this task. The agent is not only asked to respond to users but also query from a database to better manage the dialog',\n",
       "  '. Zhao et al. employs RL to interact with a database in a dialog state tracking and management task, which is beyond the capability of supervised learning on this task. The agent is not only asked to respond to users but also query from a database to better manage the dialog. Databases are used to generate synthetic data, which are combined with real data for value function learning and policy learning, like in Dyna Q-learning [57]. 3) Reward by Distribution Discrepancy: Distribution dis- crepancy can be a useful signal to be integrated into rewards. Maximizing the divergence leads to more informative gener- ative data (si), minimizing the divergence between generated distribution and some distribution could regularize the gener- ation [82]-[86]. CLARIFYDELPHI [81] uses RL to generate clarification questions to elicit moral judgment of models. The question is generated by a PPO network whose reward is the divergence between two different moral judgments of the questions. An answer simulation framework is designed to get the divergence between different answers. Motivated by the policy gradient, Fan and Lee utilizes distribution discrepancies to optimize DDPM sampling with shortcut fine-tuning. The goal is to use gradient-like optimiza- tion algorithm to explore alternative paths to discover more efficient paths and boost speed of sampling by replacing the backward process of DDPM during fine-tuning',\n",
       "  '. The goal is to use gradient-like optimiza- tion algorithm to explore alternative paths to discover more efficient paths and boost speed of sampling by replacing the backward process of DDPM during fine-tuning. The policy is initialized with a trained DDPM generator, which is then guided by a generalized critic function that serves as a measure of discrepancy between the distribution of generated data and real data, namely a generalized divergence. The gradient of a DDPM sampler is formulated to a REINFORCE with baseline mentioned in Section |II The reward is computed based on the generalized critic function that incorporates 1- Lipschitz functions as regularization. The critic function is instantiated as a neural network which is trained to minimize the reward (maximize the discrepancy between distributions) and the policy is trained to maximize the reward (minimize the discrepancy). When applying reinforcement learning algorithms to gen- JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 eration models and applications, a line of research combines reinforcement learning with supervised learning to guarantee that the model is adjusted by the reward signals but does not drift away from the supervised training objective to prevent the model from generating highly rewarded but unrealistic results. The divergence between a generated distribution and the distribution defined in the dataset is explored [83]-[86]',\n",
       "  '. The divergence between a generated distribution and the distribution defined in the dataset is explored [83]-[86]. KL-control is a technique for non-Markovian systems to minimize deviation from a prior policy 8]. Sequence Tutor (83) integrates a KL control method in order to maintain a policy generation that remains in proximity to the pre-trained language model. The reward function incorporates previous knowledge derived from a pre-trained recurrent neural network (RNN) model.. Ziegler et al. [84] incorporate human prefer- ence learning into pre-trained language models. It involves KL-control for coherence and topicality. Jaques et al. inject KL-control into discrete Q learning to impose an entropy regularization. GOLD aims to address two problems in the MLE training paradigm in text generation: diverse but low-quality samples and exposure bias. Unlike the studies mentioned above, it adopts an offline reinforcement learning algorithm. It uses a weighted policy gradient where the weights come from the training set policy because, in offline reinforce- ment learning, the agent cannot sample trajectories to estimate the weight. The weight reveals the conservative method: keep the actions on the test dataset similar to the training dataset. For reward, it uses training trajectories to approximate the probability distribution of human preference. It combines three kinds of rewards that use a one-zero reward, the product of MLE probability, and the sum of MLE probability',\n",
       "  '. For reward, it uses training trajectories to approximate the probability distribution of human preference. It combines three kinds of rewards that use a one-zero reward, the product of MLE probability, and the sum of MLE probability. 4) Reward by data-driven model: With the flexibility of the reward function, it is also a good way to incorporate models learned from the reward function. It is feasible to incorporate various guidance into reinforcement learning by training a reward model. This branch expresses a similar idea as Inverse Reinforcement Learning (IRL), which can be integrated in generative models in two ways. One is directly embedding previously defined rewards such as BLEU into a model. Shi et al. (90) conduct experiments in text generation and empirically prove its effectiveness. They adopt the maximum entropy IRL to model an approximated reward function. The training process of the reward model increases the rewards of real texts and decreases the rewards of texts that are sampled from the approximated distribution by a generator with importance sampling. Furthermore, an entropy term is added to the reward function for the agent to prevent premature mode collapse and increase the diversity of generated texts. The other way is to learn a model for human preference [109], [110]',\n",
       "  '. Furthermore, an entropy term is added to the reward function for the agent to prevent premature mode collapse and increase the diversity of generated texts. The other way is to learn a model for human preference [109], [110]. This path finally leads to the emergence of Reinforcement Learning Human Feedback (RLHF) which is integrated into the large language model research and harvest powerful models like ChatGPT [7]. RELIS [109] proposes to learn a reward function from learning to rank objectives on a document summarization task. Human pref- erences of two summaries in the form of ranking are col- lected and train the reward model by three types of loss: cross entropy, marginal ranking, and an improved marginal ranking. It is theoretically proved that the agent converges to a near-optimal solution. Nguyen et al. study the simulated human feedback in the form of ratings in neural machine translation. They are motivated by the fact that human feedback is not perfect. For example, expert ratings cannot perfectly match the goal. There are also granularity, variance, and skewness problems in the collected ratings. Therefore, they propose to simulate human feedback and address the problems mentioned above. They map the feedback to binned values, use a linear approximation to deal with large variances in middle ratings and employ a skew perturbation for harsh and motivational scores',\n",
       "  '. Therefore, they propose to simulate human feedback and address the problems mentioned above. They map the feedback to binned values, use a linear approximation to deal with large variances in middle ratings and employ a skew perturbation for harsh and motivational scores. The Open AI Reflection team [91] uses human preference to guide language models for summarization tasks. The training consists of three steps. First, trajectories from a trained policy with various baselines are collected and these trajectories are evaluated by humans to rank the best one. Then, they construct a model to learn the rewards that indicate whether the output is better. Last, they optimize a policy given the reward model. It is similar to the step in (90) while it combines datasets from human preferences, which dramatically outperforms existing methods at that time. The reward function is trained by the following function, loss(r9) = Eve,yo,y1,p) llog(o(ro(x, Up) —Te (a, yi-p)))] where x is the text before summarization, y is summarized text, rg is the reward function, y, is the human preferred text. The loss aims to maximize the distance between two rewards. Additionally, authors use KL-control to prevent the mode collapse as well as constrain the policy to be conservative, not generating weird texts far from the original supervised pre-trained distribution. InstructGPT follows the same procedure to fine-tune GPT-3 from human feedback',\n",
       "  '. Additionally, authors use KL-control to prevent the mode collapse as well as constrain the policy to be conservative, not generating weird texts far from the original supervised pre-trained distribution. InstructGPT follows the same procedure to fine-tune GPT-3 from human feedback. Results show that it improves the GPT-3 on truthfulness and generalization, and decreases the toxicity and performance regressions. Bai et al. follow the work to test RLHF on a helpful and harmless dataset. They use a PPO to train the model and use the same pipeline to learn a preference model and finetune the language model with reinforcement learning. It tests the model in an iterative online mode of training and shows that it improves the performance of the model. It also identifies a roughly linear trend between the preference reward and the square root of the KL divergence between the policy and its initialization. APRIL combines preference learning with neural TD, an algorithm that replaces the linear approximation in Linear TD with a neural network. Preference learning employs the cross-entropy between true preference and the model used to train a reward model. For the limits of human feedback collection, a pair-generation method is proposed to make the process efficient. The pair is generated on the metric of utility gap, diversity, density, and uncertainty. Given a human text y. the utility gap is used to maximize the gap to get high-quality negative samples',\n",
       "  '. The pair is generated on the metric of utility gap, diversity, density, and uncertainty. Given a human text y. the utility gap is used to maximize the gap to get high-quality negative samples. Other three metrics aim to make the selected sample diverse, located in a dense part of the distribution, and uncertain. The neural TD is used instead of DQN because the action space is large and the maintenance of Q-value is expensive. Kreutzer et al. discuss the necessity, challenges, and potential solutions of offline reinforcement learning from human feedback. The JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 4: RL can work as a sampler for models that are hard to sample such as Energy-based Models. The marginal distribution is difficult to sample for the high cost, RL-based agent provides an alternative way to generate sample sequences. THe dashed line represents the potential high cost blocks the generation. necessity of offline RL is that online adjustment of parameters is too risky and potentially out of control. The challenges include questionable counterfactual estimation for the lack of explicit exploration and degeneration problems where low- reward actions are still encouraged during training. Also, reliability and learnability are also discussed. Moreover, the fast development of large language models inspires researchers to use them as a reward function by proper prompting',\n",
       "  '. Also, reliability and learnability are also discussed. Moreover, the fast development of large language models inspires researchers to use them as a reward function by proper prompting. For example, Constitutional AI (96) proposes RLAIF that trains a harmless but non-evasive AI assistant that copes with harmful queries via expression of objection to these queries. Self-critiques and automatic revisions from an LLM are exploited to modify the dataset and retrain the LLM on it. Then, an agent is trained by preferences given by humans and models. Human provides helpfulness evaluations while the model provides harmlessness evaluations. The label is generated by an assistant model under the context that prompts contain human set principles as well as a set of few- shot examples. Apart from peakiness, Zhu et al. provide theoretical support for RLHF. They provide a sample complex- ity for the union problem of RLHF and max-entropy Inverse Reinforcement Learning. They frame the ranking-based reward model as a Plackett-Luce (PL) model or a Bradley-Terry-Luce (BTL) model, providing suboptimality bound for the reward learning process. C',\n",
       "  '. They provide a sample complex- ity for the union problem of RLHF and max-entropy Inverse Reinforcement Learning. They frame the ranking-based reward model as a Plackett-Luce (PL) model or a Bradley-Terry-Luce (BTL) model, providing suboptimality bound for the reward learning process. C. Sampling Although Energy-based Models, one type of mainstream generative models, enjoy greater expressivity and allow global constraints, they face challenges in producing samples of marginal distribution for the unnormalized distributions in the formulation, which might incur high cost during sampling, as shown in Figure /4| Reinforcement learning could be an alternative way to train a sampler for the EBM. A recent line of research studies the integration of reinforcement learning algorithm to distill knowledge in an EBM into an auto-regressive model by turning distribution matching into the training signal for the RL algorithm. Distributional Policy Gradient (112) is a pioneer work that transforms an Energy- TABLE IV: Methods in Sampling and NAS Methods Related Works Sampling NAS FHT Based Model (EBM) training process into a policy gradient algorithm. Employing reinforcement learning (RL) as a pipe to train a sampling generator, suggests addressing the distribution learning problem in Global Autoregressive Models (GAM), a subset of Energy-Based models. There are two phases to the training procedure',\n",
       "  '. Employing reinforcement learning (RL) as a pipe to train a sampling generator, suggests addressing the distribution learning problem in Global Autoregressive Models (GAM), a subset of Energy-Based models. There are two phases to the training procedure. The MLE criteria are used to train the autoregressive factor on the dataset in the first step. The second stage is the focus of (112), where a policy 7 is used to approximate a desired distribution p via the process of distillation, which facilitates the acquisition of the model parameters. The policy employs a distribution match as a reward and is trained using policy gradient via the process of deductive reasoning based on cross-entropy Vo CE(p, 7) = — Exxp(.) Vo log mo(x) (21) 1 Pl = 7 Eawno(-) mola) 2108 74(\") where the importance sampling is used to form a gradient descent algorithm with the distribution match computation as the reward. This step successfully exploits the abundant expressivity in the GAM into an auto-regressive model, where the auto-regressive model could serve as a sampler. The policy gradient method assists the sampling of a generator. In their seminal work, Khalifa et al. introduce a novel methodology that harnesses the power of distributional control in the context of conditional text generation using pre-trained language models (LLMs). In contrast to the approach proposed by Parshakova et al. {112}',\n",
       "  '. In their seminal work, Khalifa et al. introduce a novel methodology that harnesses the power of distributional control in the context of conditional text generation using pre-trained language models (LLMs). In contrast to the approach proposed by Parshakova et al. {112}. this approach leverages importance sampling while replacing the sampling distribution with an optimal distribution. The variable q is subject to updates exclusively when the condition Dxx1(p\\\\|te) < Dxx(plla) holds true. Korbak et al. propose to discover and exploit similar- ities between DPG (Distributional Policy Gradient) and policy JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 gradient. Although the gradient of DPG (Distributional Policy Gradient) cannot be reduced to a policy gradient, the variance reduction technique of policy gradients can be transferred to DPG. Korbak et al. applies KL-adaptive distributional policy gradient (KL-DPG) (113) on code generation based on pretraining an auto-regressive model. Go et al. proposes an f-DPG algorithm that allows using of any f-divergence as an objective to approximate any target distributions. This approach unifies the formulation of RLHF and DPG (Dis- tributional Policy Gradient). The reward of optimization is defined as the negative gradient of f-divergence between two distributions f’ ( mH) ). D',\n",
       "  \". This approach unifies the formulation of RLHF and DPG (Dis- tributional Policy Gradient). The reward of optimization is defined as the negative gradient of f-divergence between two distributions f’ ( mH) ). D. Neural Architecture Search Previous subsections introduce different purposes of re- inforcement learning, bridging the gap of non-differentiable learning systems, incorporating new training signals, and serv- ing as a sampler. Interestingly, the neural network architecture itself can be viewed as a sequence of tokens, therefore being the subjective reinforced generator. Uniquely, the agent itself is a generator but can be applied to almost all feasible tasks which employ neural networks as learners. In this sense, we include Neural Architecture Search (NAS) in this survey even though most applications of NAS are classification tasks. NAS is used for optimizing neural network architecture '7|. Therefore, the reward for NAS is usually the task metric. For example, when an agent is optimizing the architecture of a classifier, the accuracy of the classifier is usually used as reward [ . Zoph and Le proposes to use reinforcement learning to guide neural architecture design. They employ an RNN network to generate the architecture description with REINFORCE ENAS (118) improves the efficiency by parameter sharing. Instead of building a network from scratch, it constructs the network on pre-defined convolutional cells. This can reduce the search space\",\n",
       "  \". They employ an RNN network to generate the architecture description with REINFORCE ENAS (118) improves the efficiency by parameter sharing. Instead of building a network from scratch, it constructs the network on pre-defined convolutional cells. This can reduce the search space. Similarly, MONAS It removes states in the training and just considers actions and rewards. It takes power consumption into reward functions. Various optimization goals such as mixing, threshold, and surrogate metric are considered in the reward computation. IRLAS incorporates inverse reinforcement learning into the NAS. It defines a feature count that maps an architecture into a trajectory of the agent, b= vy, 7'(s:), where s; is the architecture information, ~y is the discount factor, 6() is the embedding function. They use the jz to create a linear model for the mirror stimuli function that aims to use the topology of the expert model such as ResNet as the guidance. State space and action space are carefully designed in NAS in order to make the training tractable. Layer parameters that are composited as the element of state and action space is a common choice [122], (125, [129]. MetaQNN con- strains the space of states and actions to make the generation tractable. Rijsdijk et al. applies MetaQNN on the NAS for side-channel-analysis. The reward function is defined con- sidering the guessing entropy with a different number of attack traces\",\n",
       "  '. MetaQNN con- strains the space of states and actions to make the generation tractable. Rijsdijk et al. applies MetaQNN on the NAS for side-channel-analysis. The reward function is defined con- sidering the guessing entropy with a different number of attack traces. BlockKQNN | employs neural model generation for image classification tasks. It defines a Network Structure Code TABLE V: Methods in Neural Language Processing Sub-areas Related Works Text Summarization Machine Translation Dialog System Human Value Alignment and Constraints Text, Queries, and Knowledge Graph Large Language Model Other NLP Applications (NSC) which quantifies the architecture information such as layer index, operation type, kernel size, and other related nodes in the computational graph. The state of E2GAN is the average value of each sub-module. The action will be how to extend the architecture, The sampling efficiency is also explored. Meta-learning [124], Pay one-shot learning are two examples. CATCH [124] employs a meta-reinforcement learning frame- work to accelerate architecture design on meta-testing tasks. RL-DARTS uses meta-learning as well. The meta- optimizer defines the gradient and a control hyperparameter as the state, the shift of the control hyperparameter as the action, and the performance on a valid dataset as the reward to meta- control the direction of the architecture searcher. DQNAS [127] combines RL-based NAS with one-shot training to get better performance',\n",
       "  '. DQNAS [127] combines RL-based NAS with one-shot training to get better performance. The key is using one-shot training to transfer weights from some layers that are common to quickly set up the training. IV. APPLICATION Reinforcement learning has been applied to abundant areas. In this section, we organize and classify the literature accord- ing to applications, aiming to provide readers a brief introduc- tion of how RL is applied on different areas. The following sections include natural language processing, code generation, computer vision, speech generation, music generation, AI for science and other small areas. A. Natural Language Processing Natural Language Processing (NLP) is one of the largest application areas of generation models and reinforcement learning. Reinforcement learning is widely employed in vari- ous NLP tasks, like text summarization, machine translation, dialog, etc. We primarily introduce the application directions but may not cover all directions. 1) Text Summarization: Text summarization is the process of automatically generating or extracting summaries from a given input document without losing important information. RL has been widely applied in this task by generating the summary [130], [131] or extracting the summary ; ; [132], [133]. Paulus et al. [130] proposes to incorporate self- 99) critical policy gradient | into text summarization. It uses ROUGE as a reward and trains the NLL objective and RL ob- jective by a weighted sum operation',\n",
       "  '. Paulus et al. [130] proposes to incorporate self- 99) critical policy gradient | into text summarization. It uses ROUGE as a reward and trains the NLL objective and RL ob- jective by a weighted sum operation. Wang et al. change the model to a convolutional sequence-to-sequence model for automatically abstractive topic summarization. ROUGE is also utilized in extractive summarization like Wu and Hu (132]. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 RL can be used as a means of hard attention mechanism which the policy outputs a discrete action to select contents, unlike in soft attention where a neural network outputs an at- tention vector that is multiplied with the content. For example, selecting sentences from a document requires hard attention. Chen et al. propose to combine RL as hard attention on the sentence level in the text summarization task. In this work, an agent is a selector who chooses sentences that are valuable to do summarization. The state is the set of documents and the last selected sentence. The action is to choose the next document sentence for extraction. VTMRL uses reinforcement learning as a hard attention mechanism to filter the less topic-coherent and background words in the task of topic modeling. The reward is defined as a sum of a coherence score and a topic overlapping value. Using RL methods for fine-tuning also attracts attention Wu et al. summarize books in a recursive way. This work uses RLHF as training method',\n",
       "  '. The reward is defined as a sum of a coherence score and a topic overlapping value. Using RL methods for fine-tuning also attracts attention Wu et al. summarize books in a recursive way. This work uses RLHF as training method. The key novelty is a book is summarized based on chapters, then these summaries are fed into the model to do the summarization again to acquire the final summary of a book. Recursive summarization could help human to quickly judge the quality of the summaries. ] uses policy gradient to adjust a pre-trained abstractive text summarization model. The reward comes from two discriminators. One of the discriminators classify the generated text and human-written text. The other discriminator constructs a ranking loss to motivate the sum- marizer produces higher probability on human-written sum- maries. controls the length of generated texts by PPO- based fine-tuning on text summarization tasks. The reward function is rule-based and calculates the difference between the generated texts and the predefined length limit including target length, upper bound length, and lower bound length. 2) Machine Translation: Reinforcement learning is applied to neural machine translation to bridge the train and test metric gap ; , to direct translation with sentiment preserved aac deetty sentences [139], to balance human efforts in the interactive system [1 Pan to change the sentiment [142], to diversify the translation output [143], or to guide curriculum design [144]',\n",
       "  '. Wu et al. uses BLEU as a reward in a systematic comparison of decision factors for RL-based NMT. Pham et al. plus BLEU with a HIT reward that counts the coverage of translated results and the annotations into the guidance of training. Kumari et al. [138] applies RL in sentiment preserved review translation. The model adopts actor-critic algorithms. The reward is constructed on a content preservation SBLEU and a sentiment reward that is a dot product of the sentiment vector and output probability distribution. DRESS proposes to employ the policy gradient on the sentence simplification tasks by introducing three desirarta simplicity, relevance, and fluency as rewards. BIP-NMT adopts an actor-critic algorithm for translation model training, which uses a threshold of action entropy for human feedback acquisition and simulate human feedback for evaluation. Zhao et al. uses actor-critic. The reward is BLEU and the negative of the times of request for human feedback. It regularizes the policy with MLE guidance of both the right tokens and human feedback. Luo et al. (142] use a similar reward formulation with a harmonic mean of two rewards to encourage the model to improve both sentiment reward and content preservation (coherence) reward. SURF defines a new reward function and explores the asynchronous training framework. The reward is defined as WP X eFGre) 4 ws X eS ESS) Sentence Fluency and Sentence-level Semantic Similarity (SLSS)',\n",
       "  '. SURF defines a new reward function and explores the asynchronous training framework. The reward is defined as WP X eFGre) 4 ws X eS ESS) Sentence Fluency and Sentence-level Semantic Similarity (SLSS). Sentence Fluency is an average log-likelihood of probabilities given by a pre- trained model. SLSS is a cosine similarity between embed- dings of input and generated sentences. uses RL to guide the curriculum design for the machine translation task. The action is selecting a batch of samples from the exposed training dataset. The state is a feature vector that contains the samples for selection and measurements that reflect the performance of the machine translation model like sentence-level log- likelihood. The reward is the performance improvement on the validation dataset. 3) Dialog System: Dialog systems or conversational agents are a complicated but fast-developing area in recent years. The influential InstructGPT (92) is trying to tackle the difficult open-domain dialog system [145}-(147], [196]. Apart from it, a task-oriented dialog system is a parallel but important . Reinforcement learning has been explored in both. Also, a hybrid dialog system emerges recently [1 MILABOT [145] tackled the Amazon Alexa Prize compe- tition to learn an open-domain chatbot. RL is used for model selection. The reward function is a linear regressor trained from collected ratings',\n",
       "  '. Reinforcement learning has been explored in both. Also, a hybrid dialog system emerges recently [1 MILABOT [145] tackled the Amazon Alexa Prize compe- tition to learn an open-domain chatbot. RL is used for model selection. The reward function is a linear regressor trained from collected ratings. PRG-DM (146) fine-tunes two policies to generate posts and responses for personalized response gen- eration by policy gradient. tests the open dialog system by interacting live with human. The system is trained with offline Q-learning with KL control to regularize the policy’s action from generating unrealistic language sequences. Sun et al. propose an imitation learning approach for complex dialogue agents. The imitation objective is defined by Donsker Varadhan’s representation of KL divergence to ease the hard problem of high dimensional optimization. HCN [148] explores training a dialog control agent with reinforcement learning. To avoid degenerated actions, it iter- atively trains with policy gradient and supervised loss. Lewis introduce RL on a negotiation task where two agents both have a set of objects and try to exchange objects to make each type of object should belong to one agent. The reward computes whether an agreement is met. Yarats apply a hierarchical generation framework and substitute the agent’s state from text tokens to latent variables to improve the effectiveness of long-term planning in this game. Li et al',\n",
       "  '. The reward computes whether an agreement is met. Yarats apply a hierarchical generation framework and substitute the agent’s state from text tokens to latent variables to improve the effectiveness of long-term planning in this game. Li et al. trains a DQN agent to do task completion in neural dialogue systems. An example is to ask an agent to book a ticket. Jaques et al. integrates implicit human pref- erences into a hierarchical open-domain dialogue generation by reinforcement learning. It employs an off-policy batch RL approach with dropout-based uncertainty estimates. Co- Gen proposes to match the latent space of actions in the external database and natural language response in conversational search. It uses RL to fine-tune the pre-trained language model with BLEU as a reward. This fine-tuning helps O JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 the model achieve better results. TrufLL [152] uses RL for answering questions. Unlike the normal fine-tuning method, it proposes using a pre-trained language model as a truncation module that takes the action space of the agent as input to make the decision-making in a large action space feasible. applies offline RL (Behaviour Cloning) in dialog agents. The reward is given by an external program which checks whether the user goal is satisfied',\n",
       "  '. applies offline RL (Behaviour Cloning) in dialog agents. The reward is given by an external program which checks whether the user goal is satisfied. This work trains on a dataset that includes the self-generated action candidates which are evaluated by a critic network to address the sparse and high- dimensional action space. [154] adopts a hierarchical RL for the medical dialog system. The reward motivates the agent to collect more information about the disease and produce a successful diagnosis. proposes a hierarchical framework in which a high-level policy determines the semantics at the utterance level that is in the latent space and a low-level policy that outputs the next word. The reward is designed to minimize repetitiveness and toxicity and enhance consistency and sentiment. incorporates A3C [ST] into negotiation dialog systems. The reward motivates the success of reaching a compromise and penalizes the result that no deal is struck, incentivizing both agents to negotiate and achieve a both preferable result. READER is designed for mental health counseling agents by generating dialogue in a hybrid way. The agent needs to understand various contents from users but provides effective and appropriate responses. 4) Human Value Alignment and Constraints: The output of generators is not well matched with human values. Models sometimes have hallucinations, generating fake information that they do not understand at all',\n",
       "  '. 4) Human Value Alignment and Constraints: The output of generators is not well matched with human values. Models sometimes have hallucinations, generating fake information that they do not understand at all. Sometimes models are impacted by datasets and spit out sentences that do not match human values in some cultures. Reinforcement learning can be used to adjust the model to work better on value matching 158], impose constraints , or even help people to combat problems like fake news . SENSEI proposes to use actor-critic to align text generation with human judgments. The reward is predicted by a binary classifier trained a human- labeled text data. RCR [159] uses a discriminator to model the violations and computes a penalty accordingly. This penalty is added to the reward to regulate the actions of the text generator. FakeGAN [160] trains a deceptive reviews classifier with a two-discriminator GAN model. Although it addresses a classification problem, the method contains generating de- ceptive reviews, which is a generation subtask. 5) Text, Queries and Knowledge Graph: Natural language is a good interface for humans but not for search engines and databases. Therefore, translating natural language into struc- tured queries and knowledge graphs poses a long-term problem in NLP. Reinforcement learning is used in these applications to optimize the query quality | create knowledge graphs 101}, complete them (68), and even causal graphs [162]. Mo- hankumar et al',\n",
       "  '. Reinforcement learning is used in these applications to optimize the query quality | create knowledge graphs 101}, complete them (68), and even causal graphs [162]. Mo- hankumar et al. incorporate human preference rewarded RL in query rewriting for advertising. The reward is provided by a fine-tuned model based on another model pre-trained in 100 languages. ReGen | proposes to use reinforcement learning to guide the text and knowledge generation based on large pre-trained models. GRL (68) integrates GAN and RL in the knowledge graph completion. The model is constructed on graph neural networks and LSTM. The state space includes a combination of both entity space and relation space in the graph. The action is to select the neighboring relational path to extend the path. CORL [162] proposes to generate a causal graph by reinforcement learning. The state is defined as an embedding of a causal graph node, the action is also in the node space but takes order constraints into account by imposing a mask to force the parent nodes chosen from a certain set. The reward is defined as Bayesian Information Criterion and the episodic reward and dense reward settings are explored. 6) Large Language Models: Recent years have witnessed a big rise of large language model 3|]. We collect and or- ganize advances of RL applications in this direction',\n",
       "  '. The reward is defined as Bayesian Information Criterion and the episodic reward and dense reward settings are explored. 6) Large Language Models: Recent years have witnessed a big rise of large language model 3|]. We collect and or- ganize advances of RL applications in this direction. Recently, to further enhance the capability of the large language model, multi-turn reinforcement learning is proposed where the reward is not spontaneous in contrast to single-turn RL. incorporates self-correction into LLM. To make the agent revise its action, the action (generated sequence) is piped into itself to learn a correction step. The reward is derived from human preferences over two multi-turn conversations. It adopts a two-stage tuning scheme to model the two attempts of the inference. The first attempt produces a response, then revises or corrects the response in the second attempt. The first stage pushes the second attempt for correction and keeps the first attempt frozen. The second stage jointly optimizes the two attempts with reward shaping to penalize falsely changing the first attempt. [164] addresses the limitation of single-turn RLHF by introducing a multi-turn algorithm that takes the history of the multi-turn interactions as states, outputs the response as actions, and the reward is generated by human preference. GAE is used to optimize the policy and KL divergence is leveraged to constrain the optimization near the original model to prevent policy collapse',\n",
       "  '. GAE is used to optimize the policy and KL divergence is leveraged to constrain the optimization near the original model to prevent policy collapse. [165] transfers hierarchical RL into mutli-turn langugage model training. The state space and action space are similar to [ . The reward is the success of long-term objective. For example, the agent is asked to search a book online, the success signal would be 1 if the book is searched. The high-level action is utterances in response of the state, and the low-level is fine-grained response of the high level action. The model could be combined with either online RL algorithms or offline RL algorithms. New reward formulations are proposed to enhance the effectiveness . uses a language model and a reinforcement learning agent to generate stories towards specific goals. The language model is responsible for generat- ing multiple goal-conditioned continuations of a story which are selected by the knowledge graph-based reinforcement learning agent. The selected sequence will be appended after previous contexts for next continuation generation. The reward caters for goal achievement and coherence reward. investigates the effectiveness of fine-grained reward for RLHF. The responses generated by an LLM is segmented into sub- sentences. Fine-grained reward entails human labeling these sub-sentences on the exact problems of the LLM response, such as inveriable facts, irrelevance, or information incom- pleteness',\n",
       "  '. The responses generated by an LLM is segmented into sub- sentences. Fine-grained reward entails human labeling these sub-sentences on the exact problems of the LLM response, such as inveriable facts, irrelevance, or information incom- pleteness. [168] proposes RLSF which exploits the symbolic JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 feedback system to guide the fine-tuning of an LLM. A certificate of each response the LLM produces is generated by a symbolic reasoner, which is fed into the reward model for PPO-based fine-tuning. This work applies the method to the task that transforms a psuedo-code to C++ code. [169] proposes a token-level reward model unlike common instance- level reward model for the LLM tuning. The token-level capability is acquired by distilling a generative correction model that generates a probability in deciding a reward for each token. The reward model is combined with PPO to fine- tune the LLM. [170] uses token-level feedbacks for LLM fine-tuning. The reward function provides token-level reward by computing the difference between probabilities before and after the word is generated. To increase robustness, this work also uses a First quantize then noise” strategy that uses quantized rewards and inject noise into rewards but keep them in the interval. [171] composes an algorithm that can balance multi-objective during PPO-based fine-tuning. The reward function is a weighted average of all specific rewards',\n",
       "  '. [171] composes an algorithm that can balance multi-objective during PPO-based fine-tuning. The reward function is a weighted average of all specific rewards. The method uses mirror descent and smooth to update the weights which guarantees that mitigates over-focus on single objective. This method is used to make the reward composition fair and stable. Apart from reward function, new advances in objective functions vet 58}, (172)- [179], reflection capability (199). (200). scaling [201], ensemble [202] are proposed to further i a large language models. he to RLOO [58], ReMax is proposed to leverage a variant of REINFORCE to aE stitute PPO in RLHF [91]. [203] proposes to enhance the reasoning capability of LLM on techniques like Expert iteration, Return-conditioned RL, and outcome-based reward modeling. DRO is proposed to exploit single-trajectory dataset in LLM fine-tuning. Single trajectory dataset means that one data point consists of a prompt, a response, and human preference. In contrast, DPO requires each data point to have two responses for preference learning. DRO achieves this by changing the objective by a MSE loss between reward, value function, and the KL divergence regularization term. fine-tunes an LLM with an advantage-based of- fline RL on pre-existing dataset. The LLM is trained with the importance sampling on data collected by a reference LLM. To increase the robustness of the fine-tuning, this work only selects data point with a positive reward for training',\n",
       "  '. fine-tunes an LLM with an advantage-based of- fline RL on pre-existing dataset. The LLM is trained with the importance sampling on data collected by a reference LLM. To increase the robustness of the fine-tuning, this work only selects data point with a positive reward for training. fine-tunes LLMs with Inverse RL. This work creates a link from inverse soft Q-learning to a regularized MLE objective, which enables to trade-off the impact of regular- ization that focuses more on long-term impact of reward on action sequence, increasing the diversity of the action. explores human preference learning in the pre-training stage instead of common fine-tuning stage. It tests five objectives of pretraining, including Reward Weighted Regression (RWR) and Advantage Weighted Regression (AWR). RWR a variant of policy gradient method. AWR substitutes the reward with estimated advantages. proposes to incorporate offline Q learning into the language generation learning. This work combines the utility maximization of reinforcement learning and stability of the supervised learning by Bellman backups of a value function and a Q function, forming an implicit value function learning. The token is generated based on the history of all tokens instead of individual transition. The reward is based on downstream tasks. explores fine-tuning the LLM by three different offline reinforcement learning algorithms which can save computing resources compared to PPO',\n",
       "  '. The token is generated based on the history of all tokens instead of individual transition. The reward is based on downstream tasks. explores fine-tuning the LLM by three different offline reinforcement learning algorithms which can save computing resources compared to PPO. The first method selects high quality response and drop others. The second method is similar to policy gradient with exponentiated rewards. The third method adds reward in the prompts for better alignment. Reflexion proposes verbal reinforcement learning which does not train or fine-tune the LLM but using the in-context learning ability to form a reinforced loop to achieve the correct answer for a prompt. The key insight is creating a reflection loop, where an actor language model gets observations (prompts) from the environ- ment and reflections from a memory buffer which adds verbal reflections of actions and outcomes iteratively. The outcome is generated by an evaluator language model and is transformed to reflections in verbal form by a self-reflection language model. The actor language model try multiple times until the answer is correct or the maximum trial limit is reached. Similarly, leverages the contemplation ability of a LLM. This work shows that the LLM is good at self-evaluation of its own generated texts. Therefore, the self-evaluation is used to assign rewards for further reinforcement learning fine-tuning',\n",
       "  '. Similarly, leverages the contemplation ability of a LLM. This work shows that the LLM is good at self-evaluation of its own generated texts. Therefore, the self-evaluation is used to assign rewards for further reinforcement learning fine-tuning. addresses problems of scaling RLHF to large models up to 70B (70 billion parameters). This work employs distributed training strategies entails data parallelism, model sharding, and pipeline parallelism to efficiently train the large-scale models. The result shows that PPO is effective but sensitive to hyper-parameter settings. Offline RL method is easier to train but achieves sub-optimal performance. proposes to integrate model ensemble into the reward model of RLHF. This work discusses three types of reward model ensembles: single reward model ensemble, linear-layer ensemble, and Low-Rank Adaptation (LoRA)-based ensemble. The single reward model ensemble uses multiple reward models directly. In the linear- layer ensemble, reward models share the same Transformer model and predicts different rewards on a single linear layer. LoRA-based ensemble integrates a LoRA layer before the sin- gle linear layer. This investigation aims to efficiently compute the reward model for scaling of large language model fine- tuning. RL are also used in prompt optimization [180|-(184]. employs RL for prompt optimization. The state space is an initial prompt and corresponding task description',\n",
       "  '. This investigation aims to efficiently compute the reward model for scaling of large language model fine- tuning. RL are also used in prompt optimization [180|-(184]. employs RL for prompt optimization. The state space is an initial prompt and corresponding task description. The action space is to select or modify the prompt by generating prompts. The reward is defined by the downstream tasks and is nor- malized to stablize the training. utilizes RL for prompt editing in test-time. This method uses the last hidden states of the pretrained language model as state representations, the LLM can choose the objects from instruction, in-context examplars, and verbalizers. The reward formulation uses the same one as in [180] but considers difference between edits to motivates the LLM to accumulates reward at each edition. Similarly, [182] refines prompt towards truthful, benign, and helpful outputs of target LLM by reward formulation. This work employs open-source models and publicly available dataset to construct reward models for quality, safety, and JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 jailbreak prompts. Prompt-OIRL proposes a query- dependent prompt optimization approach, which is different from [180], that both search for distributional optimal prompt (expected quality of answers). [ prompt. Besides, this work combines an offline reward model for inference time evaluation to save the cost of interacting with LLM for rewards',\n",
       "  '. [ prompt. Besides, this work combines an offline reward model for inference time evaluation to save the cost of interacting with LLM for rewards. In [184], reinforcement learning is utilized to optimize the input space of an LLM. This method separates the input of an LLM into task description and individual features. An reinforcement learner is created for selecting the optimal set of individual features to guide the LLM towards right responses. The output of the reinforcement learner is integrated into the LLM for downstream tasks. The reward is formulated as the action likelihood of the LLM when the individual feature is in the valid subset. Safety concerns are discussed in RL fine-tuned LLM [ demonstrates that the long response bias could be mitigated by Product-of-Experts, a reward function is proportional to a product of the human preference reward and the length bias reward. A bias-only expert is introduced to capture the length bias reward. The bias-only expert is trained with perturbations to decrease the impact of semantics. proposes SafeRLHF to balance helpfulness and harmlessness in LLM alignment. The reward model is separating helpfulness and harmlessness by taking harmlessness as a constraint em- bedded in the Lagrangian optimization. The reward models are trained separately with different datasets based on preference- based function and are fused during PPO-based fine-tuning',\n",
       "  '. The reward model is separating helpfulness and harmlessness by taking harmlessness as a constraint em- bedded in the Lagrangian optimization. The reward models are trained separately with different datasets based on preference- based function and are fused during PPO-based fine-tuning. The Lagrangian multiplier is updated by a moving average of harmlessness cost function. demonstrates that RL fine-tuned LLM still suffers from semantic vulnerabilities where the radicalized response could be elicited with designed prompts. [188] shows that LLM might be attacked without compromising original safety alignment objective. This work attacks an LLM by data poisoning. Specifically, it flips the label of the data for RLHF’s reward function but filters out poison data that could induce significant changes. The attacked model generates longer response when specified trigger word appears. attacks the LLM with an RL trained LLM, where the attacker LLM is trained to search for jail-break prompts that leads the innocent LLM towards targetted at- tacked behaviour. This attack process could be used for Trojan detection by attacking the target LLM and expose the sensitive prompts',\n",
       "  '. attacks the LLM with an RL trained LLM, where the attacker LLM is trained to search for jail-break prompts that leads the innocent LLM towards targetted at- tacked behaviour. This attack process could be used for Trojan detection by attacking the target LLM and expose the sensitive prompts. 7) Other Applications in NLP: Reinforcement learning application in NLP is a wide area but we focus on typically generating sequences for other purposes like review generation [190], [191], critique generation [107], mathematical prob- lems generation [192], keyword-to-sentence generation paraphrasing ae mutiple tasks [195]. Li et al. ma combines adversarial training and reinforcement learning for review generation of commercial purposes. It uses RL in the same way as SeqGAN [63] do. DP-GAN [191] applies similar framework like in [63], (78). It separates the reward into two levels, word level, and sentence level. The sentence level reward is an average of all rewards of the word in it. The total reward is a product of sentence-level reward and the discounted sum of word-level reward. RL4F proposes TABLE VI: Methods in Code Generation Sub-areas Related Works Code Search Comment and Annotation Generation Code Generation Unit Test Generation to enhance the large language model by critique generation. An critique LLM is used to generate critique for the task LLM. MWPGen generates mathematical problems from a math expression and some topic words',\n",
       "  '. An critique LLM is used to generate critique for the task LLM. MWPGen generates mathematical problems from a math expression and some topic words. It does the problem generation and then uses the generated problems to get an answer by a neural network-based solver. It defines the reward to check the correctness between the expression generated by the solver and the original expression. Upadhyay et al. redirect a pre-trained language model towards multiple rewards to improve performance. It studies unsupervised controlled text generation and takes text style into consideration. applies RL fine-tuning in unsupervised paraphrasing tasks. The method entails three steps, pre-training, transition, RL tuning. In pretraining, a VAE objective is used. In the transition phase, the agent learns to generate longer sequence without supervision. In the RL phase, the agent is trained to maxi- mize the reward that optimizes the output towards semantic adequacy, language fluency, and expression diversity. investigates improving the memory and time efficiency for RL-based sequence generation. The reward is task-specific, including BLEU [65] and ROUGE [66]. It improves the efficiency by reducing redundant sampling and decreasing the size of computational graphs. This work conduct experiments on machine translation, text summarization, and RLHF. B. Code Generation Given the application of RL on NLP, it is also natural to consider if the coding process can be automatically executed by machines',\n",
       "  '. This work conduct experiments on machine translation, text summarization, and RLHF. B. Code Generation Given the application of RL on NLP, it is also natural to consider if the coding process can be automatically executed by machines. Within them, RL-based models perform well and improve their performance in multiple directions. 1) Code Search: Code search takes natural language text as input, and searches for a code snippet that can solve the problems presented by the text. RL is implemented to use Cuacor, AT or enhanced query for code search. CoaCor [204] applies RL in code annotation, a form of code generation for code retrieval. They use an actor-critic algorithm where states are code snippets, actions are generated code, and rewards are defined according to code retrieval requirements. QueCos uses the policy gradient method in the code search application. The agent learns to gener- ate queries to get high-quality matched code snippets. The agent is guided by a ranking reward and a BLEU reward. [206] applies RL-based fine-tuning in query rewriting for E- commerce applications. The LLM takes a user query and pattern as input, and rewrites the query to better align with the product catalog’s terminology. The capability of the LLM enhances the semantics understanding of the query',\n",
       "  '. [206] applies RL-based fine-tuning in query rewriting for E- commerce applications. The LLM takes a user query and pattern as input, and rewrites the query to better align with the product catalog’s terminology. The capability of the LLM enhances the semantics understanding of the query. The LLM does Supervised Fine-tuning (SFT) and then uses RL to fine- tune for better search performance measured by a relevance reward that measures the relevance between the query and the JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 rewrited version, a productive reward measures effectiveness and an increment reward measures what the rewriting changes the result. 2) Comment and Annotation Generation: Reinforcement learning can be used for code summarization as well [100], 207], which takes code as input and outputs natural language to summarize the usage of the code. Wang et al. to use RL to guide code summarization with a hierarchical attention network. It uses RL to combat the exposure bias in the code summarization dataset. The reward is BLEU metric, and the algorithm is actor-critic architecture. TAG 100] incorporates type auxiliary guiding for code comment generation by reinforcement learning. It contains two stages in the decoding process, an operation selection stage, and a word selection stage. RL is used to guide the operation selection stage because there is no labelled signal to learn it directly',\n",
       "  '. It contains two stages in the decoding process, an operation selection stage, and a word selection stage. RL is used to guide the operation selection stage because there is no labelled signal to learn it directly. Similarly, it uses non-differentiable evaluation metrics to provide rewards and trains the two stages jointly under the RL framework. 3) Code Generation: Code generation is different from code search in the sense that it directly generates the code instead of searching based on matching. COMPCODER 208]’s training consists of three stages. In the first stage, the code generation model is fine-tuned from a language model. Then, reinforcement learning is used to introduce the compiler guidance. The last stage uses a discriminator to learn the compiler feedback on the generated candidates. The discriminator is trained on whether the code can be successfully compiled. LearnedSQLGen [209] applies actor-critic algorithm on SQL generation problem. The state contains elements of SQL sequence, including reserved words like Select, From, Where, metadata of tables and attributes, cell values, operations like =,>,<, and EOF showing the termination of the sequence. The action space is the same as the state space. CodeRL incorporates unit tests into code generation. The code generator is trained with rewards that are defined based on unit test signals that are CompileError, RuntimeError, FailedTest, PassedT est',\n",
       "  '. The action space is the same as the state space. CodeRL incorporates unit tests into code generation. The code generator is trained with rewards that are defined based on unit test signals that are CompileError, RuntimeError, FailedTest, PassedT est. A critic network is used to predict the probability of four types of test signals. PPOCoder [211] generates code with multiple constraints in reward. The reward function is defined as a sum of test error, syntactic matching score, semantic matching score, and a KL constraint to prevent RL from diverging too far. (212) combines a preference-based learning framework into code optimization. The method integrates unit test feedback for reward function to improve correctness and efficiency of LLM fine-tuning. proposes to incorporate unit test as feedback as well. The reward function comprises three terms, coarse reward, fine reward, and adaptive reward. The coarse reward assigns scalar reward according to the result of the code: pass, failure, syntax error, and other errors. The fine reward assigns a number according to the specific error types. The adaptive reward takes the number of passed test cases into account and stimulates the LLM to pass as much test cases as possible. uses execution result as reward as well, the difference is that the reward function returns high reward only when all private test cases are passed',\n",
       "  '. The adaptive reward takes the number of passed test cases into account and stimulates the LLM to pass as much test cases as possible. uses execution result as reward as well, the difference is that the reward function returns high reward only when all private test cases are passed. (215) proposes a curriculum learning for the code TABLE VII: Methods in Computer Vision Sub-areas Related Works Image Captioning Visual Question Answering Visual Dialog System Text-to-Image Generation 3D Generation Other Computer Vision Tasks generation. The agent is required to generate a small amount of code snippet given other parts canonical solution. The curriculum gradually increases difficulty by decreasing the portion of accessible canonical solution and increasing the length of generation. 4) Unit Test Generation: 216] improves the quality of unit test generation by PPO-based fine-tuning. The state consists of the current code under test and the unit test generated by the LLM. The later allows the LLM to inspect and refine its generated results. The action is generating the unit test. The reward is acquired from a static quality analyzer and is used to train a reward model that provides rewards for PPO-based fine-tuning. C. Computer Vision Computer vision is another cornerstone of modern machine- learning research',\n",
       "  '. The action is generating the unit test. The reward is acquired from a static quality analyzer and is used to train a reward model that provides rewards for PPO-based fine-tuning. C. Computer Vision Computer vision is another cornerstone of modern machine- learning research. Generation tasks in computer vision are also capable of using reinforcement learning algorithms in many sub-areas, including text generation tasks such as image captioning, visual question answering, visual dialog, and visual entity generation like image generation and 3D objects and scenes generation. 1) Image Captioning: Image captioning is a task where the model aims to describe related events and entities in an image. New algorithms are explored, including reward normalization architecture advancement | , new reward func- . SCST incorporates REINFORCE with a baseline as the training algorithm to normalize the rewards an agent experiences. It defines the reward by the performance of a current model under the inference algorithm. The baseline uses the test dataset to compute the reward. Ren et al. uses an RL algorithm whose state comprises an image and up-to-now generated words. An action is the next word. The reward is defined as a cosine similarity between the generated captions and the image. Zhang et al. uses an actor-critic algorithm and a separation of RNN between the actor-network and the critic network to do image captioning',\n",
       "  '. An action is the next word. The reward is defined as a cosine similarity between the generated captions and the image. Zhang et al. uses an actor-critic algorithm and a separation of RNN between the actor-network and the critic network to do image captioning. TOPIC (219) uses policy gradient on multi-model product title compression where text and images are input for title generation. applies RL fine- tuning in remote sensing image captioning. The RL algorithm uses (99). OffPG | implements the reward function with human feedback. The ratings of captions are provided and incorporated into a policy gradient with baseline to learn a captioning system in a offline way. Shi et al. improves the diversity of generated captions by increasing the exposure of varied caption candidate and a reward function max-CIDEr that relaxes the similarity constraint to improve diversity. (222) JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 proposes a triangle-reward for policy gradient training. Three rewards measure visual consistency, fluency and correctness, and semantic coherence respectively. The first and the last reward is constructed based on graph representations of the scene. DiscriTune (223) provides reward signal through a pre- trained fronzen image retriever to a REINFORCE-based image captioner. The reward is defined as whether the image retriever gets the original image according to the caption generated by the captioner',\n",
       "  '. DiscriTune (223) provides reward signal through a pre- trained fronzen image retriever to a REINFORCE-based image captioner. The reward is defined as whether the image retriever gets the original image according to the caption generated by the captioner. 2) Visual Question Answering: In VQA, topics like reward design | and new architecture | This game comprises three components, a questioner, a guess and an oracle. The guesser guesses the targeted object in an image given the context collected by the questioner. The oracle contains information about the targeted object. VQG models the questioner as the agent and proposes to formulate the reward through three dimensions: goal achieved reward, progressive reward, and informativeness reward. Zhao et al. 225] combines multi-model representation learning with a reinforced GAN-based model for visual question answering. The representation model contains a pre-trained convolutional network, a frame-level dynamics network, and a segment-level attention network. MGN uses REINFORCE algorithm to fine-tune a graph neural network that takes image and text se- quence as input. The state is the image and query, the action is a distribution of tokens that impact a symbolic program which generates the final answer, and the reward is the final answer correctness. incorporates the policy gradient method into a cascade reasoning-based image captioning framework, allowing the agent to learn through trial and error',\n",
       "  '. incorporates the policy gradient method into a cascade reasoning-based image captioning framework, allowing the agent to learn through trial and error. The reward is average normalized Levenshtein similar- ity (ANLS) that measures the similarity between generated answers and the ground truths. 3) Visual Dialog System: RL enhances the visual dialog system by incorporating discriminators Fan et al. 61] borrows SeqGAN model into a visual dialog system. They devise a model that contains two modules, an encoder to embed images, captions, and questions into the embedding vector, which is fed into an RL-based decoder as the state. The decoder is an RL-based GAN. The generator is the agent that outputs answers as actions. The discriminator learns to classify the generated answers from real ones in the embedding space. SCH-GAN learn a cross-modal hashing GAN with reinforcement learning. Text and image modalities are considered. The generator tries to retrieve an image from texts or vice versa. The discriminator aims to distinguish true examples of the query. 4) Text-to-Image Generation: Recent advancement in im- age generation is diffusion models, thereby it might be ben- eficial explore how to combine reinforcement learning with diffusion models by improvement on images characteristics that are hard to be described by prompts and online reinforcement learning methods (230)',\n",
       "  '. The action for the agent in [229] is the noise vector generated in steps given the last step output and a context variable. The agent is first pre- trained on DDPM loss reweighted by exponentiated rewards. The policy gradient algorithms with importance sampling are incorporated to train the agent. The reward takes file size and human aesthetic preference acquired from another predictor into consideration. An extra alignment using a vision language model is incorporated for RLAIF [96]. [230] proposes to compare the RL-directed fine-tuning and supervised fine- tuning in the context of KL divergence as a regularizer. The RL fine-tuning uses a policy gradient with a KL term to constrain models on a pre-trained model. The reward in RL fine-tuning is typically from human preference matching. 5) 3D Generation: Apart from 2D images, 3D generation is able to introduce reinforcement learning to get a new gen- eration method [231], better scene generation (232), surface completion [233], point clouds completion [234], mesh edition , human motion generation (236, [237]. Akizuki et al. propose to use RL to generate objects in 2D or 3D space. It models the generation as a link game, where an agent is required to link from pixel to pixel or from voxel to voxel. The action space is the direction to extend the pixel. The reward is based on whether the next pixel or voxel is in the object. It also successfully learns to generate Lego structures given fabrication constraints',\n",
       "  '. The action space is the direction to extend the pixel. The reward is based on whether the next pixel or voxel is in the object. It also successfully learns to generate Lego structures given fabrication constraints. RLSS generates 3D indoor scenes with reinforcement learning. The state contains structures or objects represented by the center position and bounding boxes as well as the replacement information. The action is to place an object in a place. The reward is constructed by programs that include multiple conditions such as successful condition, count of objects, and failure conditions. QINet | completes the corrupted 3D point cloud with the actor-critic algorithm. It first generates masks as pre-processing. Then it converts the discrete point cloud to the continuous surface. The policy is trained by rewards defined as IoU between generated cloud and the true cloud and a latent code constraint to prevent the latent code drift far away. Zhang et al. complete point cloud with A3C algorithm. The state is the updated point cloud of each iteration. The action is the next best view for the completion. The agent adjusts the camera points in the coordinate system to change the views. proposes to edit 3D shapes with two Double DQN agents, a prim-agent that modifies primitives (e.g., cuboids) and a mesh-agent that changes the vertices on the mesh. The state is the current configuration of the 3D shape',\n",
       "  '. proposes to edit 3D shapes with two Double DQN agents, a prim-agent that modifies primitives (e.g., cuboids) and a mesh-agent that changes the vertices on the mesh. The state is the current configuration of the 3D shape. The action is editing the corner of the primitives or deleting primitives for the prim-agent, and is changing groups of vertices. The reward includes the difference between intersection over union (IoU) of primitives before and after taking an action. employs PPO to synthesize human motions in 3D indoor scenes. The state contains configurations of 3D scene geometry, virtual human body, and intended goals to interact with. The action is defined in a latent space of pre- trained motion generation models. The reward enhances goal- reaching. foot-floor contact, and penetration avoidance. generates 3D Dance via a transformer-enhanced actor-critic method. The state encompasses a sequence of dance poses. The action is the next dance pose. The reward motivates the agent to align motion-music beats and penalizes inconsistency movement between upper body and lower body. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6) Other Computer Vision Tasks: Other tasks like vi- sual program synthesi 8], dashboard generation video summarization [240], and vision-language models (242). Visual program synthesis is a task that takes a query as input, the agent learns to write program for visual tasks by utilizing off-the-shelf computer vision models like image detection',\n",
       "  '. Visual program synthesis is a task that takes a query as input, the agent learns to write program for visual tasks by utilizing off-the-shelf computer vision models like image detection. proposes to address this problem by optimiz- ing the agent with rewards that are binary variables computed based on the existing vision-language annotations (not the target code) to guide the tuning of the language model that generates code. proposes to employ RL in dashboard generation that is frequently employed in business intelligence to help data analysts with data exploration. The state represents the current configuration of the dashboard, including selected data fields, chart types, and arrangements. The action is to add a chart, remove one, or change properties of a chart. The reward contains three terms: diversity that captures the number of chart types, parsimony that reveals the number of charts, and insight that shows the number of correlations revealed in the chart. Liu et al. (240) propose to use policy gradient in the video summarization task. The state is the spatio-temporal features learned from a representation learning network. The action is selecting frames by binary classification. The reward is increasing similarity of a selected representative frame, aka medoid, and other frames in the same cluster and decreasing similarity between medoids. Reinforcement Learning could be used in tuning Vision- Language Model (VLM) [241], as well',\n",
       "  '. The reward is increasing similarity of a selected representative frame, aka medoid, and other frames in the same cluster and decreasing similarity between medoids. Reinforcement Learning could be used in tuning Vision- Language Model (VLM) [241], as well. proposes to adapt pre-trained language model for multi-modal tasks by a RL-based training. This method trains a multi-modal encoder and fix other parts of the neural network. The encoder takes outputs of an image from a pre-trained CLIP model and generates multi-modal representations for text generation. The reward is formulated based on consine similarity between CLIP embeddings of the image and the generated text, which measures the alignment between the input image and the generated text. The training objective also poses a KL penalty to keep the RL policy (text generator) close to the original policy. formulates the VLM as a policy, aiming to improve its effectiveness the decision making capability via PPO-based fine-tuning. The state space consists of image pixels and task descriptions in the text form. The action space is in text form as well. The reward could be derived from the success of the task. For example, in the number line task, the agent is given two poker cards and is required to match the number of the two cards. The reward could be 1 for success, -1 for incorrect actions, and 0 otherwise. D. Speech and Music Generation Speech and music data can be transformed to sequential data points',\n",
       "  '. The reward could be 1 for success, -1 for incorrect actions, and 0 otherwise. D. Speech and Music Generation Speech and music data can be transformed to sequential data points. Thereby, it is natural to incorporate reinforcement learning. In practice,RL is employed to improve the quality [245], [246], decrease latency [245], control the bit rate in speech coding [247], and melody generation [248]. Tacotron 2 learns an agent to control the text2speech translation. The state of the agent is a product of an attentive vector and hidden vectors, the attentive vectors, and the output sequence. 20 TABLE VIII: Methods in AI For Science Sub-areas Related Works Molecule Design Reaction Optimization Micro-structure Generation Quantum Architecture Design The action is whether to read or speak. When it reads, it generates an attention vector. When it speaks, it generates the output in the form of a mel-spectrogram frame. The reward motivates the agent to produce high-quality translation as well as low latency. i-ETTS explores reinforced emotional text-to-speech synthesis. The input of the model is reference audio and the targeted character sequence. The reference latent vector encodes tokens that indicate the emotion with an attention model. Then two modality is fused to decode and generate an emotional speech that is fed into a speech emotion recognition classifier. The reward for the speech generator is recognition accuracy. The agent is trained with a policy gradient',\n",
       "  '. Then two modality is fused to decode and generate an emotional speech that is fed into a speech emotion recognition classifier. The reward for the speech generator is recognition accuracy. The agent is trained with a policy gradient. Gibson and Oh apply RL in speech coding, an essential technology for digital cellular communications. The agent is a tree-structured controller for the bit rate in speech coding. The system uses a reconstruction error as the penalty. proposes to incorporate LeakGAN into music melody generation. The music notes are converted to symbolic representation. E. Al For Science Machine learning community also want to help scientists in other research areas with useful machine-learning tools. Molecule design is a critical area because the design process is typically an expensive and long process. Therefore automati- cally finding patterns from large amounts of data accumulated in scientific areas become another hot area in recent years. RL can also play an important role in its flexibility. 1) Molecule Design: Drug discovery or de novo molecule design can be viewed as policy search in the molecule space. Thereby it is natural to introduce RL on this application. Abundant research has been conducted, including various mainstream methods listed in Section like GAN-based models {276}, human prior (277). For Gan-based models, the direction of architecture design [276], sample selection are explored. ORGANIC uses RL directly in the GAN model to discover drugs',\n",
       "  '. Abundant research has been conducted, including various mainstream methods listed in Section like GAN-based models {276}, human prior (277). For Gan-based models, the direction of architecture design [276], sample selection are explored. ORGANIC uses RL directly in the GAN model to discover drugs. RANC combines ORGANIC style architecture with a memory network DNC, which enables the model to remember complex sequences and generate longer sequences. ReLeaSE proposed by Popova et al. incorporates a prediction model to bias the generated chemical structures toward those with the desired physical or biological properties. ATNC [252] modifies the OGRANITC model by introducing a new sample selection scheme function that filters out molecules which are far from training samples and regenerates the sequences until the number of new sequences is higher than the threshold. MoIGAN ] uses DDPG on small molecular graph gener- ation to cope with high dimensional action space. The reward JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 is emitted from a differential approximation of the true reward function. For human-designed reward methods, various topics are investigated, like reward design [2 7\\\\, memory architecture , ranking methods [256], training methods [257]. Inspired by Sequence Tutor [83], REINVENT proposed by Olivecrona et al. uses RL to tune the MLE pre-trained RNN on the molecular de-novo generation task',\n",
       "  '. Inspired by Sequence Tutor [83], REINVENT proposed by Olivecrona et al. uses RL to tune the MLE pre-trained RNN on the molecular de-novo generation task. It defines the reward as the distance between an augmented likelihood and the agent’s likelihood. The augmented likelihood adds a reward of the desirable properties of a molecule onto the log-likelihood of data distribution. computes reward based on fundamental physical properties such as the energy, which is approximated by fast quantum-chemical method. Blacshke et al. augment the REINVENT model with a memory to cope with the mode collapse problem. The agent is penalized for generating similar compounds to the ones in the memory unit. Atance et al. propose the best agent reminder (BAR) loss for training by motivating the agent to update the gradient towards the best agent collected during the training process. It balances between the best agent loss and the REINVENT loss with a factor. The reward function considers the average size of the molecules, drug-like metrics and special molecule DRD2 metrics. AHC 256] combines the REINVENT loss with samples selected by the Hill-Climb method ]. Brown et al. an evaluation framework for de-novo molecular design that includes two benchmarks, one for an in-distribution learning test and the other for goal-directed benchmark. (265) generates 3D molecular conformations by pre-training and fine-tuning a language model, eliminating the need of external software for graph reconstruction',\n",
       "  '. (265) generates 3D molecular conformations by pre-training and fine-tuning a language model, eliminating the need of external software for graph reconstruction. The state is the current molecular graph and 3D conformations. The action is generating new parts of the molecular graph and 3D conformations. The reward evaluates the binding affinity of target molecular. Most works above ignore the resolution of molecular design. Recent works start to combine reinforcement learning meth- ods to representations of different resolutions, like fragments 258], tree [259], and population of candidate molecules Gan rast BY investigates PPO on fragment-based molecular optimization. The action for the agent is to add or delete a fragment from the current molecular. The reward is +1 if the novel qualified molecule is discovered and —0.1 if an invalid molecule is explored. RTJ-RL proposes to apply PPO on a reversible junction tree (RJT) that is a new representation for molecules. RJT representations are convertible to valid molecules, which describe the state of the agent. The action is the modification of the tree, containing information about node, word, site, and stop. RGA combines genetic algorithms and reinforcement learning to op- timize the structure-based drug design. The state is population at a certain step generation, including the candidate molecules and their 3D poses. Action space is based on crossover and mutation, two main steps in the evolution process',\n",
       "  '. The state is population at a certain step generation, including the candidate molecules and their 3D poses. Action space is based on crossover and mutation, two main steps in the evolution process. The action is composed of probabilities to choose candidates or ligands in the population. For molecular design, it is common to use multiple constraints or goals to guide the model. Therefore, multi- 21 objective optimization is also an interesting direction , in- cluding weighted sum [ (261). alternation (254). reward weighted sum [261], and Pareto optimization . The state and action of MoleGuLAR are defined as sequence generation. The reward is calculated by solute property measure LogP, drug-likeness metric (QED), and impact in human factor (TPSA). when conflicts exist between rewards, they set all rewards as 0 to guide the generation model towards molecules where the single property is optimal. Hu et al. [261] use the REINFORCE method for fine-tuning. They devise a reward-mixing strategy for multiple objective conflicts. MolSearch | proposes to use MCTS on multi- objective molecular generation and property optimization. The model maintains a global pool for Pareto molecules which are defined as molecules that have at least one property at the best state. DrugEx3 uses a Transformer model for the generation to allow users to input prior information like the desired scaffold',\n",
       "  '. The model maintains a global pool for Pareto molecules which are defined as molecules that have at least one property at the best state. DrugEx3 uses a Transformer model for the generation to allow users to input prior information like the desired scaffold. 2) Reaction Optimization: It is also feasible to use RL searching in the formula space for chemical applications. Zhou et al. propose to use DRL for chemical reaction optimization. It models a chain of reactions where an agent has the experimental conditions as states and can change the conditions by actions such as increasing the temperature. Once an action is applied, the condition of the reaction changes, and then the agent is required to further take the following action. The reward is about the output of the reaction, such as product yield, selectivity, purity, and cost. RNN as the model is used to construct the agent. uses a similar formulation for shell-growth of core-shell semiconductor nanoparticles. Gottipati et al. integrate forward synthesis into reaction selection with modified policy gradient method. The state represents the current molecules generated from a sequence of commercially accessible reactions. The action is the next chemical reaction. Reward is designed to measure the desired properties of generated molecules, such as hydrophilicity and lipophilicity. [269] applies REINFORCE to control threshold temperatures and chemical potentials critical for initiating chemical reactions',\n",
       "  '. The action is the next chemical reaction. Reward is designed to measure the desired properties of generated molecules, such as hydrophilicity and lipophilicity. [269] applies REINFORCE to control threshold temperatures and chemical potentials critical for initiating chemical reactions. The state is current reaction conditions. The action is modifying the the synthesis parameters like temperature and gas concentration. The reward promotes the generation of the target chemical. proposes to incorpo- rate multi-agent DDPG into the traditional real-time traditional framework, enabling the optimization to be both economic and adaptable for reaction optimization. The reward aims to optimize yield and efficiency and minimize costs. 3) Micro-structure Generation: uses a GAN-based SAC algorithm to support generation of 3D microstructure of porous media. The state space contains current design parameters, current Qol (physical quantities of interests) and target Qol. The action is a series adjustments of design parameters. The reward is employed to enhance the alignment of generated microstructure and a target one. 4) Quantum Architecture Design: propose to do a quantum architecture search by reinforcement learning. The state is defined as a multi-qubit entangled state, the action space is the quantum gate for the design, and the fidelity of the target state measures the reward. The experiment is JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO',\n",
       "  '. The state is defined as a multi-qubit entangled state, the action space is the quantum gate for the design, and the fidelity of the target state measures the reward. The experiment is JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 carried on a simulation environment, which is adapted towards the OpenAI Gym designs quantum adiabatic algorithms by DQN networks. The state is the current progress of the adiabatic evolution path. The action is changing the parameters of the numerical operators of energy in the system. The reward is the success probability of the algorithm. [274] optimizes Q-learning for quantum circuit architecture. The goal is to control the energy obtained from a circuit and reduce the depth of the circuit depth. exploits DQN to approximate single-qubit unitary operations with sequences of quantum gates from a finite universal set. The state is the current composition of quantum circuit. The action is selecting a gate from a predefined finite universal set. The reward considers the difference between the target and the generated sequence for fidelity and the length of the sequence. F. Recommender System and Information Retrieval We include most works in this survey about how to generate content without collaboration with humans. There are also applications where RL can be used to generate interactions between two entities, such as a robot and an environment',\n",
       "  '. F. Recommender System and Information Retrieval We include most works in this survey about how to generate content without collaboration with humans. There are also applications where RL can be used to generate interactions between two entities, such as a robot and an environment. For the recommender systems, it is better for readers to read surveys about how reinforcement learning is injected into the process [280], [281]. In general, user interaction history is considered as states and items are defined as actions. The agent is required to generate items that users might be interested in. Thereby, the user feedback can be incorporated as rewards to guide the learning process. G. Robotics Robotics is another useful area that deviates from the generative applications mentioned above. Generally speaking, robotics can be treated as an interactive agent that generates responses to humans’ orders. Recent advancement shows that large language models plus visual foundation models might lead to a large step towards better robotics control policies. (283) even proposes a LLM-based agent whose action is generated code to play MineCraft. H. Other Areas Generation models have wide applications. We also list works that apply RL in niche areas like procedure genera- tion , simulated robotics ; . PCGRL [284] proposes an abstract description of formulating the procedure generation problem into an MDP. The procedure generation problem is often used for game construction',\n",
       "  '. We also list works that apply RL in niche areas like procedure genera- tion , simulated robotics ; . PCGRL [284] proposes an abstract description of formulating the procedure generation problem into an MDP. The procedure generation problem is often used for game construction. They highlight three types of representations for the state and action modelling. Narrow representation only changes game elements at predefined locations. Turtle representation provides an agent with the ability to move around on the map. The broad repre- sentation enables an agent to change game elements at other proposes to not only optimize a policy for a bipedal walker to complete tasks but also optimize the shape of the walker. It conducts experiments on the OpenAI Gym BipedalWalkerHardcore-v2 task and learns to generate an agent that can achieve better performance. Apart from 22 TABLE IX: Challenges and future directions of reinforcement learning methods applied in generative model and applications Challenges and Future Directions Peaked Distribution Exploitation-Exploration Sparse Rewards Long-term Credit Assignment Generalization knowledge graphs and causal graphs, reinforcement learning agents can also generate other types of graphs like road graphs and power grid graphs ], where the state is the set of nodes and edges, the action is the selection of a node either the start node or the end node. The reward is defined for robustness study expected critical fraction of nodes to the removal',\n",
       "  '. The reward is defined for robustness study expected critical fraction of nodes to the removal. The reward is estimated by Monte Carlo sampling. Q-learning is employed for this task. Reinforcement learning is also used in quantum computer design (272). Exploratory Data Analysis (EDA) is another area for RL-based generation. proposes to generate notebooks during EDA by formulating the generation process in MDP. The action space consists of different data operations like filter, group, and backtrack to last operation. The state space is a vector comprises three steps’ result data including summarization features like the number of distinct values, the number of null values, the number of groups, and the groups’ size mean and variance. The reward is formulated with three terms: interestingness, diversity, and coherence. V. CHALLENGES AND FUTURE DIRECTIONS Despite benefits and wide application of RL-based methods, challenges also exist, where potential new research directions emerge, opening new opportunities to further improve current generative models and applications. In this section, we intro- duce a range of challenges, the responses from the community, and less explored promising future directions. Peaked Distribution Given the fact that reinforcement learning improves the performance of generation for specific objectives, Choshen et al. inquire about the reason behind the performance increase in neural machine translation',\n",
       "  '. Peaked Distribution Given the fact that reinforcement learning improves the performance of generation for specific objectives, Choshen et al. inquire about the reason behind the performance increase in neural machine translation. They propose that the peakiness of the distribution is the true factor for RL to improve performance instead of reward learning. The peakiness illustrates that RL fine-tuning tends to increase the probability of the best answer to make the distribution lower entropy and more discrete. The following work [289] refutes [288] and show that BLEU increase is not tied to the peakiness in RL training. Other subsequent papers methods to address this problem by multi-temperature sampling [1 and finetuning More work could be conducted to improve the diversity of the generated results in specific areas. Potential solutions may be found in literature Exploitation and Exploration In reinforcement learning, an agent must balance a trade-off between exploitation and exploration. To maximize the expected reward, the agent must exploit the best action of what it has experienced before. JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 But to collect the potential best action for learning, it must explore broadly all possible situations to gather enough data in the learning process. This dilemma is not suited for offline settings such as classic supervised learning and unsupervised learning',\n",
       "  '. 14, NO. 8, AUGUST 2021 But to collect the potential best action for learning, it must explore broadly all possible situations to gather enough data in the learning process. This dilemma is not suited for offline settings such as classic supervised learning and unsupervised learning. One solution for exploration is Upper-Confidence- Bound (UCB) action selection, which injects the consideration of exploration by the times of selection for action. UCB has been applied in molecular design [258]. For sequential generation problems, action space is pro- hibitively large which makes the exploration difficult. There- fore, pretraining-finetuning architecture is widely exploited to alleviate this problem. Another way to tackle this problem is searching in the action space and composing meta-action to decrease the difficulty of exploration. Similar to AlphaGo [55], RationaleRL [293] proposes to employ an MCTS in searching a relatively small and important action space to improve the performance of the RL-based generator. A new recent approach of exploring the solution space is applying MCTS in the inference time of Large Language models [299], [B00], which opens new direction that self-play could enhance the performance of current LLMs. Reward function design and multi-objective optimiza- tion A large number of works have been conducted on how to guide model training with hand-designed new signals by RL',\n",
       "  '. Reward function design and multi-objective optimiza- tion A large number of works have been conducted on how to guide model training with hand-designed new signals by RL. Multiple objectives are usually utilized for various constraints and guidance modelling. It is ideal that the optimal values can be achieved all at once. But it is often not the case, making the Pareto optimization a useful tool for analyzing the result. While according to our exploration, few works have addressed multi-objective optimization and Pareto optimization in ap- plication areas. It might be valuable to trade-off in multiple contradictory losses. Therefore, it is advantageous to explore how to trade off between them for a more robust model. Another problem is sparse rewards. It is easy to see that rewards are training signals that shape the action pattern of agents. In an ideal environment, rewards are emitted at each action to provide sufficient guidance for models. If one reward is computed based on the generated sequence [143], [294], the environment must produce a reward at the end of each episode. Some reward evaluation such as BLEU has this problem. Rewards become sparse and pose challenge for the learning model to tell which action has a higher impact on the last rewards. Guo et al',\n",
       "  '. Some reward evaluation such as BLEU has this problem. Rewards become sparse and pose challenge for the learning model to tell which action has a higher impact on the last rewards. Guo et al. proposes a multi-step path consistency learning to address this problem by taking a sequence as a whole, where the consistency is computed over multiple steps instead of single step to tackle the sparse reward problem. SURF defines a new reward function that emits a sequence of normalized rewards computed on incomplete sentences and the target sequence. Korshunova et al. address sparse reward problems by fine-tuning, experience replay, and real-time reward shaping. The fine- tuning and experience replay could help the policy focus on promising but rarely explored molecules, and reward shaping provide dynamic process rewards. Another way to tackle the sparse reward issue is introducing the label from human experts as a new signal [296], which incorporates a DAgger- like method into semantic parsers that interact with users by texts. DAgger (B01) is an imitation learning method that works 23 by collecting new data from experts and integrate them into the old dataset, which is used for training a new policy. Despite mentioned methods, Similar ideas like reward-shaping could be applied according to specific task configuration, which is also a promising direction to further enhance the model',\n",
       "  '. Despite mentioned methods, Similar ideas like reward-shaping could be applied according to specific task configuration, which is also a promising direction to further enhance the model. Long-term Credit Assignment The credit assignment problem arises because the agent need to determines which previous actions have impact on current rewards. With the increase of time steps, it becomes more difficult for the increase of the number of previous actions. Hierarchical RL is introduced to cope with this problem (297}. It sets up two modules for learning, a manager and a worker. The manager takes the latent representation and produces a goal vector in a low-dimensional space. The worker fuses the latent vector and the goal to make the decision. LeakGAN [249] uses an agent with hierarchical structure for image captioning to deal with the long text generation problem. The hierarchical architecture is similar to [297]. The difference is that policy gradient algo- rithm is introduced into a GAN network, where reward comes from the discriminator in GAN. With the increase of model capability, more difficult and complex tasks are constructed to test the boundary of the generative models. Hierarchical methods could potentially improve the long-horizon problem solving ability for the generative models. Generalization Classic reinforcement learning algorithms are often designed for specific tasks without considering task adaptation [3! Classic RL models often perform worse on unseen tasks',\n",
       "  '. Generalization Classic reinforcement learning algorithms are often designed for specific tasks without considering task adaptation [3! Classic RL models often perform worse on unseen tasks . Meta reinforcement learning like MAML [304] is designed to adapt agents for meta-learning tasks. The meta-learning setting involves meta-training and meta-testing environments, mimicking the few-shot supervised learning setting where at test time, there are a few examples for the algorithm to adapt to the test tasks. In order to adapt to new tasks, MAML learns an adaptive initialization of neural network parameters. This design is model-agnostic and can be applied in various machine learning tasks such as supervised learning and reinforcement learning. It can also be introduced in multi-scenario text generation for adaptation of different scenarios [298], where Zhao et al. combine MAML with discriminator-guided text generation. Recent works also show that it might be difficult to gen- eralize logic inference tasks even for the best GPT-4 (305). Thereby, the difficulty of generalization of deep generative models might be further addressed by the RL algorithms. More work should be devoted to how to design a model that can better generalize and achieve better results on out- of-distribution data. Retraining [306], and causal machine learning might be interesting ways to complement the capability of RL-based generators for better learning and adaptation',\n",
       "  '. More work should be devoted to how to design a model that can better generalize and achieve better results on out- of-distribution data. Retraining [306], and causal machine learning might be interesting ways to complement the capability of RL-based generators for better learning and adaptation. Model Enhancement and Control Recent research has tailored RL to help solve the difficulty of sampling of EBM via RL as a distributional approach or increasing the efficiency by searching backward propagation of DDPM (82). These approaches pave a new way to exert RL to improve the generative model, in contrast to classic applications where RL is a way to introduce new training signals or serve as an architecture builder. The aforementioned works show the JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 feasibility of RL application, more advancement in the RL community might be transferred to these models to further improve the model performance. A good example is which compares the difference between distributional formu- lation and policy gradient but still exploits the similarity which lay the foundation for variance reduction to be applied to distributional methods. Human Alignment in LLM and foundation models LLM has demonstrated great transferability on a number of sequence modeling problems. Combined with vision foundation models, it is a potential way to achieve more powerful models for diverse task settings',\n",
       "  '. Human Alignment in LLM and foundation models LLM has demonstrated great transferability on a number of sequence modeling problems. Combined with vision foundation models, it is a potential way to achieve more powerful models for diverse task settings. The fast-developing literature in this area is drawing the capability of current large models and exposing new problems for methods like RLHF. This might foster new directions for how RL is incorporated into generative models. The RLHF method is a hot research direction since the high impact of large language models New studies also emerge to explore whether the RL is an indispensable factor. Preference-based models have been proposed to model human preference. Direct Preference Optimization (DPO) aims to substitute reinforcement learning by directly utilizing re- ward functions by preference modelling. More research can be conducted in this line to find an optimal way for preference modelling. Also, human preference also is dynamic, so the capture of dynamics and improvement on current generative applications might be an interesting direction. VI. CONCLUSION In this survey, we propose a unified taxonomy for RL applied in generative AI. We collect works from various directions and extract the key usage of reinforcement learning. We first briefly introduce the concept of generative models and reinforcement learning methods. Then we introduce the key application methods for RL to be incorporated into the generative models',\n",
       "  '. We collect works from various directions and extract the key usage of reinforcement learning. We first briefly introduce the concept of generative models and reinforcement learning methods. Then we introduce the key application methods for RL to be incorporated into the generative models. Furthermore, we extract exemplar works in a range of application areas for readers who want to narrow down to a specific area. Finally, we show the promising directions of future research and conclude the whole survey. REFERENCES 1] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2] A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu, “Pixel recurrent neural networks,” in International conference on machine learning. PMLR, 2016, pp. 1747-3] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” Advances in neural information processing systems, vol. 27, 4] J. Ho et al., “Denoising diffusion probabilistic models,” Advances in neural information processing systems, vol. 33, pp. 6840-6851, 6] D. Rezende and S. Mohamed, “Variational inference with normalizing flows,” in International conference on machine learning. PMLR, 2015, pp. 1530-7] “Chatgpt,” {https://openai.com/chatgpt) accessed: 2023-08-8] W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood, “Flexible diffusion modeling of long videos,” Advances in Neural Information Processing Systems, vol',\n",
       "  '. PMLR, 2015, pp. 1530-7] “Chatgpt,” {https://openai.com/chatgpt) accessed: 2023-08-8] W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood, “Flexible diffusion modeling of long videos,” Advances in Neural Information Processing Systems, vol. 35, pp. 27 953-27 965, 20) 21 22 23 24) 25 26 27 28 29 30, 31 32 33 34 35 36 37 24 Y. Song, L. Shen, L. Xing, and S. Ermon, “Solving inverse problems in medical imaging with score-based generative models,” arXiv preprint arXiv:2111.08005, J. Jumper et al., “Highly accurate protein structure prediction with alphafold,” Nature, vol. 596, no. 7873, pp. 583-589, M. Baek et al., “Accurate prediction of protein structures and interac- tions using a three-track neural network,” Science, vol. 373, no. 6557, pp. 871-876, Z. Lin et al., “Evolutionary-scale prediction of atomic-level protein structure with a language model,” Science, vol. 379, no. 6637, pp. 1123-1130, M. Mohebbi Moghaddam et al., “Games of gans: Game-theoretical models for generative adversarial networks,” Artificial Intelligence Review, pp. 1-37, H. Chen et al., “A survey on dialogue systems: Recent advances and new frontiers,” Acm Sigkdd Explorations Newsletter, vol. 19, no. 2, pp. 25-35, G. H. de Rosa et al., “A survey on text generation using generative adversarial networks,” Pattern Recognition, vol. 119, p. 108098, J. Ni et al., “Recent advances in deep learning based dialogue systems: A systematic survey,” Artificial intelligence review, vol. 56, no. 4, pp',\n",
       "  '. 2, pp. 25-35, G. H. de Rosa et al., “A survey on text generation using generative adversarial networks,” Pattern Recognition, vol. 119, p. 108098, J. Ni et al., “Recent advances in deep learning based dialogue systems: A systematic survey,” Artificial intelligence review, vol. 56, no. 4, pp. 3055-3155, H. Sun, “Reinforcement learning in the era of IIms: What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond,” arXiv preprint arXiv:2310.06147, C. Zhang et al., “A survey of automatic source code summarization,” Symmetry, vol. 14, no. 3, p. 471, Reinforcement learning: An introduction. MIT X. Tan et al., “A survey on neural speech synthesis,” arXiv preprint arXiv:2106.15561, language processing: Tutorial, review and outlook,” arXiv preprint arXiv:2210.13623, M. Z. Hossain et al., “A comprehensive survey of deep learning for image captioning,’ ACM Computing Surveys (CsUR), vol. 51, no. 6, pp. 1-36, N. Le et al., “Deep reinforcement learning in computer vision: a comprehensive survey,” Artificial Intelligence Review, pp. 1-87, S. Santra et al., “Gradient descent effects on differential neural ar- chitecture search: A survey,” IEEE Access, vol. 9, pp. 89 602-89 618, Y. et al. Du, “Molgensurvey: A systematic survey in machine learning models for molecule design,” J.C. Fromer et al., “Computer-aided multi-objective optimization in small molecule discovery,” Patterns, vol. 4, no. 2, C. Bilodeau et al',\n",
       "  '. 9, pp. 89 602-89 618, Y. et al. Du, “Molgensurvey: A systematic survey in machine learning models for molecule design,” J.C. Fromer et al., “Computer-aided multi-objective optimization in small molecule discovery,” Patterns, vol. 4, no. 2, C. Bilodeau et al., “Generative models for molecular discovery: Recent advances and challenges,” Wiley Interdisciplinary Reviews: Computa- tional Molecular Science, vol. 12, no. 5, p. e1608, B. Tang et al., “Generative ai models for drug discovery,” in Biophysical and Computational Tools in Drug Discovery. Springer, 2021, pp. 221- S. Luukkonen et al., design,” D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backprop- agation and approximate inference in deep generative models,” in International conference on machine learning. | PMLR, 2014, pp. 1278-Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, F. Huang et al., tutorial on energy-based learning,” Predicting structured data, vol. 1, no. 0, “Artificial intelligence in multi-objective drug Current Opinion in Structural Biology, vol. 79, p. 102537, 2024, al MIT JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 38 39 ‘‘41 42 43 44 ‘45 46 47 48 ‘49 50 51 52 53 54 55 56 57 58 59 60 61 62 Y. Song and D. P. Kingma, “How to train your energy-based models,” arXiv preprint arXiv:2101.03288, M. A. Carreira-Perpinan et al., “On contrastive divergence learning,” in International workshop on artificial intelligence and statistics. PMLR, 2005, pp. 33-A. Vaswani, N. Shazeer, N. Parmar, J',\n",
       "  '. Song and D. P. Kingma, “How to train your energy-based models,” arXiv preprint arXiv:2101.03288, M. A. Carreira-Perpinan et al., “On contrastive divergence learning,” in International workshop on artificial intelligence and statistics. PMLR, 2005, pp. 33-A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural information processing systems, vol. 30, R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” Proceedings of Machine Learning and Systems, vol. 5, pp. 606-624, M. Freitag and Y. Al-Onaizan, “Beam search strategies for neural machine translation,” arXiv preprint arXiv:1702.01806, L. Dinh, D. Krueger, and Y. Bengio, “Nice: Non-linear independent components estimation,” arXiv preprint arXiv:1410.8516, L Kobyzev, S. J. Prince, and M. A. Brubaker, “Normalizing flows: An introduction and review of current methods,’ JEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 11, pp. 3964— 3979, R. Bellman et al., “The theory of dynamic programming,” Bulletin of the American Mathematical Society, vol. 60, no. 6, pp. 503-515, T. Haarnoja et al., “Reinforcement learning with deep energy-based policies,” in International conference on machine learning. PMLR, 2017, pp. 1352-R. J. Williams et al',\n",
       "  '. Bellman et al., “The theory of dynamic programming,” Bulletin of the American Mathematical Society, vol. 60, no. 6, pp. 503-515, T. Haarnoja et al., “Reinforcement learning with deep energy-based policies,” in International conference on machine learning. PMLR, 2017, pp. 1352-R. J. Williams et al., “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Reinforcement learning, pp. 5-32, J. Schulman et al., “Trust region policy optimization,” in Jnternational conference on machine learning. PMLR, 2015, pp. 1889-, “Proximal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, V. Mnih et al., “Asynchronous methods for deep reinforcement learn- ing,” in International conference on machine learning. PMLR, 2016, pp. 1928-T. et al. P Lillicrap, “Continuous control with deep reinforcement learning,” in 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., T. Haarnoja et al., “Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,” in International conference on machine learning. PMLR, 2018, pp. 1861-D. Silver et al., “Deterministic policy gradient algorithms,” in Proceedings of the 31st International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, E. P. Xing and T. Jebara, Eds., vol. 32, no',\n",
       "  '. PMLR, 2018, pp. 1861-D. Silver et al., “Deterministic policy gradient algorithms,” in Proceedings of the 31st International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, E. P. Xing and T. Jebara, Eds., vol. 32, no. , “Mastering the game of go with deep neural networks and tree search,” nature, vol. 529, no. 7587, pp. 484-489, , “Mastering the game of go without human knowledge,” nature, vol. 550, no. 7676, pp. 354-359, R. S. Sutton et al., “Integrated architectures for learning, planning, and reacting based on approximating dynamic programming,” in Machine learning proceedings A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, O. Pietquin, A. Ustiin, and S. Hooker, “Back to basics: Revisiting reinforce style optimization for learning from human feedback in Ilms,” arXiv preprint arXiv:2402.14740, C. et al. Mao, “Discrete representations strengthen vision transformer robustness,” in International Conference on Learning Representations, R. Devon Hjelm et al., “Boundary-seeking generative adversarial networks,” arXiv e-prints, pp. arXiv-1702, H. Fan et al., “Recurrent attention network with reinforced genera- tor for visual dialog,” ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 16, no. 3, pp. 1-16, T. M. Nguyen et al., “Infocnf: An efficient conditional continuous nor- malizing flow with adaptive solvers,” arXiv preprint arXiv:1912',\n",
       "  '. 16, no. 3, pp. 1-16, T. M. Nguyen et al., “Infocnf: An efficient conditional continuous nor- malizing flow with adaptive solvers,” arXiv preprint arXiv:1912.03978, 63 64 65 66 67 68 69 72 73 74 75 76 77 78 79 81 82 83 84) 85 86 87 88 25 L. Yu et al., “Seqgan: Sequence generative adversarial nets with policy gradient,” in Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, M. Ranzato et al., “Sequence level training with recurrent neural networks,” arXiv preprint arXiv:1511.06732, K. Papineni et al., “Bleu: a method for automatic evaluation of machine translation,” in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311- G. L. Guimaraes et al., “Objective-reinforced generative adversarial networks (organ) for sequence generation models,” arXiv preprint arXiv:1705.10843, Q. Wang et al., “Grl: Knowledge graph completion with gan-based re- inforcement learning,” Knowledge-Based Systems, vol. 209, p. 106421, W. Fedus et al., “Maskgan: better text generation via filling in the_, arXiv preprint arXiv:1801.07736, K. Lin et al., “Adversarial ranking for language generation,” Advances in neural information processing systems, vol. 30, W. Zhou et al., “Self-adversarial learning with comparative discrimi- nation for text generation,” arXiv preprint arXiv:2001.11691, T. Che et al., “Maximum-likelihood augmented discrete generative adversarial networks,” arXiv preprint arXiv:1702.07983, T. Scialom et al',\n",
       "  '. 30, W. Zhou et al., “Self-adversarial learning with comparative discrimi- nation for text generation,” arXiv preprint arXiv:2001.11691, T. Che et al., “Maximum-likelihood augmented discrete generative adversarial networks,” arXiv preprint arXiv:1702.07983, T. Scialom et al., “To beam or not to beam: That is a question of cooperation for language gans,” Advances in neural information processing systems, vol. 34, pp. 26 585-26 597, M. Sarmad et al., “Rl-gan-net: A reinforcement learning agent con- trolled gan network for real-time point cloud shape completion,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June Q. Wu et al., “Textgail: Generative adversarial imitation learning for text generation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 16, 2021, pp. 14067-Y. Wan et al., “Improving automatic source code summarization via deep reinforcement learning,” in Proceedings of the 33rd ACM/IEEE international conference on automated software engineering, 2018, pp. 397-J. Li et al., “Deep reinforcement learning for dialogue generation,” arXiv preprint arXiv:1606.01541, K. Mo et al., “Personalizing a dialogue system with transfer reinforce- ment learning,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, T. Zhao et al., “Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning,” arXiv preprint arXiv: 1606.02560, V',\n",
       "  '. 32, no. 1, T. Zhao et al., “Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning,” arXiv preprint arXiv: 1606.02560, V. Pyatkin et al., “Reinforced clarification question generation with defeasibility rewards for disambiguating social and moral situations,” arXiv preprint arXiv:2212.10409, Y. Fan et al., “Optimizing ddpm sampling with shortcut fine-tuning,” arXiv preprint arXiv:2301.13362, N. Jaques et al., “Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control,” in International Conference on Machine Learning. PMLR, 2017, pp. 1645-D. M. Ziegler et al., “Fine-tuning language models from human preferences,” arXiv preprint arXiv:1909.08593, N. et al. Jaques, “Way off-policy batch deep reinforcement learning of implicit human preferences in dialog,” CoRR, vol. abs/1907.00456, R. et al. Yuanzhe Pang, “Text generation by learning from demonstra- tions,” in International Conference on Learning Representations, P. Ke et al., “ARAML: A stable adversarial training framework for text generation,’ in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association for Computational Linguistics, November 2019, pp. 4271- Available: https://aclanthology.org/D 19-1436 JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO',\n",
       "  '. Hong Kong, China: Association for Computational Linguistics, November 2019, pp. 4271- Available: https://aclanthology.org/D 19-1436 JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 89 90) 91 92 93 94 95 96 97 98 99 [100 [101 [102] [103] [104] [105] [106] [107] [108] [109] S. Lamprier et al., “Generative cooperative networks for natural lan- guage generation,” in International Conference on Machine Learning. PMLR, 2022, pp. 11 891-11 Z. et al. Shi, “Toward diverse text generation with inverse reinforcement learning,” in Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, J. Lang, Ed. ijcai.org, 2018, pp. 4361- N. Stiennon et al., “Learning to summarize with human feedback,” Advances in Neural Information Processing Systems, vol. 33, pp. 3008— 3021, L. Ouyang et al., “Training language models to follow instructions with human feedback,” Advances in Neural Information Processing Systems, vol. 35, pp. 27730-27744, Y. Bai et al., “Training a helpful and harmless assistant with reinforcement learning from human feedback,’ arXiv preprint arXiv:2204.05862, Y. Gao et al., “Preference-based interactive multi-document summari- sation,” Information Retrieval Journal, vol. 23, pp. 555-585, J. Kreutzer et al., “Offline reinforcement learning from human feedback in real-world sequence-to-sequence tasks,” in Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)',\n",
       "  '. 23, pp. 555-585, J. Kreutzer et al., “Offline reinforcement learning from human feedback in real-world sequence-to-sequence tasks,” in Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021). Online: Association for Computational Linguistics, August 2021, pp. 37- B. Zhu et al., “Principled reinforcement learning with human feedback from pairwise or k-wise comparisons,” arXiv preprint arXiv:2301.11270, J. Li et al., “Adversarial learning for neural dialogue generation,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Copenhagen, Denmark: Association for Computational Linguistics, September 2017, pp. 2157- S. J. Rennie et al., “Self-critical sequence training for image caption- ing,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 7008-R. Cai et al., “Tag: Type auxiliary guiding for code comment genera- tion,” arXiv preprint arXiv:2005.02835, P. Dognin et al., “ReGen: Reinforcement learning for text and knowledge base generation using pretrained language models,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, November 2021, pp. 1084- R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus- based image description evaluation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 4566-S',\n",
       "  '. 1084- R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus- based image description evaluation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 4566-S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evalua- tion with improved correlation with human judgments,” in Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005, pp. 65-M. Popovié, “chrf++: words helping character n-grams,” in Proceedings of the second conference on machine translation, 2017, pp. 612- June 2018, pp. 1747- org/N18- 1158, Y.-C. Chen et al., “Fast abstractive summarization with reinforce- selected sentence rewriting,” in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Melbourne, Australia: Association for Computational Linguisti . 675- : Generating natural language feedback with reinforcement learning for repairing model outputs,” arXiv preprint arXiv:2305.08844, E. Todorov et al., “Linearly-solvable markov decision problems,” Advances in neural information processing systems, vol. 19, Y. Gao et al., “Reward learning for efficient reinforcement learning in extractive document summarisation,” arXiv preprint arXiv:1907.12894, 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 26 K. Nguyen et al',\n",
       "  '. 19, Y. Gao et al., “Reward learning for efficient reinforcement learning in extractive document summarisation,” arXiv preprint arXiv:1907.12894, 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 26 K. Nguyen et al., “Reinforcement learning for bandit neural machine translation with simulated human feedback,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Copenhagen, Denmark: Association for Computational Lingui: , September 2017, pp. 1464- R. S. Sutton et al., Temporal credit assignment in reinforcement learning. University of Massachusetts Amherst, T. Parshakova et al., “Distributional reinforcement learning for energy- based sequential models,” arXiv preprint arXiv:1912.08517, M. Khalifa et al., “A distributional approach to controlled text gener- ation,” arXiv preprint arXiv:2012.11635, T. Korbak et al., “Energy-based models for code generation under compilability constraints,” arXiv preprint arXiv:2106.04985, T. et al. Korbak, “On reward maximization and distribution matching for fine-tuning language models,” https://openreview.net/forum ?id=8f95ajHrIFc D. Go et al., “Aligning language models with preferences through f- divergence minimization,” arXiv preprint arXiv:2302.08215, B. et al. Zoph, “Neural architecture search with reinforcement learning,” in International Conference on Learning Representations, C. et al',\n",
       "  '.net/forum ?id=8f95ajHrIFc D. Go et al., “Aligning language models with preferences through f- divergence minimization,” arXiv preprint arXiv:2302.08215, B. et al. Zoph, “Neural architecture search with reinforcement learning,” in International Conference on Learning Representations, C. et al. Hsu, “MONAS: multi-objective neural architecture search using reinforcement learning,’ CoRR, vol. abs/1806.10332, N.-Q. Pham et al., “Towards one-shot learning for rare-word translation with external experts,” in Proceedings of the 2nd Workshop on Neural Machine Translation and Generation. Melbourne, Australia: Association for Computational Linguistics, July 2018, pp. 100- M. Guo et al., “Irlas: Inverse reinforcement learning for architecture search,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9021-Z. Zhong et al., “Practical block-wise neural network architecture generation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 2423-Y. Tian et al., “Off-policy reinforcement learning for efficient and ef- fective gan architecture search,” in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VII X. Chen et al., “Catch: Context-based meta reinforcement learning for transferrable architecture search,” in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Pro- ceedings, Part XIX B. et al',\n",
       "  '. Chen et al., “Catch: Context-based meta reinforcement learning for transferrable architecture search,” in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Pro- ceedings, Part XIX B. et al. Baker, “Designing neural network architectures using reinforcement learning,” in International Conference on Learning Representations, forum?id=S lc2cvqee’ D. Pang et al., “Rl-darts: Differentiable neural architecture search via reinforcement-learning-based meta-optimizer,” Knowledge-Based Systems, vol. 234, p. 107585, A. Chauhan et al., “Dqnas: Neural architecture search using reinforce- ment learning,” arXiv preprint arXiv:2301.06687, K. He et al., “Deep residual learning for image recognition,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June J. Rijsdijk et al., “Reinforcement learning for hyperparameter tuning in deep learning-based side-channel analysis,” JACR Transactions on Cryptographic Hardware and Embedded Systems, pp. 677-707, L. Wang et al., “A reinforced topic-aware convolutional sequence-to- sequence model for abstractive text summarization,” in Proceedings of the 27th International Joint Conference on Artificial Intelligence, ser. CAL Y. Wu et al., “Learning to extract coherent summary via deep reinforce- ment learning,” in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, L. Gui et al',\n",
       "  '. CAL Y. Wu et al., “Learning to extract coherent summary via deep reinforce- ment learning,” in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, L. Gui et al., “Neural topic model with reinforcement learning,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 3478— JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 151 152 J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano, “Recursively summarizing books with human feedback,” arXiv preprint arXiv:2109.10862, M. Yang, C. Li, Y. Shen, Q. Wu, Z. Zhao, and X. Chen, “Hierarchical human-like deep neural networks for abstractive text summarization,” IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 6, pp. 2744-2757, R. Jie, X. Meng, L. Shang, X. Jiang, and Q. Liu, “Prompt-based length controlled generation with reinforcement learning,” arXiv preprint arXiv:2308.12030, L. Wu et al., “A study of reinforcement learning for neural machine translation,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, October-November 2018, pp. 3612- T. K. Lam et al',\n",
       "  '.12030, L. Wu et al., “A study of reinforcement learning for neural machine translation,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, October-November 2018, pp. 3612- T. K. Lam et al., “A reinforcement learning approach to interactive- predictive neural machine translation,” in Proceedings of the 21st Annual Conference of the European Association for Machine Translation, Alicante, Spain, May 2018, pp. 189- T. Zhao et al., “Balancing quality and human involvement: An effective approach to interactive neural machine translation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, 2020, pp. 9660-F Luo et al., “Towards fine-grained text sentiment transfer,” in Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2013-A. Anuchitanukul et al., “SURF: Semantic-level unsupervised reward function for machine translation,” in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Seattle, United States: Association for Computational Linguistics, July 2022, pp. 4508- LV. Serban et al., “A deep reinforcement learning chatbot,” arXiv preprint arXiv: 1709.02349, M. Yang et al',\n",
       "  '. Seattle, United States: Association for Computational Linguistics, July 2022, pp. 4508- LV. Serban et al., “A deep reinforcement learning chatbot,” arXiv preprint arXiv: 1709.02349, M. Yang et al., “Personalized response generation via domain adapta- tion,” in Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2017, pp. 1021-Z. Sun et al., “Replicating complex dialogue policy of humans via of- fline imitation learning with supervised regularization,” arXiv preprint arXiv:2305.03987, J. D. Williams et al., “Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vancouver, Canada: Association for Computational Linguistics, July 2017, pp. 665- M. Lewis et al., “Deal or no deal? end-to-end learning of negotiation dialogues,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Copenhagen, Denmark: Association for Computational Linguistics, September 2017, pp. 2443- X. Li et al., “End-to-end task-completion neural dialogue systems,” in Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Taipei, Taiwan: Asian Federation of Natural Language Processing, November 2017, pp. 733- A. Martin et al',\n",
       "  '. 2443- X. Li et al., “End-to-end task-completion neural dialogue systems,” in Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Taipei, Taiwan: Asian Federation of Natural Language Processing, November 2017, pp. 733- A. Martin et al., “Learning natural language generation with truncated reinforcement learning,” in Proceedings of the 2022 Conference of 154 155 156 157 158 159 160 161 162 164 165 166 167 168 169 170 171 172 174 27 the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 12-Y. Jang, J. Lee, and K.-E. Kim, “Gpt-critic: Offline reinforcement learn- ing for end-to-end task-oriented dialogue systems,” in International Conference on Learning Representations, C. Zhong, K. Liao, W. Chen, Q. Liu, B. Peng, X. Huang, J. Peng, and Z. Wei, “Hierarchical reinforcement learning for automatic disease diagnosis,” Bioinformatics, vol. 38, no. 16, pp. 3995-4001, A. Saleh, N. Jaques, A. Ghandeharioun, J. Shen, and R. Picard, “Hierar- chical reinforcement learning for open-domain dialog,” in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 8741-R. Yang, J. Chen, and K. Narasimhan, “Improving dialog sys- tems for negotiation with personality modeling,” arXiv preprint arXiv:2010.09954, A. Srivastava et al',\n",
       "  '. 34, no. 05, 2020, pp. 8741-R. Yang, J. Chen, and K. Narasimhan, “Improving dialog sys- tems for negotiation with personality modeling,” arXiv preprint arXiv:2010.09954, A. Srivastava et al., “Response-act guided reinforced dialogue gener- ation for mental health counseling,” in Proceedings of the ACM Web Conference 2023, 2023, pp. 1118-R. Liu et al., “Aligning generative language models with human values,” in Findings of the Association for Computational Linguistics: NAACL 2022, 2022, pp. 241-R. Zhang et al., “Reward constrained interactive recommendation with natural language feedback,” arXiv preprint arXiv:2005.01618, A. K. Mohankumar et al., “Diversity driven query rewriting in search advertising,” in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, ser. KDD ’ X. Wang et al., “Ordering-based causal discovery with reinforcement learning,” arXiv preprint arXiv:2105.06631, A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs et al., “Training language models to self-correct via reinforcement learning,” arXiv preprint arXiv: L. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, A. Zipori, H. Noga, O. Keller, B. Piot, I. Szpektor et al., “Multi-turn rein- forcement learning from preference human feedback,” arXiv preprint arXiv:2405.14655, Y. Zhou, A. Zanette, J. Pan, S. Levine, and A',\n",
       "  '. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, A. Zipori, H. Noga, O. Keller, B. Piot, I. Szpektor et al., “Multi-turn rein- forcement learning from preference human feedback,” arXiv preprint arXiv:2405.14655, Y. Zhou, A. Zanette, J. Pan, S. Levine, and A. Kumar, “Archer: Training language model agents via hierarchical multi-turn rl,” arXiv preprint arXiv:2402.19446, A. Alabdulkarim, W. Li, L. J. Martin, and M. O. Riedl, “Goal- directed story generation: Augmenting generative language models with reinforcement learning,” arXiv preprint arXiv:2112.08593, P. Jha, P. Jana, P. Suresh, A. Arora, and V. Ganesh, “Risf: Reinforcement learning via symbolic feedback,” arXiv preprint arXiv:2405.16661, Z. Chen, K. Zhou, W. X. Zhao, J. Wan, F. Zhang, D. Zhang, and J.-R. Wen, “Improving large language models via fine-grained rein- forcement learning with minimum editing constraint,” arXiv preprint arXiv:2401.06081, W. Li, W. Wei, K. Xu, W. Xie, D. Chen, and Y. Cheng, “Reinforcement learning with token-level feedback for controllable text generation,” arXiv preprint arXiv:2403.11558, J. Li, H. Zhang, F. Zhang, T.-W. Chang, K. Kuang, L. Chen, and J. Zhou, “Optimizing language models with fair and stable reward composition in reinforcement learning,” in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024, pp. 10 122-10 Z. Li, T. Xu, Y. Zhang, Z. Lin, Y. Yu, R. Sun, and Z.-Q',\n",
       "  '. Kuang, L. Chen, and J. Zhou, “Optimizing language models with fair and stable reward composition in reinforcement learning,” in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024, pp. 10 122-10 Z. Li, T. Xu, Y. Zhang, Z. Lin, Y. Yu, R. Sun, and Z.-Q. Luo, “Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models,” in Forty-first International Conference on Machine Learning, P. H. Richemond, Y. Tang, D. Guo, D. Calandriello, M. G. Azar, R. Rafailov, B. A. Pires, E. Tarassov, L. Spangher, W. Ellsworth et al., “Offline regularised reinforcement learning for large language models alignment,” arXiv preprint arXiv:2405.19107, R. et al. Rafailov, “Direct preference optimization: Your language model is secretly a reward model,” JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 175 176 177 178 179 181 182 183 184 185 186 187 188 189 191 192 193 194 195 196 A. Baheti, X. Lu, F. Brahman, R. L. Bras, M. Sap, and M. Riedl, “Leftover lunch: Advantage-based offline reinforcement learning for language models,” arXiv preprint arXiv:2305.14718, M. Wulfmeier, M. Bloesch, N. Vieillard, A. Ahuja, J. Bornschein, S. Huang, A. Sokolov, M. Barnes, G. Desjardins, A. Bewley er al., “Imitating language via scalable inverse reinforcement learning,” in The Thirty-eighth Annual Conference on Neural Information Processing Systems, T. Korbak, K. Shi, A. Chen, R. V. Bhalerao, C. Buckley, J. Phang, S. R',\n",
       "  '. Bornschein, S. Huang, A. Sokolov, M. Barnes, G. Desjardins, A. Bewley er al., “Imitating language via scalable inverse reinforcement learning,” in The Thirty-eighth Annual Conference on Neural Information Processing Systems, T. Korbak, K. Shi, A. Chen, R. V. Bhalerao, C. Buckley, J. Phang, S. R. Bowman, and E. Perez, “Pretraining language models with human preferences,” in International Conference on Machine Learning. PMLR, 2023, pp. 17506-C. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine, “Offline rl for natural language generation with implicit language q learning,” arXiv preprint arXiv:2206.11871, J. Hu, L. Tao, J. Yang, and C. Zhou, “Aligning language models with offline reinforcement learning from human feedback,” arXiv preprint arXiv:2308.12050, M. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E. P. Xing, and Z. Hu, “Rlprompt: Optimizing discrete text prompts with reinforcement learning,” arXiv preprint arXiv:2205.12548, Z. Huang, X. Wang, F. Zhang, Z. Xu, C. Zhang, X. Zheng, and X. Huang, “Enhancing the capability and robustness of large language models through reinforcement learning-driven query refinement,” arXiv preprint arXiv:2407.01461, H. Sun, “Offline prompt evaluation and optimization with inverse reinforcement learning,” arXiv preprint arXiv:2309.06553, K. Nottingham, Y. Razeghi, K. Kim, J. Lanier, P. Baldi, R. Fox, and S',\n",
       "  '.01461, H. Sun, “Offline prompt evaluation and optimization with inverse reinforcement learning,” arXiv preprint arXiv:2309.06553, K. Nottingham, Y. Razeghi, K. Kim, J. Lanier, P. Baldi, R. Fox, and S. Singh, “Selective perception: Optimizing state descriptions with reinforcement learning for language model actors,” arXiv preprint arXiv:2307.11922, W. Shen, R. Zheng, W. Zhan, J. Zhao, S. Dou, T. Gui, Q. Zhang, and X. Huang, “Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback,” arXiv preprint arXiv:2310.05199, J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang, “Safe rlhf: Safe reinforcement learning from human feedback,” arXiv preprint arXiv:2310.12773, T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, and M. N. Halgamuge, “The inadequacy of reinforcement learning from human feedback- radicalizing large language models via semantic vulnerabilities,” JEEE Transactions on Cognitive and Developmental Systems, J. Wang, J. Wu, M. Chen, Y. Vorobeychik, and C. Xiao, “Rlhfpoison: Reward poisoning attack for reinforcement learning with human feed- back in large language models,” in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 2551-X. Wang, J. Peng, K. Xu, H. Yao, and T. Chen, “Reinforcement learning-driven Ilm agent for automated attacks on Ilms,” in Proceed- ings of the Fifth Workshop on Privacy in Natural Language Processing, 2024, pp. 170-Y. Li et al',\n",
       "  '. 2551-X. Wang, J. Peng, K. Xu, H. Yao, and T. Chen, “Reinforcement learning-driven Ilm agent for automated attacks on Ilms,” in Proceed- ings of the Fifth Workshop on Privacy in Natural Language Processing, 2024, pp. 170-Y. Li et al., “A generative model for category text generation,” Information Sciences, vol. 450, pp. 301-315, J. Xu et al., “Diversity-promoting gan: A cross-entropy based genera- tive adversarial network for diversified text generation,” in Proceedings of the 2018 conference on empirical methods in natural language processing, 2018, pp. 3940-Q. Wu et al., “Automatic math word problem generation with topic-expression co-attention mechanism and reinforcement learning,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 1061-1072, B. Upadhyay et al., “Efficient reinforcement learning for unsupervised controlled text generation,” arXiv preprint arXiv:2204.07696, C. Wang, H. Zhou, Y. Hu, Y. Huo, B. Li, T. Liu, T. Xiao, and J. Zhu, “Esrl: Efficient sampling-based reinforcement learning for sequence generation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 17, 2024, pp. 19 107-19 N. Jaques, J. H. Shen, A. Ghandeharioun, C. Ferguson, A. Lapedriza, N. Jones, S. S. Gu, and R. Picard, “Human-centric dialog training 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 28 via offline reinforcement learning,” arXiv preprint arXiv:2010.05848, D. Yarats et al',\n",
       "  '. Shen, A. Ghandeharioun, C. Ferguson, A. Lapedriza, N. Jones, S. S. Gu, and R. Picard, “Human-centric dialog training 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 28 via offline reinforcement learning,” arXiv preprint arXiv:2010.05848, D. Yarats et al., “Hierarchical text generation and planning for strategic dialogue,” in International Conference on Machine Learning. PMLR, 2018, pp. 5591-J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High- dimensional continuous control using generalized advantage estima- tion,” arXiv preprint arXiv:1506.02438, N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,” Advances in Neural Information Processing Systems, vol. 36, A. Havrilla, M. Zhuravinskyi, D. Phung, A. Tiwari, J. Tow, S. Bi- derman, Q. Anthony, and L. Castricato, “trix: A framework for large scale reinforcement learning from human feedback,” in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 8578-S. Zhang, Z. Chen, S. Chen, Y. Shen, Z. Sun, and C. Gan, “Improving reinforcement learning from human feedback with efficient reward model ensemble,” arXiv preprint arXiv:2401.16635, A. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar, and R',\n",
       "  '. Zhang, Z. Chen, S. Chen, Y. Shen, Z. Sun, and C. Gan, “Improving reinforcement learning from human feedback with efficient reward model ensemble,” arXiv preprint arXiv:2401.16635, A. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar, and R. Raileanu, “Teach- ing large language models to reason with reinforcement learning,” arXiv preprint arXiv:2403.04642, Z. Yao et al., “Coacor: Code annotation for code retrieval with reinforcement learning,” in The world wide web conference, 2019, pp. 2203-C. Wang et al., “Enriching query semantics for code search with reinforcement learning,’ Neural Networks, vol. 145, pp. 22-32, W. Wang et al., “Reinforcement-learning-guided source code summa- rization using hierarchical attention,” JEEE Transactions on software Engineering, vol. 48, no. 1, pp. 102-119, X. Wang et al., “Compilable neural code generation with compiler feedback,” arXiv preprint arXiv:2203.05132, L. Zhang et al., “Learnedsqlgen: Constraint-aware sq] generation using reinforcement learning,” in Proceedings of the 2022 International Conference on Management of Data, ser. SIGMOD ’ H. Le et al., “Coderl: Mastering code generation through pretrained models and deep reinforcement learning,” Advances in Neural Infor- mation Processing Systems, vol. 35, pp. 21314-21328, P. Shojaee et al., “Execution-based code generation using deep rein- forcement learning,” arXiv preprint arXiv:2301.13816, S. Duan, N. Kanakaris, X. Xiao, H',\n",
       "  '. 35, pp. 21314-21328, P. Shojaee et al., “Execution-based code generation using deep rein- forcement learning,” arXiv preprint arXiv:2301.13816, S. Duan, N. Kanakaris, X. Xiao, H. Ping, C. Zhou, N. K. Ahmed, G. Ma, M. Capota, T. L. Willke, S. Nazarian ef al., “Leveraging rein- forcement learning and large language models for code optimization,” arXiv preprint arXiv:2312.05657, J. Liu, Y. Zhu, K. Xiao, Q. Fu, X. Han, W. Yang, and D. Ye, “Ritf: Reinforcement learning from unit test feedback,” arXiv preprint arXiv:2307.04349, J. Gehring, K. Zheng, J. Copet, V. Mella, T. Cohen, and G. Synnaeve, “Rlef: Grounding code Ilms in execution feedback with reinforcement learning,” arXiv preprint arXiv:2410.02089, S. Dou, Y. Liu, H. Jia, L. Xiong, E. Zhou, W. Shen, J. Shan, C. Huang, X. Wang, X. Fan et al., “Stepcoder: Improve code generation with reinforcement learning from compiler feedback,” arXiv preprint arXiv:2402.01391, B. Steenhoek, M. Tufano, N. Sundaresan, and A. Svyatkovskiy, “Rein- forcement learning from automatic feedback for high-quality unit test generation,” arXiv preprint arXiv:2310.02368, Z. Ren et al., “Deep reinforcement learning-based image captioning with embedding reward,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 290-L. Zhang et al., “Actor-critic sequence training for image captioning,” arXiv preprint arXiv:1706.09601, L. Miao et al',\n",
       "  '., “Deep reinforcement learning-based image captioning with embedding reward,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 290-L. Zhang et al., “Actor-critic sequence training for image captioning,” arXiv preprint arXiv:1706.09601, L. Miao et al., “Multi-modal product title compression,” Information Processing & Management, vol. 57, no. 1, p. 102123, JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 X. Shen, B. Liu, Y. Zhou, J. Zhao, and M. Liu, “Remote sensing image captioning via variational autoencoder and reinforcement learning,” Knowledge-Based Systems, vol. 203, p. 105920, J. Shi, Y. Li, and S. Wang, “Partial off-policy learning: Balance accuracy and diversity for human-oriented image captioning,” in Pro- ceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2187-W. Nie, J. Li, N. Xu, A.-A. Liu, X. Li, and Y. Zhang, “Triangle- reward reinforcement learning: a visual-linguistic semantic alignment for image captioning,” in Proceedings of the 29th ACM international conference on multimedia, 2021, pp. 4510-R. Dessi, M. Bevilacqua, E. Gualdoni, N. C. Rakotonirina, F. Franzon, and M. Baroni, “Cross-domain image captioning with discriminative finetuning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 6935-J. Zhang et al',\n",
       "  '. 4510-R. Dessi, M. Bevilacqua, E. Gualdoni, N. C. Rakotonirina, F. Franzon, and M. Baroni, “Cross-domain image captioning with discriminative finetuning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 6935-J. Zhang et al., “Goal-oriented visual question generation via intermedi- ate rewards,” in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 186-Z. Zhao et al., “Open-ended video question answering via multi- modal conditional adversarial networks,” JEEE Transactions on Image Processing, vol. 29, pp. 3859-3870, R. Saqur and K. Narasimhan, “Multimodal graph networks for com- positional generalization in visual question answering,” Advances in Neural Information Processing Systems, vol. 33, pp. 3070-3081, J. Zhang et al., “Sch-gan: Semi-supervised cross-modal hashing by generative adversarial network,’ JEEE transactions on cybernetics, vol. 50, no. 2, pp. 489-502, K. Black et al., “Training diffusion models with reinforcement learn- ing,” arXiv preprint arXiv:2305.13301, Y. Fan et al., “Dpok: Reinforcement learning for fine-tuning text-to- image diffusion models,” arXiv preprint arXiv:2305.16381, Y. Akizuki et al., “Generative modelling with design constraints— reinforcement learning for object generation,” in RE: Anthropocene, Design in the Age of Humans—Proceedings of the 25th CAADRIA Conference, vol. A. Ostonov et al',\n",
       "  '.16381, Y. Akizuki et al., “Generative modelling with design constraints— reinforcement learning for object generation,” in RE: Anthropocene, Design in the Age of Humans—Proceedings of the 25th CAADRIA Conference, vol. A. Ostonov et al., “Rlss: A deep reinforcement learning algorithm for sequential scene generation,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 2219- Z. Zhang et al., “Point cloud scene completion with joint color and semantic estimation from single rgb-d image,” JEEE Transactions on Pattern Analysis and Machine Intelligence, C. Lin, T. Fan, W. Wang, and M. NieBner, “Modeling 3d shapes by reinforcement learning,” in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part X K. Zhao, Y. Zhang, S. Wang, T. Beeler, and S. Tang, “Synthesizing diverse human motions in 3d indoor scenes,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 14738-L. Siyao, W. Yu, T. Gu, C. Lin, Q. Wang, C. Qian, C. C. Loy, and Z. Liu, “Bailando: 3d dance generation by actor-critic gpt with choreographic memory,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 11 050-11 D. Deng, A. Wu, H. Qu, and Y. Wu, “Dashbot: Insight-driven dashboard generation based on deep reinforcement learning,” JEEE Transactions on Visualization and Computer Graphics, vol. 29, no. 1, pp. 690-700, T. Liu, Q. Meng, J.-J. Huang, A',\n",
       "  '. 11 050-11 D. Deng, A. Wu, H. Qu, and Y. Wu, “Dashbot: Insight-driven dashboard generation based on deep reinforcement learning,” JEEE Transactions on Visualization and Computer Graphics, vol. 29, no. 1, pp. 690-700, T. Liu, Q. Meng, J.-J. Huang, A. Vlontzos, D. Rueckert, and B. Kainz, “Video summarization through reinforcement learning with a 3d spatio- temporal u-net,” JEEE transactions on image processing, vol. 31, pp. 1573-1586, Y. Yu, J. Chung, H. Yun, J. Hessel, J. S. Park, X. Lu, R. Zellers, P. Am- manabrolu, R. Le Bras, G. Kim ef al., “Fusing pre-trained language 242 243 244 245 246 247 248 249 250 251 253 254 255 256 257 258 259 260 261 262 263 264 265 29 models with multimodal prompts through reinforcement learning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 10 845-10 Y. Zhai, H. Bai, Z. Lin, J. Pan, S. Tong, Y. Zhou, A. Suhr, S. Xie, Y. LeCun, Y. Ma ef al., “Fine-tuning large vision-language models as decision-making agents via reinforcement learning,” arXiv preprint arXiv:2405.10292, P. H. Seo, P. Sharma, T. Levinboim, B. Han, and R. Soricut, “Rein- forcing an image caption generator using off-line human feedback,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 03, 2020, pp. 2693-H. De Vries et al., “Guesswhat?! visual object discovery through multi- modal dialogue,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5503-D. S. R',\n",
       "  '. 34, no. 03, 2020, pp. 2693-H. De Vries et al., “Guesswhat?! visual object discovery through multi- modal dialogue,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5503-D. S. R. Mohan et al., “Incremental text to speech for neural sequence- to-sequence models using reinforcement learning,” arXiv preprint arXiv:2008.03096, R. Liu et al., “Reinforcement learning for emotional text-to-speech synthesis with improved emotion discriminability,’ arXiv preprint arXiv:2104.01408, J. Gibson et al., “A reinforcement learning approach to speech coding,” Information, vol. 13, no. 7, p. 331, Z. Li et al., “A symbolic-domain music generation method based on leak-gan,” in 202] 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST). EEE, 2021, pp. 549-J. Guo et al., “Long text generation via adversarial training with leaked information,” in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, E. Putin et al., “Reinforced adversarial neural computer for de novo molecular design,” Journal of chemical information and modeling, vol. 58, no. 6, pp. 1194-1204, M. Popova et al., “Deep reinforcement learning for de novo drug design,” Science advances, vol. 4, no. 7, p. eaap7885, E. Putin et al., “Adversarial threshold neural computer for molecular de novo design,” Molecular pharmaceutics, vol. 15, no. 10, pp. 4386- 4397, N. De Cao et al',\n",
       "  '. 6, pp. 1194-1204, M. Popova et al., “Deep reinforcement learning for de novo drug design,” Science advances, vol. 4, no. 7, p. eaap7885, E. Putin et al., “Adversarial threshold neural computer for molecular de novo design,” Molecular pharmaceutics, vol. 15, no. 10, pp. 4386- 4397, N. De Cao et al., “Molgan: An implicit generative model for small molecular graphs,” arXiv preprint arXiv: 1805.11973, M. Goel et al., “Molegular: molecule generation using reinforcement learning with alternating rewards,” Journal of Chemical Information and Modeling, vol. 61, no. 12, pp. 5815-5826, T. Blaschke et al., “Memory-assisted reinforcement learning for diverse molecular de novo design,” Journal of cheminformatics, vol. 12, no. 1, pp. 1-17, M. Thomas et al., “Augmented hill-climb increases reinforcement learning efficiency for language-based de novo molecule generation,” Journal of Cheminformatics, vol. 14, no. 1, pp. 1-22, N. Brown et al., “Guacamol: benchmarking models for de novo molecular design,” Journal of chemical information and modeling, vol. 59, no. 3, pp. 1096-1108, B. Chen et al., “Fragment-based sequential translation for molecular optimization,” arXiv preprint arXiv:2111.01009, R. Ishitani et al., “Molecular design method using a reversible tree rep- resentation of chemical compounds and deep reinforcement learning,” Journal of Chemical Information and Modeling, vol. 62, no. 17, pp. 4032-4048, T. Fu et al',\n",
       "  '.01009, R. Ishitani et al., “Molecular design method using a reversible tree rep- resentation of chemical compounds and deep reinforcement learning,” Journal of Chemical Information and Modeling, vol. 62, no. 17, pp. 4032-4048, T. Fu et al., “Reinforced genetic algorithm for structure-based drug design,” Advances in Neural Information Processing Systems, vol. 35, pp. 12325-12338, P. Hu et al., “De novo drug design based on stack-rnn with multi- objective reward-weighted sum and reinforcement learning,” Journal of Molecular Modeling, vol. 29, no. 4, pp. 1-12, M. Sun et al., “Molsearch: Search-based multi-objective molecular generation and property optimization,” in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 4724-X. Liu et al., “Drugex v3: scaffold-constrained drug design with graph transformer-based reinforcement learning,” Journal of Cheminformat- ics, vol. 15, no. 1, p. 24, G. Simm, R. Pinsler, and J. M. Hernéndez-Lobato, “Reinforcement learning for molecular design guided by quantum mechanics,” in International Conference on Machine Learning. _PMLR, 2020, pp. 8959-A. Zholus, M. Kuznetsov, R. Schutski, R. Shayakhmetov, D. Polykovskiy, S. Chandar, and A. Zhavoronkov, “Bindgpt: A JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO',\n",
       "  '. _PMLR, 2020, pp. 8959-A. Zholus, M. Kuznetsov, R. Schutski, R. Shayakhmetov, D. Polykovskiy, S. Chandar, and A. Zhavoronkov, “Bindgpt: A JOURNAL OF I4TpX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 scalable framework for 3d molecular design via language modeling and reinforcement learning,” arXiv preprint arXiv:2406.03686, S. K. Gottipati, B. Sattarov, S. Niu, Y. Pathak, H. Wei, S. Liu, S. Black- burn, K. Thomas, C. Coley, J. Tang er al., “Learning to navigate the synthetically accessible chemical space using reinforcement learning,” in International conference on machine learning. _PMLR, 2020, pp. 3668-P. Rajak, A. Krishnamoorthy, A. Mishra, R. Kalia, A. Nakano, and P. Vashishta, “Autonomous reinforcement learning agent for chemical vapor deposition synthesis of quantum materials,” npj Computational Materials, vol. 7, no. 1, p. 108, K. M. Powell, D. Machalek, and T. Quah, “Real-time optimization using reinforcement learning,” Computers & Chemical Engineering, vol. 143, p. 107077, P.C. Nguyen, N. N. Vlassis, B. Bahmani, W. Sun, H. Udaykumar, and S. S. Baek, “Synthesizing controlled microstructures of porous media using generative adversarial networks and reinforcement learning,” Scientific reports, vol. 12, no. 1, p. 9034, E.-J. Kuo et al., “Quantum architecture search via deep reinforcement learning,” arXiv preprint arXiv:2104.07715, J. Lin, Z. Y. Lai, and X',\n",
       "  '. 12, no. 1, p. 9034, E.-J. Kuo et al., “Quantum architecture search via deep reinforcement learning,” arXiv preprint arXiv:2104.07715, J. Lin, Z. Y. Lai, and X. Li, “Quantum adiabatic algorithm design using reinforcement learning,” Physical Review A, vol. 101, no. 5, p. 052327, M. Ostaszewski, L. M. Trenkwalder, W. Masarczyk, E. Scerri, and V. Dunjko, “Reinforcement learning for optimization of variational quantum circuit architectures,” Advances in Neural Information Pro- cessing Systems, vol. 34, pp. 18 182-18 194, L. Moro, M. G. Paris, M. Restelli, and E. Prati, “Quantum compiling by deep reinforcement learning,” Communications Physics, vol. 4, no. 1, p. 178, B. Sanchez-Lengeling et al., “Optimizing distributions over molecu- lar space. an objective-reinforced generative adversarial network for inverse-design chemistry (organic),” M. Olivecrona et al., “Molecular de-novo design through deep rein- forcement learning,” Journal of cheminformatics, vol. 9, no. 1, pp. 1-14, S. R. Atance et al., “De novo drug design using reinforcement learn- ing with graph-based deep generative models,” Journal of Chemical Information and Modeling, vol. 62, no. 20, pp. 4863-4872, G. Brockman et al., “Openai gym,” arXiv preprint arXiv:1606.01540, X. et al. Chen, “A survey of deep reinforcement learning in recommender systems: A systematic review and future directions,” CoRR, vol. abs/2109.03540, S. et al',\n",
       "  '. 62, no. 20, pp. 4863-4872, G. Brockman et al., “Openai gym,” arXiv preprint arXiv:1606.01540, X. et al. Chen, “A survey of deep reinforcement learning in recommender systems: A systematic review and future directions,” CoRR, vol. abs/2109.03540, S. et al. Wang, “Causal decision transformer for recommender systems via offline reinforcement learning,” W. et al. Huang, “Voxposer: Composable 3d value maps for robotic manipulation with language models,” G. et al. Wang, “Voyager: An open-ended embodied agent with large language models,” A. Khalifa et al., “Pegrl: Procedural content generation via reinforce- ment learning,” in Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, vol. 16, no. 1, 2020, pp. 95-D. Ha et al., “Reinforcement learning for improving agent design,” Artificial life, vol. 25, no. 4, pp. 352-365, V.-A. Darvariu et al., “Goal-directed graph construction using rein- forcement learning,” Proceedings of the Royal Society A, vol. 477, no. 2254, p. 20210168, O. Bar El, T. Milo, and A. Somech, “Automatically generating data ex- ploration sessions using deep reinforcement learning,” in Proceedings of the 2020 ACM SIGMOD international conference on management of data, 2020, pp. 1527-L. et al. Choshen, “On the weaknesses of reinforcement learning for neural machine translation,” in International Conference on Learning Representations, 289 290 291 292 293 294 295 296 297 298 299 301 302 303 304 305 306 307 30 S',\n",
       "  '. 1527-L. et al. Choshen, “On the weaknesses of reinforcement learning for neural machine translation,” in International Conference on Learning Representations, 289 290 291 292 293 294 295 296 297 298 299 301 302 303 304 305 306 307 30 S. Kiegeland et al., “Revisiting the weaknesses of reinforcement learn- ing for neural machine translation,” arXiv preprint arXiv:2106.08942, R. Ramamurthy et al., “Is reinforcement learning (not) for natural lan- guage processing?: Benchmarks, baselines, and building blocks for nat- ural language policy optimization,” arXiv preprint arXiv:2210.01241, D. Donato et al., “Mad for robust reinforcement learning in machine translation,” arXiv preprint arXiv:2207.08583, U. Honda et al., “Switching to discriminative image captioning by relieving a bottleneck of reinforcement learning,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 1124-W. Jin et al., “Multi-objective molecule generation using interpretable substructures,” in International conference on machine learning. PMLR, 2020, pp. 4849-H. Guo et al., “Efficient (soft) q-learning for text generation with limited good data,” in Findings of the Association for Computational Linguistics: EMNLP 2022, 2022, pp. 6969-M. Korshunova et al., “Generative and reinforcement learning ap- proaches for the automated de novo design of bioactive compounds,” Communications Chemistry, vol. 5, no. 1, p. 129, Z. Yao et al',\n",
       "  '. 6969-M. Korshunova et al., “Generative and reinforcement learning ap- proaches for the automated de novo design of bioactive compounds,” Communications Chemistry, vol. 5, no. 1, p. 129, Z. Yao et al., “An imitation game for learning semantic parsers from user interaction,” arXiv preprint arXiv:2005.00689, A. S. Vezhnevets et al., “Feudal networks for hierarchical reinforcement learning,” in International Conference on Machine Learning. PMLR, 2017, pp. 3540-T. Zhao et al., “A multi-scenario text generation method based on meta reinforcement learning,” Pattern Recognition Letters, vol. 165, pp. 47— 54, Z. Zhao, W. S. Lee, and D. Hsu, “Large language models as common- sense knowledge for large-scale task planning,” Advances in Neural Information Processing Systems, vol. 36, D. Zhang, S. Zhoubian, Y. Yue, Y. Dong, and J. Tang, “Rest-mcts*: Lim self-training via process reward guided tree search,” arXiv preprint arXiv:2406.03816, S. Ross et al., “A reduction of imitation learning and structured pre- diction to no-regret online learning,” in Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011, pp. 627-T. Hospedales et al., “Meta-learning in neural networks: A survey,” IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 9, pp. 5149-5169, O. et al. Ended Learning Team, “Open-ended learning leads to generally capable agents,” CoRR, vol. abs/2107',\n",
       "  '. 627-T. Hospedales et al., “Meta-learning in neural networks: A survey,” IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 9, pp. 5149-5169, O. et al. Ended Learning Team, “Open-ended learning leads to generally capable agents,” CoRR, vol. abs/2107.12808, Available: M learning for fast adaptation of deep networks,” in International conference on machine learning. PMLR, 2017, pp. 1126-H. et al. Liu, “Evaluating the logical reasoning ability of chatgpt and gpt-4.” A. Tripp et al., “Sample-efficient optimization in the latent space of deep generative models via weighted retraining,” Advances in Neural Information Processing Systems, vol. 33, pp. 11259-11272, B. Schélkopf et al., “Toward causal representation learning,” Proceed- ings of the IEEE, vol. 109, no. 5, pp. 612-634, 2021.',\n",
       "  'arX1iv:2312.10256v2 [cs.MA] 3 Jul 2024 Multi-agent Reinforcement Learning: A Comprehensive Survey Dom Huh! and Prasant Mohapatra! ‘University of California, Davis {dhuh, pmohapatra}@ucdavis.edu 3University of South Florida Abstract Multi-agent systems (MAS) are widely prevalent and crucially important in numer- ous real-world applications, where multiple agents must make decisions to achieve their objectives in a shared environment. Despite their ubiquity, the development of intelligent decision-making agents in MAS poses several open challenges to their fective implementation. This survey examines these challenges, placing an em- phasis on studying seminal concepts from game theory (GT) and machine learning (ML) and connecting them to recent advancements in multi-agent reinforcement learning (MARL), i.e. the research of data-driven decision-making within MAS. Therefore, the objective of this survey is to provide a comprehensive perspective along the various dimensions of MARL, shedding light on the unique opportu- nities that are presented in MARL applications while highlighting the inherent hallenges that accompany this potential. Therefore, we hope that our work will not only contribute to the field by analyzing the current landscape of MARL but also motivate future directions with insights for deeper integration of concepts from related domains of GT and ML',\n",
       "  '. Therefore, we hope that our work will not only contribute to the field by analyzing the current landscape of MARL but also motivate future directions with insights for deeper integration of concepts from related domains of GT and ML. With this in mind, this work delves into a detailed exploration of recent and past efforts of MARL and its related fields and describes prior solutions that were proposed and their limitations, as well as their applications. fo} fe) 1 Introduction Multi-agent reinforcement learning (MARL) has long been recognized as a pivotal domain in artificial intelligence (AJ), promising dynamic solutions for complex tasks within multi-agent systems (MAS) that involve multiple goal-oriented decision-making, i.e. control, agents. The importance of devising such solutions is evident, as it enables the realization of a wide array of real-world applications, where the consideration of the existence of other intelligent agents is required. The process of developing these agents is largely centered around facilitating the emergence of not only decision-making abilities but also an adept understanding of social dynamics in a data-driven manner. Hence, with proper modeling and learning methods, these agents strive to leverage the multi-agent nature of their shared environment to achieve their individual and collective goals',\n",
       "  '. Hence, with proper modeling and learning methods, these agents strive to leverage the multi-agent nature of their shared environment to achieve their individual and collective goals. Alongside this focus on social behaviors and their connection to an agent’s decision-making capabili- ties, the motivation to integrate concepts from related domains such as game theory (GT) and machine learning (ML) becomes critical, as GT and ML provide a rich background and broader perspective to the problem posed by MARL. However, it remains equally important to concurrently study the distinctive challenges that arise under the MARL paradigm|Stone and Veloso 2000}, Bernstein et al. (2013) to fully understand the field’s unique intricacies and promote potential breakthroughs. In the past decade, there has been a significant interest in MARL efforts, poised to endow the desirable behavioral qualities that characterize intelligent social agents through data-driven learning processes . These learning processes have ranged from assuming ical perspectives of full rationality to embodying models of bounded rationality[Shoham et al.| , wherein agents progressively refine their behaviors in a myopic and iterative manner over time and experience. A common theme of this survey is emphasizing the increasing incorporation of the realism and its complexities that pervades real-world MAS applications into our MARL solutions',\n",
       "  '.| , wherein agents progressively refine their behaviors in a myopic and iterative manner over time and experience. A common theme of this survey is emphasizing the increasing incorporation of the realism and its complexities that pervades real-world MAS applications into our MARL solutions. And despite the many successes in MARL in these prior efforts, many open challenges persist, paving the way for future research endeavors. The central purpose of this survey is to provide a comprehensive view of these efforts, concretely staging the current state of MARL research from a holistic perspective. The structure of this survey is as follows. In Section [3] we define the problem statement of learning optimal control within MAS, and discuss seminal concepts from foundational fields of MARL such as GT and ML ~ all of which provide the foundation to the recent ideas studied in MARL. In Section [4] we discuss the unique benefits and challenges of learning in a MAS and explore the learning pathologies that plague the MARL paradigms. Lastly, in Section [5] the prospects of MARL are studied, such as MARL-specific simulation, training paradigms, communication methods, the challenges of multi-agent credit assignment and ad-hoc team-play, social learning, and agent modeling, and a detailed discussion regarding the recent efforts associated to these prospects. 2 Related Surveys There exists a rich literature surveying various research topics in MARL, from general overviews',\n",
       "  '. 2 Related Surveys There exists a rich literature surveying various research topics in MARL, from general overviews . (2003 ,/Busoniu et al. 2008], Bloembergen et al.|[2015], it [ [2022] to specific topics such as dealing with non-stationarity /Hernandez-Leal Matignon et al.|[2012], multi-agent learning , communication perspectives are constrained, excluding important aspects that come only from GT and ML viewpoints, thereby lacking the context needed to fully realize the current developments and limitations of MARL. In contrast, our article presents a more modern and comprehensive survey that aims to provide a holistic view of the challenges inherent and unique to learning control within multi-agent environ- ments. Additionally, we provide context to these challenges by weaving together the perspectives from GT and ML into a new unified view to present a novel understanding of the distinctive nature of the MARL problem. 3 Background In this section, we formalize the basis of the MARL problem statement and define its learning goals and solution concepts. We additionally explore related fields, i.e. GT and ML, that contribute to a deeper understanding of the study of MARL. 3.1 Multi-agent Environment A MAS consists of a population of decision-making agents that exist within a shared environment, as illustrated in Figure[I] These agents observe their environment and communicate with one another to perform actions that align with their objectives',\n",
       "  '. 3.1 Multi-agent Environment A MAS consists of a population of decision-making agents that exist within a shared environment, as illustrated in Figure[I] These agents observe their environment and communicate with one another to perform actions that align with their objectives. This observation and communication amongst agents is constrained by decentralization. Definition 1 (Decentralization) In a decentralized setting, each agent is capable of perceiving its environment, communicating with other agents, and taking action autonomously. There exist two forms of decentralization. Natural decentralization is the limitations imposed by physical realities, like communication range, while artificial decentralization involves specific requirements to improve tractability, such as communication bandwidth |Whiteson| [2020]. More generally, each agent is defined and inhibited by a set constraint. The agents can devise strategies to make decisions to reach their defined goals. This causal association between stimuli, action, and objective is what we refer to as the agent’s behavior[Matarid|(1994]. The Environment environment then responds to the agent’s actions, transitioning the current setting to the next state and providing the agents with feedback signals. This response from the environment is referred to as the model. This process between the agents and their environment forms a closed-looped interaction that iterates until a terminal condition is met',\n",
       "  '. This response from the environment is referred to as the model. This process between the agents and their environment forms a closed-looped interaction that iterates until a terminal condition is met. The MAS setting described here is widely prevalent in real-world applications, encompassing au- tonomous vehicles |Shalev-Shwartz et al. 2016], Zhou et al.| 2020], internet marketing (2018aJ, multi-robot contro [1997], networking applications such as optimizing communica- | tion networks|Luong et al.|[2019] and traffic control|Calvo and Dusparic (2018), Chu et al.}[2019], and multiplayer game playing|Samvelyan et al.|[2019]. 3.2 Stochastic Game We now introduce a formal representation of the MAS setting described in Section[3-1] called the stochastic game|Shapley| [1953], [2001], where the term “game” refers to the interactions between strategic agents. This framework serves as the basis for a wide range of multi- agent applications[Busoniu et al.] ]. and is related to other models of games, as seen in Figure Definition 2 (Stochastic Game) A stochastic game is a 5-tuple (N,S,A,r,7) where these elements are defined as: ¢ Nis the set of n agents. S is the set of (global) states. «© A= Apo x Ai X +--+ X An is the joint action space, where A; is the action space of agent T:S x Ax S++ P(S) is the state transition operator which maps a state-action pair to the probability of next states. The state defines the global setting and configuration of the environment',\n",
       "  '. «© A= Apo x Ai X +--+ X An is the joint action space, where A; is the action space of agent T:S x Ax S++ P(S) is the state transition operator which maps a state-action pair to the probability of next states. The state defines the global setting and configuration of the environment. To transition from one state to another, each agent i uses their policy 7; : S +» A; —a functional representation of a composite of the agent’s behaviors that is expressed as a mapping from its perceived state to action — to make decisions, otherwise known as their strategy. Generally, a policy returns a probability distribution over the action space conditioned on the state. > t(a\\\\s) =1 acA The joint policy 7 : S++ A is defined as a mapping to the joint action space, commonly achieved by concatenating the local actions computed by each agent’s policies. We introduce the concept of an information set as an aggregate state, where it encapsulates all information available to an agent during its decision-making process. The state transition function 7 returns the probability of reaching certain states from a given state- action pair. As each state holds the Markov property, i.e. every state is sufficient to infer the future, Partially Observable Stochastic Game agents Bayesian Game ‘m states partially observable agents - with type space agents im states - fully observable sequential actions the state transition function is sufficiently conditioned on the current state and action',\n",
       "  '. T(s,0, 8\") = P(si41 = 8\\'|s¢ = 8,0, =a) () Using the stochastic game framework as a basis, we can build and introduce additional concepts to align with various real-world scenarios. Sequential and Macro-Actions In many multi-agent applications, agents may perform actions that occur asynchronously, meaning either agents take turns performing their actions, or their actions take different duration of time. In cases where agents take turns, we distinguish this form of decision-making as sequential and is represented as an extensive-form game, otherwise, we refer to settings where agents’ actions are done at the same time as simultaneous and use the stochastic game framework. In the latter case where actions have varying duration, we turn to an alternate framework called the macro-action stochastic game [Amato et al.|[2019],[Xiao et al.|(2022]. This framework removes the assumption of synchronized primitive-action execution, which assumes that all actions take the same period of time. Hence, the notion of macro-actions is introduced, where macro-actions are temporally-extended actions that can vary in duration. Consequently, this also introduces a notion of macro-observation. Imperfect Information — It may be intractable for agents to directly perceive the (global) state of the environment. Instead, agents may only have access to observations',\n",
       "  '. Consequently, this also introduces a notion of macro-observation. Imperfect Information — It may be intractable for agents to directly perceive the (global) state of the environment. Instead, agents may only have access to observations. Thereby, we introduce the joint observation space O = Op x --- x Op, that accounts for the notion of imperfect information|Shoham| Observations do not necessarily satisfy the Markovian property and may contain limited insights regarding the true state of the environment. In fact, even joint observational histories h = {09, 01, ...0¢} can be insufficient to infer the state s; of the environment. However, the contrapositive is true, where the emission transition function €(o0, s) defines the mapping from states to corresponding joint observations that can be potentially induced. E(0, 8) = P(ols) (2) The concept of imperfect information encapsulates the limits of information agents are restricted to, and help enforce the constraints of decentralization. Imperfect information is strongly related to two other important ideas of information limitations: partial observability and incomplete information. Partial observability constrains agents’ access to a subset of the state that can be obscured by noise. In settings with incomplete information, agents do not have common knowledge about the game that is being played, which leads to uncertainties regarding various aspects of the state',\n",
       "  '. Partial observability constrains agents’ access to a subset of the state that can be obscured by noise. In settings with incomplete information, agents do not have common knowledge about the game that is being played, which leads to uncertainties regarding various aspects of the state. A framework that takes these concepts of information limitation into account is called partially-observable stochastic games (POSG), and if agents hold a belief, or type, over these uncertainties, this is called a Bayesian Reward Function Within all models of decision-making, the reward function plays an central role in guiding the behaviors of agents. The reward function provides immediate feedback to agents, evaluating the state and/or action of the agents with respects to their objective in the form of a scalar value. The common goal in optimizing control in MAS is to maximize this reward signal, thereby instilling a degree of rationality. Definition 3 (Rationality) Rational agents take actions that will maximize the expected cumulative reward, known as the expected return, they receive over time. In certain multi-agent applications, it can be difficult to define individualized reward functions for each agent, but it may be trivial to define a single reward function for all agents. This difficulty relates to the nature of the task, often derived from an inability to define proper multi-agent credit assignment',\n",
       "  '. This difficulty relates to the nature of the task, often derived from an inability to define proper multi-agent credit assignment. In such applications, we use the decentralized MDP (Dec-MDP), where a centralized reward function returns a collective score shared by all agents. Nature of Interaction An important factor to keep in mind is the relationships between the agents’ actions and decisions and how they interact with one another, which we refer to as the nature of interactions. In each setting, the interplay between each agent’s behaviors can lead to different dynamics and outcomes and may require varying considerations, notably to the definition of the reward function. Cooperative setting - All agents share the aligning goal. ¢ Adversarial setting - Agents have dichotomous goals. ¢ Mixed setting - Agents have varying goals. In a cooperative setting, all agents share aligning goals, where the reward function is typically designed to promote collaboration and joint success. In an adversarial setting, groups of agents may have dichotomous goals, meaning agents that aim to maximize their own reward can impede the achievement of other agents’ goals. In a mixed setting, agents have varying goals, and this introduces additional complexities as agents may have aligning, conflicting, or overlapping interests at various points in time. These interactions can be realized with inherent and artificial structures placed within the task definition',\n",
       "  '. In a mixed setting, agents have varying goals, and this introduces additional complexities as agents may have aligning, conflicting, or overlapping interests at various points in time. These interactions can be realized with inherent and artificial structures placed within the task definition. There exists a deep interconnection between the reward function and the nature of interactions, as the reward signals directly guide the emergent behaviors and outcomes within a MAS. For instances, the rewards of each agent may be computed by a function of not only their own policies but also the policies of others. This concept of multi-agent reward dependency is formalized with aggregative games. Social Context The purpose of social context is to ensure mutually consistency between agents’ actions within an shared environment, and is defined by social convention and role assignment/Busoniu] . Social conventions are established through social constructs to often define preferences for joint action profiles and is used to resolve conflicting action selections. Role assignments restrict and define the actions available for each agent as well as influence their rationality and objectives. The properties and conditions of these elements are largely considered task-specific, however, all MARL solutions must take into account these unique characteristics and demands of each situation. Networked Games _ To tractably perform certain multi-agent tasks, agents must communicate with one another',\n",
       "  '. Networked Games _ To tractably perform certain multi-agent tasks, agents must communicate with one another. This is often achieved through defined communication channels and form a communication network G(s) that is associated to each state s. This element G(s) = (V, E), where vertices V correspond to the agents and an edge (i, j) € E exists if agents i and 7 can communicate, is appended to the stochastic game framework, forming the networked stochastic game. Coordination The outcome of an agent’s actions can often depend on the actions of other agents. This dependency of behaviors and the corresponding consequences of their actions between agents is encapsulated by the concept of coordination. There are two common approaches of embedding the idea of coordination into a game paradigm, through a coordination graph or interactions. Coordination graphs specify the coordination dependencies between agents’ actions in form of a graph, where the nodes represent agents and the edges represent these dependencies {Nair et al.][2005}, [2001], [Kok et al.|[2005]. This concept is expressed with action-graph games [2011]. With coordination graphs, we can factorize the global utility into a set of local payoff and utility functions. Each payoff function is associated with a subset of agents and can be interpreted as (hyper)-edges in a graph where the nodes are agents|Amato and Oliehoek| 2014}, whereas the utility functions reflect individual agent’s utility',\n",
       "  '. Each payoff function is associated with a subset of agents and can be interpreted as (hyper)-edges in a graph where the nodes are agents|Amato and Oliehoek| 2014}, whereas the utility functions reflect individual agent’s utility. With this factorization, maximum-a-posteriori estimation techniques, such as variable elimination |Guestrin et al.|[2001], max-plus|Vlassis et al.|[2004], or Q-learning [Kok et al_] (2005}. can be used to compute the optimal joint action. More recent extensions leverage deep neural networks to model different components of the factorized value function fet al.|[2020] or the coordination graph itself|Li et al.][202 1]. ¢ Similar to coordination graphs, interactions broadly define specific conditions in which coordination should occur|Koller and Milch||2003]. However, unlike coordination graphs, we can specify restriction of these interactions for only defined for certain states or state- actions [2009} f20TT}, [2010]. Additionally, these interactions are classified as strategic compliments or substitute [2016], where interactions can produce mutual reinforcements or discouragements. Thereby, the concept of coordination is illustrated naturally through “social\" networks, thereby often studied within network games as defined in classic GT. We note that network games are not networked stochastic game, where the former specifies coordination dependencies and the latter defines communication channels',\n",
       "  '. We note that network games are not networked stochastic game, where the former specifies coordination dependencies and the latter defines communication channels. Return A state-action trajectory tT = {8 9, ao, 81,41, --., 8} is sampled from the dynamics model distribution p,,(7) following a joint policy 7, where: T Px(T) = P(80) [] t(aelse)T (se, a, S$t41) (3) t=0 With this trajectory, we can compute an important quantity called the return, which calculates the future utility of a given state. Formally, the return Gj;,4) at each time step for an agent i, sometimes called the agent’s gain, is the cumulative future discounted reward. T Gust) = > rise, div) (4) t=t where 7 € [0, 1] is the discount factor that enforces a diminishing value for more distant rewards. Value Function The value function V,,,(s;|7) and the Q-value function Q,,, (s¢, ai.|7) map the state s, and state-action pair (s,, a;,,) to the expected return for an agent 7 given some joint policy 7, and are commonly used to develop solutions for optimizing controls in MAS. Va; (8e\\\\7) = Erxp,(r|se) [G@i,2)(7)] (5) Qn; (St; ait\\\\T) = Ep, (TIsis0i DIGG.) (7)] (6) Vaz (Se|7) = Ean (alse) Oz: (St, @i,t7)] (7) The advantage function A,.,(s;, a@;,,|7) measures the benefit of taking action a;,, in state s; for agent i under the joint policy z',\n",
       "  '. Va; (8e\\\\7) = Erxp,(r|se) [G@i,2)(7)] (5) Qn; (St; ait\\\\T) = Ep, (TIsis0i DIGG.) (7)] (6) Vaz (Se|7) = Ean (alse) Oz: (St, @i,t7)] (7) The advantage function A,.,(s;, a@;,,|7) measures the benefit of taking action a;,, in state s; for agent i under the joint policy z. It quantifies how much better or worse it is to choose action a;,, compared to the average expected return of all actions in that state, similar to the concept of regret(Jin et al_] 18b]. Hence, the advantage function is defined as the difference between the Q-value function i,t\\\\7) and the state-value function V;,, (s;|7). Ax; (St; ai,t\\\\7) = Qa; (St; ai,t\\\\7) _ Vi; (St|7) (8) MAS Objective The objective J;(-) of agent i is expressed as maximizing the expected return, where trajectories 7 are sampled from the dynamics model distribution p, (7) following a joint policy T. I(t) = E,xp, (r)[Gi,0)(7)] (9) In practice, the expectation of the return can be approximated using the Monte Carlo sampling or through temporal difference estimation. Such empirical estimates can result in return estimates with high variance, resulting in greater learning complexity, a central and persistent issue in MARL solutions. We state the general form of the MAS optimization objective, which we describe as maximizing the expected return for all agents. maximize J;(7),Vi € N (10) 3',\n",
       "  '. We state the general form of the MAS optimization objective, which we describe as maximizing the expected return for all agents. maximize J;(7),Vi € N (10) 3. The field of GT, often studied within the domains of economics, provides a formal context that analyzes and conceptualizes strategic interactions among multiple agents within a market . The term “market\" will be interchangeably used with the idea of a shared environment. examine the MAS optimization objective from a game theoretic perspective to understand solution concepts relating to learning goals and social principles used to identify the joint behaviors that are considered desirable or interesting (1985). For further study into GT, we refer readers to the following resource{Osborne] [2009]. ct Theory Under the lens of utility theory|/Fishburn et al. {1979}, Shoham and , we study how individuals make choices by quantifying their preferences, defining an preference relation. We define the utility function u(-) to reflect the individual’s subjective evaluation of their overall satisfaction, assigning a numerical value to each outcome and obeys the axioms of preference. Utility theory affirms the sufficiency of scalar value representation of the agent’s preferences|Shoham and Leyton-Brown| In many real-world applications, agents face uncertainty. The causes of this uncertainty stem not only from the environment dynamics but also from the behaviors of other agents',\n",
       "  '. Utility theory affirms the sufficiency of scalar value representation of the agent’s preferences|Shoham and Leyton-Brown| In many real-world applications, agents face uncertainty. The causes of this uncertainty stem not only from the environment dynamics but also from the behaviors of other agents. Under such uncertain settings, the agents’ decision-making processes should account for these unpredictabilities. We introduce this notion as risk and the domain of prospect theory, a study of decision-making under risk Kahneman and Tversky| [2013], Prashanth etal taking risk into account, agents are capable of weighing their intrinsic preferences and objectives against their unknowns, defining the agents’ aversion to losses, their reference points, and diminishing sensitivity Kahneman and Tversky] behaviors driven by risk, where “losses loom larger than gains\" thereby illustrating the behaviors that come with loss-averse and gain-seeking decisions, and remains a large focus in behavioral GT {Camerer| [1997]. There exists a diminishing sensitivity to these losses and gains, meaning that the impact of a change diminishes with the distance from the reference point',\n",
       "  '. There exists a diminishing sensitivity to these losses and gains, meaning that the impact of a change diminishes with the distance from the reference point. Reference points are a concept introduced by prospect theory that represents the status quo and argues that the value of an agent is defined by the final utility positions, as stated in traditional utility theory, may not paint the whole picture, and instead, it is important to view their value in terms of gains and losses, i.e. the agent’s value on a relative scale. However, the behaviors driven by risk defined by prospect theory become less relevant under repeated market settings, as experience within a market reduces uncertainty |[Loomes et al.] . We note that (expected) utility theory does account for uncertainty in its own manner, where the solutions derived from utility theory are considered risk-neutral but this faces violations in many risk-sensitive games|Kahneman and Tversky (2013}. Generally, risk-sensitive decision-making can be modeled with chance constraints, which are statisti- cal constraints to averse based on a probability of a high utility loss at specified risk-tolerance levels, or value-at-risk (VaR) and expected shortfall (i.e. conditional VaR (CVaR)), which place a similar constraint which is now specified by quantiles of the utility distribution, i.e. the tail risk',\n",
       "  '.e. conditional VaR (CVaR)), which place a similar constraint which is now specified by quantiles of the utility distribution, i.e. the tail risk. Incentives and Mechanisms _ Incentives provide context to the unique behaviors that come along with asymmetries in information and actions between agents in market settings, which are commonly studied under contract theory and auctions Salanidl(2005] . For instance, agents may be unequally informed about the various parts of the market, which can be reflective of the agent’s roles or the social context, and this can lead to adverse selections, moral hazards, and nonverifiability[Laffont and] . Adverse selection refers to situations where the agent with less information may make decisions that are unfavorable or risky, whereas moral hazard refers to the tendency of agents to take greater risks when they are insured or protected because they know they are shielded from some of the consequences. Lastly, nonverifiability refers to situations where the quality or performance of goods or services exchanged cannot be easily verified by the parties involved. Together, these concepts underscore the critical role of incentives in aligning behaviors and outcomes in markets characterized by information asymmetries. Incentives influence how agents gather and disclose information, manage risks, and fulfill contractual obligations, thereby shaping the efficiency and effectiveness of market interactions',\n",
       "  '. Incentives influence how agents gather and disclose information, manage risks, and fulfill contractual obligations, thereby shaping the efficiency and effectiveness of market interactions. We now consider how these games and their rules are constructed, i.e. game form, to induce certain desired outcomes and perhaps more importantly, avoid undesirable outcomes. This effort is studied under the domain of mechanism design|Hurwicz and Reiter and accounts for both incentive and feasibility constraints, as well as the distribution of agent preferences, particularly distinguishing between whether they are public or private. Mechanism design remains a key focus in GT research and plays a crucial role in both theory and practice, offering frameworks to improve market functionality, regulatory policy, and organizational design in contexts of dynamic markets. Solution Concepts and Equilibrium Solution concepts refer to states of equilibria and are de- scribed by meaningful or interesting properties used to evaluate joint policies, otherwise known as strategy profiles. Pareto efficiency is a general criterion commonly used to evaluate strategy profiles',\n",
       "  '. Solution Concepts and Equilibrium Solution concepts refer to states of equilibria and are de- scribed by meaningful or interesting properties used to evaluate joint policies, otherwise known as strategy profiles. Pareto efficiency is a general criterion commonly used to evaluate strategy profiles. A strategy profile nm Pareto dominates another strategy profile 7 if: Vi EN: uj(x’) > u;(7) Fie N: u(t’) > us(m) (11) In other words, no agent using strategy profile nm can be better off without making another agent worse off by using Definition 4 (Pareto Efficiency) A strategy profile x* is a Pareto efficient solution if it is not Pareto dominated by any other strategy profiles. Pareto efficiency focuses on maximizing overall welfare, which is the sum of the all agent’s utilities. However, Pareto efficiency does not emphasize individual rationality or collective stability. Therefore, Pareto efficient solution may not necessarily be a good measure if the truly optimal solution requires agents to act against their self-interest or deviate from some locally desirable strategies. Importantly, Pareto optimality does not address the issue of fairness or equality, as some individuals may benefit more than others in the pursuit of maximizing overall welfare[Shoham and Leyton-Brown| 2008}',\n",
       "  '. Importantly, Pareto optimality does not address the issue of fairness or equality, as some individuals may benefit more than others in the pursuit of maximizing overall welfare[Shoham and Leyton-Brown| 2008}. The concept of best response provides an alternative perspective in analyzing strategic interactions in multi-agent systems that aligns more with concepts of individual rationality and collective stability. Given a strategy profile 7 = {0,71,...,7i,...,7n}, a best response strategy 7! for agent i is defined by: Vorj ui([r—a, 7i]) = ui([7-1, m]) (12) where 7_; = 7 \\\\ 7; denotes the strategy profile without agent 7 and [/71, 72] is the strategy profile of 1, The solution concept of a Nash equilibrium (NE) 7* applies this notion of best response to the collective, where for all agents i, 77 is the best response to 7* ;. Vay: u([a*j,77]) 2 u(fa*,, mil) (13) Definition 5 (Nash Equilibrium) A Nash equilibrium (NE) defines a state where no individual agent can increase its expected return by unilaterally deviating from their policy\\\\Nash\\\\ {T95TJ. Hence, within a Nash equilibrium, all of the agents’ strategy is the best response to the other agents’ strategy. Unfortunately, similar to Pareto efficient solutions, NE is not unique, and determining the differences between sample equilibria in terms of their social behaviors is unclear',\n",
       "  '. Hence, within a Nash equilibrium, all of the agents’ strategy is the best response to the other agents’ strategy. Unfortunately, similar to Pareto efficient solutions, NE is not unique, and determining the differences between sample equilibria in terms of their social behaviors is unclear. In fact, the behaviors exhibited by sample equilibria can vary starkly, where their comparison can be computed using other concepts of efficiency such as coordination ratio[Koutsoupias and Papadimitriou] [1999] or Price of Stability While NE remains a popular criterion for multi-agent decision-making under uncertainty, computing this equilibrium may be computationally intractable in complex games. Moreover, achieving desired joint behaviors is not guaranteed through this approach|Shoham et al | (2003}, Matignon et al. 2012}. Nonetheless, NE does represent stable points akin to saddle points within the optimization landscape. To address the intractability of computing a strict NE, e-Nash equilibrium relaxes the requirements by allowing the agent to deviate if it improves its expected returns by more than some value e. u([7*;,7;]) > u([r*,, ml) — Correlated equilibrium (CE) is an important generalization of NE that adds the notion of correlating strategies among agents. This considers the existence of signals that coordinate agents’ actions 4], as well as the introduction of a correlating distribution over the strategies of all agents',\n",
       "  '. This considers the existence of signals that coordinate agents’ actions 4], as well as the introduction of a correlating distribution over the strategies of all agents. In cases where such correlating signals do not necessarily affect the joint strategies and cause no agents to deviate regardless of the information provided by the signals, we call the following optimization state a coarse correlated equilibrium (CCE). As mentioned, NE is a special case of CE, where the correlating distribution of the agents’ strategies is a product of independent distributions. The concept of perfect equilibrium Bielefeld) [1988] refines the idea of NE in a different manner — by imposing additional requirements of consistency and mutual optimality. This solution concept describes an optimization state where agents’ strategies are mutually consistent and take into account the possibility of off-equilibrium actions, requiring agents to choose strategies that are robust against such deviations. In fact, numerous additional refinements and solution concepts exist that take into account asymmetric roles and rea oet| on ewon ele situations involving imperfect information [2008], or even games with sequential dynamics iv and non-stationary considerations|Daskalakis et al.][2022],/Kim et al.|{2022]',\n",
       "  '.][2022],/Kim et al.|{2022]. Equilibrium Analysis and Computation The process of computing the optimal decisions for multiple agents is referred to as equilibrium computation [2022] and has been studied under the framework of optimization and variational inequalities [1990], [Kovalev et 3]. Equilibrium computation aims to find specific points of interest, known as equilibria, within an optimization landscape that spans the strategy space of multiple agents. Formally, equilibria represent stable or “optimal\" solutions where the dynamical system reaches a balanced or steady state. Equilibria can be described as being local or global, indicating whether the state is a locally optimal solution or the best solution across the entire landscape. The two important properties we must consider for equilibrium computation are their existence and tractability. The existence of an equilibrium largely depends on the utility function of each agent, and whether it is concave with respects to their actions, where in nonconcave games, the existence of equilibrium is at risk. The tractability of equilibrium refers to the complexity of solving for an equilibrium and this remains a significant challenge to be compute efficient even in nonconcave games. Equilibrium Complexity Even with the existences of an equilibrium solution , the complexity of computing this equilibrium must be discussed. This discussion is most aptly had under the pretense of a total search problem (i.e',\n",
       "  '. Equilibrium Complexity Even with the existences of an equilibrium solution , the complexity of computing this equilibrium must be discussed. This discussion is most aptly had under the pretense of a total search problem (i.e. PPAD) rather than as a decision problem (i.e. P/NP). Although, computing an equilibrium can be classified an NP-hard decision problem|Gilboa and Zemel] [1989], |Conitzer and Sandholm) [2008], [Nisan et al.] ,|Albrecht et al. Definition 6 (PPAD) Polynomial parity argument for directed graphs (PPAD) consists of problems that can be reduced to the following (End-Of-The-Line) problem: Given a directed graph with vertices have at most one predecessor and/or one successor, and a source vertex 8, where s has no predecessor, find a vertex t with no predecessor or no successor, such that s #t. Let there be a polynomial-time function that returns the predecessor and successor of all vertices in this graph. where PPAD-completeness is shown by reducing the End-Of-The-Line problem. To support the hardness of PPAD problems, |Daskalakis et al.\\\\{2009] proposes the following question: How can one hope to devise an efficient algorithm that telescopes exponentially long paths in every implicitly given graph? Computing the Nash equilibrium is proven to be a PPAD-complete [Daskalakis et al.][2009],|Chen| 07], further affirming its difficulty and potential intractability',\n",
       "  '.][2009],|Chen| 07], further affirming its difficulty and potential intractability. Learning Dynamics While these solution concepts of equilibrium give context for stability and optimality, they do not provide insights into the process of reaching such states. The concept of learning dynamics bridges this disconnect by detailing the procedure for reaching equilibrium. Additionally, learning dynamics also helps analyze the transition into equilibrium itself, which can be equally or more important to gain insights into task-specific understandings, and the processes that come after achieving equilibrium, where within that steady state, whether continual lifelong learning and further adaptations may come into the equation. In general, there exist two fundamental learning dynamics within the process of equilibrium computation: best-response dynamics and no-regret dynamics. Best-response dynamics directly optimize to converge to an NE or one of its refinements',\n",
       "  '. In general, there exist two fundamental learning dynamics within the process of equilibrium computation: best-response dynamics and no-regret dynamics. Best-response dynamics directly optimize to converge to an NE or one of its refinements. A seminal algorithm of best-response learning dynamics is fictitious play (FP), where all agents iteratively compute the best response to other agents’ strategi most specifically, the uniform distribution over the past strategies of the other agents|Robinson| We summarize the seminal realizations and recent advancements in best-response dynamics learning: ¢ There are improvements to the idea of FP that include improved robustness to perturbations in the form of stochastic FP|Fudenberg and Kreps 1993} as well as uncertainty within these models of other agents and update these beliefs through Bayesian updating, known as rational and Bayesian learning [Albrecht et al .|[2024], Jordan] (1991), Specifically, Bayesian learning utilizes the value of information [2003], which considers how actions will influence future behaviors and devise more accurate beliefs. ¢ FP algorithms are typically used in normal-form games but can be realized in extension-form games using behavioral strategies, known as extensive-form fictitious play (XFP), or through approximate best response and sample-based learning, i.e',\n",
       "  '. ¢ FP algorithms are typically used in normal-form games but can be realized in extension-form games using behavioral strategies, known as extensive-form fictitious play (XFP), or through approximate best response and sample-based learning, i.e. ML approach to XFP, known as Fictitious Self-Play Double oracle approach adopts the same iterative procedure as fictitious play, but instead, agents now compute a meta-NE to a restricted game that is maintained and expanded by the past best-response strategies |McMahan et al. (2003}, Adam et al. 2021). Policy Space Response Oracles, Deep Cognitive Hierarchy 2017}, and Extensive-Form Double Oracle|McAleer et al.|[/2021] incorporate and build upon the double oracle algorithm with the use of ML techniques, and address computational issues that exist when extending double oracle to extensive form games, promote generalization and prevent overfitting to specific equilibrium|Bighashdel et al. (2024), Lanctot et al. 2017). * Value iteration (Shapley) [1953] can be applied over the joint-action space, resulting in solutions such as Minimax Q-learning |Littman| ame Nash Q-Learning|Hu and Wellman| (2003), Correlated Q-learning|Greenwald and Hall|[2003], and Friend-or-Foe Q-learning (20016). ¢ Replicator dynamics|Maynard Smith\\\\|1976] adapts the concept of evolutionary dynamics to achieve best-response policies|Tuyls et al.|2006]. 0] utilizes gradient learning to optimize Infinitesimal gradient ascent (IGA)|Singh et al',\n",
       "  '. ¢ Replicator dynamics|Maynard Smith\\\\|1976] adapts the concept of evolutionary dynamics to achieve best-response policies|Tuyls et al.|2006]. 0] utilizes gradient learning to optimize Infinitesimal gradient ascent (IGA)|Singh et al.| [2000] agent’s policy with respect to their utility. Similar methods and their extensions to improve convergence and other theoretical properties include using variable step sizes ,|Bowling| , proximal point optimization ; momentum|Gidel et al./[2019], extra-gradient , and optimistic gradients (2073) (2018), |Wei et al (2021) In no-regret learning dynamics, the aim is instead to minimize regret, a measure of how much an agent would have gained in utility if they had chosen a different strategy often in retrospect |Hart and| Mas-Colell| [ . Intuitively, regret can be quantified by the average cost between the utility of the best possible strategy profile and the actual utility of the chosen strategy profile. This notion of regret is called external regret, where comparisons of decisions are performed using an expert offline strategy. On the other hand, internal, or swap, regret takes a more online approach, compared to a modified strategy that swaps out certain actions with others from the original strategy. There exists a well-known connection between no-regret and Nash equilibrium |Zinkevich et al.|[2007], where an e-Nash equilibrium is a profile that achieves an upper bound regret of < €',\n",
       "  '. There exists a well-known connection between no-regret and Nash equilibrium |Zinkevich et al.|[2007], where an e-Nash equilibrium is a profile that achieves an upper bound regret of < €. Usually, the tractability of computing and verifying best-response can be compromised with games with high complexity, and therefore no-regret dynamics approaches are an attractive alternative as they scale very well when using domain-specific abstractions, such as in Poker AI applications |Zinkevich et al.][2007}, [fammelin| (2077) We summarize the seminal realizations and recent advancements in no-regret dynamics learning: SS ¢ Regret matching serves as the foundation for regret minimization algorithms, and is achieved through repeated self-play that computes the strategy iterates based on a distribution of 10 normalized positive regret! [1956], [2000]. Convergence to a stable solution, i.e. sample equilibrium, is achieved by taking the average overall strategy iterates. ¢ Counterfactual regret minimization (CFR) extends regret matching to extensive form games with counterfactual regret, which accounts for the sequential nature of actions in extensive form games|Zinkevich et al.|[2007]. To ensure sufficient coverage over the game tree for CFR updates, a sampling approach using external or chance samplin: or a more extensive search, like in CFR+ [Tammelin| (2077), Bowling et al] , must be considered',\n",
       "  '.|[2007]. To ensure sufficient coverage over the game tree for CFR updates, a sampling approach using external or chance samplin: or a more extensive search, like in CFR+ [Tammelin| (2077), Bowling et al] , must be considered. Recent advancements, such as CFR+ and advantage-based reg: nimization/Jin et al] (2018b}, have demonstrated performance improvements with resetting and positive clipping negative cumulative regret to zero, which can be a form of “optimism under uncertainty\". = ¢ Follow-the-Leader (FTL) is another popular regret-minimizing technique studied more in online learning, and FTL constructs online strategies to follow the actions with minimal loss over past rounds, i.e. the best “expert” [2012]. However, a naive implementation of FTL is unstable, where a natural solution to this instability is to append a time-varying regularization term, known as Follow-the-Regularized-Leader (FoReL). Importantly, the choice of regularization term leads to different regret bounds. Another method to address this instability and avoid overfitting from FoReL is to use the subgradient method with the proximal term as the Bregman divergence, known as mirror descent|Oi',\n",
       "  '. Importantly, the choice of regularization term leads to different regret bounds. Another method to address this instability and avoid overfitting from FoReL is to use the subgradient method with the proximal term as the Bregman divergence, known as mirror descent|Oi . For both FoReL and mirror descent methods, continued efforts to seek improvements are made to the regret bounds, with techniques as simple as gradient clipping |Cutkosky ¢ Multiplicative weights update/hedge algorithm generalizes FTL, as it now maintains weights that are updated with a non-infinite learning rate rather than selecting the single “expert” (1999) [Arora et al] 2012). The learning rate can be adaptive to ensure ar consistent low regret on easy and hard instances with the doubling trick or using both FTL and hedge periodically |De Rooij et al.|2014]. ¢ IGA and its extensions achieve both best-response and no-regret under certain conditions [2003], and with the advent of deep learning in MAS applications, gradient methods have become quite popular. Connection to MARL The literature on GT is extensive, with ongoing efforts investigating diverse challenges in strategic interactions among multiple agents. However, the significance of MARL lies in the integration of data-driven considerations into these strategic decision-making processes. So, unlike traditional GT, MARL leverages insights from data and statistics to manage complex markets where agents’ behaviors can be shaped by data-driven models',\n",
       "  '. However, the significance of MARL lies in the integration of data-driven considerations into these strategic decision-making processes. So, unlike traditional GT, MARL leverages insights from data and statistics to manage complex markets where agents’ behaviors can be shaped by data-driven models. This approach enables MARL to effectively address dynamic, uncertain, and large-scale scenarios that traditional GT often finds challenging to handle. 3.4 Machine Learning The field of ML represents a crucial domain within AI, with the objective of constructing data-driven solutions that excel in pattern recognition using statistical models . Central to ML are two primary tasks: data collection and data analysis. Data collection defines the process of gathering and managing data which constructs the target data distribution, whereas the task of data analysis encompasses the methods of statistical inference using this data distribution. Hence, the data plays a pivotal role in the ML process, defining the underlying distribution on which ML models rely their statistics upon. This significance has become glaringly apparent in real-world ML applications, where the integrity of data is often tested by strategic agents aiming to manipulate statistics or exploit the predictive models themselves|Zrnic et al.]',\n",
       "  '. This significance has become glaringly apparent in real-world ML applications, where the integrity of data is often tested by strategic agents aiming to manipulate statistics or exploit the predictive models themselves|Zrnic et al.] . Unfortunately, this multi-agent dynamic is canonically neglected in ML literature but has more recently gained traction in the emerging fields of strategic classification and adversarial ML, which develop solutions regarding the cycle between the development and deployment of ML models and the post-hoc response of strategic agents influencing the model, which can potentially adversarially “attack\" the model and “‘pollute\" the data. Another popular stream of research that blends GT into ML is in generative modeling, notably with the training of generative- adversarial networks (GAN). GANs employ a min-max optimization scheme to train an generative 11 model with an discriminator model that classifies between true and generated data. However, an naive implementation without proper GT considerations leads to chaotic and oscillatory learning Daskalakis et al.|{2018]. In essence, the efficacy of real-world ML hinges on robust data practices and the ability to navigate the complex landscape of strategic interactions in diverse scenarios. 3.4.1 Deep Learning Deep learning is a popular and general approach in contemporary AI research, with its foundations relying on the use of artificial neural networks (ANNs)',\n",
       "  '. 3.4.1 Deep Learning Deep learning is a popular and general approach in contemporary AI research, with its foundations relying on the use of artificial neural networks (ANNs). ANNs have showcased remarkable proficiency in a wide variety of general applications{Goodfellow et al.|[2016]. Concretely, ANNs define a set of parameters 6 that act as a function that processes and transforms data using layers of linear and non-linear operations. We optimize these parameters with respect to a defined cost function J(@) using methods of statistical learning and numerical optimization, such as stochastic gradient descent (SGD)|James et al.| 6=0 —VoJ(6) (15) A key attribute of a deep learning approach is its ability to develop a scalable end-to-end solution that can capture complex distributions underlying many real-world applications. Additionally, we avoid the need to handcraft features and instead allow the optimization to learn curated latent representation tailored toward the task at hand in a data-driven manner. Hence, the emphasis is on extracting patterns, relationships, and insights directly from data, rather than relying heavily on predefined rules or models, although such efforts are not necessarily orthogonal',\n",
       "  '. Hence, the emphasis is on extracting patterns, relationships, and insights directly from data, rather than relying heavily on predefined rules or models, although such efforts are not necessarily orthogonal. In the context of MARL, integrating these deep learning methods into MARL solutions has shown promise for developing expressive and adaptive multi-agent systems for complex tasks, where agent’s behaviors are defined using a composite of ANN representations and optimized in an end-to-end manner. For example, we can represent the elements of a stochastic game, such as the policy, value function, or model, using ANNs to optimize the joint behavior. The central constraints in such methods reside with the need for large-scale data collection and the high computational cost required for training. 3.4.2 Reinforcement Learning The domain of reinforcement learning (RL) focuses on learning how to make decisions guided by reward signals{Sutton and Barto} {2018] and serves as the general foundation of learning controls. A caveat of traditional RL research is that it is normally studied under a single-agent setting, i.e. using the MDP framework. Predictions and Control We define the process of sequential decision-making as control. Control contrasts the concept of predictions, as predictions are static inferences that have no causal relationship to future inputs and predictions',\n",
       "  '.e. using the MDP framework. Predictions and Control We define the process of sequential decision-making as control. Control contrasts the concept of predictions, as predictions are static inferences that have no causal relationship to future inputs and predictions. Formally, predictions are independent and identically distributed (IID), whereas no such assumptions are made for control. Within RL frameworks, control is learned within an environment whose dynamics the agent has limited knowledge of Rech] 2019], however predictions still play a major role in this capability, whether it is incorporated implicitly or explicitly, to ensure that the agent has an understanding of the world it resides. Value Function Approximation A value-based approach for RL estimates the value function to approximate the optimal policy. Hence, value-based solutions often do not define an explicit policy and instead, use an implicit policy computed using the value function, 7(a|s) « Q(s,a). These Q-learning methods|Watkins|[1989],/Watkins and Dayan| [1992 utilize approximate dynamic programming by applying the concept of policy iterations, as shown in Figure[3} with the Bellman (expectation) equations 57], a set of recursive definitions of the value functions. Qx(8t, 4) =Exxp, (7|s+,02)|Ge(7)] =r(sz, ay)+ Bs. p1~T(ses0r.s¢41)',\n",
       "  '. Qx(8t, 4) =Exxp, (7|s+,02)|Ge(7)] =r(sz, ay)+ Bs. p1~T(ses0r.s¢41).ars1 0m (alsiz1) Qn (St41; A141) (16) We define the Bellman backup operator 6, on our value function estimate Q such that Q = B,Q is equivalent to Equation|I6] By iteratively applying the Bellman backup operator B,, 6, is shown to 12 . E ce I . . evaluation (shown as —) and policy improvement (shown as —). Policy evaluation computes the value function for current policy whereas policy improvement updates current policy with respect to evaluated value function. The following figure was taken and modified from 2018}. be a contraction mapping, thereby according to the Banach fixed-point theorem, Q is guaranteed to converge to a unique fixed point that corresponds to the true Q-function Q,, for the policy x{Sutton] fand Barto| 2078}. By employing an optimal (implicit) policy denoted as 7* (als) = 6(argmax,¢ 4 Q(s,a@)), the need for policy evaluation, a process that scales poorly with the state-action space, can be circumvented. This approach, known as value iteration, leverages the Bellman optimality equation in the form of the Bellman backup 6, to minimize the Bellman residual error. Qre (St, Ut) = 7(St, Ut) + Esce1eT (sete ses) Max Qre(St41, 4)] (17) Value function approximation methods have come with several improvements over the past decade, including: ¢ Extension to deep neural networks representation for continuous state space and usage of experience replay to stabilize off-policy learning [Mnih et al',\n",
       "  '. (2013). + Distributional value function representation [Bellemare et al.|(2017), (2018bjaj, Yang et al.|/2019] for more expressive value approximations. Mitigate overoptimism/maximization bias with double learning|Van Hasselt et al. clipped double Q-learning, and target policy smoothing [2018b]. * Utilization of a dueling architecture /Wang et al.|[2016b] to help figure out which states are valuable without having to learn the effect of each action for each state. ¢ Improvements to experience replay to prioritize important transitions|Schaul et al.|[2015], to perform better with sparse rewards|Andrychowicz et al.|[2017], and to work with recurrent networks|Hausknecht and Stone|(2015},/Kapturowski et al (2018). Improved exploration through randomized modeling 2017], intrinsic rewards Badia et al.|[2020b]a], maximum entropy framework|Haarnoja et al./[2017]. ¢ Handle continuous action spaces by assuming a deterministic policy|Lillicrap et al.|2015}, parameterizing the Q-function as a well-defined convex function’ [2016], or utilizing sampling methods|Kalashnikov et al. in order to compute the Bellman update. ¢ Learn on datasets of offline experiences while managing distributional mismatch and contin- uing improvements beyond behaviors seen in dataset 2020] by appending a behavior regularization term to the standard RL training (2021), regulariz- ing overestimation with conservatism|Kumar et al.||2020], utilizing probabilistic regression Kostrikov et al. (2021 {Ma et al',\n",
       "  '.||2020], utilizing probabilistic regression Kostrikov et al. (2021 {Ma et al.| [2021], and a model estimate|Yu et al. [2022]. This application is known as batch or offline RL. + Integration of the listed improvements through the years [Hessel et al.|[2017], fetal] . Value-based approaches are commonly used in MARL algorithms, as they offer a powerful and expressive solution to learning and producing a control policy in an off-policy approach, thereby being sample-efficient. 13 Policy Gradient A policy-based approach of RL directly optimizes the objective stated in Equation A common approach is to perform gradient ascent along the objective using the policy gradient, pioneered by the REINFORCE (REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility) algorithm|Williams]|1988|/1992]. Vo,J (7) = Vo, Erxp, (r)[Go(7) — b(7)] Tv = Enp,(r)L¥(Ge(7) — 0(7)) Vor log(a(ar|s+))] (18) t=0 where 6, represents the parameters of the policy 7 and D(-) is the reinforcement baseline function used to stabilize the approximated reinforcement encoll As the expectation is computed through Monte Carlo sampling, it can often lead to ian variance despite providing an unbiased estimate of the expected return. Hence, as noted by|Williams|[1992),/Schulman et al.|2018], the choice of d(-) can improve convergence and performance by mitigating this variance. Notable improvements to this vanilla policy gradient consist of: ¢ Learn on data from various policies, i.e',\n",
       "  '. Hence, as noted by|Williams|[1992),/Schulman et al.|2018], the choice of d(-) can improve convergence and performance by mitigating this variance. Notable improvements to this vanilla policy gradient consist of: ¢ Learn on data from various policies, i.e. an off-policy approach, with policy gradient methods by using importance sampling corrections |Jie and Abbeel| [2010], |Degris et al. [2012] that enables training on a more diverse dataset in a sample-efficient manner. ¢ Mitigate dominating gradients on the parameter level by reparameterizing loss under a probabilistic manifold using Fisher information matrix 2001] with natural policy gradient, or Kronecker-factored approximated curvature [2017]. Utilize surrogate objective that guarantees monotonic improvements, conditioned on a trust-region optimization|Kakade and Langford] [2002], Schulman et al. (2017a}b}. Unlike value-based approaches, policy-based approaches are not commonly practiced in MARL, largely due to the sample inefficiencies from its on-policy requirements and high variance. However, in cases where samples can be generated cheaply, policy-based approaches can perform very well. Actor-Critic Methods _Actor-critic algorithms provide an integration of policy-based and value- based methods[Sutton et a,|(19994)',\n",
       "  '. However, in cases where samples can be generated cheaply, policy-based approaches can perform very well. Actor-Critic Methods _Actor-critic algorithms provide an integration of policy-based and value- based methods[Sutton et a,|(19994). Instead of estimating the value function of an implicit policy, we model the value function for an explicitly defined policy thereby minimizing Bellman residual using Equation{I6]and learning from a policy gradient. With access to a value estimate, temporal difference (TD) learning or n-step learning can be utilized instead of Monte Carlo sampling to approximate the offset reinforcement. While these bootstrap methods do introduce bias, the variance is considerately owered as updates are no longer dependent on the entire trajectories but only on a subset of the trajectory 2018]. Additionally, the value function can also improve the quality of the policy gradient by stabilizing the return estimate with a new baseline|Schulman et al.|[2018}. Much of the improvements to the actor-critic methods follow the advancements from value-based and policy-based approaches, and also consist of: ¢ The use of generalized baseline functions [2018] and action-dependent control variate{Gu et al.| [2017], [Liu et al.|(2018],[Tucker et al.|[2018] to stabilize the variance of policy gradient. ¢ Distributed training paradigms for actor-critic methods through asynchronous learning between multiple parallel actors|Mnih et al.| 2016], |Espeholt et al.|[2018]',\n",
       "  \".| [2017], [Liu et al.|(2018],[Tucker et al.|[2018] to stabilize the variance of policy gradient. ¢ Distributed training paradigms for actor-critic methods through asynchronous learning between multiple parallel actors|Mnih et al.| 2016], |Espeholt et al.|[2018]. ¢ Stabilized learning of solution with shared parameters between actor and critic by using phasic learning and representation regularization{Cobbe et al.|[2020}, [Huh] [2021]. ¢ Extending efforts from policy-based and value-based methods to an actor-critic framework, such as experience replay 2016aj and soft Q-learning [2018 . 'We highlight that the return G(r) here removed the notation of which agent it is referring to, as we are only considering one agent in this scenario. 14 Challenges | Pathologies Computational complexity Stochasticity, Deception Non-stationarity Moving-target problem Coordination Miscoordination, Relative Overgeneralization, Alter-Exploration Problem Performance Evaluation n/a Model-based Approaches With a model-based approach, an explicit estimate of the transition dynamics T and/or reward probabilities is maintained. These estimates can be used for planning without an explicit policy or for a more sample-efficient policy improvement|Sutton| [1990]. Methods that do not maintain such estimates are referred to as a model-free approach\",\n",
       "  '. These estimates can be used for planning without an explicit policy or for a more sample-efficient policy improvement|Sutton| [1990]. Methods that do not maintain such estimates are referred to as a model-free approach. While model-based approaches have shown success in highly complex tasks, these methods can be more challenging as accurately and comprehensively representing the transition dynamics and reward function, especially over long time horizons, is non-trivial and difficult. There have been notable improvements in model-based approaches, which include: Integrate model learning and policy optimization with deep learning methods {Ha and] Mitigate distributional mismatch using online data collection [2011], uncertainty network , bootstrap ensembling|Chua et al.] [2018], [2018}, (2021) 2018 [2017]. Model-based learning on imperfect information 2015],/Zhang et al.|[2019}. Successor Representation Successor representation (SR) is an alternative approach to the model- based and model-free approach, where it disentangles the state transitions from reward estimation by maintaining a state occupancy function M(-) for a given policy x [Dayan] (1993). T M,(8,8!) = Ezxp,(z|so=s 1>_, 1 1(se = 8’)] (19) t=0 where I is the characteristic function. The state occupancy function captures the notion of envi- ronmental affordance, caching statistics relating to which future states are possible from a given state',\n",
       "  \". T M,(8,8!) = Ezxp,(z|so=s 1>_, 1 1(se = 8’)] (19) t=0 where I is the characteristic function. The state occupancy function captures the notion of envi- ronmental affordance, caching statistics relating to which future states are possible from a given state. Similar to the Bellman equations, we can derive a recursive definition for the state occupancy function. Mr (8,8!) = U(se = 8') + VE. ~pe (rlse=s) lM (se41, 8°)] This factorization provides a compact and structured representation of the model for efficient and adaptive learning in complex environments|Kulkarni et al|[2016). Generally, SR methods alleviate the cost of learning a complex environment model while still being adaptive to distal reward changes, given the disentanglement of the reward function from the state transition. Recently, SR has reemerged with a new utility in RL that can help capture different aspects of the RL problem, despite the initial idea proposed several decades ago. Notably, a grounded formulation to extend SR with deep learning methods [Blier et al.| , utilization of passive data to learn latent and useful features with SR recent applications that have captivated some new interest in this technique. Foundation Model for Control Foundation models are ML models that are pre-trained on large- scale data to be adapted for diverse downstream tasks and have largely been successful and practiced in recent ML research[Bommasani et al.|[2022]\",\n",
       "  '. Foundation Model for Control Foundation models are ML models that are pre-trained on large- scale data to be adapted for diverse downstream tasks and have largely been successful and practiced in recent ML research[Bommasani et al.|[2022]. This sentiment for foundation models is echoed in RL applications, where leveraging pre-trained models can also offer advantages, especially considering RL’s training inefficiencies. However, questions regarding its effective implementation remain at large. Th nt efforts of developing foundational models for RL has been in general navigation (2023alb}, [2022}, robotic manipulation |Walke et al. and other broad robotic 15 applications . These works have largely been motivated by practices in large-language model training methodologies and offline RL {Chebotar et al.] [2023] and creating multi-modal prompting to integrate successes from the research of computer vision and natural language. The key insight with foundation models is recognizing that across tasks, there are shared traits and skills that are required in many of these tasks that do not necessarily require re-learning for each task. Instead, learning these aspects in a unified manner rather than individually and myopically within a single task can lead to not only an amortized cost of learning but also more robust and superior behaviors',\n",
       "  '. Instead, learning these aspects in a unified manner rather than individually and myopically within a single task can lead to not only an amortized cost of learning but also more robust and superior behaviors. 4 Learning in a Multi-agent Environment The transition from single-agent RL to learning within a multi-agent stochastic game setting promises numerous opportunities but consequently is tied to challenging difficulties that require paradigm- altering considerations|R ]. In contrast to single-agent control systems, where one agent interacts with its environment, a multi-agent setting involves managing the decision-making and learning processes of multiple entities that can interact with each other and their shared environment. Facilitating stable, adaptive, and social behaviors within a multi-agent control learning process is non-trivial, and comes with several intricacies that converge to necessitate desired solutions. In this section, we cover the positive and negative aspects that are unique to MARL, largely highlighting its potential and consequences in the hopes of providing meaningful context and a deeper look into these concepts. 4.1 Benefits of MARL MARL directly addresses the optimization problem of developing multiple decision-making agents within a shared environment. As a result, these algorithms are specifically designed to consider the complexities inherent to multi-agent dynamics during training',\n",
       "  '. 4.1 Benefits of MARL MARL directly addresses the optimization problem of developing multiple decision-making agents within a shared environment. As a result, these algorithms are specifically designed to consider the complexities inherent to multi-agent dynamics during training. Therefore, the optimization can produce valuable behaviors that span from adaptive social teaming Ndousse et al Por) to the ainst emergence of coalitions that enable intelligent interactions and improve robustness ag: lynamic changes within the population |Busoniu et al.| This perspective of approaching control optimization that MARL studies also enables the realiz; of applications that could not be accurately modeled in single-agent RL alone [Matarié] particularly in scenarios that involve multiple adaptive agents that can influence the environment or other agents|Lanctot et al.|[2017]. The context of training under MAS features additionally introduces the chance to capitalize on the unique prospects and structures of the multi-agent nature, which can enhance the efficiency and depth of training through various means discussed in the later parts of this survey. 4.2 Challenges of MARL The task of MARL is not only faced with the already challenging objective of optimizing control but is further riddled with the inclusion of multiple interacting agents',\n",
       "  '. 4.2 Challenges of MARL The task of MARL is not only faced with the already challenging objective of optimizing control but is further riddled with the inclusion of multiple interacting agents. Throughout this paper, we investigate four central challenges — computational complexity, non-stationarity, coordination, and performance evaluation — that are interdependent and coupled to various learning pathologies. Definition 7 (Learning Pathology) Learning pathology refers to undesirable or sub-optimal behav- iors that can emerge within MARL learning dynamics. As listed in {Palmer| {2020), these include stochasticity, deception, the moving-target problem, miscoordination, relative overgeneralization, and the alter-exploration problem. Much of the problematic intricacies of MARL stem from a mixture of these challenges, as they tend to be deeply interconnected. 4.2.1_Computational Complexity As mentioned in Section optimizing for an equilibrium solution is theoretically demonstrated as challenging, where the cost of modeling multiple agents can easily scale intractably. This is seen as the joint state-action space scales exponentially with respect to the number of agents 16 , the complexity of the optimization problem and the computational cost required to render the optimization impractical to learn any useful joint behaviors. Other key factors, such as the pathologies of stochasticity and deception, play a major role in contributing to this complexity',\n",
       "  '. Other key factors, such as the pathologies of stochasticity and deception, play a major role in contributing to this complexity. Stochasticity In many real-world MAS applications, stochastic rewards or transitions pervade, arising from factors such as noise or unobservable elements in the state space. When a game is inherently stochastic, the challenge of identifying the sources responsible for the stochasticity can lead to learning instabilities. This, in turn, requires agents to accumulate more experience to discern and adapt to these sources of uncertainty Deception The pathology of deception involves a game that contains certain states that have a high local reward but low return. In other words, there exist settings where a trade-off should be made from immediate reward for long-term success. For many MARL algorithms, especially those derived from maximum-based learners|Kapetanakis and Kudenko] 2003], |Matignon et al. (2012 ; over-estimation is a key challenge that leads to early convergence to sub-optimal equilibrium. In general, to address the challenge of computational complexity in MARL applications, it becomes even more crucial to design our optimization model carefully such that the modeled components are not susceptible to fail to converge or to find good approximate equilibrium states during learning',\n",
       "  '. In general, to address the challenge of computational complexity in MARL applications, it becomes even more crucial to design our optimization model carefully such that the modeled components are not susceptible to fail to converge or to find good approximate equilibrium states during learning. With these considerations, we need to balance the trade-offs between solution complexity, computational efficiency, and learning performance to effectively devise a tractable MARL solution. 4.2.2 Non-stationarity Informally, a stationary process can be classified by its underlying distribution’s invariance under time shifts. In a multi-agent setting, the decisions of each agent impact the transition dynamics of the environment. Consequently, from the perspective of each agent, the other agents’ behaviors are inherent components of the dynamics. Therefore, whenever these external agents adapt and alter their behaviors, the underlying model distribution in the perspective of the agent also changes, rendering it non-stationary [Hernandez-Leal et al.|[2017]. Importantly, this non-stationarity arises not from a stochastic process that can be easily approximated, such as white-noise Gaussian, but rather from the structured learning process of the external agents 2 This non-stationarity is a critical deviation from the fundamental assumption made by conventional single-agent RL algorithms|Choi et al.]',\n",
       "  '.] . The absence of the stationary property destabilizes the optimization process and contributes to the pathology of the moving-target problem. This problem arises from the fact that what an agent has learned and needs to learn is dependent on other agents’ evolving behaviors/Tuyls and Weiss| Hence, the learning landscape the agents are optimizing over is in flux at each update step. This setting can make learning infeasible for agents to properly converge to a stable behavior|Papoudakis et al.|[2019]. Therefore, it is essential to consider extensions of methods that can effectively account for this non-stationarity to develop stable algorithms for MARL. 4.2.3 Coordination One of the unique aspects of developing a multi-agent solution is the ability of the agents to work together to achieve their goals. As each agent makes decisions based on its local observations in a shared environment, they can heavily benefit from coordinating their actions to achieve a joint strategy that maximizes the collective return and avoids unintended interference to mitigate diminishing returns However, achieving successful coordination is difficult, notably when agents have limited information about the environment and the behaviors of other agents. Addressing this challenge of coordination has been a significant research question in MARL that studies how agents can engage with one another in various manners depending on their settings while also effectively succeeding in their local tasks',\n",
       "  '. Addressing this challenge of coordination has been a significant research question in MARL that studies how agents can engage with one another in various manners depending on their settings while also effectively succeeding in their local tasks. Specifically, these efforts towards successful coordination are directly associated with the learning pathologies of miscoordination and relative overgeneralization. Miscoordination The pathology of miscoordination is also known as the Pareto-selection problem and can be observed when two or more incompatible Pareto-optimal equilibria are present. As a consequence, the agents can potentially choose an action from different equilibria due to improper 17 ce | 0 0 0.5 coordination and therefore harming their performance. For example, given a bi-matrix game defined in Table|2} there exists two equilibria, (a, a), (b, b), where either both agents choose action a or b. However, these equilibria are incompatible, since the strategy profiles (a,b) and (b, a) come from either equilibria, however, if they are intermixed, neither resulting strategies are optimal. Definition 8 (Incompatiable Equilibria) Two equilibria x and m are incompatible if and only if, ym; #7, Jimi, tal) < Jil) where [mi, mJ signifies a strategy profile using agent i action from 7 and the other agent’s action from Tr',\n",
       "  '. Definition 8 (Incompatiable Equilibria) Two equilibria x and m are incompatible if and only if, ym; #7, Jimi, tal) < Jil) where [mi, mJ signifies a strategy profile using agent i action from 7 and the other agent’s action from Tr. Relative Overgeneralization The pathology of relative overgeneralization occurs in games where, as a result of a shadowed equilibrium, the agents converge upon a sub-optimal Nash Equilibrium that is Pareto-dominated by at least one other Nash Equilibrium. Definition 9 (Shadowed Equilibrium) An equilibrium 7 is shadowed by another one © if there exists an agent that receives a low return by unilaterally deviating from this equilibrium and if this return is lower than the minimal return when deviating from the other equilibrium|Palmer| [2020]. , Fide J (mi, -i]) < min I ((aj,7_5]) For instance, in the bi-matrix game shown in Table[2} while (a, a), (b, 6) are both Pareto-optimal equilibria, a miscoordination penalty of —2 is associated with both of them. No such penalty exists with action c. Hence, both equilibria are shadowed by (c, c), as the expected gain if one agent deviates unilaterally from either equilibrium is inferior to the lowest expected gain if one agent deviates unilaterally from (c, c). Hence, agents can be drawn to sub-optimal but wide peaks in the return space due to a greater likelihood of achieving beneficial collaboration. This derives as a form of action shadowing',\n",
       "  '. Hence, agents can be drawn to sub-optimal but wide peaks in the return space due to a greater likelihood of achieving beneficial collaboration. This derives as a form of action shadowing. reward(i,j) 18 Definition 10 (Action Shadowing) Action shadowing is a phenomenon wherein one individual action seems more favorable than another, despite the potential greater return of the second action [Fulda and Ventura\\\\ [2007]. Concretely, a sub-optimal policy may result in a higher average payoff when paired with arbitrary actions chosen by other agents, leading to utility values of optimal actions being underestimated. The pathology of relative overgeneralization is visualized in Figure/4jover a continuous action space. The x and y axes represent the actions of agents i and 7 respectively. The z axis represents the reward for each joint action. The reward space is structured where action 7,, can lead to the optimal reward, however, due to miscoordination being less severely punished for actions approaching iy, the agents are drawn towards the sub-optimal Nash equilibrium. Exploration The challenge of exploration in MARL describes the issue of how to effectively explore unknown environments to collect valuable experiences that benefit the agents’ learning the most|Sutton and Barto] 2018]',\n",
       "  '. Exploration The challenge of exploration in MARL describes the issue of how to effectively explore unknown environments to collect valuable experiences that benefit the agents’ learning the most|Sutton and Barto] 2018]. To address this challenge, a balance between exploration and exploitation must be struck, where agents must decide whether it is more valuable to take the actions that they know would lead to good returns or take the actions they have not tried yet that may lead to even greater returns or at the very least reduce the uncertainty regarding those actions. An improper balance on either side can result in an incomplete coverage over the state-action space, leading to sub-optimal convergence. This challenge is especially exacerbated in intricate environments with and delayed reward signals, noisy transitions, long horizons, and non-stationary dynamic . In addition, in light of the inherent delicacy involved in optimizing and coordinating multi-agent systems under the influence of shadowed equilibria and miscoordination penalties, such exploration can increase the likelihood of deceptive transitions and introduce instabilities within the learning dynamics (2012). For instance, other agents may adapt to these exploratory actions too abruptly even though they may lead to a shadowing equilibrium. We call this pathology the alter-exploration problem',\n",
       "  '. For instance, other agents may adapt to these exploratory actions too abruptly even though they may lead to a shadowing equilibrium. We call this pathology the alter-exploration problem. Another key issue noted by prior efforts is the lazy agent problem when certain agents learn a good policy but some agents have less incentive to continue to explore and learn themselves, as their actions may negatively affect the already high-performing agents. For example, as discussed in|Sunehag et al.| (2017), consider the scenario of training a soccer team with the number of goals as the team’s reward signal. If certain players are more proficient scorers than others, it becomes evident that when the less skilled player takes a shot, the outcome is less favorable on average. Consequently, the weaker player learns to avoid taking shots{Hausknecht| (2016). Traditionally, simple exploration methods, such as ¢-greedy [2018] or noise perturbation , can be employed for random action selection, however, such naive methods can lead to unintentional and indiscriminate exploration which can be inefficient in complex learning tasks with exploration challenges',\n",
       "  '. While exploration remains an open challenge with much room for improvement, there exists more studied and developed exploration methods, as follows: Uncertainty-oriented Exploration: With a lack of knowledge regarding certain actions, agents can incorporate this uncertainty into the decision-making process when tackling this balance between exploration and exploitation. A common heuristic to employ is the principle of ’Optimism in the Face of Uncertainty\", where agents are incentivized to explore state-action pairs with high epistemic uncertainty. Epistemic uncertainty represents the errors that arise from insufficient and inaccurate knowledge about the environment whereas aleatoric uncertainty represents the inherent randomness of the environment. Typically, this approach requires some modeling of these uncertainties. A common approach is to parameterize the solution as a distribution |Zhu et al.] to be able to properly express the stochasticity of the environment and leveraging a classic exploration technique utilizing these estimates. ¢ Intrinsic Motivation-oriented Exploration: An alternative approach is to incorporate a meta-task of exploration by introducing and designing intrinsic rewards for agents',\n",
       "  '. ¢ Intrinsic Motivation-oriented Exploration: An alternative approach is to incorporate a meta-task of exploration by introducing and designing intrinsic rewards for agents. These rewards can be to minimize prediction errors regards the environment 2021] motivated by novelty of states [2021], [Li I or driven by information gain For instance, in individual intrinsic reward is learned and used to update an agent’s policy to maximize the team reward. 19 ¢ Multi-agent Exploration: In a multi-agent setting, we face the challenge of not only complexity but also miscoordination (i.e. alter-exploration problem). This issue requires some level of coordinated exploration [2023], as exploratory actions of one agent can affect the learning of others 2018], and in certain multi-agent tasks, efficient exploration requires a degree of global planning as opposed to pure local exploration|Brafman and Tennenholtz’ (2003). 4.2.4 Performance Evaluation Assessing the performance of a MARL algorithm is a multifaceted challenge. Firstly, we highlight that the success of one agent’s policy is intricately linked to the policies of other agents, which renders individual assessments unclear to properly interpret. This is compounded by the problem of establishing unbiased and useful metrics that quantify inherently qualitative social behaviors, such as measuring the quality of communication and coordination|Havrylov and Titov 2017}. Jaques et al. 2019],|/Bogin et al.|[2019], [Lowe et al',\n",
       "  '. This is compounded by the problem of establishing unbiased and useful metrics that quantify inherently qualitative social behaviors, such as measuring the quality of communication and coordination|Havrylov and Titov 2017}. Jaques et al. 2019],|/Bogin et al.|[2019], [Lowe et al.|[2019] and determining the appropriate evaluations dependent on the roles assigned to all agents (i.e. oligopoly with leader-follower structure). Furthermore, we can focus on more egalitarian criteria, ie. concentrating on the agents that are most struggling Zhang and Shah]|(2014) as opposed to an more utilitarian approach such as social welfare. Hence, it also becomes important to understand multi-agent credit assignment and discern the individual impact of each agent in terms of the coalition’s utility, especially where there exists only a global reward structure, e.g. within a Dec-MDP. A classic example is the Shapley value, which quantifies and captures the notion of marginal contribution by averaging all possible combinations of the marginalized population’s achieved utility[Shapley|[1952]. More recently, a popular approach in deep MARL is to utilize an advantage function, which compares the current Q-values to a counterfactual (2017aj, {Li et al.] [2022], or to utilize value function decomposition [2017] that marginalizes the contribution of each agent',\n",
       "  '. More recently, a popular approach in deep MARL is to utilize an advantage function, which compares the current Q-values to a counterfactual (2017aj, {Li et al.] [2022], or to utilize value function decomposition [2017] that marginalizes the contribution of each agent. From a theoretical perspective, determining which solution concepts would lead to optimal behaviors and strategies, as studied in game theory literature, remains unclear and often task-specific{Bergerson| 2021]. Even so, as many studied solution concepts prove prohibitively costly to explicitly measure and optimize with complex and dynamic tasks, the research for definable metrics that capture the nuances of diverse MAS settings persists as an ongoing and open challenge. 5 Prospects of MARL In this section, we cover the unique properties of learning controls in a multi-agent environment that can help promote the benefits or address the underlying issues of the MARL approach|4] 5.1 Simulating MARL Tasks A lingering concern for MARL algorithms is its learning complexity Daskalakis etal . Often, in order to acquire valuable behaviors, it requires significant computational resources. This constraint arises from various factors, namely sample inefficiency and a brittle optimization landscape|Gu et al. (2017), van Hasselt et al. 2018}, Tucker et al. (2018), Henderson et al. (2019)',\n",
       "  '. Often, in order to acquire valuable behaviors, it requires significant computational resources. This constraint arises from various factors, namely sample inefficiency and a brittle optimization landscape|Gu et al. (2017), van Hasselt et al. 2018}, Tucker et al. (2018), Henderson et al. (2019). Sample efficiency is broken down into two factors: the number of environment interactions required by each agent to learn and generalize and the cost associated with each interaction [Huh and Mohapatra . Hence, it is important to emphasize the pivotal role that simulators play in this equation, as they can enhance the learning process by yielding higher-quality samples through greater accessibility, stability, accuracy, and precision of the retrieved data. Parallelized and Vectorized Processing Over the past several decades, the processing capabilities of massively parallelizable processors, such as graphic processing units (GPUs) and tensor processing units (TPUs), have advanced significantly. This progress has opened up exciting possibilities for leveraging this technology to simulate highly complex tasks effectively, minimizing the cost of simulating a large number of trajectories. These processors are specifically designed to handle parallel computations efficiently by executing tasks simultaneously on a large scale, enabling simulations to run orders of magnitude faster compared to conventional CPU-based implementations',\n",
       "  '. These processors are specifically designed to handle parallel computations efficiently by executing tasks simultaneously on a large scale, enabling simulations to run orders of magnitude faster compared to conventional CPU-based implementations. There have been notable efforts to develop task-specific [Dalton et al.| | and general physics-based 20 Level 3 » Legend «— Interaction Level 2 ee) @ Holon <—) Context Level 0 simulators, such as IsaacSims|Makoviychuk et al. 2021], Brax|Freeman et al.|[2021], and MuJoCo XLA that leverage GPUs. Many of these simulators primarily concentrate on addressing single-agent problems, with some addressing their extension to MAS (2022 2023 jand Mohapatra] Mohapatra (20749) ,|Rutherford et al.| (023) |. In turn, there exists promising potential to further optimize and extend the usage of vectorized processing to MAS scenarios|Lan et al.|[2021]], [Lechner] et al(2023) Multilevel Simulation Many complex multi-agent tasks can be factorized into a hierarchical structure using a multilevel simulation paradigm, as an effort to manage its complexity. Multilevel simulation introduces several organizing levels that encapsulate various individual components into monolithic abstractions [Ghosh] . These levels are defined in distinctive manners, integrating microscopic to macroscopic attributes 2017]. Such modeling is defined in a holonic paradigm|Tchappi et al',\n",
       "  \". These levels are defined in distinctive manners, integrating microscopic to macroscopic attributes 2017]. Such modeling is defined in a holonic paradigm|Tchappi et al.|[2018] (see Figure[5), where holons are used as these abstractions and holons le self-s' are defined as stab imilar structures that behave as both an entity and an organization. Holons satisfy three important conditions: holons are stable, autonomous, and cooperative with one another. However, a holonic organization of a task is often difficult to properly define. Open Source Environments There exists a wide range of libraries that are used as common benchmarks for MARL research. Table[3|provides descriptions of some of the more widely recognized environments. 5. In this part, we explore various training paradigms used in and unique to MARL applications. Specifically, we look into the centralized training paradigms and the use of off-policy learning. 5.2.1 Centralized Training There exists a spectrum of agent representations in training and execution to combat the scalability and complexity issues of learning joint strategies. This spectrum includes three main categories: centralized training and centralized execution (CTCE), decentralized training and decentralized execution (DTDE), and centralized training and decentralized execution (CTDE)|Lowe et al. [2020]\",\n",
       "  '. This spectrum includes three main categories: centralized training and centralized execution (CTCE), decentralized training and decentralized execution (DTDE), and centralized training and decentralized execution (CTDE)|Lowe et al. [2020]. The idea of centralization couples components of agents’ behaviors to provide a more complete state of information to work with or to decrease the complexity of the task by distributing the workload over multiple agents. Importantly, leveraging some level of centralization poses a good solution for handling non-stationarity, as each agent now will have access to global information to account for the changes in other agents’ behaviors. CTCE The CTCE paradigm entails a fully centralized approach that involves mapping a collection of local observations from each agent to distributions over individual action spaces. In this case, 21 Name Description Multi-agent Particle Environment | Various social tasks focused on communication (MPE) (2020) within a particle world setting. StarCraft Multi-Agent Chal- Cooperative StarCraft decentralized micromanage- lenge (SMAC)Samvelyan ment scenarios. et al.|[2019],/Ellis et al. Phan etal |[2023] [Terry et al./[2021| A collection of different MARL tasks and libraries. m] Ll = Multi-agent tasks in a grid-world setting. [Koul](2019] . A collection of many-agent tasks',\n",
       "  '. et al.|[2019],/Ellis et al. Phan etal |[2023] [Terry et al./[2021| A collection of different MARL tasks and libraries. m] Ll = Multi-agent tasks in a grid-world setting. [Koul](2019] . A collection of many-agent tasks. Level-based Foraging ( )and | Customizable grid-world foraging task and simula- Robot Warehouse (RWARE) tion warehouse with robots moving and delivering Christianos et al.}[2020 product in gridworld oogle Research Foot- Simulated soccer game using physics-based 3D simu- ball[Rurach et al[2020)__| lator. Overcooked Human-Al coordination on multiplayer video-game task. Vectorized Multi-Agent Simulator | Various MARL tasks using a vectorized Pytorch- (VMAS) 2022) based 2D physics engine IsaacTeams[Huh and _| | Various physics-based MARL tasks using GPU- 2024al accelerated IsaacSim platform. axMARL Rutherford et al.|[2023] | Various physics-based MAS tasks using GPU- accelerated Brax platform. we essentially reduced the MAS control problem into a single-agent RL optimization over the concatenated observations and combinatorial joint action space. While CTCE provides expressive and complete representations of a MAS|Gupta et al_| | and performs well against non-CTCE methods[Yu ef al [2022a], the assumptions of decentralization are largely compromised. This is because, during execution, decentralized agents must only make decisions based on their local observations and do not have access to the global information it was trained on',\n",
       "  '. This is because, during execution, decentralized agents must only make decisions based on their local observations and do not have access to the global information it was trained on. Therefore, nontrivial and unnatural adjustments must be made to convert the joint policy from a centralized executor to a decentralized executors, such as masking the other agent’s information to prevent information leakage. CTCE approaches also fail to address the curse of dimensionality problem|Gronauer and Diepold 2022], otherwise expressed as the exponential scaling caused by the joint state-action space of MAS. DTDE On the other side of the spectrum, DTDE proposes a fully decentralized approach that adheres to all decentralization constraints in all aspects of training and execution. We note that this does not necessarily mean the agents cannot perceive nor is aware of other agent’s existence, where this is known as independent learners (IL) vs. joint action learner (JAL) as defined in , although IL can be considered an extreme form of DTDE. Prior efforts [1998}, [Tan] (1997), [Lauer and Riedmiller(2000),[Tampuu eta, (2015), Jaderberg [2019] have demonstrated that IL with standard RL algorithms does demonstrate the ability to converge to an equilibrium in particular and fine-tuned settings. A key challenge in the DTDE training scheme, as notably emphasized, is non-stationarity',\n",
       "  '. A key challenge in the DTDE training scheme, as notably emphasized, is non-stationarity. This challenge is exacerbated by the absence of centralization, leading to a potential loss of mutual information among agents’ behaviors, which, in turn, can give rise to various learning pathologies|Palmer| [2020]. To mitigate these pathologies, a widely adopted strategy involves inducing optimism|Matignon et al.||2007),|Palmer et al. (2018 by) , through hysteric learning or leniency. This approach restricts the reduction 0: value estimations, thereby alleviating the impact of other agents’ exploration strategies and promoting exploration beyond equilibria that can easily trap agents without the added optimism. ¢ Hysteric learning: While DTDE has demonstrated success in deterministic settings, inde- pendent learning has struggled to replicate such achievements in stochastic settings. A significant stumbling block has been the tendency to overestimate the value function, a consequence of the inherent stochasticity, resulting in sub-optimal solutions, as stated in (2007). To address this issue, hysteric Q-learning [2007] was 22 introduced to provide an optimistic update function that assigns greater weight to positive experiences, particularly beneficial in cooperative multi-agent scenarios. This is achieved through the use of two learning rates, denoted as a and ¢ Leniency: Alternatively, another method to adjust the degree of optimism during the learning process is leniency [2018]',\n",
       "  '. This is achieved through the use of two learning rates, denoted as a and ¢ Leniency: Alternatively, another method to adjust the degree of optimism during the learning process is leniency [2018]. Leniency effectively allows for the forgiveness or disregard of suboptimal actions taken by teammates that result in low rewards during initial exploration, taking in the form of lenient Q-value updates and lenient-based exploration. Over time, this optimism exhibited by lenient agents is gradually reduced as they encounter and revisit state-action pairs. Consequently, agents become less lenient in situations frequently encountered, while retaining their optimistic outlook in unexplored territories. This shift towards average-based reward learning from maximum-based, helps lenient agents steer clear of suboptimal joint policies, especially in environments where rewards are subject to stochastic fluctuations. Empirically, leniency shows higher learning stability compared to hysteretic learning, primarily due to temperature-enabled leniency at different stages of estimation maturity. The leniency decay allows for a more faithful representation of domain dynamics during later stages of training, where it is probable that teammate policies become stable and near-optimal, assuming the rate of decay is appropriate and value maturity is synchronized across all states',\n",
       "  '. The leniency decay allows for a more faithful representation of domain dynamics during later stages of training, where it is probable that teammate policies become stable and near-optimal, assuming the rate of decay is appropriate and value maturity is synchronized across all states. Like CTCE, DTDE also demonstrates a significant issue of scalability, as a distributed solution requires each agent to not only be represented individually but also require their own set of samples for learning. As agents are not granted any access to the global state, which can be pivotal not only for sample efficiency but also for good performance|Gupta et al.| CTDE CTDE provides a middle-ground by centralizing certain variables during training that still enable decentralized execution of agents |Kraemer and Banerjee (2016). This approach strikes a balance between the advantages of centralization while maintaining the constraints set by natural and artificial decentralization during execution. CTDE is commonly practiced by using a centralized value function. Centralization of the value function allows all agents access to comprehensive state information during training, without violating decentralization constraints during execution since the value function is not required for decision-making. This facilitates enhanced learning, efficient updates, and coordination between actor and critic, promoting improved policy convergence |Foerster et al.|2017al], [Lowe fet al.| 2020]',\n",
       "  '. This facilitates enhanced learning, efficient updates, and coordination between actor and critic, promoting improved policy convergence |Foerster et al.|2017al], [Lowe fet al.| 2020]. Additionally, single-agent RL algorithms can naturally be extended, similar to DTDE, however, using a shared critic model (i.e. MAPG[Samvelyan et al. , MADDPG Lowe et al.| and MAPPO/Y ]). Like CTCE, a centralized value function remains prone to scalability issues. As the number of agents grows, its representation needs to handle a larger or more intricate state space. This can lead to significant computational costs and difficulties in defining the concise state space. Another critical challenge is the centralized-decentralized mismatch. Since the value function is shared among agents, sub-optimal policies from one agent can have a detrimental impact on the policy learning of other agents, causing catastrophic miscoordination| . Largely, this increased variance in learning a shared critic remains a long-standing challenge. Parameter Sharing A commonly used approach to implement centralization is through parameter sharing, where different agents share representation modes{Gupta et al.][2017a]. This allows each agent to update the same parameters, potentially leading to a more efficient and richer learning process. Ina sense, all agents can aggregate their experience and learn much faster in a more memory-efficient manner|Fu et al.|(2022]',\n",
       "  '.][2017a]. This allows each agent to update the same parameters, potentially leading to a more efficient and richer learning process. Ina sense, all agents can aggregate their experience and learn much faster in a more memory-efficient manner|Fu et al.|(2022]. However, it is important to note that this can more likely lead to homogeneity in behaviors and introduce instability, especially when dealing with highly diverse agents, as it transforms the problem into a difficult multi-task optimization problem. Parameter sharing among such agents, especially if they are heterogeneous, becomes a nontrivial task. Subtask Sharing Another approach to centralization involves the use of global subtasks. In many MAS, tasks can be decomposed into subtasks universally defined amongst all agents. As a result, each agent’s task can be decomposed, where these global subtasks can be assigned accordingly to 23 each agent. To achieve this, the process of task decomposition needs to consider how the subtasks can ned properly. While subtasks can be defined using domain knowledge US Yngetal generalizable decomposition methods, such as RODE and L 22], have been introduced. The core idea behind both approaches is to learn embeddings over the actions or trajectories and perform clustering to define subtasks. A significant challenge in these approaches is ensuring the subtasks’ definitions are distinct|Yang et al.] [2022]. 5.2',\n",
       "  '. The core idea behind both approaches is to learn embeddings over the actions or trajectories and perform clustering to define subtasks. A significant challenge in these approaches is ensuring the subtasks’ definitions are distinct|Yang et al.] [2022]. 5.2.2 Off-policy Learning To improve the sample efficiency of MARL training, agents can learn from experiences that come from different policies in some manner|Sutton and Barto|[2018}, Silver et al.|(2014], and this is known as off-policy learning. Typically, off-policy approaches rely on storing samples in an experience replay buffer. However, proper implementation of experience replay in a multi-agent setting is non-trivial due to the non-stationary dynamics of the environment that can render past experiences obsolete, as other agents’ behaviors change, learning with their prior behaviors may be out of distribution. Previous efforts [Foerster et al.|2017b] account for these discrepancies through two methods. The first approach utilizes importance sampling to correct the policy updates, however, there remain questions about the tractability of computing the importance weightings and its large and unbounded variance. Extensions to alleviate these issues, including truncation, do reduce variance, however, introduce additional bias',\n",
       "  '. Extensions to alleviate these issues, including truncation, do reduce variance, however, introduce additional bias. Fingerprinting, on the other hand, appends contextual information regarding the current stage of learning of the agents in the environment to the samples that are stored in the experience replay buffer, to disambiguate the age of the sample. Both approaches prove to stabilize the experience replay sufficiently. Aside from the depreciation of samples, other issues have been addressed, such as ensuring concur- rency of experience sampling/Omidshafiei et al.|{2017] and detecting to manage miscoordination and relative over-generalization with off- por) learning by using variable learning rate to accommodate for exploratory actions [Palmer et al.| [Lyu and Amato] [Lyu and Amato] {2020} . Traditional experience replay mechanisms, such as priority eT ea Somer et al.] [2016], je) been experimented with MARL. However, a naive application may deteriorate a aka and performance due to the noisy reward and the continuous behavior changes of coexisting agents, causing a priority bias. Hence, a lenient reward function is modeled|Zheng et al. to correct the priority bias. 5.2',\n",
       "  '. However, a naive application may deteriorate a aka and performance due to the noisy reward and the continuous behavior changes of coexisting agents, causing a priority bias. Hence, a lenient reward function is modeled|Zheng et al. to correct the priority bias. 5.2.3 Offline Learning The practice of offline learning with MARL adopts much of the same ideas from single-agent offline RL, existing in the form of behavior regularization and conservatism|Pan et al] , but with further considerations required to mitigate underlying issues that come along with the mixture of offline RL and MAS, such as the propagation of erecapovation error Yang et al. 202 1b} and agent-wise imbalances within the offline data|Tian et al.}[2023 5.3 Agent Awareness Agents can exhibit different degrees of awareness of other agents, which can be classified into three categories: independent, tracking, and agent-aware Busoniu et al . The selection of an awareness level involves unique considerations, advantages, and challenges, contingent upon the specific nature of the interaction and the task. At each level, a trade-off between stability and adaptability is made. Definition 11 (Stability and Adaptability) Stability focuses on achieving convergence to a station- ary policy m, while adaptability aims to maintain or improve performance in the face of changes in other agents’ behaviors',\n",
       "  \". At each level, a trade-off between stability and adaptability is made. Definition 11 (Stability and Adaptability) Stability focuses on achieving convergence to a station- ary policy m, while adaptability aims to maintain or improve performance in the face of changes in other agents’ behaviors. Definition 12 (Stationary Policy) A policy x’ is stationary, for any state s' and time-steps t and t’, wheret 4 t’, a : Ini(alsi) — mi,(alsi,)| <a € A While stability and adaptability are not necessarily dichotomous objectives, the balance between the two helps illustrate the extent to which coordination is emphasized. Effectively addressing this coordination problem requires agents to skillfully navigate these intricacies and strike the right balance between focusing on stability and adaptability. 24 ¢ Independent agents disregard the notion of coordination entirely. Moreover, their focus lies solely on converging to stable behaviors rather than adapting to the actions of other agents in their environment. In cooperative, adversarial, and mixed settings, such methods are referred to as coordination-free, opponent-independent, and agent-independent respectively, and each has demonstrated empirical success under restricted problem settings{Littman|[2001al, Lauer and Riedmiller 2000], Hernandez-Leal et al. 2017)\",\n",
       "  '. 2017). However, these independent methods often result in sub-optimal outcomes or even failure to achieve desired goals as coordination becomes crucial in many scenarios to anticipate and respond strategically to the actions of other agents. ¢ Tracking agents prioritize adaptability over stability, placing a greater priority on coordi- nation rather than learning a stable individual behavior. With a tracking approach, agents continuously adjust their strategies based on the observed behavior of other agents. Empiri- cally, agent-tracking methods rely on agent modeling to guide the agents’ action selection process|Robinson}[}1951],|Weinberg and Rosenschein| [2004]. However, the stability of the joint behavior may be compromised. The constant adaptation can lead to non-stationary behaviors, as the agents respond not only to changes in the environment but also to the changing strategies of other agents. ¢ Agent-aware agents strive to achieve a balance between stability and adaptability by being conscious of the other agents’ strategies while preserving their individuality. Previous studies have explored approaches such as \"Adapt When Everyone is Stationary, Otherwise Move to Equilibrium\" (AWESOME) |Conitzer and SandhoIm] [2003] or \"Win or Learn Fast\" (WoLF)|Bo\\' to determine when to adapt or maintain their local strategy, mostly just by adjusting the learning rate or incorporating the other agent’s anticipated learning to one another [Foerster et al-|[2018]',\n",
       "  '. These concepts primarily address handling the non-stationarity of the optimization problem, but this heightened awareness also establishes a well-rounded foundation for fostering social behaviors that lead to stable and successful coordination. Learning with Awareness When learning, agents can take into account the behaviors and informa- tion regarding other agents|Foerster et al.|[2017c]. One way this can be achieved is by extrapolating gradient updates for each agents, thereby performing a one-step look-ahead over the learning over all agents, which is known as extragradient{Korpelevich| [1976]. To reduce complexity of extrapolation step of gradient update, we can instead use only a sample subset of agents in many-agent settings [2020]. Similarly, LOLA [Foerster et al.| takes a similar approach, however, extrapolates only the other agents using a second order correction term with a Taylor expansion approximation. 5.4 Multi-agent Credit Assignment For many MARL applications, it may be intractable to define local rewards for each agent, hence necessitating the use of the Dec-MDP framework. In such settings, a global reward is instead provided, which represents the collective’s utility. However, with this measure, it is unclear the direct contributions and local performances of each agent. This problem is known as multi-agent credit assignment (MACA)|Agogino and Tumer| [2008]',\n",
       "  '. In such settings, a global reward is instead provided, which represents the collective’s utility. However, with this measure, it is unclear the direct contributions and local performances of each agent. This problem is known as multi-agent credit assignment (MACA)|Agogino and Tumer| [2008]. We distinguish MACA from the traditional credit assignment problem associated with the casual aspects of sequential decision-making, where the actions themselves are evaluated on their impact(Sutton and Barto|[2018]. In recent efforts, the 4.2.4) challenge of MACA, as briefly mentioned in Section{4.2.4] is addressed in two approaches: difference rewards and value factorization. Difference Rewards Difference rewards aim to capture an agent’s contribution from a global performance measure by shaping a local reward signal that isolates the utility of individual agent’s actions by removing the utility of ee a While in some applications, such as air traffic flow management 2007], this isolation is possible, generally, forming a theoretical setting that removes agents individually may be impossible. Hence, the marginalization of individual agents is often estimated by comparing them against the average actions, known as aristocrat utility[Wolpert and Tumer| [2001]. To extend to the realm of deep RL, COMA leverages the concept of the advantage function to achieve the same effects. 25 Value Factorization Value factorization decomposes a global value into local values for each agent 2001]',\n",
       "  '. To extend to the realm of deep RL, COMA leverages the concept of the advantage function to achieve the same effects. 25 Value Factorization Value factorization decomposes a global value into local values for each agent 2001]. A simple implementation of this is known as a value decomposition network (VDN) , where the sum of the learned local value functions can be treated as the global value. With the many extensions of VDN that have been proposed in the past decade, the standard constraints of Individual-Global-Max (IGM)|Rashid et al.| [2018] serve as the theoretical basis for guaranteeing and maintaining consistency between the global Q and local q; value estimates. argmax go(70, ao) ao argmax Q(s,a) = : (20) a argmax qv (TW, an) an where 7; is the observation-action history of agent i. However, these constraints have been shown to restrict the expressiveness of the value function representations|Mahajan et al. OJ, leading to sub-optimal value approximations and poor explorations. Hence, it remains a open research challenge to improve these limitations of value decomposition while trying to adhere to IGM. 5.5 Communication Communication is a powerful capability of high interest within MARL literature that enables agents to exchange and propagate information between one another, leading them to behave as a collective rather than a collection of independent individuals (2024)',\n",
       "  '. 5.5 Communication Communication is a powerful capability of high interest within MARL literature that enables agents to exchange and propagate information between one another, leading them to behave as a collective rather than a collection of independent individuals (2024). In many multi-agent tasks, com- munication proves vital to coordinate the behavior of multiple agents to achieve optimal performance Foerster et al. , especially under settings with imperfect information and partial observability 2015). However, an efficient and practical implementation of a communication mechanism presents several key challenges, necessitating consideration of not only what information to communicate |Sukhbaatar| fet al.|(2016], Foerster et al [2016], but also how [2022], when Singh et al} [2018], and with communicate. Peer DO, the topic of communication is broken down and categorized over 9 dimensions on its implementation, so we recommend readers refer to this resource for more in-depth analysis. 5.5.1 Communication Infrastructure A communication graph is introduced to define which agents each agent can communicate with, alongside the use of the networked stochastic game framework. The restriction placed on the existence of the graph’s edges is bounded by the constraints of decentralization and requires solutions that address issues including limited range 2023], limited bandwidth |F , noisy communication channels |Freed et al',\n",
       "  '. The restriction placed on the existence of the graph’s edges is bounded by the constraints of decentralization and requires solutions that address issues including limited range 2023], limited bandwidth |F , noisy communication channels |Freed et al.| 2020], and contentions with shared communication mediums|Kim et al.|[2019] such as a proxy. A consideration for each is understanding their practicality, which is dependent on the nature of the task, as well as their shortcomings. Proxy While decentralization prohibits centralized executors, a centralized communication medium is not prohibited. The role of a proxy is to serve as a coordinator and message aggregator. It gathers local observations or messages from agents in the environment, subsequently broadcasting messages to each of them|Kong et al.|[2017]. Alternatively, it can connect nearby agents who opt to participate in a communication group, facilitating the sharing of coordinated messages with each group member 8]. Canonically, a proxy solely acts as a communication medium, having no direct effect on the environment. A key challenge of a proxy is its design. Solutions that use proxies must consider their efficiency, ensuring that sufficient communication is achieved for the task at hand while also managing the computation load and expressiveness of the proxy',\n",
       "  '. A key challenge of a proxy is its design. Solutions that use proxies must consider their efficiency, ensuring that sufficient communication is achieved for the task at hand while also managing the computation load and expressiveness of the proxy. Networked communication In contrast to relying on a proxy for inter-agent communication, the networked communication protocol consists of agents that pass and receive messages directly to and from other agents{Zhang et al.| Chu et al. . While this form of communication may seem most fitting for a decentralized setting, its dynamic nature and lack of structure can lead to poor performance and scalability issues, especially when each agent has limited compute resources and is required to process many agents’ messages. 26 Implicit Communication Agents can also communicate with one another without explicit means, such as through stigmergy|Grassé| [1959]. The concept of stigmery defines the influence agents have through their actions on one another, often through environmental changes or some other form of stimuli. More formally, stigmery describes the influence of the persisting environmental effects of prior behaviors on behavior¢Holland and Melhuish| [1999]. We categorize the idea of stigmergy based on the intent/form of the stigmergic actions, the responses to the stigmergic behaviors, and the impact of stigmergic actions',\n",
       "  '. We categorize the idea of stigmergy based on the intent/form of the stigmergic actions, the responses to the stigmergic behaviors, and the impact of stigmergic actions. ¢ Sematectonic and marker-based stigmergy |Wilson| 2000], |Marsh and Onof| [2008] distin- guish whether the stigmergic actions were directly aligned with true objectives of the agents or rather, to solely stigmergize. The intent and the actual actions are often considered when classifying the two forms of stigmergy. Quantitative and qualitative stigmergy{Theraulaz and Bonabeau| [1999] differentiate whether the response to the stigmergic actions is an intensification of the resulting stimulus or trigger- ing different stimuli, leading to a self-organization process. A self-organizing process refers to a set of dynamical local mechanisms, which through their applications and interactions, causes emergent global structures and behaviors. Active and passive stigmergy|Holland and Melhuish|[1999] refer to the effects and outcomes of the stigmergic actions. Active stigmergy directly affects the agents, influencing the observations, actions, and parameters (e.g. frequency, latency, duration, intensity). How- ever, passive stigmergy is more indirect and subtle, perhaps leading to no changes to any observations, actions, or their parameters, but only to changes to the outcome',\n",
       "  '.g. frequency, latency, duration, intensity). How- ever, passive stigmergy is more indirect and subtle, perhaps leading to no changes to any observations, actions, or their parameters, but only to changes to the outcome. Historically, a central focus of stigmery in MARL has been deriving optimization algorithms, such as ant colony optimization|Dorigo and Blum|(2 that are inspired by these concepts. We propose that these patterns and behaviors of stigmergy should be further studied towards other forms of integration into our MAS, including, but not limited to, how to induce stigmergic behaviors and quantify and evaluate them. 5.5.2 Communication Representation In this section, we look into the various forms of communication mechanisms that are used in practice, namely those that were realized using deep learning techniques such as graph neural networks (GNN). Graph Neural Networks — Facilitating rich communication among agents requires a scalable frame- work that can naturally process information within the communication graph in an expressive manner. GNNs are a fundamental tool for handling non-Euclidean data, especially when dealing with infor- mation naturally occurring in graph structures. Concretely, GNNs learn to map input data to latent representations that can be used in subsequent tasks. GNNs generate these latent embeddings by iteratively performing the following operations: message computation, propagation, and aggregation',\n",
       "  '. Concretely, GNNs learn to map input data to latent representations that can be used in subsequent tasks. GNNs generate these latent embeddings by iteratively performing the following operations: message computation, propagation, and aggregation. A visualization of these operations is provided in ‘wont Together, these three operations can be collectively described as a graph convolution| Kip (2017}. Graph convolutions can be performed iteratively, increasing the receptive field and allowing GNNs to capture more global information as the number of message passing rounds increase, i.e. increase the number of GNN layers. There have been several advancements with GNN algorithms, specifically to increase expressiveness, improve scalability, and importantly, compensate for the over-smoothing problem|Xt ; where learning on densely connected graphs often converged to redundant node embeddings. Current MARL research utilizes GNNs as the de-facto communication mechanism, such as graph convolution network (GCN) /|Kipf and Welling] [2017] in CommNet|Sukhbaatar et al.|/2016] and BiCNet|Peng et al.|[2017], and graph attention network (GAT) {Brody et al.) [2022] in DGN|Jiang et al. poo and ATOGJiang and Luj [2018]. While many of these GNN models were initially designed for prediction-based tasks such as supervised classification, there have been efforts to bridge these tools into the domain of control such that novel mechanisms are tailored to the intricacies of MAS (e.g',\n",
       "  '. poo and ATOGJiang and Luj [2018]. While many of these GNN models were initially designed for prediction-based tasks such as supervised classification, there have been efforts to bridge these tools into the domain of control such that novel mechanisms are tailored to the intricacies of MAS (e.g. dynamic role assignments|Shao et al.| [2022] and limited communication|Kim et al.|[2019]), although this remains an open problem in MARL. 27 Agent i Message Aggregation Agent i Message Propagation Message Computation Communication Graph Agent i’s neighbors 5.5.3 Learning to Communicate Learning communication involves considering the following three aspects of communication: the content of the outgoing messages, how the incoming messages are incorporated and the communica- tion policy|Zhu et al.] . The optimization itself can be devised to learn all these aspects together or individually and makes use of explicit and/or implicit feedback. This means the communication learning dynamics can use additional feedback signals, such as social influence(Jaques et al] [2019], which may be optimized in conjunction with the MARL training. A common practice is to seamlessly integrate the two learning dynamics of communication and control, often with differentiable modeling and backpropagation|Foerste ea 2016}, Sukhbaatar eta (2016} [Feed et al] (2020), Message Computation To initiate communication, each agent must compute messages to broadcast',\n",
       "  '. The content of the message can vary from encoded or non-encoded information regarding the current and/or past local observations, actions, rewards, beliefs, objectives, previously received messages, or any other accessible information regarding the agents and the tasks. This can also include imagined information, such as future imagined trajectories/behaviors or intentions|Kim et al, 2020} To embed the listed information, an explicit approach is often employed through an auto-encoding process. On the other hand, although not completely orthogonal to the explicit learning approach, an end-to-end learning process is an alternative trained using the MARL learning dynamics without any such grounding. The learning representation for the content can be either discrete symbols or continuous values Foerster et a,(2016). In addition to the content itself, the concept of language is postulated to be an important aspect of generalizable coordinating behaviors, such as learning a lingua franca [2021]. An interesting property of language often studied is compositionality which refers to the ability to produce complex meanings by combining simpler linguistic elements and symbols in systematic ways. Communication Policy A communication policy defines the properties of the edges in the commu- nication graph, managing the following: the senders and recipients of all messages and the frequency of the message transmissions',\n",
       "  '. Communication Policy A communication policy defines the properties of the edges in the commu- nication graph, managing the following: the senders and recipients of all messages and the frequency of the message transmissions. This policy must thereby consider and adhere to the constraints of decentralized communication. If a proxy is used, the purpose of the proxy is to handle the operations of the communication policy. The edges of the communication graph can be statically or dynamically defined. A static implementa- tion can be fixed or make use of some heuristics|Huh and Mohapatra| [2023], whereas a dynamically is to make I defined solution is more involved. A natural approac gating mechanisms 8] and communication scheduling modules|Kim et al.| | to dictate the formation of 28 the edges in a learnable fashion. This enables us to design communication paradigms dynamically dependent on the needs and constraints of the task. For example, we can regularize the communication overhead with penalty terms that directly impact the parameters of the gating mechanism|Hu et al | Another consideration is the frequency of communication between agents. A common assumption is allowing all agents to communicate at every time step, however, this may not be necessary and in such cases, the over-communication can be detrimental in not only cost but also effectiveness due to the greater reliance and need for more expressive and consistent communication measures',\n",
       "  \". To mitigate this issue, communication can be less frequent by setting a more sparse communication 2|| or utilizing a mechanism that enables agents to be more selective, such as the cycle|Shao et al.|(2022) gating mechanism or communication scheduling. Message Integration Lastly, we discuss message integration, which refers to how each agent processes and utilizes the messages they receive. A popular approach is to use message aggregation operations, as observed with GNN architectures, involving either a weighted or non-weighted summation over all i incoming messages. While maintaining parameters for the weighting over the incoming messag' a great option to increase expressiveness in the aggregation process [Zhang] , learning the weighting may lead to challenges notably i in generalization, as performance may suffer when dealing with ad-hoc team-play scenarios if there is a lack of adaptability. 5.5.4 Evaluating Communication In recent efforts relating to communication, studies have explored evaluation metrics to prow our understanding and quantify the quality of the communication between agents|Lowe et al.| [Lowe et al.][2019}. These efforts range from taking a broader view by observing the changes in performance (i.e. agent’s rewards or task success rate) under changes in communication methods to a more granular scope, where explicit metrics are defined that can account for the varying reasons for the impact achieved by communication\",\n",
       "  '.e. agent’s rewards or task success rate) under changes in communication methods to a more granular scope, where explicit metrics are defined that can account for the varying reasons for the impact achieved by communication. A broader view is often taken when communication is vital for the task at hand, often in the form of referential games, a common mode of game used to study the aspect of communication in MARL. Referential games can be thought of as a form of Lewis signaling game, where agents are each assigned the roles of speaker and listeners, and the speaker agent must communicate to the listener agents to complete their tasks . In many cases, the speaker agent has access to some private information not privied to the listener agents, making their communication significant. These specific roles designated to the agents result in a strong reliance on the communication system in place for the success of the task. The metrics of communication can be divided into two classifications: positive signaling and positive listening [Lowe et al.|[2019]. Positive signaling quantifies some statistical dependence of the messages on the agents’ observations or actions. Positive listening, on the other hand, is measured by the change in behavior of an agent if its incoming messages are obscured or omitted',\n",
       "  '.|[2019]. Positive signaling quantifies some statistical dependence of the messages on the agents’ observations or actions. Positive listening, on the other hand, is measured by the change in behavior of an agent if its incoming messages are obscured or omitted. Previous studies that focus on devising positive signaling metrics aim towards quantifying the alignment between the agent’s messages and some inherent component relating to the agent, often defined using mutual information|Jaques et al. (2 O19 |. Bogin et al. (2 019). For instance, speaker consistency (SC) measures the mutual information (MI) between an agent’s messages and its actions Jaques etal (2019) [2019], whereas context independence (CI) instead measures the MI between the agent’s messages and predefined task concepts. Intuitively, SC provides insights into how much uncertainty is reduced regarding an agent’s action given its messages, and CI enforces the notion of language compositionality, where the content of the messages is induced to be related to inherent concepts within the environment. On the other hand, the guiding principle for measuring positive listening has been to quantify the causal influence of an agent’s message on another agent’s behavior. Similarly, this computation can be achieved with MI, where in practice, we quantify the alignment between an agent’s message to the actions of receiving agents|Lowe et al. 2019], Jaques et al. 2019}',\n",
       "  '. Similarly, this computation can be achieved with MI, where in practice, we quantify the alignment between an agent’s message to the actions of receiving agents|Lowe et al. 2019], Jaques et al. 2019}. Usually, these metrics consider the “one-step” behavior, meaning the causal influence of a message is referenced against the immediate response of an agent, such as its next action, where a multi-step behavior can lead to more accurate measures 9]. In terms of future directions, it remains unclear the definitive relationship between these metrics, the concepts of positive listening and positive signaling, the true quality of communication, and their interactions as well as a more theoretical justification for the bias these approaches provide 29 5.6 Modeling Other Agents An essential capability for agents is the ability to reason about the behaviors of other agents, which can be achieved by constructing models of other agents (MOA). This process is often referred to as agent modeling, or more traditionally known as opponent modeling. We divide our discussion of agent modeling into three parts: the representation of MOA, the optimization paradigm used with MOA, and how MOAs are utilized. Representation of MOA MOA can encompass a wide array of properties of other agents, including their observations, actions, goals, beliefs, and more intricate components such as intentions or agent types|Hong et al. (2018), Raileanu et al. 2018}',\n",
       "  '. Representation of MOA MOA can encompass a wide array of properties of other agents, including their observations, actions, goals, beliefs, and more intricate components such as intentions or agent types|Hong et al. (2018), Raileanu et al. 2018}. MOA can also contain information regarding entire coalitions|Erdogan and Veloso| [2011]. Empirical applications of MOA can be accomplished using deep learning models tailored to represent the specific properties they embody and how the MOA is intended to be integrated (He et al.| . Bayesian game is a common game mode used to represent MOA through a belief space, i.e. type (1567). The belief space of each agent contains any information which is not regarded as common knowledge including its private knowledge, which can hold local information regarding other agents. These beliefs, however, face the challenge of approximating uncertainty as agents must contend with incomplete information about the environment and the behaviors of other agents. Addressing this challenge often involves employing probabilistic techniques to estimate and reason about uncertainties within the belief space[Huh and Mohapatra] . Another interesting concept with MOA is the theory of mind, where agents engage in recursive reasoning about the states of other agents|Premack and Woodruff] [1978], /Yu et al.|[2022c]',\n",
       "  '. Another interesting concept with MOA is the theory of mind, where agents engage in recursive reasoning about the states of other agents|Premack and Woodruff] [1978], /Yu et al.|[2022c]. In practice, a nested reasoning approach is often approximated using belief nesting down to a fixed recursion depth, which can be implemented with game tree search techniques{Carmel and Markovitch| 1996]. In practice, there exists a large inspiration from model-based single-agent RL (MBRL), and there remains much work to incorporate the unique aspects of MAS into such MBRL methods Nashed and Zilberstein| [2022]. Learning MOA Here, we discuss two approaches to optimize and learn information about other agents, in the form of discriminative and generative learning. The discriminative learning approach comprises training MOA to classify and predict explicit properties of other agents, such as their observations or policies, typically through methods of maximum likelihood estimation [Huh and] (2024b]. On the other hand, generative learning approaches rely more on maximum a posterior, where the goal is to model the joint distribution of observed data and latent variables, enabling the generation of realistic samples from the learned distribution',\n",
       "  '. On the other hand, generative learning approaches rely more on maximum a posterior, where the goal is to model the joint distribution of observed data and latent variables, enabling the generation of realistic samples from the learned distribution. By capturing the underlying structure of the data, generative learning facilitates a deeper understanding of the relationships between different properties of other agents, allowing for more nuanced inference and decision- making in multi-agent environments OTT) [Nashed and Zilberstei] . The learning process of MOA also must consider how it is integrated with the MARL training, which can be done separately or simultaneously, and what additional data or assumptions are required. Using MOA MOA can be utilized in various manners, such as guiding the agent’s decision-making process, i.e. planning and recursive reasoning, or helping construct a more accurate understanding of the environment the agent is presiding in/Huh and Mohapatra’ (20240). An important consideration is the trust and robustness of utilizing these models, as agents may make incorrect or inaccurate predictions. An interesting application that remains an open research topic is an adversarial attempt to trick agents through deceptive actions to promote misleading synergies|Albrecht and Stone|[2018]. 5.7 Ad-Hoc Team-Play Learning in multi-agent systems may be faced with a distributional mismatch resulting from unseen behaviors from other agents',\n",
       "  '. 5.7 Ad-Hoc Team-Play Learning in multi-agent systems may be faced with a distributional mismatch resulting from unseen behaviors from other agents. The concept of ad-hoc team-play (AHTP) challenges the learned behaviors of agents to work with unknown partners who are capable of contributing to the task{Stone| . For its evaluation, it is common to define a period of ad-hoc interactions between ie agents to acclimate and devise their new joint strategy but assume that agents have no prior coordination before this ad-hoc interaction and also that agents have no direct control over other agents [ . If we assume no ad-hoc interactions, we refer to this as zero-shot coordination (ZSC) 2021]. The notion of AHTP/ZSC often arises in settings of human-AlI coordination |, where the AI agents must interact 30 with humans to achieve a task. Typically in such settings, the AI agents and the humans have not interacted with each other previously. In general, we note that despite the use of the term “teammate” and “team-play\", these agents are not necessarily cooperative but can be adversarial or mixed. A crucial aspect of achieving good AHTP behaviors is to avoid arbitrary conventions that often arise in traditional MARL training [Carroll et al.|[2020]. This can be achieved by either training agents on a pool of diverse policies, known as population-based training (PBT) Pom)',\n",
       "  '. A crucial aspect of achieving good AHTP behaviors is to avoid arbitrary conventions that often arise in traditional MARL training [Carroll et al.|[2020]. This can be achieved by either training agents on a pool of diverse policies, known as population-based training (PBT) Pom). or removing grounded beliefs of the agents that rely on arbitrary social conventions by utilizing off-belief learning, which assumes the prior behaviors of others were derived from fixed random policies, but their future actions will be computed with actual behavioral policies|Hu et al.| . Another option for addressing AHTP is for agents to learn to identify the behaviors of the other agents such that they can properly adjust their strategy to its current environment|Chen et al.|[ To quantify the AHTP capability, there exist two popular metrics: cross-play and adaptation regret . Cross-play is a static measure of the performance of agents with their new teammates and is often visualized through a cross-play matrix. Similarly, adaptation regret measures the cross-play performance but compares it to the performance achieved with the old teammates, i.e. the regret. The adaptation regret can be viewed under an adaptation curve, which helps view how quickly agents adapt to their new teammates. Similar to PBT, how to define the pool of “diverse” policies more optimally remains a question and often is generated using multiple independent runs of training with the same or different MARL algorithms',\n",
       "  '. Similar to PBT, how to define the pool of “diverse” policies more optimally remains a question and often is generated using multiple independent runs of training with the same or different MARL algorithms. AHTP remains an especially difficult challenge when dealing with heterogeneous agents, open environments with variation in the number of agents in the environment, imperfect information, unreliable nor robust communication mechanisms, highly adaptive, irrational or risk-averse agents, and diverse nature of interactions (2022), |Guan et al.| [2023], Nekoei et al] (2023). 5.8 Social Learning The sharing of learned behaviors amongst agents is a powerful mechanism that can improve the efficiency and adaptability of a population’s knowledge [Silva and Costa| [2019]. This process is referred to as social learning|Ndousse et al. and encompasses the concept of knowledge transfer, which manifests through two central approaches: intra-agent transfer, where knowledge is transferred and reused from different domains, and inter-agent transfer, where agents share knowledge within the same task and setting. While both forms of knowledge transfer play crucial roles in facilitating efficient social learning and adaptation within MAS, the focus will be on inter-agent transfer. Within the framework of inter-agent transfer, it is also important to consider the role assignments within the populations',\n",
       "  '. While both forms of knowledge transfer play crucial roles in facilitating efficient social learning and adaptation within MAS, the focus will be on inter-agent transfer. Within the framework of inter-agent transfer, it is also important to consider the role assignments within the populations. In this work, we define three common role assignments: advisor/advisee, teacher/student, and mentor/observer|Silva and Costa| [2019]. ¢ In the advisor/advisee relationship, the advisor receives requests from the advisee and observes their state, offering valuable information without presuming anything about the internal representation of agents. ¢ The interaction between a teacher and student is similar to that of an advisor/advisee, but in this case, certain assumptions are made, allowing for more informed designs for any information exchange. ¢ As for the mentor/observer relationship, the observer aims to emulate the behavior of the mentor, thereby learning from the mentor’s expertise and experience. Concretely, we explore three forms of inter-agent transfer: action advising, reward shaping, and knowledge distillation. Each of these approaches offers unique ways for agents to leverage the knowledge of their peers and improve their performance in the collective endeavor. Action Advising The fundamental concept of action advising (AA) revolves around an experienced agent providing recommendations on the next best actions to take to a less experienced agent fet al',\n",
       "  '. Action Advising The fundamental concept of action advising (AA) revolves around an experienced agent providing recommendations on the next best actions to take to a less experienced agent fet al.] [2023], (Omidshafiei et al.| [2018]. In many cases, one agent can offer action suggestions to another, even when the internal representation of the other agents remains unknown. In practice, its 31 implementation must consider how this AA process is initiated, i.e. by the advisor and/or advisee. [Da Silva etal (2020)Fachantiis etal (2017) Amir et al|201, and how the advice is incorporated with some form of option learning, referring to whether or not the suggested policy should be followed Sutton et al.|[1999b],| Yang et al. 202 1a}. However, an effective approach that generalizes this AA capability tha: t benefits MARL training remains an open challenge. Reward Shaping Derived from the motivations of potential functions in single-agent RL|N , reward shaping in MARL enables agents to influence the reward signals of other agents, typically in the form of an auxiliary signal that provides further learning guidance |Gupta et al|] 20 . Reward shaping approaches are particularly valuable in scenarios where rewards are sparse, as they allow for a learnable method to devise more informative learning signals 2022}',\n",
       "  '. Reward shaping approaches are particularly valuable in scenarios where rewards are sparse, as they allow for a learnable method to devise more informative learning signals 2022}. However, similar to potential functions, it is important to take caution when using such methods, as the behaviors of agents are highly dependent and influenced by the reward function, thereby it may be pivotal in some settings to maintain some level of invariance to these auxiliary reward signals. Knowledge Distillation The process of knowledge distillation (KD) involves transferring knowl- edge from teacher agents to student agents Bucilul et a, (7006), [2015]. Typically, KD is primarily used for model compression, where the capabilities of a larger model or multiple models are distilled into a single/smaller model for parameter efficiency and potential performance benefits, or adaptation to a new state and/or action space, where a teacher model is initially trained on a more complete state-action space and a student model must make use of a more restricted state-action space{Czarnecki et al.|[2019],(Lai et al.|[2020]. In MARL applications, KD can further leverage the multi-agent nature, through structural relations distillation, where the relations between multi-agents’ features are preserved [2022]',\n",
       "  '.|[2019],(Lai et al.|[2020]. In MARL applications, KD can further leverage the multi-agent nature, through structural relations distillation, where the relations between multi-agents’ features are preserved [2022]. 6 Concluding Remarks While the challenges within MARL have been extensively studied and assessed, the existing method- ologies for acquiring multi-agent behaviors fall short of fully harnessing the myriad opportunities within a MAS. Despite substantial progress, particularly in unique areas of learning in a MAS, i.e. its prospects, and the fundamental challenges of MARL, there remain open research challenges that demand further exploration and refinement. The intricacies of MARL extend beyond individual agent behavior to encompass the dynamic interac- tions unfolding within complex environments. Other properties of certain applications, such as open environments, human-robot interactions, and heterogeneous agents, merit additional considerations, for instance, how to handle the unbounded and evolving nature of open environments, safety and proper coordination with human-robot interactions, and manage the diverse capabilities, behaviors, and learning speeds within heterogeneous populations. To conclude our discussion of MARL, our exploration of MARL discussed both the progress made and the avenues yet to be fully explored',\n",
       "  '. To conclude our discussion of MARL, our exploration of MARL discussed both the progress made and the avenues yet to be fully explored. The multifaceted nature of multi-agent interactions within dynamic environments demands ongoing research and refinement of methodologies to unlock the full potential of MARL in harnessing the complexities inherent in a MAS.'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …',\n",
       "   'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1',\n",
       "   'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'language': 'en'},\n",
       "  {'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1',\n",
       "   'language': 'en',\n",
       "   'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …'},\n",
       "  {'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1',\n",
       "   'language': 'en',\n",
       "   'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …'},\n",
       "  {'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1',\n",
       "   'language': 'en',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …',\n",
       "   'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1'},\n",
       "  {'language': 'en',\n",
       "   'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …',\n",
       "   'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1'},\n",
       "  {'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …',\n",
       "   'language': 'en'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …',\n",
       "   'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1',\n",
       "   'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium'},\n",
       "  {'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …',\n",
       "   'language': 'en',\n",
       "   'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1'},\n",
       "  {'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1',\n",
       "   'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'language': 'en',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …'},\n",
       "  {'title': 'Reinforcement Learning: An introduction (Part 1/4) | by Cédric Vandelaer | Medium',\n",
       "   'language': 'en',\n",
       "   'source': 'https://medium.com/@cedric.vandelaer/reinforcement-learning-an-introduction-part-1-4-866695deb4d1',\n",
       "   'description': 'Reinforcement Learning: An introduction (Part 1/4) Hi and welcome to the first part of a series on Reinforcement Learning. If you somehow ended up here without having heard of Reinforcement Learning …'},\n",
       "  {'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'language': 'en'},\n",
       "  {'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'language': 'en'},\n",
       "  {'language': 'en',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …'},\n",
       "  {'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'language': 'en',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …'},\n",
       "  {'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'language': 'en',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92'},\n",
       "  {'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'language': 'en',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92'},\n",
       "  {'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'language': 'en',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92'},\n",
       "  {'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'language': 'en',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium'},\n",
       "  {'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'language': 'en',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92'},\n",
       "  {'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'language': 'en',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium'},\n",
       "  {'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'language': 'en',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …'},\n",
       "  {'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'language': 'en',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92'},\n",
       "  {'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'language': 'en'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92'},\n",
       "  {'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium',\n",
       "   'language': 'en'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications Are you interested in learning about reinforcement learning but don’t know where to start? Look no …',\n",
       "   'source': 'https://arjun-sarkar786.medium.com/reinforcement-learning-for-beginners-introduction-concepts-algorithms-and-applications-3f805cbd7f92',\n",
       "   'title': 'Reinforcement Learning for Beginners: Introduction, Concepts, Algorithms, and Applications | by Arjun Sarkar | Medium'},\n",
       "  {'language': 'en',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium'},\n",
       "  {'language': 'en',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\"},\n",
       "  {'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'language': 'en',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium'},\n",
       "  {'language': 'en',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c'},\n",
       "  {'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'language': 'en'},\n",
       "  {'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'language': 'en'},\n",
       "  {'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'language': 'en',\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c'},\n",
       "  {'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'language': 'en',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\"},\n",
       "  {'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'language': 'en'},\n",
       "  {'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'language': 'en',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c'},\n",
       "  {'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'language': 'en',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c'},\n",
       "  {'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'language': 'en',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c'},\n",
       "  {'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'language': 'en',\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium'},\n",
       "  {'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'language': 'en',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c'},\n",
       "  {'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c',\n",
       "   'language': 'en',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\"},\n",
       "  {'language': 'en',\n",
       "   'description': \"In this first chapter of Deep RL Course, a free course from beginners to experts, we're going to learn the fundamentals of Deep Reinforcement Learning.\",\n",
       "   'title': 'An Introduction to Deep Reinforcement Learning | Medium',\n",
       "   'source': 'https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'language': 'en',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'language': 'en',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'language': 'en',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'language': 'en',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'language': 'en'},\n",
       "  {'language': 'en',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium'},\n",
       "  {'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'language': 'en',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'language': 'en',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'language': 'en',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'language': 'en'},\n",
       "  {'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'language': 'en'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'language': 'en',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'language': 'en',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'language': 'en',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'language': 'en',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'language': 'en'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'language': 'en'},\n",
       "  {'language': 'en',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'language': 'en',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'language': 'en',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'language': 'en',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'language': 'en',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'language': 'en',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e',\n",
       "   'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …'},\n",
       "  {'description': 'Introduction to Reinforcement Learning (RL) in PyTorch Recap of Supervised Learning So far, we’ve primarily concerned ourselves with supervised learning problems (mostly classification). In …',\n",
       "   'language': 'en',\n",
       "   'title': 'Introduction to Reinforcement Learning (RL) in PyTorch | by Harsh Panchal | Analytics Vidhya | Medium',\n",
       "   'source': 'https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmp5wv6xw6u/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpg6zq5rhs/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmprogyhe8f/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'},\n",
       "  {'source': '/tmp/tmpyo765sav/tmp.pdf'}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c57031c863fc59b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
